{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":113558,"databundleVersionId":14878066},{"sourceType":"datasetVersion","sourceId":14387840,"datasetId":9171323,"databundleVersionId":15201056},{"sourceType":"modelInstanceVersion","sourceId":4535,"databundleVersionId":6346563,"modelInstanceId":3327}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Paths & Select Config (CFG)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Set Paths & Select Config (CFG) (Kaggle-ready, offline)\n# REVISI FULL v3.1 (lebih kuat + siap MULTI-CFG + lebih aman anti-error)\n#\n# Upgrade v3.1 (sesuai strategi UNet+ASPP + pipeline naik score):\n# - PRED prefix priority: utamakan hasil training baru (UNet+ASPP / Fusion) jika ada\n#   -> pred_unet_aspp_cfg_* dipilih dulu sebelum pred_base_*\n# - MATCH juga prefer ada folder npz (test/train_all) + match_summary.json\n# - Optional FORCE select via env (tanpa merusak multi-cfg list):\n#     LUC_FORCE_PRED_CFG  = nama folder cfg (mis: pred_unet_aspp_cfg_xxx)\n#     LUC_FORCE_MATCH_CFG = nama folder cfg (mis: match_base_cfg_xxx)\n# - Sanity guard lebih jelas + PATHS tambah shortcut folder npz (aman untuk stage lanjut)\n#\n# Output globals (TETAP, JANGAN diganti):\n# - COMP_ROOT, OUT_DS_ROOT, OUT_ROOT\n# - PATHS (dict)\n# - MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR\n#\n# Extra globals (aman, membantu training lanjutan):\n# - MATCH_CFG_DIRS, PRED_CFG_DIRS (list[Path] TOP-K)\n# - MATCH_CFG_INFO, PRED_CFG_INFO (list[dict] detail skor)\n# - CACHE_ROOTS (list[Path]), SELECTED (dict), TRAIN_PLAN (dict)\n# ============================================================\n\nimport os, re, json, time\nfrom pathlib import Path\nimport pandas as pd\n\n# ----------------------------\n# Config knobs (boleh diubah)\n# ----------------------------\nTOPK_MATCH_CFGS = int(os.environ.get(\"TOPK_MATCH_CFGS\", \"5\"))\nTOPK_PRED_CFGS  = int(os.environ.get(\"TOPK_PRED_CFGS\",  \"8\"))\n\n# Kalau kamu mau paksa pakai dataset input tertentu:\n# export LUC_OUT_DS_ROOT=\"/kaggle/input/<nama_dataset_output>\"\nENV_OUT_DS_ROOT = os.environ.get(\"LUC_OUT_DS_ROOT\", \"\").strip()\n\n# Optional: paksa pilih cfg primary (nama folder) tanpa mematikan multi-cfg list\nFORCE_PRED_CFG_NAME  = os.environ.get(\"LUC_FORCE_PRED_CFG\", \"\").strip()\nFORCE_MATCH_CFG_NAME = os.environ.get(\"LUC_FORCE_MATCH_CFG\", \"\").strip()\n\n# ----------------------------\n# Helper: fast count CSV rows (binary newline count)\n# ----------------------------\ndef _fast_count_rows_csv(path: Path, assume_header: bool = True) -> int:\n    \"\"\"\n    Count rows quickly by counting '\\n' in binary.\n    Returns -1 if error.\n    \"\"\"\n    try:\n        if not path.exists() or not path.is_file():\n            return -1\n        nl = 0\n        with path.open(\"rb\") as f:\n            while True:\n                b = f.read(1024 * 1024)\n                if not b:\n                    break\n                nl += b.count(b\"\\n\")\n        rows = nl - (1 if assume_header else 0)\n        return int(max(rows, 0))\n    except Exception:\n        return -1\n\ndef _safe_mtime(p: Path) -> float:\n    try:\n        return float(p.stat().st_mtime)\n    except Exception:\n        return 0.0\n\ndef _is_nonempty_file(p: Path) -> bool:\n    try:\n        return p is not None and p.exists() and p.is_file() and p.stat().st_size > 0\n    except Exception:\n        return False\n\ndef _dir_has_any_npz(d: Path) -> bool:\n    try:\n        if d is None or (not d.exists()) or (not d.is_dir()):\n            return False\n        for _ in d.glob(\"*.npz\"):\n            return True\n        return False\n    except Exception:\n        return False\n\ndef _resolve_cfg_by_name(cache_roots: list, cfg_name: str) -> Path:\n    \"\"\"\n    Find cfg directory by exact folder name under any cache root (one level deep).\n    Returns Path or None.\n    \"\"\"\n    if not cfg_name:\n        return None\n    for root in cache_roots:\n        root = Path(root)\n        if not root.exists():\n            continue\n        cand = root / cfg_name\n        if cand.exists() and cand.is_dir():\n            return cand\n        # fallback: search one-level deep (avoid heavy recursion)\n        try:\n            for d in root.iterdir():\n                if d.is_dir() and d.name == cfg_name:\n                    return d\n        except Exception:\n            pass\n    return None\n\n# ----------------------------\n# Helper: find competition root\n# ----------------------------\ndef find_comp_root(preferred: str = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\") -> Path:\n    p = Path(preferred)\n    if p.exists():\n        return p\n\n    base = Path(\"/kaggle/input\")\n    if not base.exists():\n        raise FileNotFoundError(\"/kaggle/input tidak ditemukan (pastikan kamu di Kaggle Notebook).\")\n\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"sample_submission.csv\").exists() and ((d / \"train_images\").exists() or (d / \"test_images\").exists()):\n            cands.append(d)\n\n    if not cands:\n        for d in base.iterdir():\n            if not d.is_dir():\n                continue\n            for x in d.iterdir():\n                if not x.is_dir():\n                    continue\n                if (x / \"sample_submission.csv\").exists() and ((x / \"train_images\").exists() or (x / \"test_images\").exists()):\n                    cands.append(x)\n\n    if not cands:\n        raise FileNotFoundError(\n            \"COMP_ROOT tidak ditemukan. Harus ada folder yang memuat sample_submission.csv dan train_images/test_images.\"\n        )\n\n    cands.sort(key=lambda x: ((\"recodai\" not in x.name.lower()),\n                              (\"forgery\" not in x.name.lower()),\n                              x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: find output dataset root (hasil PREP)\n# ----------------------------\ndef find_output_dataset_root(preferred_names=(\n    \"recod-ailuc-dinov2-base\",\n    \"recod-ai-luc-dinov2-base\",\n    \"recodai-luc-dinov2-base\",\n    \"recodai-luc-dinov2\",\n    \"recodai-luc-dinov2-prep\",\n)) -> Path:\n    base = Path(\"/kaggle/input\")\n\n    if ENV_OUT_DS_ROOT:\n        p = Path(ENV_OUT_DS_ROOT)\n        if p.exists():\n            return p\n        else:\n            print(f\"WARNING: ENV LUC_OUT_DS_ROOT tidak ditemukan: {p}\")\n\n    for nm in preferred_names:\n        p = base / nm\n        if p.exists():\n            return p\n\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"recodai_luc\" / \"artifacts\").exists() or (d / \"recodai_luc\" / \"cache\").exists():\n            cands.append(d)\n            continue\n        inner = list(d.glob(\"*/recodai_luc/artifacts\"))\n        if inner:\n            cands.append(d)\n\n    if not cands:\n        raise FileNotFoundError(\"OUT_DS_ROOT tidak ditemukan. Harus ada /kaggle/input/<...>/recodai_luc/(artifacts|cache)/\")\n\n    cands.sort(key=lambda x: ((\"dinov2\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: resolve OUT_ROOT = <dataset>/recodai_luc\n# ----------------------------\ndef resolve_out_root(out_ds_root: Path) -> Path:\n    direct = out_ds_root / \"recodai_luc\"\n    if direct.exists():\n        return direct\n    hits = list(out_ds_root.glob(\"*/recodai_luc\"))\n    if hits:\n        return hits[0]\n    raise FileNotFoundError(f\"Folder recodai_luc tidak ditemukan di bawah {out_ds_root}\")\n\n# ----------------------------\n# Helper: pick TOP-K cfg directories by multi-criteria scoring\n# ----------------------------\ndef pick_top_cfgs(\n    cache_roots,\n    prefixes,\n    required_train_file: str,\n    prefer_files=(),\n    extra_prefer_dirs=(),\n    max_return: int = 5,\n) -> list:\n    \"\"\"\n    Return list of candidate dicts sorted by score desc.\n    Each candidate dict: {dir, score, train_rows, prefer_hits, mt, root, debug...}\n\n    IMPORTANT:\n    - prefixes order is treated as PRIORITY (earlier = more preferred).\n      We add prefix bonus so pred_unet_aspp_cfg_* will win over pred_base_* if both valid.\n    \"\"\"\n    if isinstance(prefixes, str):\n        prefixes = [prefixes]\n    prefixes = list(prefixes)\n\n    prefer_files = list(prefer_files or [])\n    extra_prefer_dirs = list(extra_prefer_dirs or [])\n\n    cands = []\n    for root in cache_roots:\n        root = Path(root)\n        if not root.exists():\n            continue\n\n        try:\n            it = list(root.iterdir())\n        except Exception:\n            continue\n\n        for d in it:\n            if not d.is_dir():\n                continue\n            name = d.name\n\n            # match prefix priority\n            px_idx = None\n            for i, px in enumerate(prefixes):\n                if name.startswith(px):\n                    px_idx = i\n                    break\n            if px_idx is None:\n                continue\n\n            train_fp = d / required_train_file\n            if not train_fp.exists():\n                continue\n\n            train_n = _fast_count_rows_csv(train_fp)\n            if train_n <= 0:\n                continue\n\n            # prefer files\n            pref_hit = 0\n            pref_rows_sum = 0\n            pref_detail = {}\n            for fn in prefer_files:\n                fp = d / fn\n                ok = _is_nonempty_file(fp)\n                pref_detail[fn] = bool(ok)\n                if ok:\n                    pref_hit += 1\n                    pref_rows_sum += max(_fast_count_rows_csv(fp), 0)\n\n            # prefer dirs (npz availability or other dirs)\n            dir_hit = 0\n            dir_detail = {}\n            for dn in extra_prefer_dirs:\n                dd = d / dn\n                ok = dd.exists() and dd.is_dir()\n                if ok and dn in (\"test\", \"train_all\"):\n                    ok = _dir_has_any_npz(dd)\n                dir_detail[dn] = bool(ok)\n                if ok:\n                    dir_hit += 1\n\n            # mtime: consider dir + train file + any prefer file\n            mt = max(_safe_mtime(d), _safe_mtime(train_fp))\n            for fn in prefer_files:\n                mt = max(mt, _safe_mtime(d / fn))\n\n            # prefix bonus (earlier prefix => bigger bonus)\n            # keep big enough so model family preference wins unless clearly invalid\n            # example: pred_unet_aspp_cfg_* should win over pred_base_* by default\n            prefix_bonus = 0.0\n            if len(prefixes) > 1:\n                prefix_bonus = 3e6 * float(len(prefixes) - px_idx)\n\n            # score design:\n            # - prefix bonus (family priority)\n            # - prefer files hits\n            # - prefer dirs hits (npz availability)\n            # - then train rows, then prefer rows\n            # - then newest mtime\n            score = 0.0\n            score += prefix_bonus\n            score += 1e6 * float(pref_hit)\n            score += 2e5 * float(dir_hit)\n            score += 1.0  * float(train_n)\n            score += 0.05 * float(pref_rows_sum)\n            score += 1e-6 * float(mt)\n\n            cands.append({\n                \"dir\": d,\n                \"root\": root,\n                \"score\": score,\n                \"prefix_idx\": int(px_idx),\n                \"prefix\": prefixes[px_idx],\n                \"train_file\": str(train_fp),\n                \"train_rows\": int(train_n),\n                \"prefer_hits\": int(pref_hit),\n                \"prefer_rows_sum\": int(pref_rows_sum),\n                \"prefer_detail\": pref_detail,\n                \"dir_hits\": int(dir_hit),\n                \"dir_detail\": dir_detail,\n                \"mtime\": float(mt),\n            })\n\n    cands.sort(key=lambda x: (-x[\"score\"], x[\"dir\"].name))\n    return cands[:max_return]\n\n# ----------------------------\n# Helper: detect DINO model dir (offline)\n# ----------------------------\ndef detect_dino_dir() -> Path:\n    base = Path(\"/kaggle/input/dinov2/pytorch\")\n    if base.exists():\n        for name in [\"large\", \"giant\", \"base\"]:\n            p = base / name / \"1\"\n            if p.exists():\n                return p\n    return Path(\"/kaggle/input/dinov2/pytorch/large/1\")\n\ndef detect_dino_cache_cfg(cache_dirs: list) -> Path:\n    \"\"\"\n    Find best cfg under cache/dino_v2_*/cfg_*/manifest_train_all.csv\n    Prefer large then giant then base, but choose whichever has best manifest size.\n    \"\"\"\n    best = None\n    best_key = None  # tuple for sorting\n    prio = {\"dino_v2_large\": 0, \"dino_v2_giant\": 1, \"dino_v2_base\": 2}\n\n    for root in cache_dirs:\n        root = Path(root)\n        if not root.exists():\n            continue\n\n        for dino_name in [\"dino_v2_large\", \"dino_v2_giant\", \"dino_v2_base\"]:\n            dino_root = root / dino_name\n            if not dino_root.exists():\n                continue\n\n            try:\n                cfgs = list(dino_root.iterdir())\n            except Exception:\n                continue\n\n            for cfg in cfgs:\n                if not (cfg.is_dir() and cfg.name.startswith(\"cfg_\")):\n                    continue\n                mf = cfg / \"manifest_train_all.csv\"\n                if not mf.exists():\n                    continue\n\n                n = _fast_count_rows_csv(mf)\n                mt = _safe_mtime(cfg)\n                key = (prio.get(dino_name, 9), -n, -mt, cfg.name)\n                if best is None or key < best_key:\n                    best = cfg\n                    best_key = key\n\n    return best  # can be None\n\n# ============================================================\n# 0) Locate roots\n# ============================================================\nCOMP_ROOT = find_comp_root(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\nOUT_DS_ROOT = find_output_dataset_root()\nOUT_ROOT = resolve_out_root(OUT_DS_ROOT)  # dataset input: .../recodai_luc\n\nWORK_OUT_ROOT = Path(\"/kaggle/working/recodai_luc\")\n\n# Cache roots: prefer working if exists, plus input\nCACHE_ROOTS = []\nif (WORK_OUT_ROOT / \"cache\").exists():\n    CACHE_ROOTS.append(WORK_OUT_ROOT / \"cache\")\nif (OUT_ROOT / \"cache\").exists():\n    CACHE_ROOTS.append(OUT_ROOT / \"cache\")\n\n# Artifact roots: prefer working if exists, plus input\nART_ROOTS = []\nif (WORK_OUT_ROOT / \"artifacts\").exists():\n    ART_ROOTS.append(WORK_OUT_ROOT / \"artifacts\")\nif (OUT_ROOT / \"artifacts\").exists():\n    ART_ROOTS.append(OUT_ROOT / \"artifacts\")\n\n# choose first existing artifact root\nART_DIR = None\nfor a in ART_ROOTS:\n    if a.exists():\n        ART_DIR = a\n        break\nif ART_DIR is None:\n    raise FileNotFoundError(\"ART_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n\n# cache dirs that exist\nCACHE_DIRS = [p for p in CACHE_ROOTS if Path(p).exists()]\nif not CACHE_DIRS:\n    raise FileNotFoundError(\"CACHE_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n\n# ============================================================\n# 1) Competition paths (raw images/masks)\n# ============================================================\nPATHS = {}\nPATHS[\"COMP_ROOT\"] = str(COMP_ROOT)\nPATHS[\"SAMPLE_SUB\"] = str(COMP_ROOT / \"sample_submission.csv\")\n\nPATHS[\"TRAIN_IMAGES\"] = str(COMP_ROOT / \"train_images\")\nPATHS[\"TEST_IMAGES\"]  = str(COMP_ROOT / \"test_images\")\nPATHS[\"TRAIN_MASKS\"]  = str(COMP_ROOT / \"train_masks\")\nPATHS[\"SUPP_IMAGES\"]  = str(COMP_ROOT / \"supplemental_images\")\nPATHS[\"SUPP_MASKS\"]   = str(COMP_ROOT / \"supplemental_masks\")\n\nPATHS[\"TRAIN_AUTH_DIR\"] = str(COMP_ROOT / \"train_images\" / \"authentic\")\nPATHS[\"TRAIN_FORG_DIR\"] = str(COMP_ROOT / \"train_images\" / \"forged\")\n\n# ============================================================\n# 2) Output dataset paths (clean artifacts + cache)\n# ============================================================\nPATHS[\"OUT_DS_ROOT\"] = str(OUT_DS_ROOT)\nPATHS[\"OUT_ROOT\"]    = str(OUT_ROOT)\n\nPATHS[\"ART_DIR\"]     = str(ART_DIR)\nPATHS[\"CACHE_DIRS\"]  = [str(x) for x in CACHE_DIRS]  # list\n\n# artifacts utama\nPATHS[\"DF_TRAIN_ALL\"] = str(Path(ART_DIR) / \"df_train_all.parquet\")\nPATHS[\"DF_TRAIN_CLS\"] = str(Path(ART_DIR) / \"df_train_cls.parquet\")\nPATHS[\"DF_TRAIN_SEG\"] = str(Path(ART_DIR) / \"df_train_seg.parquet\")\nPATHS[\"DF_TEST\"]      = str(Path(ART_DIR) / \"df_test.parquet\")\n\nPATHS[\"CV_CASE_FOLDS\"]   = str(Path(ART_DIR) / \"cv_case_folds.csv\")\nPATHS[\"CV_SAMPLE_FOLDS\"] = str(Path(ART_DIR) / \"cv_sample_folds.csv\")\n\nPATHS[\"IMG_PROFILE_TRAIN\"] = str(Path(ART_DIR) / \"image_profile_train.parquet\")\nPATHS[\"IMG_PROFILE_TEST\"]  = str(Path(ART_DIR) / \"image_profile_test.parquet\")\nPATHS[\"MASK_PROFILE\"]      = str(Path(ART_DIR) / \"mask_profile.parquet\")\nPATHS[\"CASE_SUMMARY\"]      = str(Path(ART_DIR) / \"case_summary.parquet\")\n\n# ============================================================\n# 3) Select best MATCH/PRED CFG dirs automatically (TOP-K + scoring)\n# ============================================================\n# MATCH candidates: must have match_features_train_all.csv\n# Prefer having match_features_test + manifests + match_summary + npz folders\nMATCH_PREFIXES = [\"match_base_cfg_\"]  # (kalau nanti ada match v2, tambah di depan list ini)\nMATCH_CFG_INFO = pick_top_cfgs(\n    CACHE_DIRS,\n    prefixes=MATCH_PREFIXES,\n    required_train_file=\"match_features_train_all.csv\",\n    prefer_files=[\n        \"match_features_test.csv\",\n        \"manifest_match_test.csv\",\n        \"manifest_match_train_all.csv\",\n        \"match_summary.json\",\n    ],\n    extra_prefer_dirs=[\"test\", \"train_all\"],  # match kamu memang punya npz, jadi prefer\n    max_return=max(1, TOPK_MATCH_CFGS),\n)\n\nif not MATCH_CFG_INFO:\n    raise FileNotFoundError(\"Tidak menemukan match cfg folder yang valid (match_features_train_all.csv).\")\n\nMATCH_CFG_DIRS = [x[\"dir\"] for x in MATCH_CFG_INFO]\nMATCH_CFG_DIR  = MATCH_CFG_DIRS[0]  # default primary\n\n# PRED candidates: MUST include UNet+ASPP prefix priority (kalau sudah kamu train nanti)\n# Urutan prefix = prioritas: lebih depan lebih dipilih\nPRED_PREFIXES = [\n    \"pred_unet_aspp_cfg_\",   # hasil training UNet+ASPP (yang kamu mau)\n    \"pred_fusion_cfg_\",      # jika nanti ada fusion/staking\n    \"pred_base\",             # fallback lama\n]\nPRED_CFG_INFO = pick_top_cfgs(\n    CACHE_DIRS,\n    prefixes=PRED_PREFIXES,\n    required_train_file=\"pred_features_train_all.csv\",\n    prefer_files=[\n        \"pred_features_test.csv\",\n        \"manifest_pred_test.csv\",\n        \"manifest_pred_train_all.csv\",\n        \"pred_summary.json\",\n    ],\n    extra_prefer_dirs=[\"test\", \"train_all\"],  # prefer having npz predictions\n    max_return=max(1, TOPK_PRED_CFGS),\n)\n\nif not PRED_CFG_INFO:\n    raise FileNotFoundError(\"Tidak menemukan pred cfg folder yang valid (pred_features_train_all.csv).\")\n\nPRED_CFG_DIRS = [x[\"dir\"] for x in PRED_CFG_INFO]\nPRED_CFG_DIR  = PRED_CFG_DIRS[0]  # default primary\n\n# Optional FORCE override (primary only)\n_force_match = _resolve_cfg_by_name(CACHE_DIRS, FORCE_MATCH_CFG_NAME) if FORCE_MATCH_CFG_NAME else None\n_force_pred  = _resolve_cfg_by_name(CACHE_DIRS, FORCE_PRED_CFG_NAME) if FORCE_PRED_CFG_NAME else None\n\nif _force_match is not None:\n    print(f\"FORCE: MATCH_CFG_DIR -> {_force_match.name}\")\n    MATCH_CFG_DIR = _force_match\nif _force_pred is not None:\n    print(f\"FORCE: PRED_CFG_DIR  -> {_force_pred.name}\")\n    PRED_CFG_DIR = _force_pred\n\n# DINO cache cfg (opsional)\nDINO_CFG_DIR = detect_dino_cache_cfg(CACHE_DIRS)\n\n# simpan cfg dir ke PATHS\nPATHS[\"MATCH_CFG_DIR\"]    = str(MATCH_CFG_DIR)\nPATHS[\"PRED_CFG_DIR\"]     = str(PRED_CFG_DIR)\nPATHS[\"DINO_CFG_DIR\"]     = str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\"\n\n# multi-cfg (baru, opsional)\nPATHS[\"MATCH_CFG_DIRS\"]   = [str(x) for x in MATCH_CFG_DIRS]\nPATHS[\"PRED_CFG_DIRS\"]    = [str(x) for x in PRED_CFG_DIRS]\n\n# feature paths dari cfg primary\nPATHS[\"MATCH_FEAT_TRAIN\"] = str(MATCH_CFG_DIR / \"match_features_train_all.csv\")\nPATHS[\"MATCH_FEAT_TEST\"]  = str(MATCH_CFG_DIR / \"match_features_test.csv\")\n\nPATHS[\"PRED_FEAT_TRAIN\"]  = str(PRED_CFG_DIR / \"pred_features_train_all.csv\")\nPATHS[\"PRED_FEAT_TEST\"]   = str(PRED_CFG_DIR / \"pred_features_test.csv\")  # bisa missing (warning)\n\n# manifests/summary (sering kepakai untuk infer/submission)\nPATHS[\"PRED_MAN_TRAIN\"] = str(PRED_CFG_DIR / \"manifest_pred_train_all.csv\")\nPATHS[\"PRED_MAN_TEST\"]  = str(PRED_CFG_DIR / \"manifest_pred_test.csv\")\nPATHS[\"PRED_SUMMARY\"]   = str(PRED_CFG_DIR / \"pred_summary.json\")\n\nPATHS[\"MATCH_MAN_TRAIN\"] = str(MATCH_CFG_DIR / \"manifest_match_train_all.csv\")\nPATHS[\"MATCH_MAN_TEST\"]  = str(MATCH_CFG_DIR / \"manifest_match_test.csv\")\nPATHS[\"MATCH_SUMMARY\"]   = str(MATCH_CFG_DIR / \"match_summary.json\")\n\n# shortcut ke folder npz (aman untuk stage lanjut, walau mungkin kosong)\nPATHS[\"PRED_NPZ_TRAIN_DIR\"]  = str(PRED_CFG_DIR / \"train_all\")\nPATHS[\"PRED_NPZ_TEST_DIR\"]   = str(PRED_CFG_DIR / \"test\")\nPATHS[\"MATCH_NPZ_TRAIN_DIR\"] = str(MATCH_CFG_DIR / \"train_all\")\nPATHS[\"MATCH_NPZ_TEST_DIR\"]  = str(MATCH_CFG_DIR / \"test\")\n\n# ============================================================\n# 4) DINO model dir (offline)\n# ============================================================\nDINO_DIR = detect_dino_dir()\nPATHS[\"DINO_DIR\"] = str(DINO_DIR)\n\n# ============================================================\n# 5) Sanity checks (wajib ada untuk training)\n# ============================================================\nmust_exist = [\n    (\"sample_submission.csv\", PATHS[\"SAMPLE_SUB\"]),\n    (\"df_train_all.parquet\",  PATHS[\"DF_TRAIN_ALL\"]),\n    (\"cv_case_folds.csv\",     PATHS[\"CV_CASE_FOLDS\"]),\n    (\"match_features_train_all.csv\", PATHS[\"MATCH_FEAT_TRAIN\"]),\n    (\"pred_features_train_all.csv\",  PATHS[\"PRED_FEAT_TRAIN\"]),\n]\nmissing = [name for name, p in must_exist if not Path(p).exists()]\nif missing:\n    raise FileNotFoundError(\"Missing required files: \" + \", \".join(missing))\n\n# opsional tapi penting untuk inference: test feature files\nopt_warn = []\nif not Path(PATHS[\"MATCH_FEAT_TEST\"]).exists():\n    opt_warn.append(\"match_features_test.csv (MATCH_FEAT_TEST)\")\nif not Path(PATHS[\"PRED_FEAT_TEST\"]).exists():\n    opt_warn.append(\"pred_features_test.csv (PRED_FEAT_TEST)\")\nif opt_warn:\n    print(\"WARNING: File opsional untuk inference belum ada:\")\n    for w in opt_warn:\n        print(\" -\", w)\n    print(\"Catatan: training masih aman (pakai *_train_all). Untuk inference gate ke test, file ini biasanya dibutuhkan.\")\n\n# npz dirs warning (tidak crash)\nif not _dir_has_any_npz(Path(PATHS[\"PRED_NPZ_TRAIN_DIR\"])):\n    print(f\"WARNING: PRED_NPZ_TRAIN_DIR tidak ada/empty: {PATHS['PRED_NPZ_TRAIN_DIR']}\")\nif not _dir_has_any_npz(Path(PATHS[\"PRED_NPZ_TEST_DIR\"])):\n    print(f\"WARNING: PRED_NPZ_TEST_DIR tidak ada/empty: {PATHS['PRED_NPZ_TEST_DIR']}\")\nif not _dir_has_any_npz(Path(PATHS[\"MATCH_NPZ_TRAIN_DIR\"])):\n    print(f\"WARNING: MATCH_NPZ_TRAIN_DIR tidak ada/empty: {PATHS['MATCH_NPZ_TRAIN_DIR']}\")\nif not _dir_has_any_npz(Path(PATHS[\"MATCH_NPZ_TEST_DIR\"])):\n    print(f\"WARNING: MATCH_NPZ_TEST_DIR tidak ada/empty: {PATHS['MATCH_NPZ_TEST_DIR']}\")\n\n# DINO model dir opsional (warning saja)\nif not Path(PATHS[\"DINO_DIR\"]).exists():\n    print(f\"WARNING: DINO dir tidak ditemukan: {PATHS['DINO_DIR']}\")\n\n# ============================================================\n# 6) Print summary + export helpers\n# ============================================================\nSELECTED = {\n    \"ART_DIR\": str(ART_DIR),\n    \"CACHE_DIRS\": [str(x) for x in CACHE_DIRS],\n    \"MATCH_CFG_DIR\": str(MATCH_CFG_DIR),\n    \"PRED_CFG_DIR\": str(PRED_CFG_DIR),\n    \"DINO_CFG_DIR\": str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\",\n    \"DINO_DIR\": str(DINO_DIR),\n    \"TOPK_MATCH_CFGS\": TOPK_MATCH_CFGS,\n    \"TOPK_PRED_CFGS\": TOPK_PRED_CFGS,\n    \"FORCE_MATCH_CFG_NAME\": FORCE_MATCH_CFG_NAME,\n    \"FORCE_PRED_CFG_NAME\": FORCE_PRED_CFG_NAME,\n    \"PRED_PREFIXES\": PRED_PREFIXES,\n    \"MATCH_PREFIXES\": MATCH_PREFIXES,\n}\n\n# Training plan: disiapkan untuk upgrade lanjutan (stacking + calibration + threshold tune)\nTRAIN_PLAN = {\n    \"seed\": 2025,\n    \"group_col\": \"case_id\",\n    \"target_col\": \"y_forged\",\n    \"n_folds\": 5,\n    \"use_calibration\": True,\n    \"calibration\": \"isotonic\",\n    \"tune_threshold_on_oof\": True,\n    \"multi_cfg\": {\n        \"enabled\": True,\n        \"topk_match_cfgs\": TOPK_MATCH_CFGS,\n        \"topk_pred_cfgs\": TOPK_PRED_CFGS,\n        \"primary_match\": MATCH_CFG_DIR.name,\n        \"primary_pred\": PRED_CFG_DIR.name,\n        \"pred_prefix_priority\": PRED_PREFIXES,\n        \"match_prefix_priority\": MATCH_PREFIXES,\n    },\n}\n\ndef _print_top_cfg_table(title: str, info_list: list, max_rows: int = 5):\n    print(title)\n    if not info_list:\n        print(\"  (none)\")\n        return\n    show = info_list[:max_rows]\n    for i, x in enumerate(show, 1):\n        d = x[\"dir\"]\n        tr = x[\"train_rows\"]\n        ph = x[\"prefer_hits\"]\n        dh = x[\"dir_hits\"]\n        px = x.get(\"prefix\", \"\")\n        print(f\"  #{i:02d} {d.name} | px={px} | score={x['score']:.1f} | train_rows={tr} | prefer_hits={ph} | dir_hits={dh}\")\n\nprint(\"OK — Roots\")\nprint(\"  COMP_ROOT   :\", COMP_ROOT)\nprint(\"  OUT_DS_ROOT :\", OUT_DS_ROOT)\nprint(\"  OUT_ROOT    :\", OUT_ROOT)\nprint(\"  ART_DIR(use):\", ART_DIR)\nprint(\"  CACHE_DIRS  :\", [str(x) for x in CACHE_DIRS])\n\nprint(\"\\nOK — Selected CFG (PRIMARY)\")\nprint(\"  MATCH_CFG_DIR:\", MATCH_CFG_DIR.name)\nprint(\"  PRED_CFG_DIR :\", PRED_CFG_DIR.name)\nprint(\"  DINO_CFG_DIR :\", (DINO_CFG_DIR.name if DINO_CFG_DIR else \"(not found / optional)\"))\n\nprint(\"\\nOK — Top candidates (for MULTI-CFG training)\")\n_print_top_cfg_table(\"MATCH CFG TOP:\", MATCH_CFG_INFO, max_rows=min(5, len(MATCH_CFG_INFO)))\n_print_top_cfg_table(\"PRED  CFG TOP:\", PRED_CFG_INFO,  max_rows=min(5, len(PRED_CFG_INFO)))\n\nprint(\"\\nOK — Key files (train)\")\nfor k in [\"DF_TRAIN_ALL\", \"CV_CASE_FOLDS\", \"MATCH_FEAT_TRAIN\", \"PRED_FEAT_TRAIN\", \"IMG_PROFILE_TRAIN\"]:\n    p = Path(PATHS[k])\n    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing/optional)'}\")\n\nprint(\"\\nOK — Key files (test/infer, optional)\")\nfor k in [\"MATCH_FEAT_TEST\", \"PRED_FEAT_TEST\", \"PRED_MAN_TEST\", \"PRED_SUMMARY\", \"MATCH_MAN_TEST\", \"MATCH_SUMMARY\"]:\n    p = Path(PATHS.get(k, \"\"))\n    if str(p) == \".\":\n        print(f\"  {k:16s}: (unset)\")\n    else:\n        print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing)'}\")\n\nprint(\"\\nOK — NPZ dirs (optional, for mask-based features / submission)\")\nfor k in [\"PRED_NPZ_TRAIN_DIR\", \"PRED_NPZ_TEST_DIR\", \"MATCH_NPZ_TRAIN_DIR\", \"MATCH_NPZ_TEST_DIR\"]:\n    p = Path(PATHS[k])\n    ok = _dir_has_any_npz(p)\n    print(f\"  {k:16s}: {p}  {'(npz found)' if ok else '(empty/missing)'}\")\n\nprint(\"\\nOK — DINO model dir\")\nprint(\"  DINO_DIR:\", DINO_DIR, \"(exists)\" if DINO_DIR.exists() else \"(missing)\")\n\n# export globals (kompatibilitas)\nglobals().update({\n    \"MATCH_CFG_DIR\": MATCH_CFG_DIR,\n    \"PRED_CFG_DIR\": PRED_CFG_DIR,\n    \"DINO_CFG_DIR\": DINO_CFG_DIR,\n    \"CACHE_ROOTS\": [Path(x) for x in CACHE_DIRS],\n    \"SELECTED\": SELECTED,\n    \"TRAIN_PLAN\": TRAIN_PLAN,\n\n    # extra (multi-cfg)\n    \"MATCH_CFG_DIRS\": MATCH_CFG_DIRS,\n    \"PRED_CFG_DIRS\": PRED_CFG_DIRS,\n    \"MATCH_CFG_INFO\": MATCH_CFG_INFO,\n    \"PRED_CFG_INFO\": PRED_CFG_INFO,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T16:23:33.741327Z","iopub.execute_input":"2026-01-11T16:23:33.741751Z","iopub.status.idle":"2026-01-11T16:23:34.006898Z","shell.execute_reply.started":"2026-01-11T16:23:33.741718Z","shell.execute_reply":"2026-01-11T16:23:34.006069Z"}},"outputs":[{"name":"stdout","text":"OK — Roots\n  COMP_ROOT   : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n  OUT_DS_ROOT : /kaggle/input/recod-ailuc-dinov2-base\n  OUT_ROOT    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc\n  ART_DIR(use): /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts\n  CACHE_DIRS  : ['/kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache']\n\nOK — Selected CFG (PRIMARY)\n  MATCH_CFG_DIR: match_base_cfg_f9f7ea3a65c5\n  PRED_CFG_DIR : pred_base_v3_v7_cfg_5dbf0aa165\n  DINO_CFG_DIR : cfg_3246fd54aab0\n\nOK — Top candidates (for MULTI-CFG training)\nMATCH CFG TOP:\n  #01 match_base_cfg_f9f7ea3a65c5 | px=match_base_cfg_ | score=4407205.8 | train_rows=5176 | prefer_hits=4 | dir_hits=2\nPRED  CFG TOP:\n  #01 pred_base_v3_v7_cfg_5dbf0aa165 | px=pred_base | score=7407206.3 | train_rows=5176 | prefer_hits=4 | dir_hits=2\n\nOK — Key files (train)\n  DF_TRAIN_ALL    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet  (exists)\n  CV_CASE_FOLDS   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv  (exists)\n  MATCH_FEAT_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv  (exists)\n  PRED_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv  (exists)\n  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet  (exists)\n\nOK — Key files (test/infer, optional)\n  MATCH_FEAT_TEST : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_test.csv  (exists)\n  PRED_FEAT_TEST  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_test.csv  (exists)\n  PRED_MAN_TEST   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/manifest_pred_test.csv  (exists)\n  PRED_SUMMARY    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_summary.json  (exists)\n  MATCH_MAN_TEST  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/manifest_match_test.csv  (exists)\n  MATCH_SUMMARY   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_summary.json  (exists)\n\nOK — NPZ dirs (optional, for mask-based features / submission)\n  PRED_NPZ_TRAIN_DIR: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/train_all  (npz found)\n  PRED_NPZ_TEST_DIR: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/test  (npz found)\n  MATCH_NPZ_TRAIN_DIR: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/train_all  (npz found)\n  MATCH_NPZ_TEST_DIR: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/test  (npz found)\n\nOK — DINO model dir\n  DINO_DIR: /kaggle/input/dinov2/pytorch/large/1 (exists)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Build Training Table (X, y, folds)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 2 — Build Training Table (X, y, folds) — REVISI FULL v3.2\n# FIX utama v3.2:\n# - NPZ overlap (pred vs match) dibuat SAFE: handle shape mismatch / mask mini / dim aneh\n#   -> tidak lagi crash \"operands could not be broadcast together\"\n# Output & flow lain TIDAK diubah.\n# ============================================================\n\nimport os, re, json, gc, warnings, hashlib\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# 0) Require PATHS\n# ----------------------------\nif \"PATHS\" not in globals() or not isinstance(PATHS, dict):\n    raise RuntimeError(\"Missing PATHS. Jalankan dulu STAGE 0 — Set Paths & Select Config (CFG).\")\n\n# ----------------------------\n# 1) Feature Engineering Config\n# ----------------------------\nFE_CFG = {\n    \"use_match_features\": True,\n    \"use_image_profile\": True,\n\n    \"use_npz_pair_features\": True,\n    \"npz_downsample_for_cc\": 256,\n    \"npz_bin_thr\": 0.5,\n    \"npz_max_rows\": int(os.environ.get(\"NPZ_MAX_ROWS\", \"0\")),\n    \"npz_cache_enabled\": True,\n\n    \"multi_cfg_enabled\": True,\n    \"multi_cfg_max_pred\": int(os.environ.get(\"MULTI_CFG_MAX_PRED\", \"6\")),\n    \"multi_cfg_max_match\": int(os.environ.get(\"MULTI_CFG_MAX_MATCH\", \"3\")),\n    \"multi_cfg_extra_mode\": \"core+agg\",\n\n    \"encode_variant_onehot\": True,\n    \"variant_min_count\": 1,\n\n    \"add_log_features\": True,\n    \"add_sqrt_features\": False,\n    \"add_interactions\": True,\n    \"add_missing_indicators\": True,\n\n    \"clip_by_quantile\": True,\n    \"clip_q\": 0.999,\n    \"clip_max_fallback\": 1e9,\n\n    \"fillna_value\": 0.0,\n\n    \"drop_constant_features\": True,\n    \"cast_float32\": True,\n\n    \"drop_unlabeled\": True,\n    \"positive_value\": 1,\n}\n\n# ----------------------------\n# 2) Prefer WORKING features if exist\n# ----------------------------\ndef _prefer_existing(*paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(str(p))\n        if str(p).strip() == \"\":\n            continue\n        if p.exists():\n            return p\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(str(p))\n        if str(p).strip() != \"\":\n            return p\n    return Path(\"\")\n\ndef _to_str_series(s: pd.Series) -> pd.Series:\n    return s.astype(str).replace({\"nan\": \"\", \"None\": \"\"})\n\ndef _ensure_uid(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"uid\" not in df.columns:\n        for alt in [\"sample_id\", \"id\", \"key\"]:\n            if alt in df.columns:\n                df = df.rename(columns={alt: \"uid\"})\n                break\n    if \"uid\" not in df.columns:\n        raise ValueError(\"Cannot find uid column. Expected 'uid' or 'sample_id'.\")\n    df[\"uid\"] = _to_str_series(df[\"uid\"])\n    return df\n\ndef _parse_case_variant_from_uid(uid_s: pd.Series) -> pd.DataFrame:\n    uid = _to_str_series(uid_s)\n    case1 = uid.str.extract(r\"^(\\d+)__\")[0]\n    var1  = uid.str.extract(r\"__(.+)$\")[0]\n    case2 = uid.str.extract(r\"^(\\d+)_\")[0]\n    var2  = uid.str.extract(r\"_(\\w+)$\")[0]\n    case = case1.fillna(case2)\n    var  = var1.fillna(var2).fillna(\"unk\")\n    return pd.DataFrame({\"case_id\": case, \"variant\": var})\n\ndef _ensure_case_variant(df: pd.DataFrame, df_base_map: pd.DataFrame = None) -> pd.DataFrame:\n    df = _ensure_uid(df)\n\n    if \"case_id\" in df.columns:\n        df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n    if \"variant\" in df.columns:\n        df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n\n    if df_base_map is not None and {\"uid\", \"case_id\", \"variant\"}.issubset(df_base_map.columns):\n        need_merge = (\"case_id\" not in df.columns) or (\"variant\" not in df.columns) or df[\"case_id\"].isna().any()\n        if need_merge:\n            df = df.merge(df_base_map[[\"uid\", \"case_id\", \"variant\"]], on=\"uid\", how=\"left\", suffixes=(\"\", \"_base\"))\n            if \"case_id_base\" in df.columns:\n                df[\"case_id\"] = df[\"case_id\"].fillna(df[\"case_id_base\"])\n                df = df.drop(columns=[\"case_id_base\"])\n            if \"variant_base\" in df.columns:\n                df[\"variant\"] = df[\"variant\"].where(df[\"variant\"].astype(str).str.len() > 0, df[\"variant_base\"])\n                df = df.drop(columns=[\"variant_base\"])\n\n    if (\"case_id\" not in df.columns) or (\"variant\" not in df.columns) or df[\"case_id\"].isna().any():\n        pv = _parse_case_variant_from_uid(df[\"uid\"])\n        if \"case_id\" not in df.columns:\n            df[\"case_id\"] = pd.to_numeric(pv[\"case_id\"], errors=\"coerce\")\n        else:\n            df[\"case_id\"] = df[\"case_id\"].fillna(pd.to_numeric(pv[\"case_id\"], errors=\"coerce\"))\n        if \"variant\" not in df.columns:\n            df[\"variant\"] = pv[\"variant\"]\n        else:\n            v = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n            df[\"variant\"] = v.where(v.str.len() > 0, pv[\"variant\"])\n\n    df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n    df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n    return df\n\ndef _pick_label_col(df: pd.DataFrame) -> str:\n    for cand in [\"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]:\n        if cand in df.columns:\n            return cand\n    return \"\"\n\ndef _slug(s: str) -> str:\n    s = str(s)\n    s = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", s)\n    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n    return s[:80] if len(s) > 80 else s\n\ndef _read_parquet_cols_safe(path: Path, desired_cols: list) -> pd.DataFrame:\n    path = Path(path)\n    try:\n        import pyarrow.parquet as pq\n        cols = pq.ParquetFile(path).schema.names\n        use = [c for c in desired_cols if c in cols]\n        if not use:\n            return pd.read_parquet(path)\n        return pd.read_parquet(path, columns=use)\n    except Exception:\n        df = pd.read_parquet(path)\n        use = [c for c in desired_cols if c in df.columns]\n        return df[use].copy() if use else df\n\n# ----------------------------\n# Resolve primary paths\n# ----------------------------\nmatch_cfg_name = Path(PATHS.get(\"MATCH_CFG_DIR\", \"\")).name if PATHS.get(\"MATCH_CFG_DIR\") else \"\"\npred_cfg_name  = Path(PATHS.get(\"PRED_CFG_DIR\", \"\")).name  if PATHS.get(\"PRED_CFG_DIR\") else \"\"\n\nWORK_CACHE_ROOT = Path(\"/kaggle/working/recodai_luc/cache\")\nmatch_feat_work = (WORK_CACHE_ROOT / match_cfg_name / \"match_features_train_all.csv\") if match_cfg_name else None\npred_feat_work  = (WORK_CACHE_ROOT / pred_cfg_name  / \"pred_features_train_all.csv\")  if pred_cfg_name  else None\n\nPRED_FEAT_TRAIN  = _prefer_existing(pred_feat_work,  PATHS.get(\"PRED_FEAT_TRAIN\", \"\"))\nMATCH_FEAT_TRAIN = _prefer_existing(match_feat_work, PATHS.get(\"MATCH_FEAT_TRAIN\", \"\"))\n\nDF_TRAIN_ALL      = Path(PATHS[\"DF_TRAIN_ALL\"])\nCV_CASE_FOLDS     = Path(PATHS[\"CV_CASE_FOLDS\"])\nIMG_PROFILE_TRAIN = Path(PATHS.get(\"IMG_PROFILE_TRAIN\", \"\"))\n\nfor need_name, need_path in [\n    (\"df_train_all.parquet\", DF_TRAIN_ALL),\n    (\"cv_case_folds.csv\", CV_CASE_FOLDS),\n    (\"pred_features_train_all.csv\", PRED_FEAT_TRAIN),\n]:\n    if not Path(need_path).exists():\n        raise FileNotFoundError(f\"Missing required file: {need_name} -> {need_path}\")\n\nprint(\"Using (PRIMARY):\")\nprint(\"  DF_TRAIN_ALL     :\", DF_TRAIN_ALL)\nprint(\"  CV_CASE_FOLDS    :\", CV_CASE_FOLDS)\nprint(\"  PRED_FEAT_TRAIN  :\", PRED_FEAT_TRAIN)\nprint(\"  MATCH_FEAT_TRAIN :\", MATCH_FEAT_TRAIN, \"(optional)\" if Path(MATCH_FEAT_TRAIN).exists() else \"(missing/skip)\")\nprint(\"  IMG_PROFILE_TRAIN:\", IMG_PROFILE_TRAIN, \"(optional)\" if Path(IMG_PROFILE_TRAIN).exists() else \"(missing/skip)\")\n\nPRED_NPZ_TRAIN_DIR  = Path(PATHS.get(\"PRED_NPZ_TRAIN_DIR\", str(Path(PATHS.get(\"PRED_CFG_DIR\", \"\")) / \"train_all\")))\nMATCH_NPZ_TRAIN_DIR = Path(PATHS.get(\"MATCH_NPZ_TRAIN_DIR\", str(Path(PATHS.get(\"MATCH_CFG_DIR\", \"\")) / \"train_all\")))\n\n# ----------------------------\n# 3) Load minimal inputs\n# ----------------------------\nbase_cols_want = [\"sample_id\", \"uid\", \"case_id\", \"variant\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]\ndf_base = _read_parquet_cols_safe(DF_TRAIN_ALL, base_cols_want)\ndf_cv   = pd.read_csv(CV_CASE_FOLDS)\n\ndf_pred_primary = pd.read_csv(PRED_FEAT_TRAIN, low_memory=False)\n\ndf_match_primary = None\nif FE_CFG[\"use_match_features\"] and Path(MATCH_FEAT_TRAIN).exists():\n    try:\n        df_match_primary = pd.read_csv(MATCH_FEAT_TRAIN, low_memory=False)\n    except Exception:\n        df_match_primary = None\n\ndf_prof = None\nif FE_CFG[\"use_image_profile\"] and Path(IMG_PROFILE_TRAIN).exists():\n    try:\n        df_prof = pd.read_parquet(IMG_PROFILE_TRAIN)\n    except Exception:\n        df_prof = None\n\n# ----------------------------\n# 4) Prepare base mapping from df_train_all\n# ----------------------------\ndf_base = df_base.copy()\nif \"uid\" not in df_base.columns:\n    if \"sample_id\" in df_base.columns:\n        df_base = df_base.rename(columns={\"sample_id\": \"uid\"})\n    elif (\"case_id\" in df_base.columns and \"variant\" in df_base.columns):\n        df_base[\"uid\"] = _to_str_series(df_base[\"case_id\"]) + \"__\" + _to_str_series(df_base[\"variant\"])\n\ndf_base = _ensure_uid(df_base)\n\nif \"case_id\" in df_base.columns:\n    df_base[\"case_id\"] = pd.to_numeric(df_base[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\nif \"variant\" in df_base.columns:\n    df_base[\"variant\"] = df_base[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n\nlabel_col = _pick_label_col(df_base)\nif not label_col:\n    raise ValueError(\"Cannot find label column in df_train_all (y_forged/has_mask/is_forged/forged).\")\n\ndf_base_map = df_base.drop_duplicates(subset=[\"uid\"], keep=\"first\").copy()\n\n# ----------------------------\n# 5) Prepare folds\n# ----------------------------\nif \"case_id\" not in df_cv.columns or \"fold\" not in df_cv.columns:\n    raise ValueError(\"cv_case_folds.csv must contain columns: case_id, fold\")\n\ndf_cv = df_cv[[\"case_id\", \"fold\"]].copy()\ndf_cv[\"case_id\"] = pd.to_numeric(df_cv[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\ndf_cv[\"fold\"]    = pd.to_numeric(df_cv[\"fold\"], errors=\"coerce\").astype(\"Int64\")\ndf_cv = df_cv.dropna().astype({\"case_id\": int, \"fold\": int}).drop_duplicates(\"case_id\")\n\n# ----------------------------\n# 6) Start from PRIMARY pred features\n# ----------------------------\ndf_pred_primary = _ensure_case_variant(df_pred_primary, df_base_map=df_base_map)\nif df_pred_primary[\"uid\"].duplicated().any():\n    df_pred_primary = df_pred_primary.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n\ndf_train = df_pred_primary.copy()\n\ndf_train = df_train.merge(\n    df_base_map[[\"uid\", label_col]].rename(columns={label_col: \"y\"}),\n    on=\"uid\", how=\"left\"\n)\n\nif df_train[\"y\"].isna().any():\n    miss = int(df_train[\"y\"].isna().sum())\n    raise ValueError(f\"Label merge produced NaN in y: {miss} rows. Check df_train_all vs pred_features alignment.\")\n\ndf_train[\"y\"] = pd.to_numeric(df_train[\"y\"], errors=\"coerce\")\n\nif FE_CFG[\"drop_unlabeled\"]:\n    before = len(df_train)\n    df_train = df_train[df_train[\"y\"].isin([0, 1])].copy()\n    after = len(df_train)\n    if before != after:\n        print(f\"NOTE: Dropped unlabeled rows (y not in {{0,1}}): {before-after} rows\")\n\ndf_train[\"y\"] = df_train[\"y\"].astype(int)\n\ndf_train = df_train.drop(columns=[\"fold\"], errors=\"ignore\").merge(df_cv, on=\"case_id\", how=\"left\")\nif df_train[\"fold\"].isna().any():\n    miss = int(df_train[\"fold\"].isna().sum())\n    raise ValueError(f\"Missing fold after merging cv_case_folds.csv: {miss} rows.\")\ndf_train[\"fold\"] = df_train[\"fold\"].astype(int)\n\n# ----------------------------\n# 7) Optional merge PRIMARY match features\n# ----------------------------\nif df_match_primary is not None:\n    dfm = _ensure_case_variant(df_match_primary, df_base_map=df_base_map)\n    if dfm[\"uid\"].duplicated().any():\n        dfm = dfm.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n\n    base_cols = set(df_train.columns)\n    new_cols = [c for c in dfm.columns if c not in base_cols and c not in [\"case_id\", \"variant\"]]\n    if new_cols:\n        df_train = df_train.merge(dfm[[\"uid\"] + new_cols], on=\"uid\", how=\"left\")\n\n# ----------------------------\n# 7.5) Aliasing\n# ----------------------------\ndef _coalesce_numeric(df: pd.DataFrame, out_col: str, candidates: list):\n    if out_col in df.columns:\n        return\n    for c in candidates:\n        if c in df.columns:\n            df[out_col] = pd.to_numeric(df[c], errors=\"coerce\")\n            return\n\n_coalesce_numeric(df_train, \"area_frac\", [\"area_frac\", \"pred_area_frac\", \"mask_area_frac\", \"area_frac_pred\", \"area_frac_unet\", \"unet_area_frac\"])\n_coalesce_numeric(df_train, \"grid_area_frac\", [\"grid_area_frac\", \"grid_mask_area_frac\", \"grid_area\"])\n_coalesce_numeric(df_train, \"best_count\", [\"best_count\", \"n_pairs\", \"pairs\", \"pair_count\"])\n_coalesce_numeric(df_train, \"best_mean_sim\", [\"best_mean_sim\", \"mean_sim\", \"sim_mean\"])\n_coalesce_numeric(df_train, \"peak_ratio\", [\"peak_ratio\", \"peak_to_mean\", \"peak_over_mean\"])\n_coalesce_numeric(df_train, \"inlier_ratio\", [\"inlier_ratio\", \"ransac_inlier_ratio\", \"inliers_ratio\"])\n_coalesce_numeric(df_train, \"has_peak\", [\"has_peak\", \"peak_found\", \"has_mode\"])\n\n_coalesce_numeric(df_train, \"n_cc_pred\", [\"n_cc_pred\", \"n_cc\", \"n_components\", \"num_components\", \"n_comp\", \"pred_n_cc\"])\n_coalesce_numeric(df_train, \"largest_cc_frac_pred\", [\"largest_cc_frac_pred\", \"largest_cc_frac\", \"largest_comp_frac\"])\n_coalesce_numeric(df_train, \"mean_prob_inside_pred\", [\"mean_prob_inside_pred\", \"mean_prob_inside\", \"mean_inside_prob\"])\n_coalesce_numeric(df_train, \"p90_prob_inside_pred\", [\"p90_prob_inside_pred\", \"p90_prob_inside\", \"p90_inside_prob\"])\n_coalesce_numeric(df_train, \"max_prob_pred\", [\"max_prob_pred\", \"max_prob\", \"pred_max_prob\"])\n\n# ----------------------------\n# 8) MULTI-CFG extras\n# ----------------------------\ndef _short_cfg_tag(cfg_dir: Path, idx: int) -> str:\n    nm = cfg_dir.name\n    m = re.search(r\"(cfg_[0-9a-f]{6,})\", nm)\n    tag = m.group(1) if m else nm\n    tag = tag.replace(\"pred_unet_aspp_cfg_\", \"pu_\").replace(\"pred_fusion_cfg_\", \"pf_\").replace(\"pred_base_\", \"pb_\")\n    tag = tag.replace(\"match_base_cfg_\", \"m_\")\n    tag = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", tag)\n    return f\"{idx:02d}_{tag[:24]}\"\n\nCORE_PRED_COLS = [\n    \"best_count\",\"best_mean_sim\",\"peak_ratio\",\"best_weight\",\"best_weight_frac\",\"inlier_ratio\",\"n_pairs_thr\",\"n_pairs_mnn\",\"has_peak\",\n    \"grid_h\",\"grid_w\",\"grid_area_frac\",\n    \"area_frac\",\"pred_area_frac\",\"mask_area_frac\",\"n_cc\",\"n_comp\",\"n_components\",\"largest_cc_frac\",\"mean_prob_inside\",\"p90_prob_inside\",\"max_prob\",\n]\nCORE_MATCH_COLS = [\n    \"best_count\",\"best_mean_sim\",\"peak_ratio\",\"best_weight\",\"best_weight_frac\",\"inlier_ratio\",\"n_pairs_thr\",\"n_pairs_mnn\",\"has_peak\",\n    \"grid_h\",\"grid_w\",\"grid_area_frac\",\n    \"area_frac\",\"mask_area_frac\",\"n_cc\",\"n_comp\",\"largest_cc_frac\",\n]\n\ndef _load_csv_core(csv_path: Path, core_cols: list) -> pd.DataFrame:\n    df = pd.read_csv(csv_path, low_memory=False)\n    df = _ensure_uid(df)\n    keep = [\"uid\"] + [c for c in core_cols if c in df.columns]\n    return df[keep].copy()\n\npred_cfg_dirs = [Path(x) for x in PATHS.get(\"PRED_CFG_DIRS\", [])] if FE_CFG[\"multi_cfg_enabled\"] else []\nmatch_cfg_dirs = [Path(x) for x in PATHS.get(\"MATCH_CFG_DIRS\", [])] if FE_CFG[\"multi_cfg_enabled\"] else []\n\nif pred_cfg_dirs:\n    pred_cfg_dirs = [d for d in pred_cfg_dirs if d.exists()]\n    pred_cfg_dirs = pred_cfg_dirs[:max(1, FE_CFG[\"multi_cfg_max_pred\"])]\nelse:\n    pred_cfg_dirs = [Path(PATHS[\"PRED_CFG_DIR\"])]\n\nif match_cfg_dirs:\n    match_cfg_dirs = [d for d in match_cfg_dirs if d.exists()]\n    match_cfg_dirs = match_cfg_dirs[:max(1, FE_CFG[\"multi_cfg_max_match\"])]\nelse:\n    match_cfg_dirs = [Path(PATHS[\"MATCH_CFG_DIR\"])] if FE_CFG[\"use_match_features\"] else []\n\nextra_pred_cfgs = [d for d in pred_cfg_dirs if str(d) != str(Path(PATHS[\"PRED_CFG_DIR\"]))]\nextra_match_cfgs = [d for d in match_cfg_dirs if str(d) != str(Path(PATHS[\"MATCH_CFG_DIR\"]))]\n\npred_core_cols_added = []\npred_core_matrix_cols = {}\n\nfor i, cfg_dir in enumerate(extra_pred_cfgs, 1):\n    fp = cfg_dir / \"pred_features_train_all.csv\"\n    if not fp.exists():\n        continue\n    try:\n        tag = _short_cfg_tag(cfg_dir, i)\n        df_extra = _load_csv_core(fp, CORE_PRED_COLS)\n        df_extra = _ensure_case_variant(df_extra, df_base_map=df_base_map)\n        ren = {}\n        for c in df_extra.columns:\n            if c == \"uid\":\n                continue\n            ren[c] = f\"{c}__{tag}\"\n            pred_core_matrix_cols.setdefault(c, []).append(ren[c])\n        df_extra = df_extra.rename(columns=ren)\n        df_train = df_train.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n        pred_core_cols_added.extend(list(ren.values()))\n    except Exception as e:\n        print(f\"WARNING: skip extra pred cfg {cfg_dir.name} due to error: {e}\")\n\nif FE_CFG[\"multi_cfg_extra_mode\"] == \"core+agg\":\n    for base_name, cols_suff in pred_core_matrix_cols.items():\n        cols_all = []\n        if base_name in df_train.columns:\n            cols_all.append(base_name)\n        cols_all.extend([c for c in cols_suff if c in df_train.columns])\n        cols_all = [c for c in cols_all if c in df_train.columns]\n        if len(cols_all) < 2:\n            continue\n        mat = df_train[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n        df_train[f\"cfg_mean_{base_name}\"] = mat.mean(axis=1, skipna=True)\n        df_train[f\"cfg_max_{base_name}\"]  = mat.max(axis=1, skipna=True)\n        df_train[f\"cfg_min_{base_name}\"]  = mat.min(axis=1, skipna=True)\n        df_train[f\"cfg_std_{base_name}\"]  = mat.std(axis=1, skipna=True)\n\nmatch_core_cols_added = []\nmatch_core_matrix_cols = {}\n\nif FE_CFG[\"use_match_features\"]:\n    for i, cfg_dir in enumerate(extra_match_cfgs, 1):\n        fp = cfg_dir / \"match_features_train_all.csv\"\n        if not fp.exists():\n            continue\n        try:\n            tag = _short_cfg_tag(cfg_dir, i)\n            df_extra = _load_csv_core(fp, CORE_MATCH_COLS)\n            df_extra = _ensure_case_variant(df_extra, df_base_map=df_base_map)\n            ren = {}\n            for c in df_extra.columns:\n                if c == \"uid\":\n                    continue\n                ren[c] = f\"{c}__{tag}\"\n                match_core_matrix_cols.setdefault(c, []).append(ren[c])\n            df_extra = df_extra.rename(columns=ren)\n            df_train = df_train.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n            match_core_cols_added.extend(list(ren.values()))\n        except Exception as e:\n            print(f\"WARNING: skip extra match cfg {cfg_dir.name} due to error: {e}\")\n\n    if FE_CFG[\"multi_cfg_extra_mode\"] == \"core+agg\":\n        for base_name, cols_suff in match_core_matrix_cols.items():\n            cols_all = []\n            if base_name in df_train.columns:\n                cols_all.append(base_name)\n            cols_all.extend([c for c in cols_suff if c in df_train.columns])\n            cols_all = [c for c in cols_all if c in df_train.columns]\n            if len(cols_all) < 2:\n                continue\n            mat = df_train[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n            df_train[f\"cfg_mean_match_{base_name}\"] = mat.mean(axis=1, skipna=True)\n            df_train[f\"cfg_max_match_{base_name}\"]  = mat.max(axis=1, skipna=True)\n            df_train[f\"cfg_min_match_{base_name}\"]  = mat.min(axis=1, skipna=True)\n            df_train[f\"cfg_std_match_{base_name}\"]  = mat.std(axis=1, skipna=True)\n\n# ----------------------------\n# 9) Optional merge image profile\n# ----------------------------\nif df_prof is not None and \"case_id\" in df_prof.columns:\n    df_prof2 = df_prof.copy()\n    df_prof2[\"case_id\"] = pd.to_numeric(df_prof2[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n    df_prof2 = df_prof2.dropna(subset=[\"case_id\"]).astype({\"case_id\": int})\n    df_prof2 = df_prof2.drop_duplicates(\"case_id\")\n\n    prof_num = [\"case_id\"] + [\n        c for c in df_prof2.columns\n        if c != \"case_id\" and pd.api.types.is_numeric_dtype(df_prof2[c])\n    ]\n    df_prof2 = df_prof2[prof_num].copy()\n\n    ren = {c: f\"profile_{c}\" for c in df_prof2.columns if c != \"case_id\"}\n    df_prof2 = df_prof2.rename(columns=ren)\n    df_train = df_train.merge(df_prof2, on=\"case_id\", how=\"left\")\n\n# ----------------------------\n# 9.5) NPZ pair features (SAFE overlap)\n# ----------------------------\ndef _dir_has_any_npz(d: Path) -> bool:\n    try:\n        if d is None or (not d.exists()) or (not d.is_dir()):\n            return False\n        for _ in d.glob(\"*.npz\"):\n            return True\n        return False\n    except Exception:\n        return False\n\ndef _uid_to_npz_path(npz_dir: Path, uid: str) -> Path:\n    uid = str(uid)\n    if uid.endswith(\".npz\"):\n        return npz_dir / uid\n    cands = [\n        npz_dir / f\"{uid}.npz\",\n        npz_dir / f\"{uid.replace('__','_')}.npz\",\n        npz_dir / f\"{uid.replace('_','__')}.npz\",\n    ]\n    if \"__authentic\" in uid:\n        cands.append(npz_dir / f\"{uid.replace('__authentic','__auth')}.npz\")\n    if \"__forged\" in uid:\n        cands.append(npz_dir / f\"{uid.replace('__forged','__forg')}.npz\")\n    for p in cands:\n        if p.exists():\n            return p\n    return cands[0]\n\ntry:\n    import scipy.ndimage as ndi\n    _HAS_NDI = True\nexcept Exception:\n    _HAS_NDI = False\ntry:\n    import cv2\n    _HAS_CV2 = True\nexcept Exception:\n    _HAS_CV2 = False\n\ndef _maybe_downsample_mask(mask: np.ndarray, ds: int):\n    if ds is None or ds <= 0:\n        return mask\n    H, W = mask.shape[:2]\n    if H <= ds and W <= ds:\n        return mask\n    if _HAS_CV2:\n        return cv2.resize(mask.astype(np.uint8), (ds, ds), interpolation=cv2.INTER_NEAREST).astype(bool)\n    ys = (np.linspace(0, H-1, ds)).astype(int)\n    xs = (np.linspace(0, W-1, ds)).astype(int)\n    return mask[np.ix_(ys, xs)]\n\ndef _count_cc_and_largest(mask_bool: np.ndarray, ds: int):\n    m = _maybe_downsample_mask(mask_bool, ds)\n    m = (m > 0).astype(np.uint8)\n    if m.sum() == 0:\n        return 0, 0\n    if _HAS_NDI:\n        lab, n = ndi.label(m)\n        if n <= 0:\n            return 0, 0\n        sizes = np.bincount(lab.ravel())\n        sizes[0] = 0\n        largest = int(sizes.max()) if sizes.size else 0\n        return int(n), int(largest)\n    if _HAS_CV2:\n        n, lab = cv2.connectedComponents(m, connectivity=8)\n        n_cc = int(max(n - 1, 0))\n        if n_cc <= 0:\n            return 0, 0\n        sizes = np.bincount(lab.ravel())\n        sizes[0] = 0\n        largest = int(sizes.max()) if sizes.size else 0\n        return int(n_cc), int(largest)\n    return -1, -1\n\ndef _extract_mask_prob_from_npz(npz_path: Path):\n    try:\n        z = np.load(npz_path, allow_pickle=True)\n        keys = list(z.files)\n        arrs = {k: z[k] for k in keys}\n    except Exception:\n        return None, None\n\n    def _pick_first(keys_like):\n        for k in keys_like:\n            if k in arrs:\n                return k\n        return None\n\n    k_prob = _pick_first([\"prob\", \"probs\", \"pred_prob\", \"mask_prob\", \"p\", \"pred\"])\n    k_mask = _pick_first([\"mask\", \"masks\", \"bin_mask\", \"pred_mask\", \"mask_bin\"])\n\n    prob = None\n    mask = None\n\n    if k_prob is not None:\n        a = arrs[k_prob]\n        if isinstance(a, np.ndarray) and a.ndim >= 2:\n            prob = a.astype(np.float32)\n\n    if k_mask is not None:\n        a = arrs[k_mask]\n        if isinstance(a, np.ndarray):\n            if a.ndim == 3:\n                a = np.max(a, axis=0)\n            mask = (a > 0).astype(bool)\n\n    if mask is None and prob is None:\n        best_k, best_n = None, -1\n        for k, a in arrs.items():\n            if not isinstance(a, np.ndarray) or a.ndim < 2:\n                continue\n            n = int(np.prod(a.shape))\n            if n > best_n:\n                best_k, best_n = k, n\n        if best_k is not None:\n            a = arrs[best_k]\n            a2 = np.max(a, axis=0) if a.ndim == 3 else a\n            if np.issubdtype(a2.dtype, np.floating):\n                prob = a2.astype(np.float32)\n            else:\n                mask = (a2 > 0).astype(bool)\n\n    if mask is None and prob is not None:\n        mask = (prob >= float(FE_CFG[\"npz_bin_thr\"]))\n\n    if prob is not None and prob.ndim == 3:\n        prob = np.max(prob, axis=0)\n    if mask is not None and mask.ndim == 3:\n        mask = np.max(mask, axis=0).astype(bool)\n\n    # squeeze final (safety)\n    if prob is not None:\n        prob = np.asarray(prob).squeeze()\n        if prob.ndim != 2:\n            prob = None\n    if mask is not None:\n        mask = np.asarray(mask).squeeze()\n        if mask.ndim != 2:\n            mask = None\n\n    return mask, prob\n\ndef _mask_stats(mask: np.ndarray, prob: np.ndarray, ds_cc: int):\n    out = {}\n    if mask is None:\n        out.update({\n            \"npz_area_frac\": np.nan,\n            \"npz_n_cc\": np.nan,\n            \"npz_largest_cc_frac\": np.nan,\n            \"npz_mean_prob_inside\": np.nan,\n            \"npz_p90_prob_inside\": np.nan,\n            \"npz_max_prob\": np.nan,\n        })\n        return out\n\n    m = (mask > 0)\n    H, W = m.shape[:2]\n    tot = float(H * W) if H and W else 0.0\n    area = float(m.sum())\n    out[\"npz_area_frac\"] = (area / tot) if tot > 0 else 0.0\n\n    n_cc, largest = _count_cc_and_largest(m, ds_cc)\n    out[\"npz_n_cc\"] = float(n_cc) if n_cc >= 0 else np.nan\n    out[\"npz_largest_cc_frac\"] = (float(largest) / area) if area > 0 and largest >= 0 else np.nan\n\n    if prob is not None and prob.shape[:2] == m.shape[:2]:\n        p = prob.astype(np.float32)\n        inside = p[m]\n        if inside.size > 0:\n            out[\"npz_mean_prob_inside\"] = float(np.mean(inside))\n            out[\"npz_p90_prob_inside\"] = float(np.quantile(inside, 0.90))\n            out[\"npz_max_prob\"] = float(np.max(inside))\n        else:\n            out[\"npz_mean_prob_inside\"] = 0.0\n            out[\"npz_p90_prob_inside\"] = 0.0\n            out[\"npz_max_prob\"] = 0.0\n    else:\n        out[\"npz_mean_prob_inside\"] = np.nan\n        out[\"npz_p90_prob_inside\"] = np.nan\n        out[\"npz_max_prob\"] = np.nan\n\n    return out\n\n# -------- FIX v3.2: SAFE overlap helpers --------\ndef _resize_bool_to_hw(mask_bool: np.ndarray, out_hw):\n    H, W = int(out_hw[0]), int(out_hw[1])\n    m = np.asarray(mask_bool).astype(np.uint8).squeeze()\n    if m.ndim != 2:\n        return None\n    if m.shape[0] == H and m.shape[1] == W:\n        return (m > 0)\n    if _HAS_CV2:\n        m2 = cv2.resize(m, (W, H), interpolation=cv2.INTER_NEAREST)\n        return (m2 > 0)\n    h, w = m.shape\n    if h <= 0 or w <= 0 or H <= 0 or W <= 0:\n        return None\n    ys = (np.linspace(0, h - 1, H)).astype(int)\n    xs = (np.linspace(0, w - 1, W)).astype(int)\n    m2 = m[np.ix_(ys, xs)]\n    return (m2 > 0)\n\ndef _safe_overlap_features(mask_a, mask_b, min_pixels=16):\n    out = {\n        \"pm_inter_frac\": np.nan,\n        \"pm_union_frac\": np.nan,\n        \"pm_iou\": np.nan,\n        \"pm_pred_minus_match_frac\": np.nan,\n        \"pm_match_minus_pred_frac\": np.nan,\n    }\n    if mask_a is None or mask_b is None:\n        return out\n    a = np.asarray(mask_a).squeeze()\n    b = np.asarray(mask_b).squeeze()\n    if a.ndim != 2 or b.ndim != 2:\n        return out\n    if a.size < int(min_pixels) or b.size < int(min_pixels):\n        return out\n\n    a = (a > 0)\n    b = (b > 0)\n\n    if a.shape != b.shape:\n        b2 = _resize_bool_to_hw(b, a.shape)\n        if b2 is None:\n            return out\n        b = b2\n\n    inter = float(np.logical_and(a, b).sum())\n    union = float(np.logical_or(a, b).sum())\n    tot = float(a.size) if a.size else 0.0\n\n    out[\"pm_inter_frac\"] = (inter / tot) if tot > 0 else 0.0\n    out[\"pm_union_frac\"] = (union / tot) if tot > 0 else 0.0\n    out[\"pm_iou\"] = (inter / union) if union > 0 else 0.0\n    out[\"pm_pred_minus_match_frac\"] = ((float(a.sum()) - inter) / tot) if tot > 0 else 0.0\n    out[\"pm_match_minus_pred_frac\"] = ((float(b.sum()) - inter) / tot) if tot > 0 else 0.0\n    return out\n# -----------------------------------------------\n\ndef _build_npz_pair_features(uids: list, pred_dir: Path, match_dir: Path, cache_path: Path):\n    if FE_CFG[\"npz_cache_enabled\"] and cache_path.exists():\n        try:\n            dfc = pd.read_parquet(cache_path)\n            dfc = _ensure_uid(dfc)\n            return dfc\n        except Exception:\n            pass\n\n    rows = []\n    ds_cc = int(FE_CFG[\"npz_downsample_for_cc\"] or 0)\n\n    max_rows = int(FE_CFG[\"npz_max_rows\"] or 0)\n    if max_rows > 0:\n        uids = uids[:max_rows]\n\n    for i, uid in enumerate(uids, 1):\n        r = {\"uid\": str(uid)}\n\n        pred_p = _uid_to_npz_path(pred_dir, uid) if pred_dir is not None else None\n        match_p = _uid_to_npz_path(match_dir, uid) if match_dir is not None else None\n\n        mp = None; pp = None\n        if pred_p is not None and pred_p.exists():\n            mp, pp = _extract_mask_prob_from_npz(pred_p)\n        pred_stats = _mask_stats(mp, pp, ds_cc)\n        for k, v in pred_stats.items():\n            r[f\"pred_{k}\"] = v\n\n        mm = None; pm = None\n        if match_p is not None and match_p.exists():\n            mm, pm = _extract_mask_prob_from_npz(match_p)\n        match_stats = _mask_stats(mm, pm, ds_cc)\n        for k, v in match_stats.items():\n            r[f\"match_{k}\"] = v\n\n        # FIX v3.2: SAFE overlap (no broadcast error)\n        r.update(_safe_overlap_features(mp, mm, min_pixels=16))\n\n        rows.append(r)\n\n        if (i % 500) == 0:\n            print(f\"  NPZ features progress: {i}/{len(uids)}\")\n\n    dfc = pd.DataFrame(rows)\n    if FE_CFG[\"npz_cache_enabled\"]:\n        try:\n            cache_path.parent.mkdir(parents=True, exist_ok=True)\n            dfc.to_parquet(cache_path, index=False)\n        except Exception:\n            pass\n    return dfc\n\nnpz_ready = FE_CFG[\"use_npz_pair_features\"] and _dir_has_any_npz(PRED_NPZ_TRAIN_DIR) and _dir_has_any_npz(MATCH_NPZ_TRAIN_DIR)\nif npz_ready:\n    OUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n    OUT_ART.mkdir(parents=True, exist_ok=True)\n\n    key = f\"npz_pair_{_slug(pred_cfg_name)}__{_slug(match_cfg_name)}__thr{FE_CFG['npz_bin_thr']}\"\n    h = hashlib.md5(key.encode()).hexdigest()[:12]\n    cache_path = OUT_ART / f\"npz_pair_features_train_all_{h}.parquet\"\n\n    print(\"\\nBuilding NPZ pair features (pred+match) ...\")\n    uids = df_train[\"uid\"].astype(str).tolist()\n    df_npz = _build_npz_pair_features(uids, PRED_NPZ_TRAIN_DIR, MATCH_NPZ_TRAIN_DIR, cache_path)\n    df_npz = _ensure_uid(df_npz)\n\n    keep_cols = [c for c in df_npz.columns if c != \"uid\"]\n    df_train = df_train.merge(df_npz[[\"uid\"] + keep_cols], on=\"uid\", how=\"left\")\n\n    _coalesce_numeric(df_train, \"area_frac\", [\"area_frac\", \"pred_npz_area_frac\", \"pred_npz_area_frac\"])\n    _coalesce_numeric(df_train, \"n_cc_pred\", [\"n_cc_pred\", \"pred_npz_n_cc\"])\n    _coalesce_numeric(df_train, \"largest_cc_frac_pred\", [\"largest_cc_frac_pred\", \"pred_npz_largest_cc_frac\"])\n    _coalesce_numeric(df_train, \"mean_prob_inside_pred\", [\"mean_prob_inside_pred\", \"pred_npz_mean_prob_inside\"])\n    _coalesce_numeric(df_train, \"p90_prob_inside_pred\", [\"p90_prob_inside_pred\", \"pred_npz_p90_prob_inside\"])\n    _coalesce_numeric(df_train, \"max_prob_pred\", [\"max_prob_pred\", \"pred_npz_max_prob\"])\n\nelse:\n    print(\"\\nNOTE: NPZ pair features skipped (missing/empty pred/match npz dirs).\")\n    print(\"  PRED_NPZ_TRAIN_DIR :\", PRED_NPZ_TRAIN_DIR, \"(npz found)\" if _dir_has_any_npz(PRED_NPZ_TRAIN_DIR) else \"(empty/missing)\")\n    print(\"  MATCH_NPZ_TRAIN_DIR:\", MATCH_NPZ_TRAIN_DIR, \"(npz found)\" if _dir_has_any_npz(MATCH_NPZ_TRAIN_DIR) else \"(empty/missing)\")\n\n# ----------------------------\n# 10) Feature engineering helpers\n# ----------------------------\ndef _num(s):\n    return pd.to_numeric(s, errors=\"coerce\")\n\ndef safe_log1p_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.log1p(x)\n\ndef safe_sqrt_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.sqrt(x)\n\ndef get_clip_cap(series: pd.Series, q: float, fallback: float):\n    s = _num(series).astype(float).replace([np.inf, -np.inf], np.nan).dropna()\n    if len(s) == 0:\n        return float(fallback)\n    s = s[np.isfinite(s)]\n    cap = float(np.quantile(np.abs(s.values), q))\n    if (not np.isfinite(cap)) or (cap <= 0):\n        return float(fallback)\n    return float(cap)\n\n# ----------------------------\n# 11) Candidate numeric feature list (pre-fill)\n# ----------------------------\nTARGET_COLS = {\"y\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"}\nSPLIT_COLS  = {\"fold\"}\nID_DROP_NUM = {\"case_id\"}\n\nfor c in df_train.columns:\n    if pd.api.types.is_numeric_dtype(df_train[c]):\n        df_train[c] = df_train[c].replace([np.inf, -np.inf], np.nan)\n\nmissing_ind_cols = []\nif FE_CFG[\"add_missing_indicators\"]:\n    for c in df_train.columns:\n        if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n            continue\n        if pd.api.types.is_numeric_dtype(df_train[c]) and df_train[c].isna().any():\n            ind = f\"isna_{c}\"\n            df_train[ind] = df_train[c].isna().astype(np.uint8)\n            missing_ind_cols.append(ind)\n\nheavy_candidates = set([\n    \"peak_ratio\", \"best_weight\", \"best_count\", \"best_weight_frac\",\n    \"pair_count\", \"n_pairs_thr\", \"n_pairs_mnn\", \"overmask_tighten_steps\",\n    \"largest_comp\", \"n_comp\", \"grid_h\", \"grid_w\",\n    \"grid_area_frac\", \"area_frac\", \"inlier_ratio\",\n    \"n_cc_pred\", \"largest_cc_frac_pred\", \"mean_prob_inside_pred\", \"p90_prob_inside_pred\", \"max_prob_pred\",\n    \"pm_iou\", \"pm_union_frac\", \"pm_inter_frac\",\n])\nfor c in df_train.columns:\n    cl = c.lower()\n    if any(k in cl for k in [\"count\", \"pairs\", \"weight\", \"ratio\", \"area\", \"comp\", \"std_\", \"iou\", \"prob\"]):\n        if pd.api.types.is_numeric_dtype(df_train[c]):\n            heavy_candidates.add(c)\n\nclip_caps = {}\nif FE_CFG[\"clip_by_quantile\"]:\n    for c in sorted(list(heavy_candidates)):\n        if c in df_train.columns and pd.api.types.is_numeric_dtype(df_train[c]):\n            clip_caps[c] = get_clip_cap(df_train[c], FE_CFG[\"clip_q\"], FE_CFG[\"clip_max_fallback\"])\n\nif FE_CFG[\"add_log_features\"] or FE_CFG[\"add_sqrt_features\"]:\n    for c, cap in clip_caps.items():\n        x = _num(df_train[c]).fillna(0.0).astype(float).values\n        x = np.clip(x, -cap, cap)\n        df_train[f\"{c}_cap\"] = x.astype(np.float32)\n\n        if FE_CFG[\"add_log_features\"]:\n            df_train[f\"logabs_{c}\"] = safe_log1p_nonneg(np.abs(x)).astype(np.float32)\n        if FE_CFG[\"add_sqrt_features\"]:\n            df_train[f\"sqrtabs_{c}\"] = safe_sqrt_nonneg(np.abs(x)).astype(np.float32)\n\nif FE_CFG[\"add_interactions\"]:\n    def getf(col, default=0.0):\n        if col in df_train.columns:\n            return _num(df_train[col]).fillna(default).astype(float).values\n        return np.full(len(df_train), default, dtype=np.float64)\n\n    best_mean_sim = getf(\"best_mean_sim\", 0.0)\n    best_count    = getf(\"best_count\", 0.0)\n    peak_ratio    = getf(\"peak_ratio\", 0.0)\n    has_peak      = getf(\"has_peak\", 0.0)\n    grid_area     = getf(\"grid_area_frac\", 0.0)\n    area_frac     = getf(\"area_frac\", 0.0)\n    n_pairs_thr   = getf(\"n_pairs_thr\", 0.0)\n    n_pairs_mnn   = getf(\"n_pairs_mnn\", 0.0)\n    inlier_ratio  = getf(\"inlier_ratio\", 0.0)\n    gh = getf(\"grid_h\", 0.0)\n    gw = getf(\"grid_w\", 0.0)\n    gridN = np.clip(gh * gw, 0.0, None)\n\n    df_train[\"sim_x_count\"]      = (best_mean_sim * best_count).astype(np.float32)\n    df_train[\"peak_x_sim\"]       = (peak_ratio * best_mean_sim).astype(np.float32)\n    df_train[\"haspeak_x_sim\"]    = (has_peak * best_mean_sim).astype(np.float32)\n    df_train[\"area_x_sim\"]       = (grid_area * best_mean_sim).astype(np.float32)\n    df_train[\"area_x_count\"]     = (grid_area * best_count).astype(np.float32)\n    df_train[\"mask_grid_ratio\"]  = (area_frac / (1e-6 + grid_area)).astype(np.float32)\n    df_train[\"mnn_ratio\"]        = (n_pairs_mnn / (1.0 + n_pairs_thr)).astype(np.float32)\n    df_train[\"pairs_per_cell\"]   = (n_pairs_thr / (1.0 + gridN)).astype(np.float32)\n    df_train[\"inlier_x_pairs\"]   = (inlier_ratio * n_pairs_thr).astype(np.float32)\n\n    df_train[\"log1p_pairs_thr\"]  = safe_log1p_nonneg(n_pairs_thr).astype(np.float32)\n    df_train[\"log1p_best_count\"] = safe_log1p_nonneg(best_count).astype(np.float32)\n    df_train[\"log1p_area_frac\"]  = safe_log1p_nonneg(np.clip(area_frac, 0, None)).astype(np.float32)\n\n    ncc   = getf(\"n_cc_pred\", 0.0)\n    lccf  = getf(\"largest_cc_frac_pred\", 0.0)\n    mpin  = getf(\"mean_prob_inside_pred\", 0.0)\n    p90in = getf(\"p90_prob_inside_pred\", 0.0)\n    mxp   = getf(\"max_prob_pred\", 0.0)\n\n    df_train[\"prob_mean_x_area\"] = (mpin * area_frac).astype(np.float32)\n    df_train[\"prob_p90_x_area\"]  = (p90in * area_frac).astype(np.float32)\n    df_train[\"prob_max_x_area\"]  = (mxp * area_frac).astype(np.float32)\n    df_train[\"comp_x_area\"]      = (ncc * area_frac).astype(np.float32)\n    df_train[\"largestcc_x_area\"] = (lccf * area_frac).astype(np.float32)\n    df_train[\"prob_contrast\"]    = (p90in - mpin).astype(np.float32)\n\n    iou = getf(\"pm_iou\", 0.0)\n    inter = getf(\"pm_inter_frac\", 0.0)\n    union = getf(\"pm_union_frac\", 0.0)\n    df_train[\"iou_x_area\"]       = (iou * area_frac).astype(np.float32)\n    df_train[\"inter_over_union\"] = (inter / (1e-6 + union)).astype(np.float32)\n\nnum_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\ndf_train[num_cols] = df_train[num_cols].fillna(FE_CFG[\"fillna_value\"])\n\n# ----------------------------\n# 12) Variant one-hot\n# ----------------------------\nvariant_dummy_cols = []\nif FE_CFG[\"encode_variant_onehot\"]:\n    vc = df_train[\"variant\"].astype(str).fillna(\"unk\")\n    counts = vc.value_counts()\n    keep = set(counts[counts >= int(FE_CFG[\"variant_min_count\"])].index.tolist())\n    vc = vc.where(vc.isin(keep), other=\"rare\")\n    dummies = pd.get_dummies(vc, prefix=\"v\", dummy_na=False).astype(np.uint8)\n    variant_dummy_cols = dummies.columns.tolist()\n    df_train = pd.concat([df_train, dummies], axis=1)\n\n# ----------------------------\n# 13) Select final feature columns\n# ----------------------------\nfeature_cols = []\nfor c in df_train.columns:\n    if not pd.api.types.is_numeric_dtype(df_train[c]):\n        continue\n    if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n        continue\n    feature_cols.append(c)\n\ndropped_constant = []\nif FE_CFG[\"drop_constant_features\"]:\n    nun = df_train[feature_cols].nunique(dropna=False)\n    nonconst = nun[nun > 1].index.tolist()\n    dropped_constant = sorted(set(feature_cols) - set(nonconst))\n    feature_cols = nonconst\n\nif FE_CFG[\"cast_float32\"]:\n    df_train[feature_cols] = df_train[feature_cols].astype(np.float32)\n\nFEATURE_COLS = list(feature_cols)\n\n# ----------------------------\n# 14) Final outputs\n# ----------------------------\nbase_out_cols = [\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]\ndf_train_tabular = df_train[base_out_cols + FEATURE_COLS].copy()\n\nX_train = df_train_tabular[FEATURE_COLS]\ny_train = df_train_tabular[\"y\"].astype(int)\nfolds   = df_train_tabular[\"fold\"].astype(int)\n\nprint(\"\\nOK — Training table built\")\nprint(\"  df_train_tabular:\", df_train_tabular.shape)\nprint(\"  X_train:\", X_train.shape, \"| y pos%:\", float(y_train.mean()) * 100.0)\nprint(\"  folds:\", int(folds.nunique()), \"unique folds\")\nprint(\"  feature_cols:\", int(len(FEATURE_COLS)))\nif dropped_constant:\n    print(\"  dropped_constant_features:\", len(dropped_constant))\nif variant_dummy_cols:\n    print(\"  variant_dummies:\", len(variant_dummy_cols))\nif missing_ind_cols:\n    print(\"  missing_indicators:\", len(missing_ind_cols))\nif pred_core_cols_added:\n    print(\"  extra_pred_core_cols:\", len(pred_core_cols_added))\nif match_core_cols_added:\n    print(\"  extra_match_core_cols:\", len(match_core_cols_added))\n\nif X_train.shape[0] != y_train.shape[0]:\n    raise RuntimeError(\"X_train and y_train row mismatch\")\nif y_train.isna().any():\n    raise RuntimeError(\"y_train contains NaN\")\nif folds.isna().any():\n    raise RuntimeError(\"folds contains NaN\")\n\nprint(\"\\nFeature head:\", FEATURE_COLS[:25])\nprint(\"Feature tail:\", FEATURE_COLS[-15:])\n\n# ----------------------------\n# 15) Save schema\n# ----------------------------\nOUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_ART.mkdir(parents=True, exist_ok=True)\n\nwith open(OUT_ART / \"feature_cols.json\", \"w\") as f:\n    json.dump(FEATURE_COLS, f, indent=2)\n\nschema = {\n    \"fe_cfg\": FE_CFG,\n    \"label_col_source\": label_col,\n    \"clip_caps\": {k: float(v) for k, v in clip_caps.items()},\n    \"dropped_constant_features\": dropped_constant,\n    \"variant_dummy_cols\": variant_dummy_cols,\n    \"missing_indicator_cols\": missing_ind_cols,\n    \"primary_cfg\": {\n        \"pred_cfg_dir\": str(Path(PATHS[\"PRED_CFG_DIR\"])),\n        \"match_cfg_dir\": str(Path(PATHS[\"MATCH_CFG_DIR\"])) if PATHS.get(\"MATCH_CFG_DIR\") else \"\",\n        \"pred_npz_train_dir\": str(PRED_NPZ_TRAIN_DIR),\n        \"match_npz_train_dir\": str(MATCH_NPZ_TRAIN_DIR),\n    },\n    \"multi_cfg\": {\n        \"pred_cfg_dirs_used\": [str(Path(PATHS[\"PRED_CFG_DIR\"]))] + [str(d) for d in extra_pred_cfgs],\n        \"match_cfg_dirs_used\": [str(Path(PATHS[\"MATCH_CFG_DIR\"]))] + ([str(d) for d in extra_match_cfgs] if FE_CFG[\"use_match_features\"] else []),\n        \"extra_pred_core_cols_added\": pred_core_cols_added[:200],\n        \"extra_match_core_cols_added\": match_core_cols_added[:200],\n    },\n    \"n_features\": int(len(FEATURE_COLS)),\n    \"example_feature_head\": FEATURE_COLS[:30],\n}\nwith open(OUT_ART / \"feature_schema.json\", \"w\") as f:\n    json.dump(schema, f, indent=2)\n\ndf_train_tabular.to_parquet(OUT_ART / \"df_train_tabular.parquet\", index=False)\n\nprint(f\"\\nSaved -> {OUT_ART/'feature_cols.json'}\")\nprint(f\"Saved -> {OUT_ART/'feature_schema.json'}\")\nprint(f\"Saved -> {OUT_ART/'df_train_tabular.parquet'}\")\n\nglobals().update({\n    \"df_train_tabular\": df_train_tabular,\n    \"FEATURE_COLS\": FEATURE_COLS,\n    \"X_train\": X_train,\n    \"y_train\": y_train,\n    \"folds\": folds,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T16:23:34.008721Z","iopub.execute_input":"2026-01-11T16:23:34.008997Z"}},"outputs":[{"name":"stdout","text":"Using (PRIMARY):\n  DF_TRAIN_ALL     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet\n  CV_CASE_FOLDS    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv\n  PRED_FEAT_TRAIN  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv\n  MATCH_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv (optional)\n  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet (optional)\n\nBuilding NPZ pair features (pred+match) ...\n  NPZ features progress: 500/5176\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Build & Export Test Feature Table","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 2.5 — Build & Export Test Feature Table (pred_features_test*)\n# ONE CELL (Kaggle-ready) — REVISI FULL v2.0 (SELARAS Step 2 v3.1)\n#\n# Tujuan (FIX utama v2.0):\n# - Bukan sekadar \"merge kolom yang ketemu\".\n# - v2.0 MEMBANGUN fitur TEST dengan *feature engineering yang sama* seperti Step 2:\n#   * missing indicators (sesuai schema)\n#   * clipping + *_cap + logabs_*\n#   * interactions\n#   * variant one-hot (kolom sama persis)\n#   * (opsional) multi-CFG core+agg (pred+match)\n#   * (opsional) NPZ pair features (pred+match) + overlap (cache parquet)\n#\n# Output:\n#   /kaggle/working/recodai_luc_gate_artifacts/pred_features_test.csv\n#   /kaggle/working/recodai_luc_gate_artifacts/pred_features_test_cfg_<hash>.csv\n#\n# REQUIRE (minimal):\n# - FEATURE_COLS (list) dari Step 2\n# - feature_schema.json (disarankan; dibuat oleh Step 2 v3.1)\n# - PATHS dari STAGE 0 (untuk lokasi pred/match cfg + test meta)\n# ============================================================\n\nimport os, re, json, hashlib, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 0) Require FEATURE_COLS\n# ----------------------------\nif \"FEATURE_COLS\" not in globals():\n    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 (Build Training Table) dulu.\")\nFEATURE_COLS = list(FEATURE_COLS)\nif len(FEATURE_COLS) == 0:\n    raise RuntimeError(\"FEATURE_COLS kosong. Cek Step 2.\")\n\n# ----------------------------\n# 1) Load schema (from Step 2)\n# ----------------------------\nSCHEMA_PATH = OUT_DIR / \"feature_schema.json\"\nif not SCHEMA_PATH.exists():\n    print(\"WARNING: feature_schema.json tidak ditemukan. Akan jalan mode fallback (lebih lemah).\")\n    schema = {}\nelse:\n    schema = json.loads(SCHEMA_PATH.read_text())\n\nFE_CFG = schema.get(\"fe_cfg\", {}) or {}\nclip_caps = schema.get(\"clip_caps\", {}) or {}\nvariant_dummy_cols = schema.get(\"variant_dummy_cols\", []) or []\nmissing_indicator_cols = schema.get(\"missing_indicator_cols\", []) or []\n\n# fallback defaults (jaga aman)\nFE_CFG.setdefault(\"fillna_value\", 0.0)\nFE_CFG.setdefault(\"npz_bin_thr\", 0.5)\nFE_CFG.setdefault(\"npz_downsample_for_cc\", 256)\nFE_CFG.setdefault(\"use_npz_pair_features\", True)\nFE_CFG.setdefault(\"use_match_features\", True)\nFE_CFG.setdefault(\"multi_cfg_enabled\", True)\nFE_CFG.setdefault(\"multi_cfg_extra_mode\", \"core+agg\")\nFE_CFG.setdefault(\"multi_cfg_max_pred\", int(os.environ.get(\"MULTI_CFG_MAX_PRED\", \"6\")))\nFE_CFG.setdefault(\"multi_cfg_max_match\", int(os.environ.get(\"MULTI_CFG_MAX_MATCH\", \"3\")))\n\n# ----------------------------\n# 2) Helpers\n# ----------------------------\ndef _read_table_any(p: Path):\n    p = Path(p)\n    if not p.exists():\n        return None\n    if p.suffix.lower() == \".parquet\":\n        return pd.read_parquet(p)\n    return pd.read_csv(p)\n\ndef _pick_first_existing(paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(str(p))\n        if p.exists() and p.is_file():\n            return p\n    return None\n\ndef _to_str_series(s: pd.Series) -> pd.Series:\n    return s.astype(str).replace({\"nan\": \"\", \"None\": \"\"})\n\ndef _ensure_uid_case_variant(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Pastikan ada: uid, case_id, variant (kalau bisa).\n    PRIORITAS uid format: case_id__variant (double underscore) karena NPZ kamu pakai itu.\n    \"\"\"\n    df = df.copy()\n    cols = set(df.columns)\n\n    # normalize case_id\n    if \"case_id\" not in cols:\n        for alt in [\"case\", \"caseid\", \"image_id\", \"img_id\", \"id\"]:\n            if alt in cols:\n                df = df.rename(columns={alt: \"case_id\"})\n                cols = set(df.columns)\n                break\n\n    if \"variant\" not in cols:\n        for alt in [\"var\", \"aug\", \"view\", \"split_variant\"]:\n            if alt in cols:\n                df = df.rename(columns={alt: \"variant\"})\n                cols = set(df.columns)\n                break\n\n    if \"case_id\" in df.columns:\n        df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n\n    if \"variant\" not in df.columns:\n        df[\"variant\"] = \"test\"\n    df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"test\", \"None\": \"test\", \"\": \"test\"})\n\n    # uid\n    if \"uid\" not in df.columns:\n        if \"case_id\" in df.columns:\n            # best: case_id__variant\n            df[\"uid\"] = _to_str_series(df[\"case_id\"]) + \"__\" + _to_str_series(df[\"variant\"])\n        else:\n            df[\"uid\"] = np.arange(len(df)).astype(str)\n\n    df[\"uid\"] = df[\"uid\"].astype(str)\n    return df\n\ndef _infer_join_keys(dfA: pd.DataFrame, dfB: pd.DataFrame):\n    A = set(dfA.columns); B = set(dfB.columns)\n    for keys in ([\"uid\"], [\"case_id\",\"variant\"], [\"case_id\"]):\n        if all(k in A for k in keys) and all(k in B for k in keys):\n            return keys\n    return None\n\ndef _slug(s: str) -> str:\n    s = str(s)\n    s = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", s)\n    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n    return s[:80] if len(s) > 80 else s\n\ndef _coalesce_numeric(df: pd.DataFrame, out_col: str, candidates: list):\n    if out_col in df.columns:\n        return\n    for c in candidates:\n        if c in df.columns:\n            df[out_col] = pd.to_numeric(df[c], errors=\"coerce\")\n            return\n\n# ----------------------------\n# 3) Load base TEST meta\n# ----------------------------\ndef _get_base_test_df():\n    if \"df_test_tabular\" in globals() and isinstance(globals()[\"df_test_tabular\"], pd.DataFrame):\n        return globals()[\"df_test_tabular\"].copy(), \"globals(df_test_tabular)\"\n    if \"df_test\" in globals() and isinstance(globals()[\"df_test\"], pd.DataFrame):\n        return globals()[\"df_test\"].copy(), \"globals(df_test)\"\n    if \"PATHS\" in globals() and isinstance(globals()[\"PATHS\"], dict):\n        cand = _pick_first_existing([\n            globals()[\"PATHS\"].get(\"DF_TEST\", None),\n            globals()[\"PATHS\"].get(\"DF_TEST_ALL\", None),\n            globals()[\"PATHS\"].get(\"TEST_META\", None),\n        ])\n        if cand is not None:\n            df = _read_table_any(cand)\n            if df is not None:\n                return df.copy(), f\"PATHS({cand})\"\n    cand = _pick_first_existing([\n        Path(\"/kaggle/working/df_test.csv\"),\n        Path(\"/kaggle/working/test.csv\"),\n        Path(\"/kaggle/working/df_test.parquet\"),\n    ])\n    if cand is not None:\n        df = _read_table_any(cand)\n        if df is not None:\n            return df.copy(), f\"fallback({cand})\"\n    raise FileNotFoundError(\"Tidak menemukan df_test/meta. Pastikan PATHS[DF_TEST] ada atau df_test sudah dibuat.\")\n\ndf_base, base_src = _get_base_test_df()\ndf_base = _ensure_uid_case_variant(df_base)\nprint(\"Base TEST source:\", base_src, \"| shape:\", df_base.shape)\n\n# satu row per uid\nif df_base[\"uid\"].duplicated().any():\n    df_base = df_base.drop_duplicates(\"uid\").reset_index(drop=True)\n\nid_cols = [c for c in [\"uid\",\"case_id\",\"variant\"] if c in df_base.columns]\ndf_out = df_base[id_cols].copy()\n\n# ----------------------------\n# 4) Merge PRIMARY pred/match test tables (langsung dari PATHS jika ada)\n# ----------------------------\ndef _merge_feat(path, name):\n    nonlocal df_out\n    if path is None:\n        return 0\n    p = Path(str(path))\n    if (not p.exists()) or (not p.is_file()):\n        return 0\n    try:\n        df = _read_table_any(p)\n        if df is None or len(df) == 0:\n            return 0\n        df = _ensure_uid_case_variant(df)\n        join_keys = _infer_join_keys(df_out, df)\n        if join_keys is None:\n            return 0\n        # keep only what helps (raw columns, not limited to FEATURE_COLS yet)\n        # (kita butuh base cols untuk bikin derived features)\n        df = df.drop_duplicates(join_keys if len(join_keys)>1 else join_keys[0])\n        before_cols = set(df_out.columns)\n        df_out = df_out.merge(df, on=join_keys, how=\"left\", suffixes=(\"\", \"_dup\"))\n\n        # resolve dup\n        dup_cols = [c for c in df_out.columns if c.endswith(\"_dup\")]\n        for dc in dup_cols:\n            basec = dc[:-4]\n            if basec in df_out.columns:\n                df_out[basec] = df_out[basec].fillna(df_out[dc])\n            df_out = df_out.drop(columns=[dc])\n\n        gained = len(set(df_out.columns) - before_cols)\n        print(f\"  merged {name}: {p.name} | join_keys={join_keys} | gained_cols={gained}\")\n        return gained\n    except Exception as e:\n        print(f\"  skip {name} (merge error): {p} | err={repr(e)}\")\n        return 0\n\nPRED_FEAT_TEST  = PATHS.get(\"PRED_FEAT_TEST\", None) if \"PATHS\" in globals() else None\nMATCH_FEAT_TEST = PATHS.get(\"MATCH_FEAT_TEST\", None) if \"PATHS\" in globals() else None\n\nprint(\"\\nMerging primary test feature CSVs (if exist):\")\ng1 = _merge_feat(PRED_FEAT_TEST, \"PRED_FEAT_TEST\")\ng2 = _merge_feat(MATCH_FEAT_TEST, \"MATCH_FEAT_TEST\")\n\n# ----------------------------\n# 5) MULTI-CFG extra core+agg (test) — harus konsisten dg Step 2\n# ----------------------------\ndef _short_cfg_tag(cfg_dir: Path, idx: int) -> str:\n    nm = cfg_dir.name\n    m = re.search(r\"(cfg_[0-9a-f]{6,})\", nm)\n    tag = m.group(1) if m else nm\n    tag = tag.replace(\"pred_unet_aspp_cfg_\", \"pu_\").replace(\"pred_fusion_cfg_\", \"pf_\").replace(\"pred_base_\", \"pb_\")\n    tag = tag.replace(\"match_base_cfg_\", \"m_\")\n    tag = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", tag)\n    return f\"{idx:02d}_{tag[:24]}\"\n\nCORE_PRED_COLS = [\n    \"best_count\",\"best_mean_sim\",\"peak_ratio\",\"best_weight\",\"best_weight_frac\",\"inlier_ratio\",\"n_pairs_thr\",\"n_pairs_mnn\",\"has_peak\",\n    \"grid_h\",\"grid_w\",\"grid_area_frac\",\n    \"area_frac\",\"pred_area_frac\",\"mask_area_frac\",\"n_cc\",\"n_comp\",\"n_components\",\"largest_cc_frac\",\"mean_prob_inside\",\"p90_prob_inside\",\"max_prob\",\n]\nCORE_MATCH_COLS = [\n    \"best_count\",\"best_mean_sim\",\"peak_ratio\",\"best_weight\",\"best_weight_frac\",\"inlier_ratio\",\"n_pairs_thr\",\"n_pairs_mnn\",\"has_peak\",\n    \"grid_h\",\"grid_w\",\"grid_area_frac\",\n    \"area_frac\",\"mask_area_frac\",\"n_cc\",\"n_comp\",\"largest_cc_frac\",\n]\n\ndef _load_csv_core(csv_path: Path, core_cols: list) -> pd.DataFrame:\n    df = pd.read_csv(csv_path, low_memory=False)\n    df = _ensure_uid_case_variant(df)\n    keep = [\"uid\"] + [c for c in core_cols if c in df.columns]\n    return df[keep].copy()\n\nif FE_CFG.get(\"multi_cfg_enabled\", True) and (\"PATHS\" in globals()) and isinstance(PATHS, dict):\n    pred_cfg_dirs = [Path(x) for x in PATHS.get(\"PRED_CFG_DIRS\", [])] if PATHS.get(\"PRED_CFG_DIRS\") else []\n    match_cfg_dirs = [Path(x) for x in PATHS.get(\"MATCH_CFG_DIRS\", [])] if PATHS.get(\"MATCH_CFG_DIRS\") else []\n\n    if pred_cfg_dirs:\n        pred_cfg_dirs = [d for d in pred_cfg_dirs if d.exists()]\n        pred_cfg_dirs = pred_cfg_dirs[:max(1, int(FE_CFG.get(\"multi_cfg_max_pred\", 6)))]\n    else:\n        if PATHS.get(\"PRED_CFG_DIR\"):\n            pred_cfg_dirs = [Path(PATHS[\"PRED_CFG_DIR\"])]\n\n    if match_cfg_dirs:\n        match_cfg_dirs = [d for d in match_cfg_dirs if d.exists()]\n        match_cfg_dirs = match_cfg_dirs[:max(1, int(FE_CFG.get(\"multi_cfg_max_match\", 3)))]\n    else:\n        if PATHS.get(\"MATCH_CFG_DIR\"):\n            match_cfg_dirs = [Path(PATHS[\"MATCH_CFG_DIR\"])]\n\n    primary_pred = str(Path(PATHS.get(\"PRED_CFG_DIR\",\"\"))) if PATHS.get(\"PRED_CFG_DIR\") else \"\"\n    primary_match = str(Path(PATHS.get(\"MATCH_CFG_DIR\",\"\"))) if PATHS.get(\"MATCH_CFG_DIR\") else \"\"\n\n    extra_pred_cfgs = [d for d in pred_cfg_dirs if str(d) != primary_pred]\n    extra_match_cfgs = [d for d in match_cfg_dirs if str(d) != primary_match]\n\n    pred_core_matrix_cols = {}\n    for i, cfg_dir in enumerate(extra_pred_cfgs, 1):\n        fp = cfg_dir / \"pred_features_test.csv\"\n        if not fp.exists():\n            continue\n        try:\n            tag = _short_cfg_tag(cfg_dir, i)\n            df_extra = _load_csv_core(fp, CORE_PRED_COLS)\n            ren = {}\n            for c in df_extra.columns:\n                if c == \"uid\": continue\n                ren[c] = f\"{c}__{tag}\"\n                pred_core_matrix_cols.setdefault(c, []).append(ren[c])\n            df_extra = df_extra.rename(columns=ren)\n            df_out = df_out.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n            print(f\"  extra pred cfg merged: {cfg_dir.name}\")\n        except Exception as e:\n            print(f\"  WARNING: skip extra pred cfg {cfg_dir.name}: {e}\")\n\n    if FE_CFG.get(\"multi_cfg_extra_mode\",\"core+agg\") == \"core+agg\":\n        for base_name, cols_suff in pred_core_matrix_cols.items():\n            cols_all = []\n            if base_name in df_out.columns:\n                cols_all.append(base_name)\n            cols_all.extend([c for c in cols_suff if c in df_out.columns])\n            cols_all = [c for c in cols_all if c in df_out.columns]\n            if len(cols_all) < 2:\n                continue\n            mat = df_out[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n            df_out[f\"cfg_mean_{base_name}\"] = mat.mean(axis=1, skipna=True)\n            df_out[f\"cfg_max_{base_name}\"]  = mat.max(axis=1, skipna=True)\n            df_out[f\"cfg_min_{base_name}\"]  = mat.min(axis=1, skipna=True)\n            df_out[f\"cfg_std_{base_name}\"]  = mat.std(axis=1, skipna=True)\n\n    match_core_matrix_cols = {}\n    if FE_CFG.get(\"use_match_features\", True):\n        for i, cfg_dir in enumerate(extra_match_cfgs, 1):\n            fp = cfg_dir / \"match_features_test.csv\"\n            if not fp.exists():\n                continue\n            try:\n                tag = _short_cfg_tag(cfg_dir, i)\n                df_extra = _load_csv_core(fp, CORE_MATCH_COLS)\n                ren = {}\n                for c in df_extra.columns:\n                    if c == \"uid\": continue\n                    ren[c] = f\"{c}__{tag}\"\n                    match_core_matrix_cols.setdefault(c, []).append(ren[c])\n                df_extra = df_extra.rename(columns=ren)\n                df_out = df_out.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n                print(f\"  extra match cfg merged: {cfg_dir.name}\")\n            except Exception as e:\n                print(f\"  WARNING: skip extra match cfg {cfg_dir.name}: {e}\")\n\n        if FE_CFG.get(\"multi_cfg_extra_mode\",\"core+agg\") == \"core+agg\":\n            for base_name, cols_suff in match_core_matrix_cols.items():\n                cols_all = []\n                if base_name in df_out.columns:\n                    cols_all.append(base_name)\n                cols_all.extend([c for c in cols_suff if c in df_out.columns])\n                cols_all = [c for c in cols_all if c in df_out.columns]\n                if len(cols_all) < 2:\n                    continue\n                mat = df_out[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n                df_out[f\"cfg_mean_match_{base_name}\"] = mat.mean(axis=1, skipna=True)\n                df_out[f\"cfg_max_match_{base_name}\"]  = mat.max(axis=1, skipna=True)\n                df_out[f\"cfg_min_match_{base_name}\"]  = mat.min(axis=1, skipna=True)\n                df_out[f\"cfg_std_match_{base_name}\"]  = mat.std(axis=1, skipna=True)\n\n# ----------------------------\n# 6) Optional: NPZ pair features for TEST (pred+match) + overlap (cache)\n# ----------------------------\ndef _dir_has_any_npz(d: Path) -> bool:\n    try:\n        if d is None or (not d.exists()) or (not d.is_dir()):\n            return False\n        for _ in d.glob(\"*.npz\"):\n            return True\n        return False\n    except Exception:\n        return False\n\ndef _uid_to_npz_path(npz_dir: Path, uid: str) -> Path:\n    uid = str(uid)\n    if uid.endswith(\".npz\"):\n        p = npz_dir / uid\n        return p\n    cands = [\n        npz_dir / f\"{uid}.npz\",\n        npz_dir / f\"{uid.replace('__','_')}.npz\",\n        npz_dir / f\"{uid.replace('_','__')}.npz\",\n    ]\n    for p in cands:\n        if p.exists():\n            return p\n    return cands[0]\n\ntry:\n    import scipy.ndimage as ndi\n    _HAS_NDI = True\nexcept Exception:\n    _HAS_NDI = False\ntry:\n    import cv2\n    _HAS_CV2 = True\nexcept Exception:\n    _HAS_CV2 = False\n\ndef _maybe_downsample_mask(mask: np.ndarray, ds: int):\n    if ds is None or ds <= 0:\n        return mask\n    H, W = mask.shape[:2]\n    if H <= ds and W <= ds:\n        return mask\n    if _HAS_CV2:\n        return cv2.resize(mask.astype(np.uint8), (ds, ds), interpolation=cv2.INTER_NEAREST).astype(bool)\n    ys = (np.linspace(0, H-1, ds)).astype(int)\n    xs = (np.linspace(0, W-1, ds)).astype(int)\n    return mask[np.ix_(ys, xs)]\n\ndef _count_cc_and_largest(mask_bool: np.ndarray, ds: int):\n    m = _maybe_downsample_mask(mask_bool, ds)\n    m = (m > 0).astype(np.uint8)\n    if m.sum() == 0:\n        return 0, 0\n    if _HAS_NDI:\n        lab, n = ndi.label(m)\n        if n <= 0:\n            return 0, 0\n        sizes = np.bincount(lab.ravel())\n        sizes[0] = 0\n        largest = int(sizes.max()) if sizes.size else 0\n        return int(n), int(largest)\n    if _HAS_CV2:\n        n, lab = cv2.connectedComponents(m, connectivity=8)\n        n_cc = int(max(n - 1, 0))\n        if n_cc <= 0:\n            return 0, 0\n        sizes = np.bincount(lab.ravel())\n        sizes[0] = 0\n        largest = int(sizes.max()) if sizes.size else 0\n        return int(n_cc), int(largest)\n    return -1, -1\n\ndef _extract_mask_prob_from_npz(npz_path: Path, thr: float):\n    try:\n        z = np.load(npz_path, allow_pickle=True)\n        keys = list(z.files)\n        arrs = {k: z[k] for k in keys}\n    except Exception:\n        return None, None\n\n    def _pick_first(keys_like):\n        for k in keys_like:\n            if k in arrs:\n                return k\n        return None\n\n    k_prob = _pick_first([\"prob\",\"probs\",\"pred_prob\",\"mask_prob\",\"p\",\"pred\"])\n    k_mask = _pick_first([\"mask\",\"masks\",\"bin_mask\",\"pred_mask\",\"mask_bin\"])\n\n    prob = None\n    mask = None\n    if k_prob is not None:\n        a = arrs[k_prob]\n        if isinstance(a, np.ndarray) and a.ndim >= 2:\n            prob = a.astype(np.float32)\n    if k_mask is not None:\n        a = arrs[k_mask]\n        if isinstance(a, np.ndarray):\n            if a.ndim == 3:\n                a = np.max(a, axis=0)\n            mask = (a > 0).astype(bool)\n\n    if mask is None and prob is None:\n        best_k, best_n = None, -1\n        for k, a in arrs.items():\n            if not isinstance(a, np.ndarray): continue\n            if a.ndim < 2: continue\n            n = int(np.prod(a.shape))\n            if n > best_n:\n                best_k, best_n = k, n\n        if best_k is not None:\n            a = arrs[best_k]\n            a2 = np.max(a, axis=0) if a.ndim == 3 else a\n            if np.issubdtype(a2.dtype, np.floating):\n                prob = a2.astype(np.float32)\n            else:\n                mask = (a2 > 0).astype(bool)\n\n    if mask is None and prob is not None:\n        m = prob\n        if m.ndim == 3:\n            m = np.max(m, axis=0)\n        mask = (m >= float(thr))\n\n    if prob is not None and prob.ndim == 3:\n        prob = np.max(prob, axis=0)\n    if mask is not None and mask.ndim == 3:\n        mask = np.max(mask, axis=0).astype(bool)\n\n    return mask, prob\n\ndef _mask_stats(mask: np.ndarray, prob: np.ndarray, ds_cc: int):\n    out = {}\n    if mask is None:\n        out.update({\n            \"npz_area_frac\": np.nan,\n            \"npz_n_cc\": np.nan,\n            \"npz_largest_cc_frac\": np.nan,\n            \"npz_mean_prob_inside\": np.nan,\n            \"npz_p90_prob_inside\": np.nan,\n            \"npz_max_prob\": np.nan,\n        })\n        return out\n\n    m = (mask > 0)\n    H, W = m.shape[:2]\n    tot = float(H * W) if H and W else 0.0\n    area = float(m.sum())\n    out[\"npz_area_frac\"] = (area / tot) if tot > 0 else 0.0\n\n    n_cc, largest = _count_cc_and_largest(m, ds_cc)\n    out[\"npz_n_cc\"] = float(n_cc) if n_cc >= 0 else np.nan\n    out[\"npz_largest_cc_frac\"] = (float(largest) / area) if area > 0 and largest >= 0 else np.nan\n\n    if prob is not None and prob.shape[:2] == m.shape[:2]:\n        p = prob.astype(np.float32)\n        inside = p[m]\n        if inside.size > 0:\n            out[\"npz_mean_prob_inside\"] = float(np.mean(inside))\n            out[\"npz_p90_prob_inside\"] = float(np.quantile(inside, 0.90))\n            out[\"npz_max_prob\"] = float(np.max(inside))\n        else:\n            out[\"npz_mean_prob_inside\"] = 0.0\n            out[\"npz_p90_prob_inside\"] = 0.0\n            out[\"npz_max_prob\"] = 0.0\n    else:\n        out[\"npz_mean_prob_inside\"] = np.nan\n        out[\"npz_p90_prob_inside\"] = np.nan\n        out[\"npz_max_prob\"] = np.nan\n    return out\n\ndef _build_npz_pair_features(uids: list, pred_dir: Path, match_dir: Path, cache_path: Path, thr: float, ds_cc: int):\n    if cache_path.exists():\n        try:\n            dfc = pd.read_parquet(cache_path)\n            dfc = _ensure_uid_case_variant(dfc)\n            return dfc\n        except Exception:\n            pass\n\n    rows = []\n    for i, uid in enumerate(uids, 1):\n        r = {\"uid\": str(uid)}\n\n        pred_p = _uid_to_npz_path(pred_dir, uid) if pred_dir is not None else None\n        match_p = _uid_to_npz_path(match_dir, uid) if match_dir is not None else None\n\n        mp = None; pp = None\n        if pred_p is not None and pred_p.exists():\n            mp, pp = _extract_mask_prob_from_npz(pred_p, thr)\n        pred_stats = _mask_stats(mp, pp, ds_cc)\n        for k, v in pred_stats.items():\n            r[f\"pred_{k}\"] = v\n\n        mm = None; pm = None\n        if match_p is not None and match_p.exists():\n            mm, pm = _extract_mask_prob_from_npz(match_p, thr)\n        match_stats = _mask_stats(mm, pm, ds_cc)\n        for k, v in match_stats.items():\n            r[f\"match_{k}\"] = v\n\n        if (mp is not None) and (mm is not None):\n            a = (mp > 0); b = (mm > 0)\n            inter = float(np.logical_and(a, b).sum())\n            union = float(np.logical_or(a, b).sum())\n            tot = float(a.size) if a.size else 0.0\n            r[\"pm_inter_frac\"] = (inter / tot) if tot > 0 else 0.0\n            r[\"pm_union_frac\"] = (union / tot) if tot > 0 else 0.0\n            r[\"pm_iou\"] = (inter / union) if union > 0 else 0.0\n            r[\"pm_pred_minus_match_frac\"] = ((float(a.sum()) - inter) / tot) if tot > 0 else 0.0\n            r[\"pm_match_minus_pred_frac\"] = ((float(b.sum()) - inter) / tot) if tot > 0 else 0.0\n        else:\n            r[\"pm_inter_frac\"] = np.nan\n            r[\"pm_union_frac\"] = np.nan\n            r[\"pm_iou\"] = np.nan\n            r[\"pm_pred_minus_match_frac\"] = np.nan\n            r[\"pm_match_minus_pred_frac\"] = np.nan\n\n        rows.append(r)\n\n        if (i % 500) == 0:\n            print(f\"  NPZ test progress: {i}/{len(uids)}\")\n\n    dfc = pd.DataFrame(rows)\n    try:\n        cache_path.parent.mkdir(parents=True, exist_ok=True)\n        dfc.to_parquet(cache_path, index=False)\n    except Exception:\n        pass\n    return dfc\n\n# locate npz dirs (primary cfg)\nPRED_CFG_DIR = Path(PATHS.get(\"PRED_CFG_DIR\",\"\")) if (\"PATHS\" in globals()) else Path(\"\")\nMATCH_CFG_DIR = Path(PATHS.get(\"MATCH_CFG_DIR\",\"\")) if (\"PATHS\" in globals()) else Path(\"\")\nPRED_NPZ_TEST_DIR = Path(PATHS.get(\"PRED_NPZ_TEST_DIR\", str(PRED_CFG_DIR / \"test\")))\nMATCH_NPZ_TEST_DIR = Path(PATHS.get(\"MATCH_NPZ_TEST_DIR\", str(MATCH_CFG_DIR / \"test\")))\n\nnpz_ready = bool(FE_CFG.get(\"use_npz_pair_features\", True)) and _dir_has_any_npz(PRED_NPZ_TEST_DIR) and _dir_has_any_npz(MATCH_NPZ_TEST_DIR)\nif npz_ready:\n    thr = float(FE_CFG.get(\"npz_bin_thr\", 0.5))\n    ds_cc = int(FE_CFG.get(\"npz_downsample_for_cc\", 256) or 0)\n\n    key = f\"npz_pair_test_{_slug(PRED_CFG_DIR.name)}__{_slug(MATCH_CFG_DIR.name)}__thr{thr}\"\n    h = hashlib.md5(key.encode()).hexdigest()[:12]\n    cache_path = OUT_DIR / f\"npz_pair_features_test_{h}.parquet\"\n\n    print(\"\\nBuilding NPZ pair features (TEST) ...\")\n    uids = df_out[\"uid\"].astype(str).tolist()\n    df_npz = _build_npz_pair_features(uids, PRED_NPZ_TEST_DIR, MATCH_NPZ_TEST_DIR, cache_path, thr, ds_cc)\n    df_npz = _ensure_uid_case_variant(df_npz)\n    keep_cols = [c for c in df_npz.columns if c != \"uid\"]\n    df_out = df_out.merge(df_npz[[\"uid\"] + keep_cols], on=\"uid\", how=\"left\")\nelse:\n    print(\"\\nNOTE: NPZ pair features TEST skipped (missing/empty dirs).\")\n    print(\"  PRED_NPZ_TEST_DIR :\", PRED_NPZ_TEST_DIR, \"(npz found)\" if _dir_has_any_npz(PRED_NPZ_TEST_DIR) else \"(empty/missing)\")\n    print(\"  MATCH_NPZ_TEST_DIR:\", MATCH_NPZ_TEST_DIR, \"(npz found)\" if _dir_has_any_npz(MATCH_NPZ_TEST_DIR) else \"(empty/missing)\")\n\n# ----------------------------\n# 7) Canonical aliases (supaya interactions stabil)\n# ----------------------------\n_coalesce_numeric(df_out, \"area_frac\", [\"area_frac\",\"pred_area_frac\",\"mask_area_frac\",\"pred_npz_area_frac\",\"pred_npz_area_frac\",\"pred_npz_area_frac\"])\n_coalesce_numeric(df_out, \"grid_area_frac\", [\"grid_area_frac\",\"grid_area\"])\n_coalesce_numeric(df_out, \"best_count\", [\"best_count\",\"pair_count\",\"n_pairs\",\"pairs\"])\n_coalesce_numeric(df_out, \"best_mean_sim\", [\"best_mean_sim\",\"mean_sim\",\"sim_mean\"])\n_coalesce_numeric(df_out, \"peak_ratio\", [\"peak_ratio\",\"peak_to_mean\",\"peak_over_mean\"])\n_coalesce_numeric(df_out, \"inlier_ratio\", [\"inlier_ratio\",\"ransac_inlier_ratio\",\"inliers_ratio\"])\n_coalesce_numeric(df_out, \"has_peak\", [\"has_peak\",\"peak_found\",\"has_mode\"])\n\n# NPZ-derived (pred)\n_coalesce_numeric(df_out, \"n_cc_pred\", [\"n_cc_pred\",\"pred_npz_n_cc\",\"pred_npz_n_cc\",\"pred_npz_n_cc\",\"pred_npz_n_cc\", \"pred_npz_n_cc\", \"pred_npz_n_cc\", \"pred_npz_n_cc\"])\n_coalesce_numeric(df_out, \"largest_cc_frac_pred\", [\"largest_cc_frac_pred\",\"pred_npz_largest_cc_frac\"])\n_coalesce_numeric(df_out, \"mean_prob_inside_pred\", [\"mean_prob_inside_pred\",\"pred_npz_mean_prob_inside\"])\n_coalesce_numeric(df_out, \"p90_prob_inside_pred\", [\"p90_prob_inside_pred\",\"pred_npz_p90_prob_inside\"])\n_coalesce_numeric(df_out, \"max_prob_pred\", [\"max_prob_pred\",\"pred_npz_max_prob\"])\n\n# ----------------------------\n# 8) Recompute engineered columns to match FEATURE_COLS\n#    (hanya buat kolom-kolom yang dibutuhkan)\n# ----------------------------\ndef _num(s):\n    return pd.to_numeric(s, errors=\"coerce\")\n\ndef safe_log1p_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.log1p(x)\n\ndef safe_sqrt_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.sqrt(x)\n\n# replace inf -> NaN (numeric)\nfor c in df_out.columns:\n    if pd.api.types.is_numeric_dtype(df_out[c]):\n        df_out[c] = df_out[c].replace([np.inf,-np.inf], np.nan)\n\n# missing indicators: pakai daftar dari schema agar kolom sama\nif missing_indicator_cols:\n    for ind in missing_indicator_cols:\n        # expected format: isna_<col>\n        if ind in df_out.columns:\n            continue\n        if ind.startswith(\"isna_\"):\n            base = ind[5:]\n            if base in df_out.columns:\n                if pd.api.types.is_numeric_dtype(df_out[base]):\n                    df_out[ind] = df_out[base].isna().astype(np.uint8)\n                else:\n                    df_out[ind] = 0\n            else:\n                df_out[ind] = 1  # benar-benar missing\nelse:\n    # fallback: create isna_* hanya jika dibutuhkan oleh FEATURE_COLS\n    for c in FEATURE_COLS:\n        if c.startswith(\"isna_\") and c not in df_out.columns:\n            base = c[5:]\n            if base in df_out.columns and pd.api.types.is_numeric_dtype(df_out[base]):\n                df_out[c] = df_out[base].isna().astype(np.uint8)\n            else:\n                df_out[c] = 1\n\n# clipped + cap + logabs/sqrtabs: gunakan clip_caps dari schema\n# hanya buat kalau kolom target ada di FEATURE_COLS\nfor c, cap in (clip_caps or {}).items():\n    try:\n        cap = float(cap)\n    except Exception:\n        continue\n\n    need_cap = (f\"{c}_cap\" in FEATURE_COLS)\n    need_log = (f\"logabs_{c}\" in FEATURE_COLS)\n    need_sqrt = (f\"sqrtabs_{c}\" in FEATURE_COLS)\n\n    if (not need_cap) and (not need_log) and (not need_sqrt):\n        continue\n    if c not in df_out.columns:\n        # kalau base tidak ada, isi 0\n        if need_cap:  df_out[f\"{c}_cap\"] = 0.0\n        if need_log:  df_out[f\"logabs_{c}\"] = 0.0\n        if need_sqrt: df_out[f\"sqrtabs_{c}\"] = 0.0\n        continue\n\n    x = _num(df_out[c]).fillna(0.0).astype(float).values\n    x = np.clip(x, -cap, cap)\n    if need_cap:\n        df_out[f\"{c}_cap\"] = x.astype(np.float32)\n    if need_log:\n        df_out[f\"logabs_{c}\"] = safe_log1p_nonneg(np.abs(x)).astype(np.float32)\n    if need_sqrt:\n        df_out[f\"sqrtabs_{c}\"] = safe_sqrt_nonneg(np.abs(x)).astype(np.float32)\n\n# interactions: hitung hanya kalau kolomnya diminta FEATURE_COLS\ndef _getf(col, default=0.0):\n    if col in df_out.columns:\n        return _num(df_out[col]).fillna(default).astype(float).values\n    return np.full(len(df_out), default, dtype=np.float64)\n\nbest_mean_sim = _getf(\"best_mean_sim\", 0.0)\nbest_count    = _getf(\"best_count\", 0.0)\npeak_ratio    = _getf(\"peak_ratio\", 0.0)\nhas_peak      = _getf(\"has_peak\", 0.0)\ngrid_area     = _getf(\"grid_area_frac\", 0.0)\narea_frac     = _getf(\"area_frac\", 0.0)\nn_pairs_thr   = _getf(\"n_pairs_thr\", 0.0)\nn_pairs_mnn   = _getf(\"n_pairs_mnn\", 0.0)\ninlier_ratio  = _getf(\"inlier_ratio\", 0.0)\ngh = _getf(\"grid_h\", 0.0)\ngw = _getf(\"grid_w\", 0.0)\ngridN = np.clip(gh * gw, 0.0, None)\n\ndef _set_if_needed(name, arr):\n    if name in FEATURE_COLS and name not in df_out.columns:\n        df_out[name] = arr.astype(np.float32)\n\n_set_if_needed(\"sim_x_count\",      best_mean_sim * best_count)\n_set_if_needed(\"peak_x_sim\",       peak_ratio * best_mean_sim)\n_set_if_needed(\"haspeak_x_sim\",    has_peak * best_mean_sim)\n_set_if_needed(\"area_x_sim\",       grid_area * best_mean_sim)\n_set_if_needed(\"area_x_count\",     grid_area * best_count)\n_set_if_needed(\"mask_grid_ratio\",  area_frac / (1e-6 + grid_area))\n_set_if_needed(\"mnn_ratio\",        n_pairs_mnn / (1.0 + n_pairs_thr))\n_set_if_needed(\"pairs_per_cell\",   n_pairs_thr / (1.0 + gridN))\n_set_if_needed(\"inlier_x_pairs\",   inlier_ratio * n_pairs_thr)\n_set_if_needed(\"log1p_pairs_thr\",  safe_log1p_nonneg(n_pairs_thr))\n_set_if_needed(\"log1p_best_count\", safe_log1p_nonneg(best_count))\n_set_if_needed(\"log1p_area_frac\",  safe_log1p_nonneg(np.clip(area_frac, 0, None)))\n\n# seg/gate stability extras (jika dibutuhkan)\nncc   = _getf(\"n_cc_pred\", 0.0)\nlccf  = _getf(\"largest_cc_frac_pred\", 0.0)\nmpin  = _getf(\"mean_prob_inside_pred\", 0.0)\np90in = _getf(\"p90_prob_inside_pred\", 0.0)\nmxp   = _getf(\"max_prob_pred\", 0.0)\n\n_set_if_needed(\"prob_mean_x_area\", mpin * area_frac)\n_set_if_needed(\"prob_p90_x_area\",  p90in * area_frac)\n_set_if_needed(\"prob_max_x_area\",  mxp * area_frac)\n_set_if_needed(\"comp_x_area\",      ncc * area_frac)\n_set_if_needed(\"largestcc_x_area\", lccf * area_frac)\n_set_if_needed(\"prob_contrast\",    p90in - mpin)\n\n# overlap extras\niou   = _getf(\"pm_iou\", 0.0)\ninter = _getf(\"pm_inter_frac\", 0.0)\nunion = _getf(\"pm_union_frac\", 0.0)\n_set_if_needed(\"iou_x_area\",       iou * area_frac)\n_set_if_needed(\"inter_over_union\", inter / (1e-6 + union))\n\n# variant one-hot: buat kolom sama persis seperti train\nif variant_dummy_cols:\n    cats = [c[len(\"v_\"):] for c in variant_dummy_cols if c.startswith(\"v_\")]\n    cats_set = set(cats)\n    v = df_out[\"variant\"].astype(str).fillna(\"unk\")\n    # map unseen -> rare jika ada\n    if \"rare\" in cats_set:\n        v2 = v.where(v.isin(cats_set), other=\"rare\")\n    else:\n        v2 = v.where(v.isin(cats_set), other=v)  # unseen => stay (kolomnya nanti 0 semua)\n    for col in variant_dummy_cols:\n        if col in df_out.columns:\n            continue\n        if col.startswith(\"v_\"):\n            cat = col[len(\"v_\"):]\n            df_out[col] = (v2 == cat).astype(np.uint8)\n\n# fill numeric NaN -> 0\nnum_cols = [c for c in df_out.columns if pd.api.types.is_numeric_dtype(df_out[c])]\ndf_out[num_cols] = df_out[num_cols].fillna(float(FE_CFG.get(\"fillna_value\", 0.0)))\n\n# ----------------------------\n# 9) Finalize: ensure all FEATURE_COLS exist + numeric float32\n# ----------------------------\nstill_missing = [c for c in FEATURE_COLS if c not in df_out.columns]\nif still_missing:\n    for c in still_missing:\n        df_out[c] = 0.0\n\nfor c in FEATURE_COLS:\n    df_out[c] = pd.to_numeric(df_out[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n\n# order final\ndf_final = df_out[id_cols + FEATURE_COLS].copy()\nprint(\"\\nFinal TEST feature table:\", df_final.shape)\n\n# ----------------------------\n# 10) Save pred_features_test*\n# ----------------------------\ncfg_hash = hashlib.sha1(json.dumps(FEATURE_COLS, ensure_ascii=False).encode(\"utf-8\")).hexdigest()[:12]\np_main = OUT_DIR / \"pred_features_test.csv\"\np_cfg  = OUT_DIR / f\"pred_features_test_cfg_{cfg_hash}.csv\"\n\ndf_final.to_csv(p_main, index=False)\ndf_final.to_csv(p_cfg, index=False)\n\nprint(\"\\nSaved:\")\nprint(\"  ->\", p_main)\nprint(\"  ->\", p_cfg)\n\n# Export global for next steps\ndf_test_tabular = df_final\nPRED_FEATURES_TEST_CSV = str(p_main)\nPRED_FEATURES_TEST_CFG_CSV = str(p_cfg)\nPRED_FEATURES_TEST_CFG_HASH = cfg_hash\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Baseline Model (Leakage-Safe CV)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 3 — Train Mask Model (UNet + ASPP) on DINOv2 Token-Grid Embeddings (Decoder-only)\n# REVISI FULL v3.1-mask (Leakage-Safe CV by fold/case_id, AMP+EMA+accum+earlystop)\n#\n# Target: UNet+ASPP untuk prediksi mask, decoder di atas token-grid embedding DINOv2.\n# - Input utama: token-grid (Htok, Wtok, D) per uid (DINOv2 patch tokens/grid).\n# - GT: mask full-res (opsional) -> downsample ke (Htok, Wtok) untuk training.\n#\n# REQUIRE (minimal):\n# - df_train_tabular: kolom wajib: uid, case_id, fold, y  (variant opsional)\n#   * y=0 -> mask dianggap kosong\n# - TOKEN CACHE: file embedding per uid (.npz/.npy) yang berisi array token grid\n#   * Auto-search beberapa direktori umum. Kalau tidak ketemu -> fallback zeros (warn sekali).\n# - TRAIN MASK DIR (opsional): kalau mask file ada (nama sama dengan uid.*)\n#\n# OUTPUT:\n# - /kaggle/working/recodai_luc_mask_artifacts/mask_folds_seed_<seed>/mask_fold_<fold>.pt\n# - /kaggle/working/recodai_luc_mask_artifacts/oof_mask_metrics.csv\n# - /kaggle/working/recodai_luc_mask_artifacts/final_mask_model.pt   (bundle: fold_packs + full_packs + recommended_thr)\n#\n# Export globals:\n# - FINAL_MASK_MODEL_PT (str)\n# - MASK_OOF_REPORT (dict)\n# ============================================================\n\nimport os, gc, json, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import roc_auc_score\n\n# ----------------------------\n# 0) Require df_train_tabular\n# ----------------------------\nneed_vars = [\"df_train_tabular\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Butuh df_train_tabular (uid/case_id/fold/y).\")\n\ndf_train_tabular = df_train_tabular.copy()\n\nrequired_cols = {\"uid\", \"case_id\", \"fold\", \"y\"}\nmissing_cols = [c for c in required_cols if c not in df_train_tabular.columns]\nif missing_cols:\n    raise ValueError(f\"df_train_tabular missing columns: {missing_cols}\")\n\ndf_train_tabular[\"uid\"] = df_train_tabular[\"uid\"].astype(str)\ndf_train_tabular[\"case_id\"] = df_train_tabular[\"case_id\"].astype(str)\ndf_train_tabular[\"fold\"] = df_train_tabular[\"fold\"].astype(int)\ndf_train_tabular[\"y\"] = df_train_tabular[\"y\"].astype(int)\n\nuids = df_train_tabular[\"uid\"].to_numpy()\ny_img = df_train_tabular[\"y\"].to_numpy(np.int64)\nfolds = df_train_tabular[\"fold\"].to_numpy(np.int64)\nunique_folds = sorted(df_train_tabular[\"fold\"].unique().tolist())\n\nprint(\"Step3-mask setup:\")\nprint(\"  rows :\", len(df_train_tabular))\nprint(\"  folds:\", len(unique_folds), unique_folds)\nprint(\"  forged%:\", float(y_img.mean())*100.0)\n\n# ----------------------------\n# 1) Device + CFG (AUTO)\n# ----------------------------\nSEED0 = 2025\n\ndef seed_everything(seed=2025):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\n\ndef get_mem_gb():\n    if device.type != \"cuda\":\n        return 0.0\n    try:\n        return float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n    except Exception:\n        return 0.0\n\nMEM_GB = get_mem_gb()\nseed_everything(SEED0)\n\nprint(\"Device:\", device, \"| AMP:\", use_amp, \"| VRAM(GB):\", MEM_GB)\n\n# decoder-only; token dim D akan di-infer dari cache\nCFG_CPU = dict(\n    seed=SEED0,\n    epochs=35,\n    batch_size=16,\n    accum_steps=1,\n    lr=3e-4,\n    weight_decay=1e-2,\n    warmup_frac=0.05,\n    grad_clip=1.0,\n    early_patience=8,\n    early_min_delta=1e-4,\n    use_ema=True,\n    ema_decay=0.999,\n\n    # model\n    base_ch=128,\n    drop=0.10,\n    aspp_rates=(1, 2, 4, 6),\n\n    # loss\n    bce_weight=1.0,\n    dice_weight=1.0,\n    focal_gamma=0.0,           # 0=off\n\n    # aug on token-grid (flip only, safe)\n    aug_hflip=0.5,\n    aug_vflip=0.2,\n\n    # threshold search (dice on OOF)\n    thr_grid=81,\n)\n\nCFG_GPU = dict(CFG_CPU)\nCFG_GPU.update(\n    epochs=50,\n    batch_size=32 if MEM_GB >= 16 else 24,\n    accum_steps=1 if MEM_GB >= 16 else 2,\n    lr=3e-4,\n    base_ch=160 if MEM_GB >= 16 else 128,\n    drop=0.10,\n)\n\nCFG_STRONG = dict(CFG_GPU)\nCFG_STRONG.update(\n    epochs=65,\n    batch_size=48 if MEM_GB >= 30 else CFG_GPU[\"batch_size\"],\n    base_ch=192 if MEM_GB >= 30 else CFG_GPU[\"base_ch\"],\n    lr=2.5e-4,\n)\n\nCFG = dict(CFG_GPU if device.type == \"cuda\" else CFG_CPU)\nCFG_NAME = \"GPU\" if device.type == \"cuda\" else \"CPU\"\nif device.type == \"cuda\" and MEM_GB >= 30:\n    CFG = dict(CFG_STRONG)\n    CFG_NAME = \"STRONG\"\n\n# ENV overrides (opsional)\nif os.environ.get(\"MASK_EPOCHS\",\"\").strip():\n    CFG[\"epochs\"] = int(os.environ[\"MASK_EPOCHS\"])\nif os.environ.get(\"MASK_BS\",\"\").strip():\n    CFG[\"batch_size\"] = int(os.environ[\"MASK_BS\"])\nif os.environ.get(\"MASK_LR\",\"\").strip():\n    CFG[\"lr\"] = float(os.environ[\"MASK_LR\"])\nif os.environ.get(\"MASK_ACCUM\",\"\").strip():\n    CFG[\"accum_steps\"] = int(os.environ[\"MASK_ACCUM\"])\n\nprint(\"CFG:\", CFG_NAME, json.dumps({k: CFG[k] for k in [\"epochs\",\"batch_size\",\"accum_steps\",\"lr\",\"base_ch\"]}, indent=2))\n\n# ----------------------------\n# 2) Directories: token cache + mask dir (auto-detect)\n# ----------------------------\ndef _first_existing(paths):\n    for p in paths:\n        if p is None: \n            continue\n        p = Path(p)\n        if p.exists():\n            return p\n    return None\n\n# mask dir candidates\nMASK_DIR = None\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    MASK_DIR = _first_existing([PATHS.get(\"TRAIN_MASK_DIR\"), PATHS.get(\"MASK_DIR\"), PATHS.get(\"TRAIN_MASKS\")])\n\nif MASK_DIR is None:\n    # common competition layouts\n    MASK_DIR = _first_existing([\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\",\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/masks_train\",\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_mask\",\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/masks\",\n        \"/kaggle/working/recodai_luc/train_masks\",\n    ])\n\nprint(\"MASK_DIR:\", str(MASK_DIR) if MASK_DIR else \"(None / will assume empty mask for y=0 and missing files)\")\n\n# token cache candidates\nTOKEN_CACHE_DIR = None\nif \"CACHE_ROOT\" in globals():\n    try:\n        cr = Path(CACHE_ROOT)\n        if cr.exists():\n            TOKEN_CACHE_DIR = cr\n    except Exception:\n        pass\n\nif TOKEN_CACHE_DIR is None:\n    # try typical cache roots\n    base_candidates = [\n        Path(\"/kaggle/working/recodai_luc/cache\"),\n        Path(\"/kaggle/working/recodai_luc/cache/dino_v2\"),\n        Path(\"/kaggle/input/recod-ailuc-dinov2-train/recodai_luc/cache\"),\n        Path(\"/kaggle/input/recod-ailuc-dinov2-train/recodai_luc/cache/dino_v2\"),\n    ]\n    for b in base_candidates:\n        if not b.exists():\n            continue\n        # pick latest cfg_* if exists\n        cfg_dirs = sorted([p for p in b.glob(\"cfg_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n        if cfg_dirs:\n            TOKEN_CACHE_DIR = cfg_dirs[0]\n            break\n        # otherwise use b directly\n        TOKEN_CACHE_DIR = b\n        break\n\nprint(\"TOKEN_CACHE_DIR:\", str(TOKEN_CACHE_DIR) if TOKEN_CACHE_DIR else \"(None)\")\n\n# ----------------------------\n# 3) Token loader (robust) + mask loader\n# ----------------------------\n_WARN_TOKEN_MISS = False\n_WARN_MASK_MISS = False\n\ndef _load_np_any(p: Path):\n    p = Path(p)\n    if p.suffix.lower() == \".npy\":\n        return np.load(p, allow_pickle=False)\n    if p.suffix.lower() == \".npz\":\n        z = np.load(p, allow_pickle=False)\n        # pick array with ndim 2 or 3\n        keys = list(z.keys())\n        best = None\n        for k in keys:\n            a = z[k]\n            if isinstance(a, np.ndarray) and a.ndim in (2,3):\n                best = a\n                break\n        if best is None and len(keys):\n            best = z[keys[0]]\n        return best\n    raise ValueError(f\"Unsupported token file: {p}\")\n\ndef _reshape_to_grid(a: np.ndarray):\n    # expected grid: (H,W,D) or (D,H,W) or (N,D)\n    a = np.asarray(a)\n    if a.ndim == 3:\n        # maybe (D,H,W)\n        if a.shape[0] in [384, 512, 768, 1024, 1536] and (a.shape[1] * a.shape[2] > 16):\n            # assume (D,H,W)\n            D,H,W = a.shape\n            return np.transpose(a, (1,2,0))  # -> (H,W,D)\n        # assume (H,W,D)\n        return a\n    if a.ndim == 2:\n        N,D = a.shape\n        s = int(round(math.sqrt(N)))\n        if s*s == N:\n            return a.reshape(s, s, D)\n        # fallback: try factorization\n        for h in range(1, int(math.sqrt(N))+1):\n            if N % h == 0:\n                w = N // h\n                if h >= 4 and w >= 4:\n                    return a.reshape(h, w, D)\n        # last resort: treat as 1xN\n        return a.reshape(1, N, D)\n    raise ValueError(f\"Token array has unsupported shape: {a.shape}\")\n\ndef find_token_file(uid: str):\n    if TOKEN_CACHE_DIR is None:\n        return None\n    uid = str(uid)\n    # common patterns\n    cand = [\n        TOKEN_CACHE_DIR / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"tokens_train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"tokens_train\" / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"train\" / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"feat_train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"feat_train\" / f\"{uid}.npy\",\n    ]\n    for p in cand:\n        if p.exists():\n            return p\n    # limited glob fallback (avoid super expensive recursive)\n    try:\n        hits = list(TOKEN_CACHE_DIR.glob(f\"**/*{uid}*.npz\"))\n        if hits:\n            return hits[0]\n        hits = list(TOKEN_CACHE_DIR.glob(f\"**/*{uid}*.npy\"))\n        if hits:\n            return hits[0]\n    except Exception:\n        pass\n    return None\n\ndef load_token_grid(uid: str):\n    global _WARN_TOKEN_MISS\n    p = find_token_file(uid)\n    if p is None:\n        if not _WARN_TOKEN_MISS:\n            print(\"[WARN] Token file tidak ketemu untuk sebagian uid. Fallback -> zeros (sekali warn).\")\n            _WARN_TOKEN_MISS = True\n        return None\n    try:\n        a = _load_np_any(p)\n        g = _reshape_to_grid(a).astype(np.float32, copy=False)  # (H,W,D)\n        # clean nan/inf\n        if not np.isfinite(g).all():\n            g = np.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0)\n        return g\n    except Exception as e:\n        if not _WARN_TOKEN_MISS:\n            print(\"[WARN] Gagal baca token file, fallback zeros. err:\", repr(e))\n            _WARN_TOKEN_MISS = True\n        return None\n\ndef find_mask_file(uid: str):\n    if MASK_DIR is None:\n        return None\n    uid = str(uid)\n    # try common extensions\n    exts = [\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\"]\n    for ex in exts:\n        p = Path(MASK_DIR) / f\"{uid}{ex}\"\n        if p.exists():\n            return p\n    # glob fallback\n    try:\n        hits = list(Path(MASK_DIR).glob(f\"{uid}.*\"))\n        if hits:\n            return hits[0]\n    except Exception:\n        pass\n    return None\n\ndef load_mask_full(uid: str):\n    global _WARN_MASK_MISS\n    p = find_mask_file(uid)\n    if p is None:\n        if not _WARN_MASK_MISS and MASK_DIR is not None:\n            print(\"[WARN] Sebagian mask file tidak ditemukan. Untuk missing -> mask kosong (sekali warn).\")\n            _WARN_MASK_MISS = True\n        return None\n    try:\n        im = Image.open(p)\n        im = im.convert(\"L\")\n        m = np.array(im, dtype=np.uint8)\n        # binarize (umum: 0/255)\n        m = (m > 127).astype(np.uint8)\n        return m\n    except Exception as e:\n        if not _WARN_MASK_MISS:\n            print(\"[WARN] Gagal baca mask file, fallback kosong. err:\", repr(e))\n            _WARN_MASK_MISS = True\n        return None\n\ndef downsample_mask_to_grid(mask_full: np.ndarray, H: int, W: int):\n    # area-like downsample (PIL BILINEAR then threshold) -> cukup stabil untuk token grid\n    if mask_full is None:\n        return np.zeros((H, W), dtype=np.float32)\n    im = Image.fromarray((mask_full.astype(np.uint8) * 255))\n    im = im.resize((W, H), resample=Image.BILINEAR)\n    m = np.array(im, dtype=np.float32) / 255.0\n    # keep as soft mask (0..1) for loss\n    return m.astype(np.float32)\n\n# infer token_dim from first available sample\nTOKEN_DIM = None\nTOKEN_HW_EX = None\nfor uid in uids[:min(80, len(uids))]:\n    g = load_token_grid(uid)\n    if g is not None and g.ndim == 3 and g.shape[0] >= 4 and g.shape[1] >= 4:\n        TOKEN_DIM = int(g.shape[2])\n        TOKEN_HW_EX = (int(g.shape[0]), int(g.shape[1]))\n        break\n\nif TOKEN_DIM is None:\n    raise RuntimeError(\n        \"Tidak menemukan token-grid embedding di TOKEN_CACHE_DIR. \"\n        \"Pastikan cache DINOv2 token-grid per uid tersedia (npz/npy).\"\n    )\n\nprint(\"Token example grid:\", TOKEN_HW_EX, \"| token_dim:\", TOKEN_DIM)\n\n# ----------------------------\n# 4) Dataset (token-grid -> mask-grid)\n# ----------------------------\nclass TokenMaskDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, is_train: bool, cfg: dict):\n        self.df = df.reset_index(drop=True)\n        self.is_train = bool(is_train)\n        self.cfg = cfg\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        uid = str(row[\"uid\"])\n        yflag = int(row[\"y\"])\n\n        g = load_token_grid(uid)  # (H,W,D) or None\n        if g is None:\n            # fallback zeros with token example H,W\n            H, W = TOKEN_HW_EX\n            g = np.zeros((H, W, TOKEN_DIM), dtype=np.float32)\n        H, W, D = g.shape\n\n        # mask: if y=0 -> empty; else load if available (if missing -> empty)\n        mask_full = None\n        if yflag == 1:\n            mask_full = load_mask_full(uid)\n        m = downsample_mask_to_grid(mask_full, H, W)  # (H,W) float 0..1\n\n        # aug (token-grid safe flips)\n        if self.is_train:\n            if self.cfg.get(\"aug_hflip\", 0) > 0 and np.random.rand() < float(self.cfg[\"aug_hflip\"]):\n                g = g[:, ::-1, :].copy()\n                m = m[:, ::-1].copy()\n            if self.cfg.get(\"aug_vflip\", 0) > 0 and np.random.rand() < float(self.cfg[\"aug_vflip\"]):\n                g = g[::-1, :, :].copy()\n                m = m[::-1, :].copy()\n\n        # to tensor\n        x = torch.from_numpy(np.transpose(g, (2, 0, 1)).astype(np.float32))  # (D,H,W)\n        y = torch.from_numpy(m[None, ...].astype(np.float32))                # (1,H,W)\n        return x, y, uid\n\n# ----------------------------\n# 5) Model: UNet + ASPP (decoder-only on token-grid)\n# ----------------------------\nclass ConvGNAct(nn.Module):\n    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, drop=0.0, groups=8):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)\n        g = min(int(groups), int(out_ch))\n        g = max(1, g)\n        self.gn = nn.GroupNorm(g, out_ch)\n        self.act = nn.SiLU(inplace=True)\n        self.drop = nn.Dropout2d(float(drop)) if float(drop) > 0 else nn.Identity()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.gn(x)\n        x = self.act(x)\n        x = self.drop(x)\n        return x\n\nclass ASPP(nn.Module):\n    def __init__(self, ch, rates=(1,2,4,6), drop=0.0):\n        super().__init__()\n        rs = list(rates)\n        self.branches = nn.ModuleList()\n        for r in rs:\n            self.branches.append(\n                nn.Sequential(\n                    nn.Conv2d(ch, ch, 3, padding=int(r), dilation=int(r), bias=False),\n                    nn.GroupNorm(min(8, ch), ch),\n                    nn.SiLU(inplace=True),\n                )\n            )\n        self.proj = nn.Sequential(\n            nn.Conv2d(ch * len(rs), ch, 1, bias=False),\n            nn.GroupNorm(min(8, ch), ch),\n            nn.SiLU(inplace=True),\n            nn.Dropout2d(float(drop)) if float(drop) > 0 else nn.Identity()\n        )\n\n    def forward(self, x):\n        xs = [b(x) for b in self.branches]\n        x = torch.cat(xs, dim=1)\n        return self.proj(x)\n\nclass UNetASPP(nn.Module):\n    def __init__(self, in_dim, base_ch=128, drop=0.10, aspp_rates=(1,2,4,6)):\n        super().__init__()\n        C = int(base_ch)\n\n        # light encoder on token-grid\n        self.stem = nn.Sequential(\n            ConvGNAct(in_dim, C, k=1, s=1, p=0, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n        self.enc1 = nn.Sequential(\n            ConvGNAct(C, C,   k=3, s=1, p=1, drop=drop),\n            ConvGNAct(C, C,   k=3, s=1, p=1, drop=drop),\n        )\n        self.down1 = ConvGNAct(C, 2*C, k=3, s=2, p=1, drop=drop)\n        self.enc2 = nn.Sequential(\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down2 = ConvGNAct(2*C, 4*C, k=3, s=2, p=1, drop=drop)\n        self.enc3 = nn.Sequential(\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down3 = ConvGNAct(4*C, 6*C, k=3, s=2, p=1, drop=drop)\n\n        self.bottleneck = nn.Sequential(\n            ConvGNAct(6*C, 6*C, k=3, s=1, p=1, drop=drop),\n            ASPP(6*C, rates=aspp_rates, drop=drop),\n            ConvGNAct(6*C, 6*C, k=3, s=1, p=1, drop=drop),\n        )\n\n        # decoder\n        self.dec3 = nn.Sequential(\n            ConvGNAct(6*C + 4*C, 4*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.dec2 = nn.Sequential(\n            ConvGNAct(4*C + 2*C, 2*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.dec1 = nn.Sequential(\n            ConvGNAct(2*C + C, C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n\n        self.head = nn.Conv2d(C, 1, kernel_size=1)\n\n    def forward(self, x):\n        # x: (B, D, H, W)\n        x0 = self.stem(x)     # (B,C,H,W)\n        s1 = self.enc1(x0)    # (B,C,H,W)\n\n        x1 = self.down1(s1)   # (B,2C,H/2,W/2)\n        s2 = self.enc2(x1)\n\n        x2 = self.down2(s2)   # (B,4C,H/4,W/4)\n        s3 = self.enc3(x2)\n\n        x3 = self.down3(s3)   # (B,6C,H/8,W/8)\n        b  = self.bottleneck(x3)\n\n        # up to s3\n        u3 = F.interpolate(b, size=s3.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d3 = self.dec3(torch.cat([u3, s3], dim=1))\n\n        # up to s2\n        u2 = F.interpolate(d3, size=s2.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d2 = self.dec2(torch.cat([u2, s2], dim=1))\n\n        # up to s1\n        u1 = F.interpolate(d2, size=s1.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d1 = self.dec1(torch.cat([u1, s1], dim=1))\n\n        logit = self.head(d1)  # (B,1,H,W) token-grid resolution\n        return logit\n\n# ----------------------------\n# 6) EMA\n# ----------------------------\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[n] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for n, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[n].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for n, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[n] = p.detach().clone()\n            p.copy_(self.shadow[n])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for n, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[n])\n        self.backup = {}\n\n# ----------------------------\n# 7) Loss + metrics\n# ----------------------------\ndef dice_score(prob, target, eps=1e-6):\n    # prob/target: (B,1,H,W) float\n    prob = prob.float()\n    target = target.float()\n    num = (prob * target).sum(dim=(2,3)) * 2.0\n    den = (prob + target).sum(dim=(2,3)).clamp_min(eps)\n    d = (num / den).mean()\n    return d\n\ndef dice_loss_from_logits(logits, target, eps=1e-6):\n    prob = torch.sigmoid(logits)\n    return 1.0 - dice_score(prob, target, eps=eps)\n\ndef focal_bce_with_logits(logits, targets, gamma=0.0):\n    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n    if gamma and gamma > 0:\n        p = torch.sigmoid(logits)\n        p_t = p * targets + (1.0 - p) * (1.0 - targets)\n        mod = (1.0 - p_t).clamp_min(0.0).pow(float(gamma))\n        bce = bce * mod\n    return bce.mean()\n\n@torch.no_grad()\ndef eval_loader(model, loader, ema=None, thr_list=None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    tot_loss = 0.0\n    tot_dice = 0.0\n    n_batches = 0\n\n    # threshold sweep (dice@thr)\n    thr_list = thr_list if thr_list is not None else [0.5]\n    thr_list = [float(t) for t in thr_list]\n    dice_thr = np.zeros(len(thr_list), dtype=np.float64)\n    cnt_thr = 0\n\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            logits = model(xb)\n            loss = focal_bce_with_logits(logits, yb, gamma=float(CFG.get(\"focal_gamma\", 0.0))) * float(CFG[\"bce_weight\"]) \\\n                 + dice_loss_from_logits(logits, yb) * float(CFG[\"dice_weight\"])\n\n        tot_loss += float(loss.item())\n        prob = torch.sigmoid(logits)\n        tot_dice += float(dice_score(prob, yb).item())\n        n_batches += 1\n\n        # sweep on CPU-friendly boolean\n        prob_np = prob.detach().float().cpu().numpy()\n        y_np = yb.detach().float().cpu().numpy()\n        for j, t in enumerate(thr_list):\n            pr = (prob_np >= t).astype(np.float32)\n            # dice (batch mean)\n            inter = (pr * y_np).sum(axis=(2,3)) * 2.0\n            den = (pr + y_np).sum(axis=(2,3)) + 1e-6\n            dice_thr[j] += float((inter / den).mean())\n        cnt_thr += 1\n\n    if ema is not None:\n        ema.restore(model)\n\n    out = {\n        \"val_loss\": tot_loss / max(1, n_batches),\n        \"val_dice_prob\": tot_dice / max(1, n_batches),\n        \"thr_list\": thr_list,\n        \"val_dice_thr\": (dice_thr / max(1, cnt_thr)).tolist(),\n    }\n    return out\n\n# ----------------------------\n# 8) Train one fold\n# ----------------------------\ndef train_one_fold(df_tr, df_va, cfg, seed):\n    seed_everything(seed)\n\n    ds_tr = TokenMaskDataset(df_tr, is_train=True, cfg=cfg)\n    ds_va = TokenMaskDataset(df_va, is_train=False, cfg=cfg)\n\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n\n    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n\n    model = UNetASPP(in_dim=TOKEN_DIM, base_ch=int(cfg[\"base_ch\"]), drop=float(cfg[\"drop\"]), aspp_rates=tuple(cfg[\"aspp_rates\"])).to(device)\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(0.9, 0.99),\n        eps=1e-8,\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n    total_steps = int(cfg[\"epochs\"]) * max(1, steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        # cosine decay after warmup\n        t = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        t = min(max(t, 0.0), 1.0)\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n\n    sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n\n    best = {\"val_loss\": 1e18, \"val_dice_prob\": -1.0, \"epoch\": -1}\n    best_state = None\n    bad = 0\n    opt_step = 0\n\n    thr_list = np.linspace(0.05, 0.95, int(cfg[\"thr_grid\"])).tolist()\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum = 0.0\n        n_sum = 0\n        micro = 0\n        t0 = time.time()\n\n        for xb, yb, _ in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = focal_bce_with_logits(logits, yb, gamma=float(cfg.get(\"focal_gamma\", 0.0))) * float(cfg[\"bce_weight\"]) \\\n                     + dice_loss_from_logits(logits, yb) * float(cfg[\"dice_weight\"])\n                loss = loss / accum_steps\n\n            scaler.scale(loss).backward()\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro % accum_steps) == 0:\n                if float(cfg.get(\"grad_clip\", 0.0)) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                opt_step += 1\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last\n        if (micro % accum_steps) != 0:\n            if float(cfg.get(\"grad_clip\", 0.0)) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            opt_step += 1\n            if ema is not None:\n                ema.update(model)\n\n        # validate (EMA)\n        ev = eval_loader(model, dl_va, ema=ema, thr_list=thr_list)\n        vloss = float(ev[\"val_loss\"])\n        vdice = float(ev[\"val_dice_prob\"])\n\n        dt = time.time() - t0\n        print(f\"  epoch {epoch+1:03d}/{cfg['epochs']} | tr_loss={loss_sum/max(1,n_sum):.5f} | val_loss={vloss:.5f} | val_dice(sigmoid)={vdice:.5f} | opt_step={opt_step} | dt={dt:.1f}s\")\n\n        improved = (best[\"val_loss\"] - vloss) > float(cfg[\"early_min_delta\"])\n        if improved:\n            best[\"val_loss\"] = vloss\n            best[\"val_dice_prob\"] = vdice\n            best[\"epoch\"] = int(epoch)\n\n            # save EMA-weighted state (since eval uses EMA)\n            if ema is not None:\n                ema.apply_shadow(model)\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n                ema.restore(model)\n            else:\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n\n            bad = 0\n        else:\n            bad += 1\n            if bad >= int(cfg[\"early_patience\"]):\n                print(f\"  early stop at epoch {epoch+1}, best_epoch={best['epoch']+1}, best_val_loss={best['val_loss']:.5f}\")\n                break\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    # load best\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    # final val sweep for best thr (dice on hard mask)\n    ev = eval_loader(model, dl_va, ema=None, thr_list=thr_list)\n    dice_thr = np.array(ev[\"val_dice_thr\"], dtype=np.float64)\n    j = int(np.argmax(dice_thr))\n    best_thr = float(ev[\"thr_list\"][j])\n    best_dice_hard = float(dice_thr[j])\n\n    pack = {\n        \"arch\": \"UNetASPP_on_DINOv2TokenGrid_v3.1\",\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"cfg\": dict(cfg),\n        \"token_dim\": int(TOKEN_DIM),\n        \"best_epoch\": int(best[\"epoch\"] + 1),\n        \"best_val_loss\": float(best[\"val_loss\"]),\n        \"best_val_dice_prob\": float(best[\"val_dice_prob\"]),\n        \"best_thr\": float(best_thr),\n        \"best_val_dice_hard\": float(best_dice_hard),\n    }\n    return pack\n\n# ----------------------------\n# 9) Multi-seed CV + save fold models + bundle\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_mask_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# seed plan (mirip pola kamu)\nN_SEEDS = 2 if (device.type == \"cuda\" and MEM_GB >= 30) else 1\nif os.environ.get(\"MASK_NSEEDS\",\"\").strip():\n    N_SEEDS = int(os.environ[\"MASK_NSEEDS\"])\nSEEDS = [int(CFG[\"seed\"]) + i*17 for i in range(max(1, N_SEEDS))]\nprint(\"SEED plan:\", SEEDS)\n\nfold_packs_all = []\nfold_rows = []\nbest_epochs_all = []\n\nfor si, seed in enumerate(SEEDS, 1):\n    print(\"\\n==============================\")\n    print(f\"== SEED {seed} ({si}/{len(SEEDS)})\")\n    print(\"==============================\")\n\n    models_dir = OUT_DIR / f\"mask_folds_seed_{seed}\"\n    models_dir.mkdir(parents=True, exist_ok=True)\n\n    for f in unique_folds:\n        print(f\"\\n[Seed {seed} | Fold {f}]\")\n\n        df_tr = df_train_tabular[df_train_tabular[\"fold\"] != f].reset_index(drop=True)\n        df_va = df_train_tabular[df_train_tabular[\"fold\"] == f].reset_index(drop=True)\n\n        pack = train_one_fold(df_tr, df_va, CFG, seed=int(seed) + int(f)*101)\n        pack[\"seed\"] = int(seed)\n        pack[\"fold\"] = int(f)\n\n        # save fold pt\n        pt_path = models_dir / f\"mask_fold_{f}.pt\"\n        torch.save({\"pack\": pack}, pt_path)\n\n        fold_packs_all.append(pack)\n        best_epochs_all.append(int(pack[\"best_epoch\"]))\n\n        fold_rows.append({\n            \"seed\": int(seed),\n            \"fold\": int(f),\n            \"best_epoch\": int(pack[\"best_epoch\"]),\n            \"best_val_loss\": float(pack[\"best_val_loss\"]),\n            \"best_val_dice_prob\": float(pack[\"best_val_dice_prob\"]),\n            \"best_thr\": float(pack[\"best_thr\"]),\n            \"best_val_dice_hard\": float(pack[\"best_val_dice_hard\"]),\n            \"pt_path\": str(pt_path),\n        })\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\ndf_fold = pd.DataFrame(fold_rows).sort_values([\"seed\",\"fold\"]).reset_index(drop=True)\ndisplay(df_fold)\ndf_fold.to_csv(OUT_DIR / \"oof_mask_metrics.csv\", index=False)\n\n# recommended_thr: median best_thr across all (seed,fold)\nrecommended_thr = float(np.median(df_fold[\"best_thr\"].to_numpy(dtype=np.float64)))\nprint(\"\\nRecommended thr (median of fold best_thr):\", recommended_thr)\n\n# ----------------------------\n# 10) Train FULL model(s) per seed (epochs = median best_epoch * 1.15, capped)\n# ----------------------------\ndef train_full(df_full, cfg, seed, epochs_full):\n    seed_everything(seed)\n\n    ds = TokenMaskDataset(df_full, is_train=True, cfg=cfg)\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n\n    dl = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                    num_workers=nw, pin_memory=pin, drop_last=False,\n                    persistent_workers=(nw > 0))\n\n    model = UNetASPP(in_dim=TOKEN_DIM, base_ch=int(cfg[\"base_ch\"]), drop=float(cfg[\"drop\"]), aspp_rates=tuple(cfg[\"aspp_rates\"])).to(device)\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(0.9, 0.99),\n        eps=1e-8,\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl) / accum_steps))\n    total_steps = int(epochs_full) * max(1, steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        t = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        t = min(max(t, 0.0), 1.0)\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n\n    sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n\n    print(f\"\\nTraining FULL mask model | seed={seed} | epochs={epochs_full}\")\n\n    for ep in range(int(epochs_full)):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n        loss_sum, n_sum, micro = 0.0, 0, 0\n\n        for xb, yb, _ in dl:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = focal_bce_with_logits(logits, yb, gamma=float(cfg.get(\"focal_gamma\", 0.0))) * float(cfg[\"bce_weight\"]) \\\n                     + dice_loss_from_logits(logits, yb) * float(cfg[\"dice_weight\"])\n                loss = loss / accum_steps\n\n            scaler.scale(loss).backward()\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro % accum_steps) == 0:\n                if float(cfg.get(\"grad_clip\", 0.0)) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        if (micro % accum_steps) != 0:\n            if float(cfg.get(\"grad_clip\", 0.0)) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        print(f\"  full epoch {ep+1:03d}/{epochs_full} | loss={loss_sum/max(1,n_sum):.5f}\")\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    used_ema = bool(ema is not None)\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    full_pack = {\n        \"arch\": \"UNetASPP_on_DINOv2TokenGrid_v3.1\",\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"cfg\": dict(cfg),\n        \"token_dim\": int(TOKEN_DIM),\n        \"epochs_full\": int(epochs_full),\n        \"seed\": int(seed),\n        \"used_ema\": bool(used_ema),\n    }\n\n    if ema is not None:\n        ema.restore(model)\n\n    return full_pack\n\nflat_best = np.array(best_epochs_all, dtype=np.int32) if len(best_epochs_all) else np.array([max(10, CFG[\"epochs\"]//2)], dtype=np.int32)\nmed_best = int(np.median(flat_best))\nepochs_full = int(max(10, round(med_best * 1.15)))\nepochs_full = int(min(epochs_full, int(CFG[\"epochs\"])))  # cap\n\nfull_packs = []\nfor seed in SEEDS:\n    full_packs.append(train_full(df_train_tabular, CFG, seed=int(seed), epochs_full=int(epochs_full)))\n\n# ----------------------------\n# 11) Save final bundle (compatible style)\n# ----------------------------\nfinal_bundle = {\n    \"type\": \"mask_unet_aspp_decoder_on_dinov2_tokengrid_v3.1\",\n    \"cfg_name\": CFG_NAME,\n    \"cfg\": CFG,\n    \"seed_plan\": SEEDS,\n    \"token_dim\": int(TOKEN_DIM),\n    \"token_grid_example\": TOKEN_HW_EX,\n    \"fold_packs\": fold_packs_all,   # list: seed x fold\n    \"full_packs\": full_packs,       # list: per seed\n    \"recommended_thr\": float(recommended_thr),\n    \"fold_metrics_csv\": str(OUT_DIR / \"oof_mask_metrics.csv\"),\n    \"notes\": \"Decoder-only UNet+ASPP trained on DINOv2 token-grid embeddings; GT mask downsampled to token grid.\",\n}\n\nFINAL_MASK_MODEL_PT = str(OUT_DIR / \"final_mask_model.pt\")\ntorch.save(final_bundle, FINAL_MASK_MODEL_PT)\n\nwith open(OUT_DIR / \"final_mask_bundle.json\", \"w\") as f:\n    json.dump({\n        \"final_mask_model_pt\": FINAL_MASK_MODEL_PT,\n        \"cfg_name\": CFG_NAME,\n        \"seeds\": SEEDS,\n        \"n_fold_packs\": int(len(fold_packs_all)),\n        \"n_full_packs\": int(len(full_packs)),\n        \"recommended_thr\": float(recommended_thr),\n        \"token_dim\": int(TOKEN_DIM),\n        \"epochs_full\": int(epochs_full),\n    }, f, indent=2)\n\nMASK_OOF_REPORT = {\n    \"cfg_name\": CFG_NAME,\n    \"seeds\": SEEDS,\n    \"recommended_thr\": float(recommended_thr),\n    \"fold_metrics_path\": str(OUT_DIR / \"oof_mask_metrics.csv\"),\n    \"epochs_full\": int(epochs_full),\n}\n\nprint(\"\\nSaved artifacts:\")\nprint(\"  fold models   ->\", OUT_DIR, \"(mask_folds_seed_*/mask_fold_*.pt)\")\nprint(\"  fold metrics  ->\", OUT_DIR / \"oof_mask_metrics.csv\")\nprint(\"  final bundle  ->\", FINAL_MASK_MODEL_PT)\nprint(\"  meta json     ->\", OUT_DIR / \"final_mask_bundle.json\")\n\nglobals().update({\n    \"FINAL_MASK_MODEL_PT\": FINAL_MASK_MODEL_PT,\n    \"MASK_OOF_REPORT\": MASK_OOF_REPORT,\n})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optimize Model & Hyperparameters (Iterative)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 4 — Optimize Mask Model & Hyperparameters (Iterative) — UNet+ASPP on DINOv2 Token-Grid\n# REVISI FULL v4.0 (2-stage search, resume-safe, AMP+EMA+accum, robust token-grid resize)\n#\n# Primary score: OOF best Dice (hard mask) over threshold grid\n#\n# Output:\n# - /kaggle/working/recodai_luc_mask_artifacts/opt_search/stage1_results.csv\n# - /kaggle/working/recodai_luc_mask_artifacts/opt_search/opt_results.csv\n# - /kaggle/working/recodai_luc_mask_artifacts/opt_search/opt_results.json\n# - /kaggle/working/recodai_luc_mask_artifacts/opt_search/opt_fold_details.csv\n# - /kaggle/working/recodai_luc_mask_artifacts/opt_search/oof_scalars_<cfg_name>.csv (top configs)\n# - /kaggle/working/recodai_luc_mask_artifacts/best_mask_config.json\n# - /kaggle/working/recodai_luc_mask_artifacts/best_mask_model.pt\n#\n# REQUIRE:\n# - df_train_tabular with columns: uid, case_id, fold, y   (variant optional)\n# - TOKEN_CACHE_DIR contains per-uid token-grid npy/npz (H,W,D) or (N,D) reshapeable\n# - MASK_DIR (optional but recommended) contains per-uid mask images (png/jpg/tif)\n# ============================================================\n\nimport os, gc, json, math, time, warnings\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom IPython.display import display\n\n# ----------------------------\n# 0) Require data\n# ----------------------------\nneed_vars = [\"df_train_tabular\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Butuh df_train_tabular (uid/case_id/fold/y).\")\n\ndf_train_tabular = df_train_tabular.copy()\nreq = {\"uid\",\"case_id\",\"fold\",\"y\"}\nmiss = [c for c in req if c not in df_train_tabular.columns]\nif miss:\n    raise ValueError(f\"df_train_tabular missing columns: {miss}\")\n\ndf_train_tabular[\"uid\"] = df_train_tabular[\"uid\"].astype(str)\ndf_train_tabular[\"case_id\"] = df_train_tabular[\"case_id\"].astype(str)\ndf_train_tabular[\"fold\"] = df_train_tabular[\"fold\"].astype(int)\ndf_train_tabular[\"y\"] = df_train_tabular[\"y\"].astype(int)\n\nuids_all  = df_train_tabular[\"uid\"].to_numpy()\ny_img_all = df_train_tabular[\"y\"].to_numpy(np.int64)\nfolds_all = df_train_tabular[\"fold\"].to_numpy(np.int64)\nunique_folds = sorted(df_train_tabular[\"fold\"].unique().tolist())\n\nprint(\"Optimize setup (MASK UNet+ASPP on token-grid):\")\nprint(f\"  rows={len(df_train_tabular)} | folds={len(unique_folds)} | forged%={float(y_img_all.mean())*100:.2f}\")\n\n# ----------------------------\n# 1) Global settings\n# ----------------------------\nSEED = 2025\nTHR_GRID = 81\n\n# 2-stage runtime controls\nSTAGE1_FOLDS = min(3, len(unique_folds))\nSTAGE1_EPOCH_CAP = 30\nSTAGE1_PAT_CAP = 6\n\nSTAGE2_TOPM = 3\nREPORT_TOPK_OOF = 3\n\nTIME_BUDGET_SEC = 0  # 0=off\n\ndef seed_everything(seed=2025):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\nprint(\"Device:\", device, \"| AMP:\", use_amp)\n\ndef get_mem_gb():\n    if device.type != \"cuda\":\n        return 0.0\n    try:\n        return float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n    except Exception:\n        return 0.0\n\nMEM_GB = get_mem_gb()\nprint(\"GPU mem GB:\", MEM_GB)\n\nif device.type == \"cuda\":\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\n# ----------------------------\n# 2) Auto-detect dirs (TOKEN_CACHE_DIR + MASK_DIR)\n# ----------------------------\ndef _first_existing(paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(p)\n        if p.exists():\n            return p\n    return None\n\n# MASK_DIR\nMASK_DIR = None\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    MASK_DIR = _first_existing([PATHS.get(\"TRAIN_MASK_DIR\"), PATHS.get(\"MASK_DIR\"), PATHS.get(\"TRAIN_MASKS\")])\nif MASK_DIR is None:\n    MASK_DIR = _first_existing([\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\",\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/masks_train\",\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/masks\",\n        \"/kaggle/working/recodai_luc/train_masks\",\n    ])\nprint(\"MASK_DIR:\", str(MASK_DIR) if MASK_DIR else \"(None)\")\n\n# TOKEN_CACHE_DIR\nTOKEN_CACHE_DIR = None\nif \"CACHE_ROOT\" in globals():\n    try:\n        cr = Path(CACHE_ROOT)\n        if cr.exists():\n            TOKEN_CACHE_DIR = cr\n    except Exception:\n        pass\n\nif TOKEN_CACHE_DIR is None:\n    base_candidates = [\n        Path(\"/kaggle/working/recodai_luc/cache/dino_v2\"),\n        Path(\"/kaggle/working/recodai_luc/cache\"),\n        Path(\"/kaggle/input/recod-ailuc-dinov2-train/recodai_luc/cache/dino_v2\"),\n        Path(\"/kaggle/input/recod-ailuc-dinov2-train/recodai_luc/cache\"),\n    ]\n    for b in base_candidates:\n        if not b.exists():\n            continue\n        cfg_dirs = sorted([p for p in b.glob(\"cfg_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n        if cfg_dirs:\n            TOKEN_CACHE_DIR = cfg_dirs[0]\n            break\n        TOKEN_CACHE_DIR = b\n        break\n\nprint(\"TOKEN_CACHE_DIR:\", str(TOKEN_CACHE_DIR) if TOKEN_CACHE_DIR else \"(None)\")\nif TOKEN_CACHE_DIR is None:\n    raise RuntimeError(\"TOKEN_CACHE_DIR tidak ditemukan. Pastikan cache token-grid DINOv2 per uid tersedia.\")\n\n# ----------------------------\n# 3) Fast path maps (token/mask) + token shape inference\n# ----------------------------\ndef _load_np_any(p: Path):\n    p = Path(p)\n    if p.suffix.lower() == \".npy\":\n        return np.load(p, allow_pickle=False)\n    if p.suffix.lower() == \".npz\":\n        z = np.load(p, allow_pickle=False)\n        keys = list(z.keys())\n        for k in keys:\n            a = z[k]\n            if isinstance(a, np.ndarray) and a.ndim in (2,3):\n                return a\n        return z[keys[0]] if keys else None\n    return None\n\ndef _reshape_to_grid(a: np.ndarray):\n    a = np.asarray(a)\n    if a.ndim == 3:\n        # (D,H,W) -> (H,W,D)\n        if a.shape[0] in [384, 512, 768, 1024, 1536] and (a.shape[1] * a.shape[2] > 16):\n            D,H,W = a.shape\n            return np.transpose(a, (1,2,0))\n        return a  # assume (H,W,D)\n    if a.ndim == 2:\n        N,D = a.shape\n        s = int(round(math.sqrt(N)))\n        if s*s == N:\n            return a.reshape(s,s,D)\n        for h in range(1, int(math.sqrt(N))+1):\n            if N % h == 0:\n                w = N // h\n                if h >= 4 and w >= 4:\n                    return a.reshape(h,w,D)\n        return a.reshape(1,N,D)\n    return None\n\ndef find_token_file_fast(uid: str):\n    uid = str(uid)\n    cand = [\n        TOKEN_CACHE_DIR / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"tokens_train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"tokens_train\" / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"train\" / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"feat_train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"feat_train\" / f\"{uid}.npy\",\n    ]\n    for p in cand:\n        if p.exists():\n            return p\n    return None\n\ndef find_mask_file_fast(uid: str):\n    if MASK_DIR is None:\n        return None\n    uid = str(uid)\n    exts = [\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\"]\n    for ex in exts:\n        p = Path(MASK_DIR) / f\"{uid}{ex}\"\n        if p.exists():\n            return p\n    # fallback single glob (not recursive)\n    hits = list(Path(MASK_DIR).glob(f\"{uid}.*\"))\n    return hits[0] if hits else None\n\n# Build maps (fast, no recursive glob)\nTOKEN_PATH = {}\nMASK_PATH = {}\n\ntoken_hw_counter = Counter()\ntoken_dim_counter = Counter()\n\nmax_probe = min(300, len(uids_all))\nfor uid in uids_all[:max_probe]:\n    tp = find_token_file_fast(uid)\n    TOKEN_PATH[uid] = tp\n    if tp is None:\n        continue\n    a = _load_np_any(tp)\n    if a is None:\n        continue\n    g = _reshape_to_grid(a)\n    if g is None or g.ndim != 3:\n        continue\n    H,W,D = int(g.shape[0]), int(g.shape[1]), int(g.shape[2])\n    if H >= 4 and W >= 4 and D >= 16:\n        token_hw_counter[(H,W)] += 1\n        token_dim_counter[D] += 1\n\n# fill token path map for all uids (fast)\nfor uid in uids_all[max_probe:]:\n    TOKEN_PATH[uid] = find_token_file_fast(uid)\n\nif MASK_DIR is not None:\n    for uid in uids_all[:max_probe]:\n        MASK_PATH[uid] = find_mask_file_fast(uid)\n    for uid in uids_all[max_probe:]:\n        MASK_PATH[uid] = find_mask_file_fast(uid)\n\nif not token_hw_counter or not token_dim_counter:\n    raise RuntimeError(\"Tidak bisa infer token grid shape/dim dari token cache. Cek format file token-grid per uid.\")\n\nTOKEN_HW = token_hw_counter.most_common(1)[0][0]\nTOKEN_DIM = token_dim_counter.most_common(1)[0][0]\n\nprint(\"Token grid mode:\", TOKEN_HW, \"| token_dim mode:\", TOKEN_DIM)\n\n# ----------------------------\n# 4) Utilities: load/resize tokens + load/resize mask\n# ----------------------------\ndef load_token_grid_from_path(p: Path):\n    a = _load_np_any(p)\n    g = _reshape_to_grid(a)\n    if g is None:\n        return None\n    g = g.astype(np.float32, copy=False)\n    if not np.isfinite(g).all():\n        g = np.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0)\n    return g  # (H,W,D)\n\ndef resize_grid_hw(g: np.ndarray, out_hw):\n    # g: (H,W,D) -> resize H,W with bilinear over spatial\n    Ht, Wt = int(out_hw[0]), int(out_hw[1])\n    if g.shape[0] == Ht and g.shape[1] == Wt:\n        return g\n    x = torch.from_numpy(np.transpose(g, (2,0,1))).unsqueeze(0)  # (1,D,H,W)\n    x = F.interpolate(x, size=(Ht,Wt), mode=\"bilinear\", align_corners=False)\n    out = x.squeeze(0).permute(1,2,0).contiguous().cpu().numpy()\n    return out.astype(np.float32, copy=False)\n\ndef load_mask_bin(path: Path):\n    im = Image.open(path).convert(\"L\")\n    m = (np.array(im, dtype=np.uint8) > 127).astype(np.uint8)\n    return m\n\ndef resize_mask_soft(mask_bin: np.ndarray, out_hw):\n    # (Hfull,Wfull)->(Ht,Wt) soft 0..1\n    Ht,Wt = int(out_hw[0]), int(out_hw[1])\n    im = Image.fromarray(mask_bin.astype(np.uint8)*255)\n    im = im.resize((Wt,Ht), resample=Image.BILINEAR)\n    m = (np.array(im, dtype=np.float32) / 255.0).astype(np.float32)\n    return m\n\n# ----------------------------\n# 5) Dataset (with optional dropping missing pos masks)\n# ----------------------------\nDROP_MISSING_POS_MASKS = True  # recommended\n\ndef build_df_filtered(df: pd.DataFrame):\n    df = df.copy()\n    tok_exists = df[\"uid\"].map(lambda u: TOKEN_PATH.get(str(u), None) is not None).astype(int)\n    df[\"token_exists\"] = tok_exists\n\n    if MASK_DIR is not None:\n        m_exists = df[\"uid\"].map(lambda u: MASK_PATH.get(str(u), None) is not None).astype(int)\n    else:\n        m_exists = 0\n    df[\"mask_exists\"] = m_exists\n\n    # drop missing tokens (training useless)\n    df = df[df[\"token_exists\"] == 1].reset_index(drop=True)\n\n    if DROP_MISSING_POS_MASKS and MASK_DIR is not None:\n        # keep all negatives; keep positives only if mask exists\n        df = df[(df[\"y\"] == 0) | (df[\"mask_exists\"] == 1)].reset_index(drop=True)\n\n    return df\n\ndf_train_filtered = build_df_filtered(df_train_tabular)\nprint(\"After filtering:\")\nprint(\"  rows:\", len(df_train_filtered),\n      \"| forged%:\", float(df_train_filtered[\"y\"].mean())*100.0,\n      \"| token missing dropped:\", int((df_train_tabular.shape[0] - df_train_filtered.shape[0])))\n\nclass TokenMaskDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, is_train: bool, cfg: dict):\n        self.df = df.reset_index(drop=True)\n        self.is_train = bool(is_train)\n        self.cfg = cfg\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        uid = str(row[\"uid\"])\n        yflag = int(row[\"y\"])\n\n        tp = TOKEN_PATH.get(uid, None)\n        g = load_token_grid_from_path(tp) if tp is not None else None\n        if g is None:\n            g = np.zeros((TOKEN_HW[0], TOKEN_HW[1], TOKEN_DIM), dtype=np.float32)\n        if g.shape[2] != TOKEN_DIM:\n            # if dim mismatch, pad/crop to TOKEN_DIM\n            D = g.shape[2]\n            if D > TOKEN_DIM:\n                g = g[:, :, :TOKEN_DIM]\n            else:\n                pad = np.zeros((g.shape[0], g.shape[1], TOKEN_DIM - D), dtype=np.float32)\n                g = np.concatenate([g, pad], axis=2)\n        g = resize_grid_hw(g, TOKEN_HW)\n\n        # mask\n        m_soft = np.zeros((TOKEN_HW[0], TOKEN_HW[1]), dtype=np.float32)\n        if yflag == 1 and MASK_DIR is not None:\n            mp = MASK_PATH.get(uid, None)\n            if mp is not None:\n                m_bin = load_mask_bin(mp)\n                m_soft = resize_mask_soft(m_bin, TOKEN_HW)\n\n        # aug: flips + token noise\n        if self.is_train:\n            if float(self.cfg.get(\"aug_hflip\", 0.0)) > 0 and np.random.rand() < float(self.cfg[\"aug_hflip\"]):\n                g = g[:, ::-1, :].copy()\n                m_soft = m_soft[:, ::-1].copy()\n            if float(self.cfg.get(\"aug_vflip\", 0.0)) > 0 and np.random.rand() < float(self.cfg[\"aug_vflip\"]):\n                g = g[::-1, :, :].copy()\n                m_soft = m_soft[::-1, :].copy()\n\n            ns = float(self.cfg.get(\"input_noise_std\", 0.0))\n            if ns > 0:\n                g = g + np.random.randn(*g.shape).astype(np.float32) * ns\n\n        x = torch.from_numpy(np.transpose(g, (2,0,1)).astype(np.float32))   # (D,H,W)\n        y = torch.from_numpy(m_soft[None, ...].astype(np.float32))          # (1,H,W)\n        return x, y, uid\n\n# ----------------------------\n# 6) Model: UNet + ASPP\n# ----------------------------\nclass ConvGNAct(nn.Module):\n    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, drop=0.0, groups=8):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)\n        g = max(1, min(int(groups), int(out_ch)))\n        self.gn = nn.GroupNorm(g, out_ch)\n        self.act = nn.SiLU(inplace=True)\n        self.drop = nn.Dropout2d(float(drop)) if float(drop) > 0 else nn.Identity()\n\n    def forward(self, x):\n        return self.drop(self.act(self.gn(self.conv(x))))\n\nclass ASPP(nn.Module):\n    def __init__(self, ch, rates=(1,2,4,6), drop=0.0):\n        super().__init__()\n        rs = list(rates)\n        self.branches = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(ch, ch, 3, padding=int(r), dilation=int(r), bias=False),\n                nn.GroupNorm(max(1, min(8, ch)), ch),\n                nn.SiLU(inplace=True),\n            ) for r in rs\n        ])\n        self.proj = nn.Sequential(\n            nn.Conv2d(ch * len(rs), ch, 1, bias=False),\n            nn.GroupNorm(max(1, min(8, ch)), ch),\n            nn.SiLU(inplace=True),\n            nn.Dropout2d(float(drop)) if float(drop) > 0 else nn.Identity()\n        )\n\n    def forward(self, x):\n        xs = [b(x) for b in self.branches]\n        return self.proj(torch.cat(xs, dim=1))\n\nclass UNetASPP(nn.Module):\n    def __init__(self, in_dim, base_ch=160, drop=0.10, aspp_rates=(1,2,4,6)):\n        super().__init__()\n        C = int(base_ch)\n\n        self.stem = nn.Sequential(\n            ConvGNAct(in_dim, C, k=1, s=1, p=0, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n        self.enc1 = nn.Sequential(\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down1 = ConvGNAct(C, 2*C, k=3, s=2, p=1, drop=drop)\n        self.enc2 = nn.Sequential(\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down2 = ConvGNAct(2*C, 4*C, k=3, s=2, p=1, drop=drop)\n        self.enc3 = nn.Sequential(\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down3 = ConvGNAct(4*C, 6*C, k=3, s=2, p=1, drop=drop)\n\n        self.bottleneck = nn.Sequential(\n            ConvGNAct(6*C, 6*C, k=3, s=1, p=1, drop=drop),\n            ASPP(6*C, rates=aspp_rates, drop=drop),\n            ConvGNAct(6*C, 6*C, k=3, s=1, p=1, drop=drop),\n        )\n\n        self.dec3 = nn.Sequential(\n            ConvGNAct(6*C + 4*C, 4*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.dec2 = nn.Sequential(\n            ConvGNAct(4*C + 2*C, 2*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.dec1 = nn.Sequential(\n            ConvGNAct(2*C + C, C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n\n        self.head = nn.Conv2d(C, 1, 1)\n\n    def forward(self, x):\n        x0 = self.stem(x)\n        s1 = self.enc1(x0)\n\n        x1 = self.down1(s1)\n        s2 = self.enc2(x1)\n\n        x2 = self.down2(s2)\n        s3 = self.enc3(x2)\n\n        x3 = self.down3(s3)\n        b  = self.bottleneck(x3)\n\n        u3 = F.interpolate(b,  size=s3.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d3 = self.dec3(torch.cat([u3, s3], dim=1))\n\n        u2 = F.interpolate(d3, size=s2.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d2 = self.dec2(torch.cat([u2, s2], dim=1))\n\n        u1 = F.interpolate(d2, size=s1.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d1 = self.dec1(torch.cat([u1, s1], dim=1))\n\n        return self.head(d1)  # (B,1,H,W)\n\n# ----------------------------\n# 7) EMA + loss/metrics\n# ----------------------------\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n        self.backup = {}\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for n,p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[n].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for n,p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[n] = p.detach().clone()\n            p.copy_(self.shadow[n])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for n,p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[n])\n        self.backup = {}\n\ndef dice_from_probs(prob, target, eps=1e-6):\n    # prob/target: (B,1,H,W)\n    num = (prob * target).sum(dim=(2,3)) * 2.0\n    den = (prob + target).sum(dim=(2,3)).clamp_min(eps)\n    return (num / den).mean()\n\ndef dice_hard_np(prob_np, target_np, thr):\n    pr = (prob_np >= thr).astype(np.float32)\n    inter = (pr * target_np).sum(axis=(2,3)) * 2.0\n    den = (pr + target_np).sum(axis=(2,3)) + 1e-6\n    return float((inter / den).mean())\n\ndef loss_bce_dice(logits, target, bce_w=1.0, dice_w=1.0, focal_gamma=0.0):\n    bce = F.binary_cross_entropy_with_logits(logits, target, reduction=\"none\")\n    if focal_gamma and focal_gamma > 0:\n        p = torch.sigmoid(logits)\n        p_t = p * target + (1.0 - p) * (1.0 - target)\n        bce = bce * (1.0 - p_t).clamp_min(0.0).pow(float(focal_gamma))\n    bce = bce.mean()\n\n    prob = torch.sigmoid(logits)\n    dsc = dice_from_probs(prob, target)\n    dloss = 1.0 - dsc\n    return float(bce_w)*bce + float(dice_w)*dloss\n\n@torch.no_grad()\ndef eval_loader(model, loader, ema=None, thr_grid=81):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    tot_loss = 0.0\n    tot_dice_soft = 0.0\n    nb = 0\n\n    thrs = np.linspace(0.05, 0.95, int(thr_grid), dtype=np.float64)\n    dice_sweep = np.zeros_like(thrs, dtype=np.float64)\n    cnt = 0\n\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n\n        if use_amp:\n            with torch.cuda.amp.autocast(enabled=True):\n                logits = model(xb)\n                loss = loss_bce_dice(\n                    logits, yb,\n                    bce_w=float(CFG_EVAL[\"bce_weight\"]),\n                    dice_w=float(CFG_EVAL[\"dice_weight\"]),\n                    focal_gamma=float(CFG_EVAL.get(\"focal_gamma\", 0.0)),\n                )\n        else:\n            logits = model(xb)\n            loss = loss_bce_dice(\n                logits, yb,\n                bce_w=float(CFG_EVAL[\"bce_weight\"]),\n                dice_w=float(CFG_EVAL[\"dice_weight\"]),\n                focal_gamma=float(CFG_EVAL.get(\"focal_gamma\", 0.0)),\n            )\n\n        prob = torch.sigmoid(logits)\n        dsoft = dice_from_probs(prob, yb)\n\n        tot_loss += float(loss.item() if hasattr(loss, \"item\") else loss)\n        tot_dice_soft += float(dsoft.item())\n        nb += 1\n\n        prob_np = prob.detach().float().cpu().numpy()\n        y_np = yb.detach().float().cpu().numpy()\n        for i, t in enumerate(thrs):\n            dice_sweep[i] += dice_hard_np(prob_np, y_np, thr=float(t))\n        cnt += 1\n\n    if ema is not None:\n        ema.restore(model)\n\n    dice_sweep = dice_sweep / max(1, cnt)\n    j = int(np.argmax(dice_sweep))\n    return {\n        \"val_loss\": tot_loss / max(1, nb),\n        \"val_dice_soft\": tot_dice_soft / max(1, nb),\n        \"best_thr\": float(thrs[j]),\n        \"best_dice_hard\": float(dice_sweep[j]),\n    }\n\n# ----------------------------\n# 8) Train one fold (mask)\n# ----------------------------\ndef train_one_fold_mask(df_tr, df_va, cfg):\n    seed_everything(int(cfg[\"seed\"]))\n\n    ds_tr = TokenMaskDataset(df_tr, is_train=True, cfg=cfg)\n    ds_va = TokenMaskDataset(df_va, is_train=False, cfg=cfg)\n\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n\n    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n\n    model = UNetASPP(\n        in_dim=int(TOKEN_DIM),\n        base_ch=int(cfg[\"base_ch\"]),\n        drop=float(cfg[\"drop\"]),\n        aspp_rates=tuple(cfg[\"aspp_rates\"]),\n    ).to(device)\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(0.9, 0.99),\n        eps=1e-8,\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n    total_steps = int(cfg[\"epochs\"]) * max(1, steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        t = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        t = min(max(t, 0.0), 1.0)\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n\n    sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp) if use_amp else None\n    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n\n    best = {\"val_loss\": 1e18, \"epoch\": -1, \"best_thr\": 0.5, \"best_dice_hard\": -1.0, \"val_dice_soft\": -1.0}\n    best_state = None\n    bad = 0\n    opt_step = 0\n\n    global CFG_EVAL\n    CFG_EVAL = cfg  # for eval_loader access\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum, n_sum, micro = 0.0, 0, 0\n        t0 = time.time()\n\n        for xb, yb, _ in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            if use_amp:\n                with torch.cuda.amp.autocast(enabled=True):\n                    logits = model(xb)\n                    loss = loss_bce_dice(\n                        logits, yb,\n                        bce_w=float(cfg[\"bce_weight\"]),\n                        dice_w=float(cfg[\"dice_weight\"]),\n                        focal_gamma=float(cfg.get(\"focal_gamma\", 0.0)),\n                    ) / accum_steps\n                scaler.scale(loss).backward()\n            else:\n                logits = model(xb)\n                loss = loss_bce_dice(\n                    logits, yb,\n                    bce_w=float(cfg[\"bce_weight\"]),\n                    dice_w=float(cfg[\"dice_weight\"]),\n                    focal_gamma=float(cfg.get(\"focal_gamma\", 0.0)),\n                ) / accum_steps\n                loss.backward()\n\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro % accum_steps) == 0:\n                if float(cfg[\"grad_clip\"]) > 0:\n                    if use_amp:\n                        scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n                if use_amp:\n                    scaler.step(opt); scaler.update()\n                else:\n                    opt.step()\n\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                opt_step += 1\n                if ema is not None:\n                    ema.update(model)\n\n        if (micro % accum_steps) != 0:\n            if float(cfg[\"grad_clip\"]) > 0:\n                if use_amp:\n                    scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n            if use_amp:\n                scaler.step(opt); scaler.update()\n            else:\n                opt.step()\n\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            opt_step += 1\n            if ema is not None:\n                ema.update(model)\n\n        ev = eval_loader(model, dl_va, ema=ema, thr_grid=int(cfg[\"thr_grid\"]))\n        dt = time.time() - t0\n        print(f\"  ep {epoch+1:03d}/{cfg['epochs']} | tr_loss={loss_sum/max(1,n_sum):.5f} | \"\n              f\"val_loss={ev['val_loss']:.5f} | val_dice_soft={ev['val_dice_soft']:.5f} | \"\n              f\"best_dice_hard={ev['best_dice_hard']:.5f}@{ev['best_thr']:.2f} | opt_step={opt_step} | dt={dt:.1f}s\")\n\n        improved = (best[\"val_loss\"] - float(ev[\"val_loss\"])) > float(cfg[\"min_delta\"])\n        if improved:\n            best[\"val_loss\"] = float(ev[\"val_loss\"])\n            best[\"val_dice_soft\"] = float(ev[\"val_dice_soft\"])\n            best[\"best_thr\"] = float(ev[\"best_thr\"])\n            best[\"best_dice_hard\"] = float(ev[\"best_dice_hard\"])\n            best[\"epoch\"] = int(epoch)\n\n            if ema is not None:\n                ema.apply_shadow(model)\n                best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n                ema.restore(model)\n            else:\n                best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= int(cfg[\"patience\"]):\n                print(f\"  early stop: best_epoch={best['epoch']+1}, best_val_loss={best['val_loss']:.5f}\")\n                break\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    pack = {\n        \"arch\": \"UNetASPP_on_DINOv2TokenGrid_opt_v4.0\",\n        \"state_dict\": {k: v.detach().cpu() for k,v in model.state_dict().items()},\n        \"cfg\": dict(cfg),\n        \"token_dim\": int(TOKEN_DIM),\n        \"token_hw\": tuple(map(int, TOKEN_HW)),\n        \"best_epoch\": int(best[\"epoch\"] + 1),\n        \"best_val_loss\": float(best[\"val_loss\"]),\n        \"best_val_dice_soft\": float(best[\"val_dice_soft\"]),\n        \"best_thr\": float(best[\"best_thr\"]),\n        \"best_val_dice_hard\": float(best[\"best_dice_hard\"]),\n    }\n    return pack\n\n# ----------------------------\n# 9) CV evaluator for a config (store only scalars OOF)\n# ----------------------------\n@torch.no_grad()\ndef infer_val_scalars(model, loader, thr=0.5):\n    model.eval()\n    area_frac = []\n    mean_prob = []\n    uids = []\n    for xb, _, uidb in loader:\n        xb = xb.to(device, non_blocking=True)\n        if use_amp:\n            with torch.cuda.amp.autocast(enabled=True):\n                logits = model(xb)\n                prob = torch.sigmoid(logits)\n        else:\n            logits = model(xb)\n            prob = torch.sigmoid(logits)\n\n        pr = (prob >= float(thr)).float()\n        af = pr.mean(dim=(1,2,3)).detach().cpu().numpy()  # (B,)\n        mp = prob.mean(dim=(1,2,3)).detach().cpu().numpy()\n        area_frac.append(af)\n        mean_prob.append(mp)\n        uids.extend(list(uidb))\n    area_frac = np.concatenate(area_frac, axis=0).astype(np.float32)\n    mean_prob = np.concatenate(mean_prob, axis=0).astype(np.float32)\n    return uids, area_frac, mean_prob\n\ndef run_cv_config(cfg, cfg_name, folds_subset=None):\n    use_folds = unique_folds if folds_subset is None else list(folds_subset)\n\n    fold_rows = []\n    fold_packs = []\n    # store per-uid scalars (not full masks)\n    oof_area = {uid: np.nan for uid in df_train_filtered[\"uid\"].tolist()}\n    oof_mean = {uid: np.nan for uid in df_train_filtered[\"uid\"].tolist()}\n\n    for f in use_folds:\n        df_tr = df_train_filtered[df_train_filtered[\"fold\"] != f].reset_index(drop=True)\n        df_va = df_train_filtered[df_train_filtered[\"fold\"] == f].reset_index(drop=True)\n\n        # unique seed per fold for stability\n        cfg_fold = dict(cfg)\n        cfg_fold[\"seed\"] = int(cfg.get(\"seed\", SEED)) + int(f)*101\n\n        print(f\"    -> fold {f} | tr={len(df_tr)} va={len(df_va)} pos_va={int(df_va['y'].sum())}\")\n\n        pack = train_one_fold_mask(df_tr, df_va, cfg_fold)\n\n        # val scalar inference (use model from pack)\n        cpu_cnt = os.cpu_count() or 2\n        nw = 2 if cpu_cnt >= 4 else 0\n        pin = (device.type == \"cuda\")\n        ds_va = TokenMaskDataset(df_va, is_train=False, cfg=cfg_fold)\n        dl_va = DataLoader(ds_va, batch_size=int(cfg_fold[\"batch_size\"]), shuffle=False,\n                           num_workers=nw, pin_memory=pin, drop_last=False,\n                           persistent_workers=(nw > 0))\n\n        model = UNetASPP(in_dim=int(TOKEN_DIM), base_ch=int(cfg_fold[\"base_ch\"]),\n                         drop=float(cfg_fold[\"drop\"]), aspp_rates=tuple(cfg_fold[\"aspp_rates\"])).to(device)\n        model.load_state_dict(pack[\"state_dict\"], strict=True)\n\n        ulist, area_frac, mean_prob = infer_val_scalars(model, dl_va, thr=float(pack[\"best_thr\"]))\n\n        for u, af, mp in zip(ulist, area_frac, mean_prob):\n            oof_area[str(u)] = float(af)\n            oof_mean[str(u)] = float(mp)\n\n        fold_rows.append({\n            \"cfg\": cfg_name,\n            \"fold\": int(f),\n            \"best_epoch\": int(pack[\"best_epoch\"]),\n            \"best_val_loss\": float(pack[\"best_val_loss\"]),\n            \"best_val_dice_soft\": float(pack[\"best_val_dice_soft\"]),\n            \"best_thr\": float(pack[\"best_thr\"]),\n            \"best_val_dice_hard\": float(pack[\"best_val_dice_hard\"]),\n        })\n\n        pack2 = dict(pack)\n        pack2[\"fold\"] = int(f)\n        fold_packs.append(pack2)\n\n        del model\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    df_f = pd.DataFrame(fold_rows)\n    # aggregate score: mean of best_val_dice_hard across folds (works for stage1 subset too)\n    score = float(df_f[\"best_val_dice_hard\"].mean()) if len(df_f) else 0.0\n    thr_med = float(np.median(df_f[\"best_thr\"].to_numpy(dtype=np.float64))) if len(df_f) else 0.5\n\n    summary = {\n        \"cfg\": cfg_name,\n        \"stage\": \"full\" if folds_subset is None else f\"subset{len(use_folds)}\",\n        \"score_mean_best_dice_hard\": score,\n        \"thr_median\": thr_med,\n\n        \"base_ch\": int(cfg[\"base_ch\"]),\n        \"drop\": float(cfg[\"drop\"]),\n        \"aspp_rates\": str(tuple(cfg[\"aspp_rates\"])),\n\n        \"epochs\": int(cfg[\"epochs\"]),\n        \"batch_size\": int(cfg[\"batch_size\"]),\n        \"accum_steps\": int(cfg.get(\"accum_steps\", 1)),\n        \"lr\": float(cfg[\"lr\"]),\n        \"weight_decay\": float(cfg[\"weight_decay\"]),\n        \"warmup_frac\": float(cfg[\"warmup_frac\"]),\n        \"grad_clip\": float(cfg[\"grad_clip\"]),\n        \"patience\": int(cfg[\"patience\"]),\n        \"min_delta\": float(cfg[\"min_delta\"]),\n        \"use_ema\": bool(cfg.get(\"use_ema\", True)),\n        \"ema_decay\": float(cfg.get(\"ema_decay\", 0.999)),\n\n        \"bce_weight\": float(cfg[\"bce_weight\"]),\n        \"dice_weight\": float(cfg[\"dice_weight\"]),\n        \"focal_gamma\": float(cfg.get(\"focal_gamma\", 0.0)),\n        \"input_noise_std\": float(cfg.get(\"input_noise_std\", 0.0)),\n        \"aug_hflip\": float(cfg.get(\"aug_hflip\", 0.0)),\n        \"aug_vflip\": float(cfg.get(\"aug_vflip\", 0.0)),\n        \"thr_grid\": int(cfg[\"thr_grid\"]),\n    }\n\n    # scalars oof (dict) only for folds evaluated\n    return summary, fold_rows, oof_area, oof_mean, fold_packs\n\n# ----------------------------\n# 10) Candidate configs (mask)\n# ----------------------------\ndef make_base_cfg():\n    if device.type == \"cuda\":\n        if MEM_GB >= 30:\n            bs, acc = 48, 1\n        elif MEM_GB >= 16:\n            bs, acc = 32, 1\n        else:\n            bs, acc = 24, 2\n    else:\n        bs, acc = 12, 1\n\n    return dict(\n        seed=SEED,\n        epochs=55 if device.type == \"cuda\" else 30,\n        batch_size=bs,\n        accum_steps=acc,\n        lr=3e-4,\n        weight_decay=1e-2,\n        warmup_frac=0.05,\n        grad_clip=1.0,\n        patience=10,\n        min_delta=1e-4,\n        use_ema=True,\n        ema_decay=0.999,\n\n        base_ch=160 if device.type == \"cuda\" else 128,\n        drop=0.10,\n        aspp_rates=(1,2,4,6),\n\n        bce_weight=1.0,\n        dice_weight=1.0,\n        focal_gamma=0.0,\n\n        input_noise_std=0.008,\n        aug_hflip=0.5,\n        aug_vflip=0.2,\n\n        thr_grid=THR_GRID,\n    )\n\nBASE = make_base_cfg()\n\ncandidates = []\ncandidates.append((\"mask_160_r1246\", dict(BASE, base_ch=160, drop=0.10, aspp_rates=(1,2,4,6), lr=3e-4, weight_decay=1e-2)))\ncandidates.append((\"mask_192_reg\",   dict(BASE, base_ch=192, drop=0.14, aspp_rates=(1,2,4,6), lr=2.5e-4, weight_decay=1.5e-2,\n                                         input_noise_std=0.010, dice_weight=1.2, bce_weight=0.9)))\ncandidates.append((\"mask_128_fast\",  dict(BASE, base_ch=128, drop=0.10, aspp_rates=(1,2,3,4), lr=3.5e-4, weight_decay=8e-3,\n                                         epochs=min(int(BASE[\"epochs\"]), 45), patience=9)))\ncandidates.append((\"mask_focal\",     dict(BASE, base_ch=160, drop=0.12, aspp_rates=(1,2,4,6), lr=3e-4, weight_decay=1.2e-2,\n                                         focal_gamma=1.5, bce_weight=1.0, dice_weight=1.1)))\n\nif device.type == \"cuda\" and MEM_GB >= 20:\n    candidates.append((\"mask_big_224\", dict(BASE, base_ch=224, drop=0.16, aspp_rates=(1,2,4,8), lr=2.0e-4, weight_decay=2.0e-2,\n                                           epochs=max(int(BASE[\"epochs\"]), 65), patience=12,\n                                           dice_weight=1.25, bce_weight=0.85, input_noise_std=0.012)))\n\nprint(f\"\\nTotal mask candidates: {len(candidates)}\")\nprint(\"Primary score: mean(best_val_dice_hard) across folds (subset for stage1, full for stage2)\")\n\n# ----------------------------\n# 11) Run 2-stage search (resume-safe)\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_mask_artifacts\")\nOPT_DIR = OUT_DIR / \"opt_search\"\nOPT_DIR.mkdir(parents=True, exist_ok=True)\n\nSTAGE1_PATH = OPT_DIR / \"stage1_results.csv\"\n\n# evenly spaced folds subset\nif STAGE1_FOLDS >= len(unique_folds):\n    folds_subset = unique_folds\nelse:\n    idxs = np.linspace(0, len(unique_folds)-1, STAGE1_FOLDS)\n    idxs = np.unique(np.round(idxs).astype(int)).tolist()\n    folds_subset = [unique_folds[i] for i in idxs]\n    if len(folds_subset) < STAGE1_FOLDS:\n        for f in unique_folds:\n            if f not in folds_subset:\n                folds_subset.append(f)\n            if len(folds_subset) >= STAGE1_FOLDS:\n                break\n\nprint(\"\\nStage-1 folds subset:\", folds_subset)\n\ndone_stage1 = set()\nif STAGE1_PATH.exists():\n    try:\n        df_prev = pd.read_csv(STAGE1_PATH)\n        if \"cfg\" in df_prev.columns:\n            done_stage1 = set(df_prev[\"cfg\"].astype(str).tolist())\n            print(f\"Resume: found {len(done_stage1)} configs already in stage1_results.csv\")\n    except Exception:\n        pass\n\nt0 = time.time()\n\nfor i, (name, cfg) in enumerate(candidates, 1):\n    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n        print(\"Time budget reached. Stop search.\")\n        break\n\n    if name in done_stage1:\n        print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] SKIP -> {name}\")\n        continue\n\n    cfg1 = dict(cfg)\n    cfg1[\"epochs\"] = int(min(int(cfg1[\"epochs\"]), int(STAGE1_EPOCH_CAP)))\n    cfg1[\"patience\"] = int(min(int(cfg1[\"patience\"]), int(STAGE1_PAT_CAP)))\n\n    print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] CV(subset) -> {name}\")\n    summ, fold_rows, _, _, _ = run_cv_config(cfg1, name, folds_subset=folds_subset)\n\n    print(f\"  stage1 score(mean dice hard): {summ['score_mean_best_dice_hard']:.6f} | thr_median: {summ['thr_median']:.3f}\")\n\n    # append to CSV (resume-safe)\n    try:\n        df_append = pd.DataFrame([summ])\n        if STAGE1_PATH.exists():\n            df_old = pd.read_csv(STAGE1_PATH)\n            df_new = pd.concat([df_old, df_append], axis=0, ignore_index=True)\n        else:\n            df_new = df_append\n        df_new.to_csv(STAGE1_PATH, index=False)\n    except Exception:\n        pass\n\n# load stage1 ranking\nif STAGE1_PATH.exists():\n    df_s1 = pd.read_csv(STAGE1_PATH)\nelse:\n    df_s1 = pd.DataFrame([])\n\nif len(df_s1) == 0:\n    raise RuntimeError(\"Stage-1 menghasilkan 0 hasil. Cek token/mask cache atau turunkan kandidat/epochs.\")\n\ndf_s1 = df_s1.sort_values([\"score_mean_best_dice_hard\", \"thr_median\"], ascending=[False, True]).reset_index(drop=True)\nprint(\"\\nStage-1 ranking (top):\")\ndisplay(df_s1.head(10))\n\ntopM = min(int(STAGE2_TOPM), len(df_s1))\nstage2_names = df_s1[\"cfg\"].head(topM).astype(str).tolist()\nprint(\"\\nStage-2 will run full CV for:\", stage2_names)\n\nall_summaries = []\nall_fold_rows = []\noof_area_store = {}\noof_mean_store = {}\npack_store = {}\n\nfor j, nm in enumerate(stage2_names, 1):\n    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n        print(\"Time budget reached. Stop stage-2.\")\n        break\n\n    cfg = None\n    for (nname, ccfg) in candidates:\n        if nname == nm:\n            cfg = ccfg\n            break\n    if cfg is None:\n        continue\n\n    print(f\"\\n[Stage-2 {j:02d}/{len(stage2_names)}] CV(full) -> {nm}\")\n    summ, fold_rows, oof_area, oof_mean, fold_packs = run_cv_config(cfg, nm, folds_subset=None)\n\n    all_summaries.append(summ)\n    all_fold_rows.extend(fold_rows)\n    oof_area_store[nm] = oof_area\n    oof_mean_store[nm] = oof_mean\n    pack_store[nm] = fold_packs\n\n    print(f\"  OOF score(mean dice hard): {summ['score_mean_best_dice_hard']:.6f} | thr_median: {summ['thr_median']:.3f}\")\n\ndf_sum = pd.DataFrame(all_summaries)\ndf_fold = pd.DataFrame(all_fold_rows)\n\nif len(df_sum) == 0:\n    raise RuntimeError(\"Stage-2 produced no results. Turunkan kandidat/epochs atau cek device/VRAM.\")\n\ndf_sum = df_sum.sort_values([\"score_mean_best_dice_hard\"], ascending=[False]).reset_index(drop=True)\n\nprint(\"\\nStage-2 top candidates (full CV):\")\ndisplay(df_sum)\n\n# save search results\ndf_sum.to_csv(OPT_DIR / \"opt_results.csv\", index=False)\nwith open(OPT_DIR / \"opt_results.json\", \"w\") as f:\n    json.dump(df_sum.to_dict(orient=\"records\"), f, indent=2)\ndf_fold.to_csv(OPT_DIR / \"opt_fold_details.csv\", index=False)\n\n# save OOF scalars for top configs\ntop_names = df_sum[\"cfg\"].head(min(REPORT_TOPK_OOF, len(df_sum))).astype(str).tolist()\nfor nm in top_names:\n    # align to df_train_filtered order\n    ulist = df_train_filtered[\"uid\"].astype(str).tolist()\n    area = np.array([oof_area_store[nm].get(u, np.nan) for u in ulist], dtype=np.float32)\n    meanp = np.array([oof_mean_store[nm].get(u, np.nan) for u in ulist], dtype=np.float32)\n\n    df_o = df_train_filtered[[\"uid\",\"y\",\"fold\"]].copy()\n    df_o[f\"oof_area_frac_{nm}\"] = area\n    df_o[f\"oof_mean_prob_{nm}\"] = meanp\n    df_o.to_csv(OPT_DIR / f\"oof_scalars_{nm}.csv\", index=False)\n\n# ----------------------------\n# 12) Choose best config + save BEST fold packs\n# ----------------------------\nbest_single = df_sum.iloc[0].to_dict()\nbest_cfg_name = str(best_single[\"cfg\"])\n\nbest_cfg = None\nfor nm, cfg in candidates:\n    if nm == best_cfg_name:\n        best_cfg = cfg\n        break\nif best_cfg is None:\n    raise RuntimeError(\"Best cfg not found in candidates list (unexpected).\")\n\nbest_fold_packs = pack_store[best_cfg_name]\nrecommended_thr = float(best_single[\"thr_median\"])\nbest_score = float(best_single[\"score_mean_best_dice_hard\"])\n\nbest_model_path = OUT_DIR / \"best_mask_model.pt\"\ntorch.save(\n    {\n        \"type\": \"unet_aspp_decoder_on_dinov2_tokengrid_opt_v4\",\n        \"cfg_name\": best_cfg_name,\n        \"cfg\": best_cfg,\n        \"seed\": SEED,\n        \"token_dim\": int(TOKEN_DIM),\n        \"token_hw\": tuple(map(int, TOKEN_HW)),\n        \"fold_packs\": best_fold_packs,\n        \"recommended_thr\": recommended_thr,\n        \"best_oof_score_mean_dice_hard\": best_score,\n        \"notes\": \"Best config from Step 4 (mask optimization). fold_packs only (no full retrain here).\",\n    },\n    best_model_path\n)\n\nbest_bundle = {\n    \"type\": \"unet_aspp_decoder_on_dinov2_tokengrid_opt_v4\",\n    \"model_name\": best_cfg_name,\n    \"random_seed\": SEED,\n    \"token_dim\": int(TOKEN_DIM),\n    \"token_hw\": tuple(map(int, TOKEN_HW)),\n\n    \"cfg\": best_cfg,\n    \"recommended_thr\": recommended_thr,\n    \"best_oof_score_mean_dice_hard\": best_score,\n\n    \"paths\": {\n        \"opt_results_csv\": str(OPT_DIR / \"opt_results.csv\"),\n        \"opt_fold_details_csv\": str(OPT_DIR / \"opt_fold_details.csv\"),\n        \"stage1_results_csv\": str(STAGE1_PATH),\n        \"best_model_pt\": str(best_model_path),\n    },\n}\n\nwith open(OUT_DIR / \"best_mask_config.json\", \"w\") as f:\n    json.dump(best_bundle, f, indent=2)\n\nprint(\"\\nSaved best artifacts:\")\nprint(\"  best model (fold packs) ->\", best_model_path)\nprint(\"  best config             ->\", OUT_DIR / \"best_mask_config.json\")\nprint(\"  opt results             ->\", OPT_DIR / \"opt_results.csv\")\nprint(\"  fold detail             ->\", OPT_DIR / \"opt_fold_details.csv\")\nprint(\"  stage1 cache            ->\", STAGE1_PATH)\n\n# Export globals for next steps\nBEST_MASK_BUNDLE = best_bundle\nBEST_MASK_CFG_NAME = best_cfg_name\nBEST_MASK_CFG = best_cfg\nOPT_RESULTS_DF_MASK = df_sum\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Training (Train on Full Data)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 5 — Final Training (Train on Full Data) — MASK ONLY\n# REVISI FULL v1.0 (match Step 4 MASK optimizer: UNet+ASPP on DINOv2 token-grid)\n#\n# Sesuai saran sebelumnya:\n# - Kalau Step 3 kamu sudah pakai UNet+ASPP mask-decoder di atas token-grid DINOv2,\n#   maka Step 5 final training juga harus train MASK model (bukan transformer gate).\n#\n# Fix/fitur:\n# - STRICT load cfg terbaik dari Step 4: best_mask_config.json (tanpa upscale kecuali ON)\n# - AMP/GradScaler aman: CPU => no-amp, no-scaler\n# - Internal val case-level (stratified by case y) untuk cari best_epoch + recommended_thr\n# - Retrain FULL data pakai epochs_fixed (best_epoch * 1.05) + EMA final-weights\n# - OOM fallback aman: turun batch -> base_ch -> accum\n# - Token-grid size guard: auto-detect mode (H,W,D) lalu resize semua token+mask ke itu\n#\n# Output:\n#   /kaggle/working/recodai_luc_mask_artifacts/final_mask_model.pt\n#   /kaggle/working/recodai_luc_mask_artifacts/final_mask_bundle.json\n# ============================================================\n\nimport os, json, gc, math, time, warnings\nfrom pathlib import Path\nfrom collections import Counter\nfrom contextlib import nullcontext\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# 0) REQUIRE\n# ----------------------------\nif \"df_train_tabular\" not in globals():\n    raise RuntimeError(\"Missing `df_train_tabular`. Pastikan Step 2 (build table) sudah jalan.\")\ndf_train_tabular = df_train_tabular.copy()\n\nneed_cols = {\"uid\",\"case_id\",\"y\"}\nmiss = [c for c in need_cols if c not in df_train_tabular.columns]\nif miss:\n    raise ValueError(f\"df_train_tabular missing columns: {miss}\")\n\ndf_train_tabular[\"uid\"] = df_train_tabular[\"uid\"].astype(str)\ndf_train_tabular[\"case_id\"] = df_train_tabular[\"case_id\"].astype(str)\ndf_train_tabular[\"y\"] = df_train_tabular[\"y\"].astype(int)\n\nprint(\"Final MASK training data:\")\nprint(f\"  rows={len(df_train_tabular)} | forged%={float(df_train_tabular['y'].mean())*100:.2f}\")\n\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_mask_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 1) Load best cfg (Step 4 output)\n# ----------------------------\nbest_bundle = None\nsource = None\n\ncfg_path = OUT_DIR / \"best_mask_config.json\"\nbest_model_candidates = [\n    OUT_DIR / \"best_mask_model.pt\",\n    OUT_DIR / \"best_mask_model.pth\",\n]\n\nif \"BEST_MASK_BUNDLE\" in globals() and isinstance(BEST_MASK_BUNDLE, dict):\n    best_bundle = BEST_MASK_BUNDLE\n    source = \"memory(BEST_MASK_BUNDLE)\"\nelif cfg_path.exists():\n    best_bundle = json.loads(cfg_path.read_text())\n    source = str(cfg_path)\n\nif best_bundle is not None and isinstance(best_bundle, dict) and isinstance(best_bundle.get(\"cfg\", None), dict):\n    base_cfg = dict(best_bundle[\"cfg\"])\n    print(\"\\nLoaded cfg from:\", source)\nelse:\n    base_cfg = {}\n    print(\"\\nNo best_mask_config found. Using strong default cfg.\")\n\n# optional: load fold_packs dari best_mask_model.pt\nfold_packs_from_step4 = None\nbest_mask_model_path = None\nfor p in best_model_candidates:\n    if p.exists():\n        best_mask_model_path = p\n        break\n\nif best_mask_model_path is not None:\n    try:\n        obj = torch.load(best_mask_model_path, map_location=\"cpu\")\n        if isinstance(obj, dict) and isinstance(obj.get(\"fold_packs\", None), list):\n            fold_packs_from_step4 = obj[\"fold_packs\"]\n            print(\"Loaded fold_packs from:\", str(best_mask_model_path))\n    except Exception as e:\n        print(\"Warning: failed to load best_mask_model.* fold_packs:\", repr(e))\n\n# ----------------------------\n# 2) Device + seed\n# ----------------------------\ndef seed_everything(seed: int):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nFINAL_SEED = 2025\nif isinstance(best_bundle, dict):\n    FINAL_SEED = int(best_bundle.get(\"seed\", best_bundle.get(\"random_seed\", 2025)))\n\nseed_everything(FINAL_SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\n\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\nvram_gb = None\nif device.type == \"cuda\":\n    vram_gb = float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n\nprint(\"\\nDevice:\", device, \"| AMP:\", use_amp, \"| VRAM_GB:\", (f\"{vram_gb:.1f}\" if vram_gb else \"CPU\"))\n\n# ----------------------------\n# 3) Training policy\n# ----------------------------\nALLOW_UPSCALE = False          # True kalau mau auto-besar sesuai VRAM\nUSE_INTERNAL_VAL = True        # cari best_epoch + recommended_thr dari internal val (case-level)\nVAL_FRAC_CASE = 0.08           # 8% case untuk val\nEARLY_STOP = True\n\nN_SEEDS = 1\n\nDROP_MISSING_POS_MASKS = True  # disarankan (pos tanpa mask => bikin training noisy)\nTIME_BUDGET_SEC = 0            # 0=off\n\n# ----------------------------\n# 4) Auto-detect dirs (TOKEN_CACHE_DIR + MASK_DIR)\n# ----------------------------\ndef _first_existing(paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(p)\n        if p.exists():\n            return p\n    return None\n\n# MASK_DIR\nMASK_DIR = None\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    MASK_DIR = _first_existing([PATHS.get(\"TRAIN_MASK_DIR\"), PATHS.get(\"MASK_DIR\"), PATHS.get(\"TRAIN_MASKS\")])\nif MASK_DIR is None:\n    MASK_DIR = _first_existing([\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\",\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/masks_train\",\n        \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/masks\",\n        \"/kaggle/working/recodai_luc/train_masks\",\n    ])\nprint(\"MASK_DIR:\", str(MASK_DIR) if MASK_DIR else \"(None)\")\n\n# TOKEN_CACHE_DIR\nTOKEN_CACHE_DIR = None\nif \"CACHE_ROOT\" in globals():\n    try:\n        cr = Path(CACHE_ROOT)\n        if cr.exists():\n            TOKEN_CACHE_DIR = cr\n    except Exception:\n        pass\n\nif TOKEN_CACHE_DIR is None:\n    base_candidates = [\n        Path(\"/kaggle/working/recodai_luc/cache/dino_v2\"),\n        Path(\"/kaggle/working/recodai_luc/cache\"),\n        Path(\"/kaggle/input/recod-ailuc-dinov2-train/recodai_luc/cache/dino_v2\"),\n        Path(\"/kaggle/input/recod-ailuc-dinov2-train/recodai_luc/cache\"),\n    ]\n    for b in base_candidates:\n        if not b.exists():\n            continue\n        cfg_dirs = sorted([p for p in b.glob(\"cfg_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n        if cfg_dirs:\n            TOKEN_CACHE_DIR = cfg_dirs[0]\n            break\n        TOKEN_CACHE_DIR = b\n        break\n\nprint(\"TOKEN_CACHE_DIR:\", str(TOKEN_CACHE_DIR) if TOKEN_CACHE_DIR else \"(None)\")\nif TOKEN_CACHE_DIR is None:\n    raise RuntimeError(\"TOKEN_CACHE_DIR tidak ditemukan. Pastikan cache token-grid DINOv2 per uid tersedia.\")\n\n# ----------------------------\n# 5) Token/mask path map + infer token grid mode (H,W,D)\n# ----------------------------\ndef _load_np_any(p: Path):\n    p = Path(p)\n    if p.suffix.lower() == \".npy\":\n        return np.load(p, allow_pickle=False)\n    if p.suffix.lower() == \".npz\":\n        z = np.load(p, allow_pickle=False)\n        keys = list(z.keys())\n        for k in keys:\n            a = z[k]\n            if isinstance(a, np.ndarray) and a.ndim in (2,3):\n                return a\n        return z[keys[0]] if keys else None\n    return None\n\ndef _reshape_to_grid(a: np.ndarray):\n    a = np.asarray(a)\n    if a.ndim == 3:\n        # (D,H,W) -> (H,W,D)\n        if a.shape[0] in [256, 384, 512, 768, 1024, 1536] and (a.shape[1] * a.shape[2] > 16):\n            D,H,W = a.shape\n            return np.transpose(a, (1,2,0))\n        return a  # assume (H,W,D)\n    if a.ndim == 2:\n        N,D = a.shape\n        s = int(round(math.sqrt(N)))\n        if s*s == N:\n            return a.reshape(s,s,D)\n        for h in range(1, int(math.sqrt(N))+1):\n            if N % h == 0:\n                w = N // h\n                if h >= 4 and w >= 4:\n                    return a.reshape(h,w,D)\n        return a.reshape(1,N,D)\n    return None\n\ndef find_token_file_fast(uid: str):\n    uid = str(uid)\n    cand = [\n        TOKEN_CACHE_DIR / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"tokens_train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"tokens_train\" / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"train\" / f\"{uid}.npy\",\n        TOKEN_CACHE_DIR / \"feat_train\" / f\"{uid}.npz\",\n        TOKEN_CACHE_DIR / \"feat_train\" / f\"{uid}.npy\",\n    ]\n    for p in cand:\n        if p.exists():\n            return p\n    return None\n\ndef find_mask_file_fast(uid: str):\n    if MASK_DIR is None:\n        return None\n    uid = str(uid)\n    exts = [\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\"]\n    for ex in exts:\n        p = Path(MASK_DIR) / f\"{uid}{ex}\"\n        if p.exists():\n            return p\n    hits = list(Path(MASK_DIR).glob(f\"{uid}.*\"))\n    return hits[0] if hits else None\n\nuids_all = df_train_tabular[\"uid\"].astype(str).tolist()\n\nTOKEN_PATH = {}\nMASK_PATH = {}\n\ntoken_hw_counter = Counter()\ntoken_dim_counter = Counter()\n\nmax_probe = min(300, len(uids_all))\nfor uid in uids_all[:max_probe]:\n    tp = find_token_file_fast(uid)\n    TOKEN_PATH[uid] = tp\n    if tp is None:\n        continue\n    a = _load_np_any(tp)\n    if a is None:\n        continue\n    g = _reshape_to_grid(a)\n    if g is None or g.ndim != 3:\n        continue\n    H,W,D = int(g.shape[0]), int(g.shape[1]), int(g.shape[2])\n    if H >= 4 and W >= 4 and D >= 16:\n        token_hw_counter[(H,W)] += 1\n        token_dim_counter[D] += 1\n\nfor uid in uids_all[max_probe:]:\n    TOKEN_PATH[uid] = find_token_file_fast(uid)\n\nif MASK_DIR is not None:\n    for uid in uids_all[:max_probe]:\n        MASK_PATH[uid] = find_mask_file_fast(uid)\n    for uid in uids_all[max_probe:]:\n        MASK_PATH[uid] = find_mask_file_fast(uid)\n\nif not token_hw_counter or not token_dim_counter:\n    raise RuntimeError(\"Tidak bisa infer token grid shape/dim dari token cache. Cek format token-grid per uid.\")\n\nTOKEN_HW = token_hw_counter.most_common(1)[0][0]\nTOKEN_DIM = token_dim_counter.most_common(1)[0][0]\n\nprint(\"Token grid mode:\", TOKEN_HW, \"| token_dim mode:\", TOKEN_DIM)\n\n# ----------------------------\n# 6) Filter df (drop missing token, optionally drop missing positive masks)\n# ----------------------------\ndef build_df_filtered(df: pd.DataFrame):\n    df = df.copy()\n    df[\"uid\"] = df[\"uid\"].astype(str)\n\n    df[\"token_exists\"] = df[\"uid\"].map(lambda u: TOKEN_PATH.get(str(u), None) is not None).astype(int)\n    if MASK_DIR is not None:\n        df[\"mask_exists\"] = df[\"uid\"].map(lambda u: MASK_PATH.get(str(u), None) is not None).astype(int)\n    else:\n        df[\"mask_exists\"] = 0\n\n    df = df[df[\"token_exists\"] == 1].reset_index(drop=True)\n\n    if DROP_MISSING_POS_MASKS and MASK_DIR is not None:\n        df = df[(df[\"y\"] == 0) | (df[\"mask_exists\"] == 1)].reset_index(drop=True)\n\n    return df\n\ndf_train_filtered = build_df_filtered(df_train_tabular)\nprint(\"After filtering:\")\nprint(\"  rows:\", len(df_train_filtered),\n      \"| forged%:\", float(df_train_filtered[\"y\"].mean())*100.0,\n      \"| token_missing_dropped:\", int((df_train_tabular.shape[0] - df_train_filtered.shape[0])))\n\nif len(df_train_filtered) < 64:\n    raise RuntimeError(\"Data terlalu sedikit setelah filtering. Cek token cache & mask dir.\")\n\n# ----------------------------\n# 7) IO utils: load/resize tokens + masks\n# ----------------------------\ndef load_token_grid_from_path(p: Path):\n    a = _load_np_any(p)\n    g = _reshape_to_grid(a)\n    if g is None:\n        return None\n    g = g.astype(np.float32, copy=False)\n    if not np.isfinite(g).all():\n        g = np.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0)\n    return g  # (H,W,D)\n\ndef resize_grid_hw(g: np.ndarray, out_hw):\n    Ht, Wt = int(out_hw[0]), int(out_hw[1])\n    if g.shape[0] == Ht and g.shape[1] == Wt:\n        return g\n    x = torch.from_numpy(np.transpose(g, (2,0,1))).unsqueeze(0)  # (1,D,H,W)\n    x = F.interpolate(x, size=(Ht,Wt), mode=\"bilinear\", align_corners=False)\n    out = x.squeeze(0).permute(1,2,0).contiguous().cpu().numpy()\n    return out.astype(np.float32, copy=False)\n\ndef load_mask_bin(path: Path):\n    im = Image.open(path).convert(\"L\")\n    m = (np.array(im, dtype=np.uint8) > 127).astype(np.uint8)\n    return m\n\ndef resize_mask_soft(mask_bin: np.ndarray, out_hw):\n    Ht,Wt = int(out_hw[0]), int(out_hw[1])\n    im = Image.fromarray(mask_bin.astype(np.uint8)*255)\n    im = im.resize((Wt,Ht), resample=Image.BILINEAR)\n    m = (np.array(im, dtype=np.float32) / 255.0).astype(np.float32)\n    return m\n\n# ----------------------------\n# 8) Dataset\n# ----------------------------\nclass TokenMaskDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, is_train: bool, cfg: dict):\n        self.df = df.reset_index(drop=True)\n        self.is_train = bool(is_train)\n        self.cfg = cfg\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        uid = str(row[\"uid\"])\n        yflag = int(row[\"y\"])\n\n        tp = TOKEN_PATH.get(uid, None)\n        g = load_token_grid_from_path(tp) if tp is not None else None\n        if g is None:\n            g = np.zeros((TOKEN_HW[0], TOKEN_HW[1], TOKEN_DIM), dtype=np.float32)\n\n        # dim fix\n        if g.shape[2] != TOKEN_DIM:\n            D = g.shape[2]\n            if D > TOKEN_DIM:\n                g = g[:, :, :TOKEN_DIM]\n            else:\n                pad = np.zeros((g.shape[0], g.shape[1], TOKEN_DIM - D), dtype=np.float32)\n                g = np.concatenate([g, pad], axis=2)\n\n        g = resize_grid_hw(g, TOKEN_HW)\n\n        # mask\n        m_soft = np.zeros((TOKEN_HW[0], TOKEN_HW[1]), dtype=np.float32)\n        if yflag == 1 and MASK_DIR is not None:\n            mp = MASK_PATH.get(uid, None)\n            if mp is not None:\n                m_bin = load_mask_bin(mp)\n                m_soft = resize_mask_soft(m_bin, TOKEN_HW)\n\n        # aug\n        if self.is_train:\n            if float(self.cfg.get(\"aug_hflip\", 0.0)) > 0 and np.random.rand() < float(self.cfg[\"aug_hflip\"]):\n                g = g[:, ::-1, :].copy()\n                m_soft = m_soft[:, ::-1].copy()\n            if float(self.cfg.get(\"aug_vflip\", 0.0)) > 0 and np.random.rand() < float(self.cfg[\"aug_vflip\"]):\n                g = g[::-1, :, :].copy()\n                m_soft = m_soft[::-1, :].copy()\n\n            ns = float(self.cfg.get(\"input_noise_std\", 0.0))\n            if ns > 0:\n                g = g + np.random.randn(*g.shape).astype(np.float32) * ns\n\n        x = torch.from_numpy(np.transpose(g, (2,0,1)).astype(np.float32))   # (D,H,W)\n        y = torch.from_numpy(m_soft[None, ...].astype(np.float32))          # (1,H,W)\n        return x, y, uid\n\ndef make_loader(ds, batch_size, shuffle):\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n    return DataLoader(\n        ds,\n        batch_size=int(batch_size),\n        shuffle=bool(shuffle),\n        num_workers=nw,\n        pin_memory=pin,\n        drop_last=False,\n        persistent_workers=(nw > 0),\n    )\n\n# ----------------------------\n# 9) Model: UNet + ASPP\n# ----------------------------\nclass ConvGNAct(nn.Module):\n    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, drop=0.0, groups=8):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)\n        g = max(1, min(int(groups), int(out_ch)))\n        self.gn = nn.GroupNorm(g, out_ch)\n        self.act = nn.SiLU(inplace=True)\n        self.drop = nn.Dropout2d(float(drop)) if float(drop) > 0 else nn.Identity()\n\n    def forward(self, x):\n        return self.drop(self.act(self.gn(self.conv(x))))\n\nclass ASPP(nn.Module):\n    def __init__(self, ch, rates=(1,2,4,6), drop=0.0):\n        super().__init__()\n        rs = list(rates)\n        self.branches = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(ch, ch, 3, padding=int(r), dilation=int(r), bias=False),\n                nn.GroupNorm(max(1, min(8, ch)), ch),\n                nn.SiLU(inplace=True),\n            ) for r in rs\n        ])\n        self.proj = nn.Sequential(\n            nn.Conv2d(ch * len(rs), ch, 1, bias=False),\n            nn.GroupNorm(max(1, min(8, ch)), ch),\n            nn.SiLU(inplace=True),\n            nn.Dropout2d(float(drop)) if float(drop) > 0 else nn.Identity()\n        )\n\n    def forward(self, x):\n        xs = [b(x) for b in self.branches]\n        return self.proj(torch.cat(xs, dim=1))\n\nclass UNetASPP(nn.Module):\n    def __init__(self, in_dim, base_ch=160, drop=0.10, aspp_rates=(1,2,4,6)):\n        super().__init__()\n        C = int(base_ch)\n\n        self.stem = nn.Sequential(\n            ConvGNAct(in_dim, C, k=1, s=1, p=0, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n        self.enc1 = nn.Sequential(\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down1 = ConvGNAct(C, 2*C, k=3, s=2, p=1, drop=drop)\n        self.enc2 = nn.Sequential(\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down2 = ConvGNAct(2*C, 4*C, k=3, s=2, p=1, drop=drop)\n        self.enc3 = nn.Sequential(\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.down3 = ConvGNAct(4*C, 6*C, k=3, s=2, p=1, drop=drop)\n\n        self.bottleneck = nn.Sequential(\n            ConvGNAct(6*C, 6*C, k=3, s=1, p=1, drop=drop),\n            ASPP(6*C, rates=aspp_rates, drop=drop),\n            ConvGNAct(6*C, 6*C, k=3, s=1, p=1, drop=drop),\n        )\n\n        self.dec3 = nn.Sequential(\n            ConvGNAct(6*C + 4*C, 4*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(4*C, 4*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.dec2 = nn.Sequential(\n            ConvGNAct(4*C + 2*C, 2*C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(2*C, 2*C, k=3, s=1, p=1, drop=drop),\n        )\n        self.dec1 = nn.Sequential(\n            ConvGNAct(2*C + C, C, k=3, s=1, p=1, drop=drop),\n            ConvGNAct(C, C, k=3, s=1, p=1, drop=drop),\n        )\n\n        self.head = nn.Conv2d(C, 1, 1)\n\n    def forward(self, x):\n        x0 = self.stem(x)\n        s1 = self.enc1(x0)\n\n        x1 = self.down1(s1)\n        s2 = self.enc2(x1)\n\n        x2 = self.down2(s2)\n        s3 = self.enc3(x2)\n\n        x3 = self.down3(s3)\n        b  = self.bottleneck(x3)\n\n        u3 = F.interpolate(b,  size=s3.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d3 = self.dec3(torch.cat([u3, s3], dim=1))\n\n        u2 = F.interpolate(d3, size=s2.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d2 = self.dec2(torch.cat([u2, s2], dim=1))\n\n        u1 = F.interpolate(d2, size=s1.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d1 = self.dec1(torch.cat([u1, s1], dim=1))\n\n        return self.head(d1)  # (B,1,H,W)\n\n# ----------------------------\n# 10) EMA + loss + eval\n# ----------------------------\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n        self.backup = {}\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for n,p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[n].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for n,p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[n] = p.detach().clone()\n            p.copy_(self.shadow[n])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for n,p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[n])\n        self.backup = {}\n\ndef dice_from_probs(prob, target, eps=1e-6):\n    num = (prob * target).sum(dim=(2,3)) * 2.0\n    den = (prob + target).sum(dim=(2,3)).clamp_min(eps)\n    return (num / den).mean()\n\ndef dice_hard_np(prob_np, target_np, thr):\n    pr = (prob_np >= thr).astype(np.float32)\n    inter = (pr * target_np).sum(axis=(2,3)) * 2.0\n    den = (pr + target_np).sum(axis=(2,3)) + 1e-6\n    return float((inter / den).mean())\n\ndef loss_bce_dice(logits, target, bce_w=1.0, dice_w=1.0, focal_gamma=0.0):\n    bce = F.binary_cross_entropy_with_logits(logits, target, reduction=\"none\")\n    if focal_gamma and focal_gamma > 0:\n        p = torch.sigmoid(logits)\n        p_t = p * target + (1.0 - p) * (1.0 - target)\n        bce = bce * (1.0 - p_t).clamp_min(0.0).pow(float(focal_gamma))\n    bce = bce.mean()\n    prob = torch.sigmoid(logits)\n    dsc = dice_from_probs(prob, target)\n    return float(bce_w)*bce + float(dice_w)*(1.0 - dsc)\n\n@torch.no_grad()\ndef eval_loader(model, loader, cfg, ema=None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n\n    thrs = np.linspace(0.05, 0.95, int(cfg[\"thr_grid\"]), dtype=np.float64)\n    dice_sweep = np.zeros_like(thrs, dtype=np.float64)\n\n    tot_loss, tot_dice_soft, nb, cnt = 0.0, 0.0, 0, 0\n\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n\n        with ctx:\n            logits = model(xb)\n            loss = loss_bce_dice(\n                logits, yb,\n                bce_w=float(cfg[\"bce_weight\"]),\n                dice_w=float(cfg[\"dice_weight\"]),\n                focal_gamma=float(cfg.get(\"focal_gamma\", 0.0)),\n            )\n            prob = torch.sigmoid(logits)\n            dsoft = dice_from_probs(prob, yb)\n\n        tot_loss += float(loss.item() if hasattr(loss, \"item\") else loss)\n        tot_dice_soft += float(dsoft.item())\n        nb += 1\n\n        prob_np = prob.detach().float().cpu().numpy()\n        y_np = yb.detach().float().cpu().numpy()\n        for i, t in enumerate(thrs):\n            dice_sweep[i] += dice_hard_np(prob_np, y_np, thr=float(t))\n        cnt += 1\n\n    dice_sweep = dice_sweep / max(1, cnt)\n    j = int(np.argmax(dice_sweep))\n\n    if ema is not None:\n        ema.restore(model)\n\n    return {\n        \"val_loss\": tot_loss / max(1, nb),\n        \"val_dice_soft\": tot_dice_soft / max(1, nb),\n        \"best_thr\": float(thrs[j]),\n        \"best_dice_hard\": float(dice_sweep[j]),\n    }\n\n# ----------------------------\n# 11) CFG merge + guards (strict match Step 4)\n# ----------------------------\nCFG = dict(\n    seed=FINAL_SEED,\n    epochs=55 if device.type == \"cuda\" else 30,\n    batch_size=32 if device.type == \"cuda\" else 12,\n    accum_steps=1,\n    lr=3e-4,\n    weight_decay=1e-2,\n    warmup_frac=0.05,\n    grad_clip=1.0,\n    patience=10,\n    min_delta=1e-4,\n    use_ema=True,\n    ema_decay=0.999,\n\n    base_ch=160 if device.type == \"cuda\" else 128,\n    drop=0.10,\n    aspp_rates=(1,2,4,6),\n\n    bce_weight=1.0,\n    dice_weight=1.0,\n    focal_gamma=0.0,\n\n    input_noise_std=0.008,\n    aug_hflip=0.5,\n    aug_vflip=0.2,\n\n    thr_grid=81,\n)\n\nfor k, v in base_cfg.items():\n    if k in CFG:\n        CFG[k] = v\n\ndef cpu_safe_cfg(cfg: dict):\n    cfg = dict(cfg)\n    if device.type != \"cuda\":\n        cfg[\"base_ch\"] = min(int(cfg[\"base_ch\"]), 128)\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 12)\n        cfg[\"epochs\"] = min(int(cfg[\"epochs\"]), 35)\n        cfg[\"accum_steps\"] = 1\n    return cfg\n\ndef maybe_upscale_cfg(cfg: dict):\n    if not (device.type == \"cuda\" and ALLOW_UPSCALE and vram_gb is not None):\n        return dict(cfg)\n    cfg = dict(cfg)\n    if vram_gb >= 24:\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 48)\n    elif vram_gb >= 16:\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 32)\n    return cfg\n\nCFG = cpu_safe_cfg(CFG)\nCFG = maybe_upscale_cfg(CFG)\n\n# effective batch heuristic (mask model is heavier; keep conservative)\nTARGET_EFF_BATCH = 48 if device.type == \"cuda\" else 12\nCFG[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(CFG[\"batch_size\"]))))\n\nprint(\"\\nCFG (final MASK):\")\nfor k in [\"base_ch\",\"drop\",\"aspp_rates\",\"batch_size\",\"accum_steps\",\"epochs\",\"lr\",\"weight_decay\",\"warmup_frac\",\"patience\",\"thr_grid\"]:\n    print(f\"  {k}: {CFG[k]}\")\n\n# ----------------------------\n# 12) Internal val split (case-level stratified)\n# ----------------------------\ndef make_case_split(df: pd.DataFrame, val_frac=0.08, seed=2025):\n    g = df.groupby(\"case_id\")[\"y\"].max().reset_index().rename(columns={\"y\": \"case_y\"})\n    pos_cases = g.loc[g[\"case_y\"] == 1, \"case_id\"].astype(str).to_numpy()\n    neg_cases = g.loc[g[\"case_y\"] == 0, \"case_id\"].astype(str).to_numpy()\n\n    rng = np.random.RandomState(int(seed))\n    rng.shuffle(pos_cases); rng.shuffle(neg_cases)\n\n    if len(pos_cases) == 0 or len(neg_cases) == 0:\n        idx = np.arange(len(df))\n        rng.shuffle(idx)\n        n_val = max(1, int(len(df) * float(val_frac)))\n        is_val = np.zeros(len(df), dtype=bool)\n        is_val[idx[:n_val]] = True\n        return is_val\n\n    n_val_pos = max(1, int(len(pos_cases) * float(val_frac)))\n    n_val_neg = max(1, int(len(neg_cases) * float(val_frac)))\n    val_cases = np.concatenate([pos_cases[:n_val_pos], neg_cases[:n_val_neg]])\n    val_set = set(val_cases.tolist())\n\n    is_val = df[\"case_id\"].astype(str).isin(val_set).to_numpy(dtype=bool)\n\n    # fallback if degenerate\n    va = df.loc[is_val, \"y\"].to_numpy()\n    if is_val.sum() < 32 or len(np.unique(va)) < 2:\n        idx_pos = np.where(df[\"y\"].values == 1)[0]\n        idx_neg = np.where(df[\"y\"].values == 0)[0]\n        rng.shuffle(idx_pos); rng.shuffle(idx_neg)\n        n_val = max(32, int(len(df) * float(val_frac)))\n        n_val_pos = max(1, int(n_val * float(df[\"y\"].mean())))\n        n_val_pos = min(n_val_pos, len(idx_pos))\n        n_val_neg = min(n_val - n_val_pos, len(idx_neg))\n        val_idx = np.concatenate([idx_pos[:n_val_pos], idx_neg[:n_val_neg]])\n        is_val = np.zeros(len(df), dtype=bool)\n        is_val[val_idx] = True\n\n    return is_val\n\n# ----------------------------\n# 13) Train with internal val -> best_epoch + recommended_thr\n# ----------------------------\ndef build_model(cfg: dict):\n    return UNetASPP(\n        in_dim=int(TOKEN_DIM),\n        base_ch=int(cfg[\"base_ch\"]),\n        drop=float(cfg[\"drop\"]),\n        aspp_rates=tuple(cfg[\"aspp_rates\"]),\n    ).to(device)\n\ndef build_opt_and_sch(model, cfg, steps_per_epoch, epochs):\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(0.9, 0.99),\n        eps=1e-8,\n    )\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    total_steps = int(max(1, int(epochs) * int(max(1, steps_per_epoch))))\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        t = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        t = min(max(t, 0.0), 1.0)\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n\n    sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n    return opt, sch\n\ndef train_with_internal_val_get_best(df_all: pd.DataFrame, cfg: dict, seed=2025):\n    seed_everything(int(seed))\n\n    is_val = make_case_split(df_all, val_frac=float(VAL_FRAC_CASE), seed=int(seed))\n    df_tr = df_all.loc[~is_val].reset_index(drop=True)\n    df_va = df_all.loc[is_val].reset_index(drop=True)\n\n    ds_tr = TokenMaskDataset(df_tr, is_train=True, cfg=cfg)\n    ds_va = TokenMaskDataset(df_va, is_train=False, cfg=cfg)\n    dl_tr = make_loader(ds_tr, cfg[\"batch_size\"], shuffle=True)\n    dl_va = make_loader(ds_va, cfg[\"batch_size\"], shuffle=False)\n\n    model = build_model(cfg)\n    opt, sch = build_opt_and_sch(\n        model, cfg,\n        steps_per_epoch=int(math.ceil(len(dl_tr) / max(1, int(cfg.get(\"accum_steps\", 1))))),\n        epochs=int(cfg[\"epochs\"])\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=True) if use_amp else None\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    best = {\"dice_hard\": -1.0, \"thr\": 0.5, \"epoch\": -1, \"val_loss\": 1e18, \"val_dice_soft\": -1.0}\n    best_state = None\n    bad = 0\n\n    print(f\"\\nInternal val split (MASK): train={len(df_tr)} | val={len(df_va)} | val_pos%={float(df_va['y'].mean())*100:.2f}\")\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum, n_sum, micro = 0.0, 0, 0\n        t0 = time.time()\n\n        for xb, yb, _ in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            with ctx:\n                logits = model(xb)\n                loss = loss_bce_dice(\n                    logits, yb,\n                    bce_w=float(cfg[\"bce_weight\"]),\n                    dice_w=float(cfg[\"dice_weight\"]),\n                    focal_gamma=float(cfg.get(\"focal_gamma\", 0.0)),\n                ) / float(accum)\n\n            if use_amp:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n            n_sum += xb.size(0)\n\n            if (micro % accum) == 0:\n                if float(cfg[\"grad_clip\"]) > 0:\n                    if use_amp:\n                        scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n                if use_amp:\n                    scaler.step(opt); scaler.update()\n                else:\n                    opt.step()\n\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        if (micro % accum) != 0:\n            if float(cfg[\"grad_clip\"]) > 0:\n                if use_amp:\n                    scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n            if use_amp:\n                scaler.step(opt); scaler.update()\n            else:\n                opt.step()\n\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        ev = eval_loader(model, dl_va, cfg, ema=ema)\n        dt = time.time() - t0\n        print(f\"  ep {epoch+1:03d}/{cfg['epochs']} | tr_loss={loss_sum/max(1,n_sum):.5f} | \"\n              f\"val_loss={ev['val_loss']:.5f} | val_dice_soft={ev['val_dice_soft']:.5f} | \"\n              f\"best_dice_hard={ev['best_dice_hard']:.5f}@{ev['best_thr']:.2f} | bad={bad} | dt={dt:.1f}s\")\n\n        improved = (ev[\"best_dice_hard\"] - best[\"dice_hard\"]) > float(cfg[\"min_delta\"])\n        if improved:\n            best[\"dice_hard\"] = float(ev[\"best_dice_hard\"])\n            best[\"thr\"] = float(ev[\"best_thr\"])\n            best[\"epoch\"] = int(epoch) + 1\n            best[\"val_loss\"] = float(ev[\"val_loss\"])\n            best[\"val_dice_soft\"] = float(ev[\"val_dice_soft\"])\n\n            # store EMA weights\n            if ema is not None:\n                ema.apply_shadow(model)\n                best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n                ema.restore(model)\n            else:\n                best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n\n            bad = 0\n        else:\n            bad += 1\n            if EARLY_STOP and bad >= int(cfg[\"patience\"]):\n                break\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if best_state is None:\n        best[\"epoch\"] = max(12, int(cfg[\"epochs\"] * 0.6))\n        best[\"thr\"] = float(best_bundle.get(\"recommended_thr\", 0.5)) if isinstance(best_bundle, dict) else 0.5\n\n    return best\n\n# ----------------------------\n# 14) Train FULL fixed epochs (use EMA at end)\n# ----------------------------\ndef train_full_fixed_epochs(df_all: pd.DataFrame, cfg: dict, epochs_fixed: int, seed=2025):\n    seed_everything(int(seed))\n\n    ds = TokenMaskDataset(df_all, is_train=True, cfg=cfg)\n    dl = make_loader(ds, cfg[\"batch_size\"], shuffle=True)\n\n    model = build_model(cfg)\n    opt, sch = build_opt_and_sch(\n        model, cfg,\n        steps_per_epoch=int(math.ceil(len(dl) / max(1, int(cfg.get(\"accum_steps\", 1))))),\n        epochs=int(epochs_fixed)\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=True) if use_amp else None\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    t0 = time.time()\n\n    for epoch in range(int(epochs_fixed)):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum, n_sum, micro = 0.0, 0, 0\n\n        for xb, yb, _ in dl:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            with ctx:\n                logits = model(xb)\n                loss = loss_bce_dice(\n                    logits, yb,\n                    bce_w=float(cfg[\"bce_weight\"]),\n                    dice_w=float(cfg[\"dice_weight\"]),\n                    focal_gamma=float(cfg.get(\"focal_gamma\", 0.0)),\n                ) / float(accum)\n\n            if use_amp:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n            n_sum += xb.size(0)\n\n            if (micro % accum) == 0:\n                if float(cfg[\"grad_clip\"]) > 0:\n                    if use_amp:\n                        scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n                if use_amp:\n                    scaler.step(opt); scaler.update()\n                else:\n                    opt.step()\n\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        if (micro % accum) != 0:\n            if float(cfg[\"grad_clip\"]) > 0:\n                if use_amp:\n                    scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n            if use_amp:\n                scaler.step(opt); scaler.update()\n            else:\n                opt.step()\n\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        print(f\"  full epoch {epoch+1:03d}/{int(epochs_fixed)} | loss={loss_sum/max(1,n_sum):.5f}\")\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    pack = {\n        \"type\": \"unet_aspp_token_grid_full_v1\",\n        \"arch\": \"UNetASPP\",\n        \"state_dict\": {k: v.detach().cpu() for k,v in model.state_dict().items()},\n        \"cfg\": dict(cfg),\n        \"seed\": int(seed),\n        \"token_dim\": int(TOKEN_DIM),\n        \"token_hw\": tuple(map(int, TOKEN_HW)),\n        \"train_rows\": int(len(df_all)),\n        \"pos_rate\": float(df_all[\"y\"].mean()),\n        \"epochs_fixed\": int(epochs_fixed),\n        \"accum_steps\": int(accum),\n        \"train_time_s\": float(time.time() - t0),\n        \"used_ema_weights\": bool(ema is not None),\n    }\n    return pack\n\n# ----------------------------\n# 15) OOM fallback\n# ----------------------------\ndef apply_oom_fallback(cfg: dict):\n    cfg = dict(cfg)\n    cfg[\"batch_size\"] = max(4, int(cfg[\"batch_size\"]) // 2)\n    cfg[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(cfg[\"batch_size\"]))))\n    cfg[\"base_ch\"] = max(96, int(cfg[\"base_ch\"]) - 32)\n    cfg[\"drop\"] = min(0.25, float(cfg[\"drop\"]) + 0.02)\n    return cfg\n\n# ----------------------------\n# 16) Train final (OOM-safe)\n# ----------------------------\nfinal_full_packs = []\ninternal_val_infos = []\n\nt_global = time.time()\n\nfor s in range(int(N_SEEDS)):\n    seed_i = FINAL_SEED + s\n    print(f\"\\n[Final MASK Train] seed={seed_i}\")\n\n    cfg_run = dict(CFG)\n    cfg_run[\"seed\"] = int(seed_i)\n\n    for attempt in range(6):\n        try:\n            # Phase A: best epoch + thr\n            if USE_INTERNAL_VAL:\n                info = train_with_internal_val_get_best(df_train_filtered, cfg_run, seed=seed_i)\n                best_epoch = int(info[\"epoch\"])\n                best_thr = float(info[\"thr\"])\n                internal_val_infos.append({\"seed\": seed_i, \"best_epoch\": best_epoch, \"best_thr\": best_thr, **info, \"cfg_used\": dict(cfg_run)})\n\n                E_FULL = int(min(int(cfg_run[\"epochs\"]), max(8, round(best_epoch * 1.05))))\n                print(f\"\\nBest_epoch={best_epoch} | best_thr={best_thr:.3f} -> Retrain FULL for E_FULL={E_FULL}\")\n            else:\n                best_thr = float(best_bundle.get(\"recommended_thr\", 0.5)) if isinstance(best_bundle, dict) else 0.5\n                E_FULL = int(cfg_run[\"epochs\"])\n                internal_val_infos.append({\"seed\": seed_i, \"best_epoch\": None, \"best_thr\": best_thr, \"cfg_used\": dict(cfg_run)})\n\n            full_pack = train_full_fixed_epochs(df_train_filtered, cfg_run, epochs_fixed=E_FULL, seed=seed_i)\n            final_full_packs.append(full_pack)\n            break\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n            if (\"out of memory\" in msg) and device.type == \"cuda\":\n                print(f\"  OOM detected (attempt {attempt+1}). Applying fallback.\")\n                torch.cuda.empty_cache()\n                cfg_run = apply_oom_fallback(cfg_run)\n                print(\"  New CFG after fallback:\")\n                for k in [\"base_ch\",\"batch_size\",\"accum_steps\",\"epochs\",\"drop\"]:\n                    print(f\"    {k}: {cfg_run[k]}\")\n                continue\n            raise\n\n    gc.collect()\n    if device.type == \"cuda\":\n        torch.cuda.empty_cache()\n\nif len(final_full_packs) == 0:\n    raise RuntimeError(\"Final training failed: no full_packs produced.\")\n\n# recommended_thr: prefer internal val median if available, else from Step4 bundle, else 0.5\nrecommended_thr = None\nif len(internal_val_infos) > 0 and all((\"best_thr\" in x and x[\"best_thr\"] is not None) for x in internal_val_infos):\n    recommended_thr = float(np.median([float(x[\"best_thr\"]) for x in internal_val_infos]))\nelif isinstance(best_bundle, dict) and best_bundle.get(\"recommended_thr\", None) is not None:\n    recommended_thr = float(best_bundle[\"recommended_thr\"])\nelse:\n    recommended_thr = 0.5\n\n# ----------------------------\n# 17) Save artifacts\n# ----------------------------\nfinal_model_path = OUT_DIR / \"final_mask_model.pt\"\n\ntorch.save(\n    {\n        \"type\": \"final_mask_v1\",\n        \"arch\": \"UNetASPP_on_DINOv2TokenGrid\",\n        \"token_dim\": int(TOKEN_DIM),\n        \"token_hw\": tuple(map(int, TOKEN_HW)),\n\n        # keep fold ensemble from Step 4 if available\n        \"fold_packs\": fold_packs_from_step4,\n\n        # full-data trained packs (list; usually 1 seed)\n        \"full_packs\": final_full_packs,\n\n        # training meta\n        \"internal_val_infos\": internal_val_infos,\n        \"recommended_thr\": float(recommended_thr),\n        \"bundle_source\": source,\n        \"seed_base\": int(FINAL_SEED),\n        \"train_time_total_s\": float(time.time() - t_global),\n    },\n    final_model_path\n)\n\nfinal_bundle = {\n    \"type\": \"final_mask_v1\",\n    \"arch\": \"UNetASPP_on_DINOv2TokenGrid\",\n    \"token_dim\": int(TOKEN_DIM),\n    \"token_hw\": tuple(map(int, TOKEN_HW)),\n\n    \"n_seeds\": int(len(final_full_packs)),\n    \"seeds\": [int(p[\"seed\"]) for p in final_full_packs],\n    \"cfg_final\": dict(CFG),\n\n    \"use_internal_val\": bool(USE_INTERNAL_VAL),\n    \"val_frac_case\": float(VAL_FRAC_CASE) if USE_INTERNAL_VAL else 0.0,\n    \"early_stop\": bool(EARLY_STOP) if USE_INTERNAL_VAL else False,\n\n    \"train_rows\": int(len(df_train_filtered)),\n    \"pos_rate\": float(df_train_filtered[\"y\"].mean()),\n\n    \"recommended_thr\": float(recommended_thr),\n    \"has_fold_packs_from_step4\": bool(fold_packs_from_step4 is not None),\n    \"best_cfg_source\": source,\n\n    \"paths\": {\n        \"final_model_pt\": str(final_model_path),\n        \"best_cfg_json\": str(cfg_path) if cfg_path.exists() else None,\n        \"best_model_pt\": str(best_mask_model_path) if best_mask_model_path is not None else None,\n    },\n\n    \"notes\": \"Final MASK model: internal case-level val -> pick best_epoch & thr -> retrain full; saves EMA weights.\",\n}\n\nfinal_bundle_path = OUT_DIR / \"final_mask_bundle.json\"\nfinal_bundle_path.write_text(json.dumps(final_bundle, indent=2))\n\nprint(\"\\nSaved final MASK training artifacts:\")\nprint(\"  model  ->\", final_model_path)\nprint(\"  bundle ->\", final_bundle_path)\nprint(\"  recommended_thr ->\", float(recommended_thr))\n\n# Export globals\nFINAL_MASK_MODEL_PT = str(final_model_path)\nFINAL_MASK_BUNDLE = final_bundle\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finalize & Save Model Bundle (Reproducible)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 6 — Finalize & Save Model Bundle (Notebook-3 / Inference Notebook)\n# REVISI FULL v5.2 (portable + robust discovery + SUPPORT Gate + Mask in one bundle)\n#\n# Upgrade utama vs v5.1:\n# - Auto-discover *both* final_gate_model.pt dan final_mask_model.pt (kalau ada)\n# - Output bundle tunggal (agar inference notebook bisa load Gate saja / Mask saja / Dual)\n# - Thresholds robust: resolve T_gate dan T_mask dengan priority bertingkat\n# - Manifest + pack menyimpan meta keduanya (sha256, cfg summary, token_hw/dim, feature_cols)\n#\n# Output (write):\n#   /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/\n#     - final_gate_model.pt (optional)\n#     - final_gate_bundle.json (optional)\n#     - feature_cols.json (required IF gate exists)\n#     - final_mask_model.pt (optional)\n#     - final_mask_bundle.json (optional)\n#     - thresholds.json (T_gate, T_mask)\n#     - model_bundle_manifest.json\n#     - model_bundle_pack.json (+ optional joblib)\n#     - model_bundle_v5_notebook3.zip\n# ============================================================\n\nimport os, json, time, platform, warnings, zipfile, hashlib, shutil\nfrom pathlib import Path\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# 0) IO roots\n# ----------------------------\nOUT_ROOT = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")   # keep as requested\nOUT_ROOT.mkdir(parents=True, exist_ok=True)\n\nBUNDLE_VERSION = \"v5_notebook3\"\nOUT_DIR = OUT_ROOT / f\"model_bundle_{BUNDLE_VERSION}\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"OUTPUT (write):\", OUT_DIR)\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef read_json_safe(p: Path, default=None):\n    try:\n        return json.loads(Path(p).read_text())\n    except Exception:\n        return default\n\ndef sha256_file(p: Path, chunk=1024 * 1024):\n    p = Path(p)\n    h = hashlib.sha256()\n    with p.open(\"rb\") as f:\n        while True:\n            b = f.read(chunk)\n            if not b:\n                break\n            h.update(b)\n    return h.hexdigest()\n\ndef file_meta(p: Path):\n    p = Path(p)\n    if not p.exists() or not p.is_file():\n        return None\n    st = p.stat()\n    return {\n        \"path\": str(p),\n        \"name\": p.name,\n        \"bytes\": int(st.st_size),\n        \"sha256\": sha256_file(p),\n        \"mtime_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(st.st_mtime)),\n    }\n\ndef safe_add(zf: zipfile.ZipFile, p: Path, arcname: str):\n    if p is None:\n        return\n    p = Path(p)\n    if p.exists() and p.is_file():\n        zf.write(p, arcname=arcname)\n\ndef pick_first_existing(paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(p)\n        if p.exists() and p.is_file():\n            return p\n    return None\n\ndef find_file_near(root: Path, filename: str, max_depth: int = 3):\n    root = Path(root)\n    if (root / filename).exists():\n        return root / filename\n\n    # depth 1\n    for p1 in root.glob(\"*\"):\n        if p1.is_dir() and (p1 / filename).exists():\n            return p1 / filename\n\n    if max_depth >= 2:\n        for p2 in root.glob(\"*/*\"):\n            if p2.is_dir() and (p2 / filename).exists():\n                return p2 / filename\n\n    if max_depth >= 3:\n        for p3 in root.glob(\"*/*/*\"):\n            if p3.is_dir() and (p3 / filename).exists():\n                return p3 / filename\n\n    return None\n\ndef copy_if_needed(src: Path, dst: Path, verbose=True):\n    src, dst = Path(src), Path(dst)\n    dst.parent.mkdir(parents=True, exist_ok=True)\n    if not src.exists():\n        raise FileNotFoundError(f\"Missing src: {src}\")\n\n    if dst.exists() and dst.is_file():\n        try:\n            if src.stat().st_size == dst.stat().st_size:\n                if sha256_file(src) == sha256_file(dst):\n                    if verbose:\n                        print(\"  [skip copy] already identical:\", dst.name)\n                    return dst\n        except Exception:\n            pass\n\n    shutil.copy2(src, dst)\n    if verbose:\n        print(\"  [copied] ->\", dst)\n    return dst\n\ndef _score_dir_gate(p: Path):\n    p = Path(p)\n    score = 0\n    if (p / \"final_gate_model.pt\").exists(): score += 100\n    if (p / \"final_gate_bundle.json\").exists(): score += 30\n    if (p / \"feature_cols.json\").exists(): score += 20\n    if (p / \"thresholds.json\").exists(): score += 5\n    if (p / \"best_gate_config.json\").exists(): score += 3\n    if (p / \"best_gate_model.pt\").exists(): score += 3\n    if (p / \"opt_search\" / \"opt_results.csv\").exists(): score += 1\n    return score\n\ndef _score_dir_mask(p: Path):\n    p = Path(p)\n    score = 0\n    if (p / \"final_mask_model.pt\").exists(): score += 100\n    if (p / \"final_mask_bundle.json\").exists(): score += 30\n    if (p / \"thresholds_mask.json\").exists(): score += 5\n    if (p / \"thresholds.json\").exists(): score += 2\n    if (p / \"best_mask_config.json\").exists(): score += 3\n    if (p / \"best_mask_model.pt\").exists(): score += 3\n    return score\n\ndef _bounded_dirs(ds: Path):\n    ds = Path(ds)\n    out = []\n    if not ds.exists() or not ds.is_dir():\n        return out\n    level0 = [ds]\n    level1 = [p for p in ds.glob(\"*\") if p.is_dir()]\n    level2 = [p for p in ds.glob(\"*/*\") if p.is_dir()]\n    level3 = [p for p in ds.glob(\"*/*/*\") if p.is_dir()]\n    return level0 + level1 + level2 + level3\n\ndef _gather_candidate_dirs(kind: str):\n    assert kind in [\"gate\", \"mask\"]\n    cands = []\n\n    # working roots\n    work_roots = [\n        Path(\"/kaggle/working/recodai_luc_gate_artifacts\"),\n        Path(\"/kaggle/working/recodai_luc_mask_artifacts\"),\n        Path(\"/kaggle/working\"),\n    ]\n    for base in work_roots:\n        if not base.exists():\n            continue\n        cands.append(base)\n        for sub in base.glob(\"model_bundle_*\"):\n            if sub.is_dir():\n                cands.append(sub)\n\n    # inputs (read-only datasets)\n    inp = Path(\"/kaggle/input\")\n    if inp.exists():\n        for ds in inp.iterdir():\n            if not ds.is_dir():\n                continue\n            for d in _bounded_dirs(ds):\n                name = d.name.lower()\n                if \"recodai\" in name or \"luc\" in name or \"bundle\" in name or \"artifacts\" in name:\n                    cands.append(d)\n                if kind == \"gate\" and (d / \"final_gate_model.pt\").exists():\n                    cands.append(d)\n                if kind == \"mask\" and (d / \"final_mask_model.pt\").exists():\n                    cands.append(d)\n                if d.name in [\"recodai_luc_gate_artifacts\", \"recodai_luc_mask_artifacts\"]:\n                    for sub in d.glob(\"model_bundle_*\"):\n                        if sub.is_dir():\n                            cands.append(sub)\n\n    # de-dup\n    seen = set()\n    out = []\n    for p in cands:\n        p = Path(p)\n        sp = str(p)\n        if sp not in seen and p.exists() and p.is_dir():\n            seen.add(sp)\n            out.append(p)\n    return out\n\ndef _pick_best_dir(kind: str):\n    cands = _gather_candidate_dirs(kind)\n    if not cands:\n        return None\n    if kind == \"gate\":\n        return max(cands, key=_score_dir_gate)\n    else:\n        return max(cands, key=_score_dir_mask)\n\n# ----------------------------\n# 1) Pick best SRC dirs (gate + mask)\n# ----------------------------\nSRC_DIR_GATE = _pick_best_dir(\"gate\")\nSRC_DIR_MASK = _pick_best_dir(\"mask\")\n\nprint(\"\\nSOURCE dirs picked:\")\nprint(\"  gate:\", SRC_DIR_GATE, \"| score:\", (_score_dir_gate(SRC_DIR_GATE) if SRC_DIR_GATE else None))\nprint(\"  mask:\", SRC_DIR_MASK, \"| score:\", (_score_dir_mask(SRC_DIR_MASK) if SRC_DIR_MASK else None))\n\n# ----------------------------\n# 2) Locate artifacts (robust) — Gate\n# ----------------------------\nfinal_gate_pt = None\nfinal_gate_bundle_json = None\nfeature_cols_path = None\nbest_gate_config_path = None\nbest_gate_model_path = None\nbaseline_report_path = None\nopt_results_csv = None\nopt_fold_csv = None\noof_baseline_csv = None\nMODEL_DIR_GATE = None\n\nif SRC_DIR_GATE is not None:\n    final_gate_pt = find_file_near(SRC_DIR_GATE, \"final_gate_model.pt\", max_depth=3)\n    if final_gate_pt is not None:\n        MODEL_DIR_GATE = final_gate_pt.parent\n        final_gate_bundle_json = find_file_near(MODEL_DIR_GATE, \"final_gate_bundle.json\", max_depth=2) \\\n                                 or find_file_near(SRC_DIR_GATE, \"final_gate_bundle.json\", max_depth=3)\n        feature_cols_path = find_file_near(MODEL_DIR_GATE, \"feature_cols.json\", max_depth=2) \\\n                            or find_file_near(SRC_DIR_GATE, \"feature_cols.json\", max_depth=3)\n\n        baseline_report_path = pick_first_existing([\n            find_file_near(MODEL_DIR_GATE, \"baseline_mhc_transformer_cv_report.json\", max_depth=2),\n            find_file_near(MODEL_DIR_GATE, \"baseline_transformer_cv_report.json\", max_depth=2),\n            find_file_near(MODEL_DIR_GATE, \"baseline_cv_report.json\", max_depth=2),\n            find_file_near(SRC_DIR_GATE,   \"baseline_mhc_transformer_cv_report.json\", max_depth=3),\n            find_file_near(SRC_DIR_GATE,   \"baseline_transformer_cv_report.json\", max_depth=3),\n            find_file_near(SRC_DIR_GATE,   \"baseline_cv_report.json\", max_depth=3),\n        ])\n\n        best_gate_config_path = pick_first_existing([\n            find_file_near(MODEL_DIR_GATE, \"best_gate_config.json\", max_depth=2),\n            find_file_near(SRC_DIR_GATE,   \"best_gate_config.json\", max_depth=3),\n        ])\n\n        best_gate_model_path = pick_first_existing([\n            find_file_near(MODEL_DIR_GATE, \"best_gate_model.pt\", max_depth=2),\n            find_file_near(SRC_DIR_GATE,   \"best_gate_model.pt\", max_depth=3),\n        ])\n\n        opt_results_csv = pick_first_existing([\n            find_file_near(MODEL_DIR_GATE, \"opt_results.csv\", max_depth=3),\n            find_file_near(SRC_DIR_GATE,   \"opt_results.csv\", max_depth=3),\n            (MODEL_DIR_GATE / \"opt_search\" / \"opt_results.csv\") if (MODEL_DIR_GATE / \"opt_search\" / \"opt_results.csv\").exists() else None,\n            (SRC_DIR_GATE   / \"opt_search\" / \"opt_results.csv\") if (SRC_DIR_GATE   / \"opt_search\" / \"opt_results.csv\").exists() else None,\n        ])\n\n        opt_fold_csv = pick_first_existing([\n            find_file_near(MODEL_DIR_GATE, \"opt_fold_details.csv\", max_depth=3),\n            find_file_near(SRC_DIR_GATE,   \"opt_fold_details.csv\", max_depth=3),\n            (MODEL_DIR_GATE / \"opt_search\" / \"opt_fold_details.csv\") if (MODEL_DIR_GATE / \"opt_search\" / \"opt_fold_details.csv\").exists() else None,\n            (SRC_DIR_GATE   / \"opt_search\" / \"opt_fold_details.csv\") if (SRC_DIR_GATE   / \"opt_search\" / \"opt_fold_details.csv\").exists() else None,\n        ])\n\n        oof_baseline_csv = pick_first_existing([\n            find_file_near(MODEL_DIR_GATE, \"oof_baseline_mhc_transformer.csv\", max_depth=2),\n            find_file_near(MODEL_DIR_GATE, \"oof_baseline_transformer.csv\", max_depth=2),\n            find_file_near(MODEL_DIR_GATE, \"oof_baseline.csv\", max_depth=2),\n            find_file_near(SRC_DIR_GATE,   \"oof_baseline_mhc_transformer.csv\", max_depth=3),\n            find_file_near(SRC_DIR_GATE,   \"oof_baseline_transformer.csv\", max_depth=3),\n            find_file_near(SRC_DIR_GATE,   \"oof_baseline.csv\", max_depth=3),\n        ])\n\n# ----------------------------\n# 2b) Locate artifacts (robust) — Mask\n# ----------------------------\nfinal_mask_pt = None\nfinal_mask_bundle_json = None\nbest_mask_config_path = None\nbest_mask_model_path = None\nMODEL_DIR_MASK = None\nsrc_thresh_mask = None\n\nif SRC_DIR_MASK is not None:\n    final_mask_pt = find_file_near(SRC_DIR_MASK, \"final_mask_model.pt\", max_depth=3)\n    if final_mask_pt is not None:\n        MODEL_DIR_MASK = final_mask_pt.parent\n        final_mask_bundle_json = find_file_near(MODEL_DIR_MASK, \"final_mask_bundle.json\", max_depth=2) \\\n                                 or find_file_near(SRC_DIR_MASK, \"final_mask_bundle.json\", max_depth=3)\n\n        best_mask_config_path = pick_first_existing([\n            find_file_near(MODEL_DIR_MASK, \"best_mask_config.json\", max_depth=2),\n            find_file_near(SRC_DIR_MASK,   \"best_mask_config.json\", max_depth=3),\n        ])\n\n        best_mask_model_path = pick_first_existing([\n            find_file_near(MODEL_DIR_MASK, \"best_mask_model.pt\", max_depth=2),\n            find_file_near(SRC_DIR_MASK,   \"best_mask_model.pt\", max_depth=3),\n        ])\n\n        src_thresh_mask = find_file_near(MODEL_DIR_MASK, \"thresholds_mask.json\", max_depth=2) \\\n                          or find_file_near(SRC_DIR_MASK, \"thresholds_mask.json\", max_depth=3)\n\n# ----------------------------\n# 2c) Hard requirement check\n# ----------------------------\nif final_gate_pt is None and final_mask_pt is None:\n    raise FileNotFoundError(\n        \"Tidak menemukan final_gate_model.pt maupun final_mask_model.pt di /kaggle/working atau /kaggle/input.\\n\"\n        \"Pastikan kamu sudah add dataset output training ke notebook ini.\"\n    )\n\nprint(\"\\nFound artifacts (read):\")\nprint(\"  [GATE] final_model      :\", final_gate_pt if final_gate_pt else \"(missing)\")\nprint(\"  [GATE] final_bundle     :\", final_gate_bundle_json if final_gate_bundle_json else \"(missing/skip)\")\nprint(\"  [GATE] feature_cols     :\", feature_cols_path if feature_cols_path else \"(missing)\")\nprint(\"  [MASK] final_model      :\", final_mask_pt if final_mask_pt else \"(missing)\")\nprint(\"  [MASK] final_bundle     :\", final_mask_bundle_json if final_mask_bundle_json else \"(missing/skip)\")\nprint(\"  [MASK] thresholds_mask  :\", src_thresh_mask if src_thresh_mask else \"(missing/skip)\")\n\n# ----------------------------\n# 3) COPY core artifacts into OUT_DIR (portable bundle)\n# ----------------------------\nprint(\"\\nCopying core files into OUT_DIR (portable):\")\nextras_dir = OUT_DIR / \"extras\"\nextras_dir.mkdir(parents=True, exist_ok=True)\nopt_dir = OUT_DIR / \"opt_search\"\nopt_dir.mkdir(parents=True, exist_ok=True)\noof_dir = OUT_DIR / \"oof\"\noof_dir.mkdir(parents=True, exist_ok=True)\n\nfinal_gate_dst = None\nfinal_gate_bundle_dst = None\nfeature_cols_dst = None\n\nif final_gate_pt is not None:\n    final_gate_dst = copy_if_needed(final_gate_pt, OUT_DIR / \"final_gate_model.pt\")\n    if final_gate_bundle_json is not None and final_gate_bundle_json.exists():\n        final_gate_bundle_dst = copy_if_needed(final_gate_bundle_json, OUT_DIR / \"final_gate_bundle.json\")\n    if feature_cols_path is None or not feature_cols_path.exists():\n        raise FileNotFoundError(\"Gate ditemukan, tapi feature_cols.json tidak ditemukan. Pastikan Step 2 ikut dibundle.\")\n    feature_cols_dst = copy_if_needed(feature_cols_path, OUT_DIR / \"feature_cols.json\")\n\n    # optional extras\n    if baseline_report_path:\n        copy_if_needed(baseline_report_path, extras_dir / Path(baseline_report_path).name, verbose=False)\n    if best_gate_config_path:\n        copy_if_needed(best_gate_config_path, extras_dir / Path(best_gate_config_path).name, verbose=False)\n    if best_gate_model_path:\n        copy_if_needed(best_gate_model_path, extras_dir / Path(best_gate_model_path).name, verbose=False)\n    if opt_results_csv:\n        copy_if_needed(opt_results_csv, opt_dir / Path(opt_results_csv).name, verbose=False)\n    if opt_fold_csv:\n        copy_if_needed(opt_fold_csv, opt_dir / Path(opt_fold_csv).name, verbose=False)\n    if oof_baseline_csv:\n        copy_if_needed(oof_baseline_csv, oof_dir / Path(oof_baseline_csv).name, verbose=False)\n\nfinal_mask_dst = None\nfinal_mask_bundle_dst = None\nthresholds_mask_dst = None\n\nif final_mask_pt is not None:\n    final_mask_dst = copy_if_needed(final_mask_pt, OUT_DIR / \"final_mask_model.pt\")\n    if final_mask_bundle_json is not None and final_mask_bundle_json.exists():\n        final_mask_bundle_dst = copy_if_needed(final_mask_bundle_json, OUT_DIR / \"final_mask_bundle.json\")\n    if best_mask_config_path:\n        copy_if_needed(best_mask_config_path, extras_dir / Path(best_mask_config_path).name, verbose=False)\n    if best_mask_model_path:\n        copy_if_needed(best_mask_model_path, extras_dir / Path(best_mask_model_path).name, verbose=False)\n    if src_thresh_mask and src_thresh_mask.exists():\n        thresholds_mask_dst = copy_if_needed(src_thresh_mask, OUT_DIR / \"thresholds_mask.json\", verbose=False)\n\n# ----------------------------\n# 4) Load metadata (from copied files)\n# ----------------------------\ngate_feature_cols = []\ngate_final_bundle = {}\nmask_final_bundle = {}\nbest_gate_config = None\nbest_mask_config = None\n\nif feature_cols_dst is not None:\n    gate_feature_cols = read_json_safe(feature_cols_dst, default=[])\n    if not isinstance(gate_feature_cols, list) or len(gate_feature_cols) == 0:\n        raise ValueError(f\"feature_cols invalid/empty: {feature_cols_dst}\")\n\nif final_gate_bundle_dst is not None:\n    gate_final_bundle = read_json_safe(final_gate_bundle_dst, default={}) or {}\n\nif final_mask_bundle_dst is not None:\n    mask_final_bundle = read_json_safe(final_mask_bundle_dst, default={}) or {}\n\nif best_gate_config_path:\n    best_gate_config = read_json_safe(best_gate_config_path, default=None)\nif best_mask_config_path:\n    best_mask_config = read_json_safe(best_mask_config_path, default=None)\n\n# recommended thr from pt(s)\nrecommended_thr_gate_from_pt = None\nrecommended_thr_mask_from_pt = None\ntoken_hw_from_mask_pt = None\ntoken_dim_from_mask_pt = None\n\ntry:\n    import torch\n    if final_gate_dst is not None:\n        objg = torch.load(final_gate_dst, map_location=\"cpu\")\n        if isinstance(objg, dict):\n            recommended_thr_gate_from_pt = objg.get(\"recommended_thr\", None)\n    if final_mask_dst is not None:\n        objm = torch.load(final_mask_dst, map_location=\"cpu\")\n        if isinstance(objm, dict):\n            recommended_thr_mask_from_pt = objm.get(\"recommended_thr\", None)\n            token_hw_from_mask_pt = objm.get(\"token_hw\", None)\n            token_dim_from_mask_pt = objm.get(\"token_dim\", None)\nexcept Exception as e:\n    print(\"Warning: failed to read *.pt metadata:\", repr(e))\n\n# ----------------------------\n# 5) Threshold resolve (T_gate + T_mask)\n# ----------------------------\ndef extract_thr_from_best_cfg(cfg: dict):\n    if not isinstance(cfg, dict):\n        return None\n    if cfg.get(\"oof_best_thr\", None) is not None:\n        try:\n            return float(cfg[\"oof_best_thr\"])\n        except Exception:\n            pass\n    sel = cfg.get(\"selection\", None)\n    if isinstance(sel, dict) and sel.get(\"oof_best_thr\", None) is not None:\n        try:\n            return float(sel[\"oof_best_thr\"])\n        except Exception:\n            pass\n    # some configs store \"recommended_thr\"\n    if cfg.get(\"recommended_thr\", None) is not None:\n        try:\n            return float(cfg[\"recommended_thr\"])\n        except Exception:\n            pass\n    return None\n\n# Gate threshold priority:\n# (a) SRC thresholds.json near gate model dir\n# (b) OUT_DIR thresholds.json if rerun\n# (c) final_gate_bundle.json recommended_thr\n# (d) final_gate_model.pt recommended_thr\n# (e) best_gate_config.json oof_best_thr\n# (f) 0.5\nT_gate = None\nsrc_thresh_gate = None\nif MODEL_DIR_GATE is not None:\n    src_thresh_gate = find_file_near(MODEL_DIR_GATE, \"thresholds.json\", max_depth=2) or \\\n                      (find_file_near(SRC_DIR_GATE, \"thresholds.json\", max_depth=3) if SRC_DIR_GATE else None)\n\nout_thresh_path = OUT_DIR / \"thresholds.json\"\nexisting_out_thresh = read_json_safe(out_thresh_path, default=None) if out_thresh_path.exists() else None\n\nif src_thresh_gate and src_thresh_gate.exists():\n    tj = read_json_safe(src_thresh_gate, default={})\n    if isinstance(tj, dict) and tj.get(\"T_gate\", None) is not None:\n        try:\n            T_gate = float(tj[\"T_gate\"])\n        except Exception:\n            T_gate = None\n\nif T_gate is None and isinstance(existing_out_thresh, dict) and existing_out_thresh.get(\"T_gate\", None) is not None:\n    try:\n        T_gate = float(existing_out_thresh[\"T_gate\"])\n    except Exception:\n        T_gate = None\n\nif T_gate is None and isinstance(gate_final_bundle, dict) and gate_final_bundle.get(\"recommended_thr\", None) is not None:\n    try:\n        T_gate = float(gate_final_bundle[\"recommended_thr\"])\n    except Exception:\n        T_gate = None\n\nif T_gate is None and recommended_thr_gate_from_pt is not None:\n    try:\n        T_gate = float(recommended_thr_gate_from_pt)\n    except Exception:\n        T_gate = None\n\nif T_gate is None and isinstance(best_gate_config, dict):\n    T_gate = extract_thr_from_best_cfg(best_gate_config)\n\nif T_gate is None:\n    T_gate = 0.5\n\n# Mask threshold priority:\n# (a) thresholds_mask.json near mask model dir\n# (b) thresholds.json existing OUT_DIR (T_mask)\n# (c) final_mask_bundle.json recommended_thr\n# (d) final_mask_model.pt recommended_thr\n# (e) best_mask_config.json oof_best_thr/recommended_thr\n# (f) 0.5\nT_mask = None\nif thresholds_mask_dst is not None and thresholds_mask_dst.exists():\n    tj = read_json_safe(thresholds_mask_dst, default={})\n    if isinstance(tj, dict) and tj.get(\"T_mask\", None) is not None:\n        try:\n            T_mask = float(tj[\"T_mask\"])\n        except Exception:\n            T_mask = None\n    elif isinstance(tj, dict) and tj.get(\"recommended_thr\", None) is not None:\n        try:\n            T_mask = float(tj[\"recommended_thr\"])\n        except Exception:\n            T_mask = None\n\nif T_mask is None and isinstance(existing_out_thresh, dict) and existing_out_thresh.get(\"T_mask\", None) is not None:\n    try:\n        T_mask = float(existing_out_thresh[\"T_mask\"])\n    except Exception:\n        T_mask = None\n\nif T_mask is None and isinstance(mask_final_bundle, dict) and mask_final_bundle.get(\"recommended_thr\", None) is not None:\n    try:\n        T_mask = float(mask_final_bundle[\"recommended_thr\"])\n    except Exception:\n        T_mask = None\n\nif T_mask is None and recommended_thr_mask_from_pt is not None:\n    try:\n        T_mask = float(recommended_thr_mask_from_pt)\n    except Exception:\n        T_mask = None\n\nif T_mask is None and isinstance(best_mask_config, dict):\n    T_mask = extract_thr_from_best_cfg(best_mask_config)\n\nif T_mask is None:\n    T_mask = 0.5\n\nthresholds = {\n    \"T_gate\": float(T_gate) if final_gate_dst is not None else None,\n    \"T_mask\": float(T_mask) if final_mask_dst is not None else None,\n    \"beta_for_tuning\": float(best_gate_config.get(\"beta_for_tuning\", 0.5)) if isinstance(best_gate_config, dict) else 0.5,\n    \"guards\": {\n        \"min_area_frac\": None,\n        \"max_area_frac\": None,\n        \"max_components\": None,\n    },\n    \"source_priority\": {\n        \"T_gate\": [\n            \"SRC thresholds.json (near gate model)\",\n            \"OUT_DIR/thresholds.json (existing)\",\n            \"final_gate_bundle.json.recommended_thr\",\n            \"final_gate_model.pt.recommended_thr\",\n            \"best_gate_config.json.oof_best_thr (or selection.oof_best_thr legacy)\",\n            \"fallback 0.5\",\n        ],\n        \"T_mask\": [\n            \"SRC thresholds_mask.json (near mask model)\",\n            \"OUT_DIR/thresholds.json (existing)\",\n            \"final_mask_bundle.json.recommended_thr\",\n            \"final_mask_model.pt.recommended_thr\",\n            \"best_mask_config.json oof_best_thr/recommended_thr\",\n            \"fallback 0.5\",\n        ],\n    },\n    \"notes\": \"Thresholds used in inference. Update after calibration/OOF tuning if needed.\",\n}\n\nthresholds_path = OUT_DIR / \"thresholds.json\"\nthresholds_path.write_text(json.dumps(thresholds, indent=2))\n\nprint(\"\\nThreshold resolved:\")\nprint(\"  T_gate:\", thresholds[\"T_gate\"])\nprint(\"  T_mask:\", thresholds[\"T_mask\"])\nprint(\"  thresholds.json ->\", thresholds_path)\n\n# ----------------------------\n# 6) cfg_meta (optional)\n# ----------------------------\ncfg_meta = {}\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    cfg_meta = {k: PATHS.get(k, None) for k in [\n        \"COMP_ROOT\",\"OUT_DS_ROOT\",\"OUT_ROOT\",\"MATCH_CFG_DIR\",\"PRED_CFG_DIR\",\"DINO_CFG_DIR\",\"DINO_LARGE_DIR\",\n        \"PRED_FEAT_TRAIN\",\"MATCH_FEAT_TRAIN\",\"DF_TRAIN_ALL\",\"CV_CASE_FOLDS\",\"IMG_PROFILE_TRAIN\"\n    ]}\n\n# ----------------------------\n# 7) Manifest (reproducible) + sha256 index\n# ----------------------------\ntask_str = \"Recod.ai/LUC — Portable Bundle — (optional) Gate + (optional) Mask\"\nmodel_format = \"torch_pt\"\n\nartifact_paths = [\n    final_gate_dst,\n    final_gate_bundle_dst,\n    feature_cols_dst,\n    final_mask_dst,\n    final_mask_bundle_dst,\n    thresholds_mask_dst,\n    thresholds_path,\n]\n# extras\nfor p in extras_dir.glob(\"*\"):\n    artifact_paths.append(p)\nfor p in opt_dir.glob(\"*\"):\n    artifact_paths.append(p)\nfor p in oof_dir.glob(\"*\"):\n    artifact_paths.append(p)\n\nartifact_paths = [p for p in artifact_paths if p is not None]\n\nartifacts_meta = {}\nfor p in artifact_paths:\n    m = file_meta(p)\n    if m is not None:\n        artifacts_meta[m[\"name\"]] = m\n\nmanifest = {\n    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"python\": platform.python_version(),\n    \"platform\": platform.platform(),\n    \"bundle_version\": BUNDLE_VERSION,\n    \"task\": task_str,\n    \"model_format\": model_format,\n\n    \"source_dirs\": {\n        \"gate\": str(SRC_DIR_GATE) if SRC_DIR_GATE else None,\n        \"mask\": str(SRC_DIR_MASK) if SRC_DIR_MASK else None,\n    },\n    \"source_model_dirs\": {\n        \"gate\": str(MODEL_DIR_GATE) if MODEL_DIR_GATE else None,\n        \"mask\": str(MODEL_DIR_MASK) if MODEL_DIR_MASK else None,\n    },\n    \"output_dir\": str(OUT_DIR),\n\n    \"artifacts_index\": artifacts_meta,\n    \"cfg_meta\": cfg_meta,\n\n    \"summary_gate\": {\n        \"present\": bool(final_gate_dst is not None),\n        \"type\": (gate_final_bundle.get(\"type\") if isinstance(gate_final_bundle, dict) else None),\n        \"n_seeds\": (gate_final_bundle.get(\"n_seeds\") if isinstance(gate_final_bundle, dict) else None),\n        \"seeds\": (gate_final_bundle.get(\"seeds\") if isinstance(gate_final_bundle, dict) else None),\n        \"train_rows\": (gate_final_bundle.get(\"train_rows\") if isinstance(gate_final_bundle, dict) else None),\n        \"pos_rate\": (gate_final_bundle.get(\"pos_rate\") if isinstance(gate_final_bundle, dict) else None),\n        \"feature_count\": int(len(gate_feature_cols)) if gate_feature_cols else None,\n        \"T_gate\": thresholds.get(\"T_gate\", None),\n        \"recommended_thr_from_pt\": recommended_thr_gate_from_pt,\n    },\n    \"summary_mask\": {\n        \"present\": bool(final_mask_dst is not None),\n        \"type\": (mask_final_bundle.get(\"type\") if isinstance(mask_final_bundle, dict) else None),\n        \"train_rows\": (mask_final_bundle.get(\"train_rows\") if isinstance(mask_final_bundle, dict) else None),\n        \"pos_rate\": (mask_final_bundle.get(\"pos_rate\") if isinstance(mask_final_bundle, dict) else None),\n        \"token_hw_from_pt\": token_hw_from_mask_pt,\n        \"token_dim_from_pt\": token_dim_from_mask_pt,\n        \"T_mask\": thresholds.get(\"T_mask\", None),\n        \"recommended_thr_from_pt\": recommended_thr_mask_from_pt,\n    },\n}\n\nmanifest_path = OUT_DIR / \"model_bundle_manifest.json\"\nmanifest_path.write_text(json.dumps(manifest, indent=2))\n\n# ----------------------------\n# 8) Bundle pack (portable JSON) + optional joblib\n# ----------------------------\nbundle_pack = {\n    \"bundle_version\": BUNDLE_VERSION,\n    \"model_format\": model_format,\n    \"bundle_files\": {\n        \"final_gate_model.pt\": \"final_gate_model.pt\" if final_gate_dst is not None else None,\n        \"final_gate_bundle.json\": \"final_gate_bundle.json\" if final_gate_bundle_dst is not None else None,\n        \"feature_cols.json\": \"feature_cols.json\" if feature_cols_dst is not None else None,\n\n        \"final_mask_model.pt\": \"final_mask_model.pt\" if final_mask_dst is not None else None,\n        \"final_mask_bundle.json\": \"final_mask_bundle.json\" if final_mask_bundle_dst is not None else None,\n        \"thresholds_mask.json\": \"thresholds_mask.json\" if thresholds_mask_dst is not None else None,\n\n        \"thresholds.json\": \"thresholds.json\",\n        \"model_bundle_manifest.json\": \"model_bundle_manifest.json\",\n        \"model_bundle_pack.json\": \"model_bundle_pack.json\",\n        \"model_bundle_pack.joblib\": \"model_bundle_pack.joblib\",\n        \"model_bundle_v5_notebook3.zip\": \"model_bundle_v5_notebook3.zip\",\n    },\n    \"thresholds\": thresholds,\n    \"feature_cols\": gate_feature_cols if gate_feature_cols else None,\n    \"token_meta\": {\n        \"token_hw\": token_hw_from_mask_pt,\n        \"token_dim\": token_dim_from_mask_pt,\n    } if final_mask_dst is not None else None,\n    \"cfg_meta\": cfg_meta,\n    \"manifest\": manifest,\n}\n\nbundle_pack_json = OUT_DIR / \"model_bundle_pack.json\"\nbundle_pack_json.write_text(json.dumps(bundle_pack, indent=2))\n\nbundle_pack_joblib = OUT_DIR / \"model_bundle_pack.joblib\"\njoblib_ok = False\ntry:\n    import joblib\n    joblib.dump(bundle_pack, bundle_pack_joblib)\n    joblib_ok = True\nexcept Exception:\n    joblib_ok = False\n\n# ----------------------------\n# 9) Create portable ZIP (writes to OUT_DIR)\n# ----------------------------\nzip_path = OUT_DIR / \"model_bundle_v5_notebook3.zip\"\n\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n    # core\n    safe_add(zf, final_gate_dst, \"final_gate_model.pt\")\n    safe_add(zf, final_gate_bundle_dst, \"final_gate_bundle.json\")\n    safe_add(zf, feature_cols_dst, \"feature_cols.json\")\n\n    safe_add(zf, final_mask_dst, \"final_mask_model.pt\")\n    safe_add(zf, final_mask_bundle_dst, \"final_mask_bundle.json\")\n    safe_add(zf, thresholds_mask_dst, \"thresholds_mask.json\")\n\n    safe_add(zf, thresholds_path, \"thresholds.json\")\n    safe_add(zf, manifest_path, \"model_bundle_manifest.json\")\n    safe_add(zf, bundle_pack_json, \"model_bundle_pack.json\")\n    if joblib_ok:\n        safe_add(zf, bundle_pack_joblib, \"model_bundle_pack.joblib\")\n\n    # extras\n    if extras_dir.exists():\n        for p in extras_dir.glob(\"*\"):\n            safe_add(zf, p, f\"extras/{p.name}\")\n\n    # opt_search\n    if opt_dir.exists():\n        for p in opt_dir.glob(\"*\"):\n            safe_add(zf, p, f\"opt_search/{p.name}\")\n\n    # oof\n    if oof_dir.exists():\n        for p in oof_dir.glob(\"*\"):\n            safe_add(zf, p, f\"oof/{p.name}\")\n\nprint(\"\\nOK — Model bundle finalized (Notebook-3 compatible)\")\nprint(\"  OUT_DIR          ->\", OUT_DIR)\nprint(\"  manifest         ->\", manifest_path)\nprint(\"  pack (json)      ->\", bundle_pack_json)\nprint(\"  pack (joblib)    ->\", (bundle_pack_joblib if joblib_ok else \"(skip; joblib not available)\"))\nprint(\"  thresholds       ->\", thresholds_path)\nprint(\"  zip              ->\", zip_path)\n\nprint(\"\\nBundle summary:\")\nprint(\"  bundle_version:\", BUNDLE_VERSION)\nprint(\"  model_format  :\", model_format)\nprint(\"  has_gate      :\", final_gate_dst is not None)\nprint(\"  has_mask      :\", final_mask_dst is not None)\nprint(\"  feature_cnt   :\", (len(gate_feature_cols) if gate_feature_cols else 0))\nprint(\"  T_gate        :\", thresholds.get(\"T_gate\"))\nprint(\"  T_mask        :\", thresholds.get(\"T_mask\"))\nprint(\"  task          :\", task_str)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}