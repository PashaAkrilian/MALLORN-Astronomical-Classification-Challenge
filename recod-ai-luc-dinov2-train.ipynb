{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113558,"databundleVersionId":14878066,"sourceType":"competition"},{"sourceId":14387840,"sourceType":"datasetVersion","datasetId":9171323},{"sourceId":4535,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3327,"modelId":986},{"sourceId":4537,"sourceType":"modelInstanceVersion","modelInstanceId":3329,"modelId":986}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Paths & Select Config (CFG)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Set Paths & Select Config (CFG) (Kaggle-ready, offline)\n# REVISI FULL v2.2 (lebih kuat + bantu performa model lewat pemilihan CFG terbaik)\n#\n# Upgrade utama v2.2:\n# - Cache root multi-sumber: utamakan /kaggle/working/recodai_luc jika ada (hasil run terbaru),\n#   fallback ke dataset input (OUT_ROOT) — jadi training pakai artefak paling update.\n# - Pemilihan CFG pakai scoring (bukan cuma train rows):\n#     * wajib: train feature ada\n#     * prefer: test feature ada (match_features_test / pred_features_test)\n#     * tie-break: train rows, test rows, latest modified time\n# - Auto-detect DINOv2 model dir (large/giant/base) offline\n# - Tambah PATHS penting untuk TRAIN/INFER: manifest_pred_*, pred_summary.json, dll.\n# - Sanity guard lebih informatif + tidak nge-crash untuk file opsional.\n#\n# Output globals (tetap, JANGAN diganti namanya):\n# - COMP_ROOT, OUT_DS_ROOT, OUT_ROOT\n# - PATHS (dict)\n# - MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR\n# Extra globals (opsional, membantu tahap training):\n# - CACHE_ROOTS (list), SELECTED (dict), TRAIN_PLAN (dict)\n# ============================================================\n\nimport os, re, json, time\nfrom pathlib import Path\nimport pandas as pd\n\n# ----------------------------\n# Helper: fast count rows CSV\n# ----------------------------\ndef _fast_count_rows_csv(path: Path) -> int:\n    try:\n        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            n = sum(1 for _ in f) - 1\n        return int(max(n, 0))\n    except Exception:\n        return -1\n\ndef _safe_mtime(p: Path) -> float:\n    try:\n        return float(p.stat().st_mtime)\n    except Exception:\n        return 0.0\n\ndef _is_nonempty_file(p: Path) -> bool:\n    try:\n        return p.exists() and p.is_file() and p.stat().st_size > 0\n    except Exception:\n        return False\n\n# ----------------------------\n# Helper: find competition root\n# ----------------------------\ndef find_comp_root(preferred: str = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\") -> Path:\n    p = Path(preferred)\n    if p.exists():\n        return p\n\n    base = Path(\"/kaggle/input\")\n    if not base.exists():\n        raise FileNotFoundError(\"/kaggle/input tidak ditemukan (pastikan kamu di Kaggle Notebook).\")\n\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"sample_submission.csv\").exists() and ((d / \"train_images\").exists() or (d / \"test_images\").exists()):\n            cands.append(d)\n\n    if not cands:\n        for d in base.iterdir():\n            if not d.is_dir():\n                continue\n            for x in d.iterdir():\n                if not x.is_dir():\n                    continue\n                if (x / \"sample_submission.csv\").exists() and ((x / \"train_images\").exists() or (x / \"test_images\").exists()):\n                    cands.append(x)\n\n    if not cands:\n        raise FileNotFoundError(\n            \"COMP_ROOT tidak ditemukan. Harus ada folder yang memuat sample_submission.csv dan train_images/test_images.\"\n        )\n\n    cands.sort(key=lambda x: ((\"recodai\" not in x.name.lower()), (\"forgery\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: find output dataset root (hasil PREP)\n# ----------------------------\ndef find_output_dataset_root(preferred_names=(\n    \"recod-ailuc-dinov2-base\",\n    \"recod-ai-luc-dinov2-base\",\n    \"recodai-luc-dinov2-base\",\n    \"recodai-luc-dinov2\",\n    \"recodai-luc-dinov2-prep\",\n)) -> Path:\n    base = Path(\"/kaggle/input\")\n\n    for nm in preferred_names:\n        p = base / nm\n        if p.exists():\n            return p\n\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"recodai_luc\" / \"artifacts\").exists():\n            cands.append(d)\n            continue\n        inner = list(d.glob(\"*/recodai_luc/artifacts\"))\n        if inner:\n            cands.append(d)\n\n    if not cands:\n        raise FileNotFoundError(\"OUT_DS_ROOT tidak ditemukan. Harus ada /kaggle/input/<...>/recodai_luc/artifacts/\")\n\n    cands.sort(key=lambda x: ((\"dinov2\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: resolve OUT_ROOT = <dataset>/recodai_luc\n# ----------------------------\ndef resolve_out_root(out_ds_root: Path) -> Path:\n    direct = out_ds_root / \"recodai_luc\"\n    if direct.exists():\n        return direct\n    hits = list(out_ds_root.glob(\"*/recodai_luc\"))\n    if hits:\n        return hits[0]\n    raise FileNotFoundError(f\"Folder recodai_luc tidak ditemukan di bawah {out_ds_root}\")\n\n# ----------------------------\n# Helper: pick best cfg directory by multi-criteria scoring\n# ----------------------------\ndef pick_best_cfg(cache_roots, prefix: str, feat_train_filename: str, feat_test_filename: str = None) -> Path:\n    \"\"\"\n    cache_roots: list[Path] kandidat cache root\n    prefix: 'match_base_cfg_' atau 'pred_base'\n    feat_train_filename: file wajib\n    feat_test_filename : file opsional yang diprefer (kalau ada)\n    \"\"\"\n    cands = []\n    for root in cache_roots:\n        root = Path(root)\n        if not root.exists():\n            continue\n        for d in root.iterdir():\n            if not d.is_dir():\n                continue\n            if not d.name.startswith(prefix):\n                continue\n\n            train_fp = d / feat_train_filename\n            if not train_fp.exists():\n                continue\n\n            train_n = _fast_count_rows_csv(train_fp)\n            test_fp = (d / feat_test_filename) if feat_test_filename else None\n            test_ok = _is_nonempty_file(test_fp) if test_fp else False\n            test_n = _fast_count_rows_csv(test_fp) if (test_fp and test_fp.exists()) else -1\n\n            # latest time: pakai dir atau file train\n            mt = max(_safe_mtime(d), _safe_mtime(train_fp), _safe_mtime(test_fp) if test_fp else 0.0)\n\n            # scoring: prefer punya test file, lalu train rows besar, lalu test rows besar, lalu terbaru\n            score = 0.0\n            if train_n <= 0:\n                score -= 1e6\n            score += 1e5 * (1.0 if test_ok else 0.0)\n            score += 1.0 * float(max(train_n, 0))\n            score += 0.1 * float(max(test_n, 0))\n            score += 1e-6 * float(mt)\n\n            cands.append((score, d, root, train_fp, test_fp, train_n, test_n, test_ok, mt))\n\n    if not cands:\n        raise FileNotFoundError(\n            f\"Tidak ada CFG folder untuk prefix='{prefix}' dengan file '{feat_train_filename}' pada cache_roots={cache_roots}\"\n        )\n\n    cands.sort(key=lambda x: (-x[0], x[1].name))\n    best = cands[0]\n    return best[1]\n\n# ----------------------------\n# Helper: detect best DINO model dir (offline)\n# ----------------------------\ndef detect_dino_dir() -> Path:\n    # urutan prefer: large -> giant -> base\n    cands = [\n        Path(\"/kaggle/input/dinov2/pytorch/large/1\"),\n        Path(\"/kaggle/input/dinov2/pytorch/giant/1\"),\n        Path(\"/kaggle/input/dinov2/pytorch/base/1\"),\n    ]\n    for p in cands:\n        if p.exists():\n            return p\n    return Path(\"/kaggle/input/dinov2/pytorch/large/1\")  # default (mungkin missing; hanya warning)\n\n# ============================================================\n# 0) Locate roots\n# ============================================================\nCOMP_ROOT = find_comp_root(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\nOUT_DS_ROOT = find_output_dataset_root()\nOUT_ROOT = resolve_out_root(OUT_DS_ROOT)  # dataset input: .../recodai_luc\n\n# Kandidat cache/artifacts:\n# - hasil run terbaru di /kaggle/working/recodai_luc (kalau ada)\n# - fallback dataset input OUT_ROOT\nWORK_OUT_ROOT = Path(\"/kaggle/working/recodai_luc\")\nCACHE_ROOTS = []\nART_ROOTS = []\n\nif (WORK_OUT_ROOT / \"cache\").exists() and (WORK_OUT_ROOT / \"artifacts\").exists():\n    CACHE_ROOTS.append(WORK_OUT_ROOT / \"cache\")\n    ART_ROOTS.append(WORK_OUT_ROOT / \"artifacts\")\n\nCACHE_ROOTS.append(Path(OUT_ROOT) / \"cache\")\nART_ROOTS.append(Path(OUT_ROOT) / \"artifacts\")\n\n# pilih artifacts root yang valid (prioritas: working)\nART_DIR = None\nfor a in ART_ROOTS:\n    if a.exists():\n        ART_DIR = a\n        break\nif ART_DIR is None:\n    raise FileNotFoundError(\"ART_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n\n# pilih cache root list yang exist\nCACHE_DIRS = [p for p in CACHE_ROOTS if Path(p).exists()]\nif not CACHE_DIRS:\n    raise FileNotFoundError(\"CACHE_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n\n# ============================================================\n# 1) Competition paths (raw images/masks)\n# ============================================================\nPATHS = {}\nPATHS[\"COMP_ROOT\"] = str(COMP_ROOT)\nPATHS[\"SAMPLE_SUB\"] = str(COMP_ROOT / \"sample_submission.csv\")\n\nPATHS[\"TRAIN_IMAGES\"] = str(COMP_ROOT / \"train_images\")\nPATHS[\"TEST_IMAGES\"]  = str(COMP_ROOT / \"test_images\")\nPATHS[\"TRAIN_MASKS\"]  = str(COMP_ROOT / \"train_masks\")\nPATHS[\"SUPP_IMAGES\"]  = str(COMP_ROOT / \"supplemental_images\")\nPATHS[\"SUPP_MASKS\"]   = str(COMP_ROOT / \"supplemental_masks\")\n\nPATHS[\"TRAIN_AUTH_DIR\"] = str(COMP_ROOT / \"train_images\" / \"authentic\")\nPATHS[\"TRAIN_FORG_DIR\"] = str(COMP_ROOT / \"train_images\" / \"forged\")\n\n# ============================================================\n# 2) Output dataset paths (clean artifacts + cache)\n# ============================================================\n# NOTE: OUT_DS_ROOT/OUT_ROOT tetap menunjuk dataset input (untuk kompatibilitas),\n# tapi ART_DIR/CACHE_DIRS bisa berasal dari /kaggle/working jika ada (lebih baru).\nPATHS[\"OUT_DS_ROOT\"] = str(OUT_DS_ROOT)\nPATHS[\"OUT_ROOT\"]    = str(OUT_ROOT)\n\nPATHS[\"ART_DIR\"]     = str(ART_DIR)\nPATHS[\"CACHE_DIRS\"]  = [str(x) for x in CACHE_DIRS]  # list\n\n# artifacts utama\nPATHS[\"DF_TRAIN_ALL\"] = str(Path(ART_DIR) / \"df_train_all.parquet\")\nPATHS[\"DF_TRAIN_CLS\"] = str(Path(ART_DIR) / \"df_train_cls.parquet\")\nPATHS[\"DF_TRAIN_SEG\"] = str(Path(ART_DIR) / \"df_train_seg.parquet\")\nPATHS[\"DF_TEST\"]      = str(Path(ART_DIR) / \"df_test.parquet\")\n\nPATHS[\"CV_CASE_FOLDS\"]   = str(Path(ART_DIR) / \"cv_case_folds.csv\")\nPATHS[\"CV_SAMPLE_FOLDS\"] = str(Path(ART_DIR) / \"cv_sample_folds.csv\")\n\nPATHS[\"IMG_PROFILE_TRAIN\"] = str(Path(ART_DIR) / \"image_profile_train.parquet\")\nPATHS[\"IMG_PROFILE_TEST\"]  = str(Path(ART_DIR) / \"image_profile_test.parquet\")\nPATHS[\"MASK_PROFILE\"]      = str(Path(ART_DIR) / \"mask_profile.parquet\")\nPATHS[\"CASE_SUMMARY\"]      = str(Path(ART_DIR) / \"case_summary.parquet\")\n\n# ============================================================\n# 3) Select best MATCH/PRED CFG dirs automatically (scoring)\n# ============================================================\nMATCH_CFG_DIR = pick_best_cfg(\n    CACHE_DIRS,\n    prefix=\"match_base_cfg_\",\n    feat_train_filename=\"match_features_train_all.csv\",\n    feat_test_filename=\"match_features_test.csv\",\n)\n\nPRED_CFG_DIR = pick_best_cfg(\n    CACHE_DIRS,\n    prefix=\"pred_base\",\n    feat_train_filename=\"pred_features_train_all.csv\",\n    feat_test_filename=\"pred_features_test.csv\",  # diprefer jika ada\n)\n\n# DINO cache cfg (opsional): cache/dino_v2_large/cfg_*/manifest_train_all.csv\nDINO_CFG_DIR = None\nfor root in CACHE_DIRS:\n    dino_root = Path(root) / \"dino_v2_large\"\n    if not dino_root.exists():\n        continue\n    dino_cands = []\n    for d in dino_root.iterdir():\n        if d.is_dir() and d.name.startswith(\"cfg_\") and (d / \"manifest_train_all.csv\").exists():\n            dino_cands.append(d)\n    if dino_cands:\n        scored = []\n        for d in dino_cands:\n            mf = d / \"manifest_train_all.csv\"\n            scored.append((_fast_count_rows_csv(mf), _safe_mtime(d), d))\n        scored.sort(key=lambda x: (-x[0], -x[1], x[2].name))\n        DINO_CFG_DIR = scored[0][2]\n        break\n\n# simpan cfg dir ke PATHS\nPATHS[\"MATCH_CFG_DIR\"] = str(MATCH_CFG_DIR)\nPATHS[\"PRED_CFG_DIR\"]  = str(PRED_CFG_DIR)\nPATHS[\"DINO_CFG_DIR\"]  = str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\"\n\n# feature paths dari cfg terpilih\nPATHS[\"MATCH_FEAT_TRAIN\"] = str(MATCH_CFG_DIR / \"match_features_train_all.csv\")\nPATHS[\"MATCH_FEAT_TEST\"]  = str(MATCH_CFG_DIR / \"match_features_test.csv\")\n\nPATHS[\"PRED_FEAT_TRAIN\"]  = str(PRED_CFG_DIR / \"pred_features_train_all.csv\")\nPATHS[\"PRED_FEAT_TEST\"]   = str(PRED_CFG_DIR / \"pred_features_test.csv\")  # bisa missing (warning)\n\n# pred manifests (sering kepakai untuk infer/submission)\nPATHS[\"PRED_MAN_TRAIN\"] = str(PRED_CFG_DIR / \"manifest_pred_train_all.csv\")\nPATHS[\"PRED_MAN_TEST\"]  = str(PRED_CFG_DIR / \"manifest_pred_test.csv\")\nPATHS[\"PRED_SUMMARY\"]   = str(PRED_CFG_DIR / \"pred_summary.json\")\n\n# match manifests (kalau ada)\nPATHS[\"MATCH_MAN_TRAIN\"] = str(MATCH_CFG_DIR / \"manifest_match_train_all.csv\")\nPATHS[\"MATCH_MAN_TEST\"]  = str(MATCH_CFG_DIR / \"manifest_match_test.csv\")\n\n# ============================================================\n# 4) DINO model dir (offline)\n# ============================================================\nDINO_DIR = detect_dino_dir()\nPATHS[\"DINO_DIR\"] = str(DINO_DIR)\n\n# ============================================================\n# 5) Sanity checks (wajib ada untuk training)\n# ============================================================\nmust_exist = [\n    (\"sample_submission.csv\", PATHS[\"SAMPLE_SUB\"]),\n    (\"df_train_all.parquet\",  PATHS[\"DF_TRAIN_ALL\"]),\n    (\"cv_case_folds.csv\",     PATHS[\"CV_CASE_FOLDS\"]),\n    (\"match_features_train_all.csv\", PATHS[\"MATCH_FEAT_TRAIN\"]),\n    (\"pred_features_train_all.csv\",  PATHS[\"PRED_FEAT_TRAIN\"]),\n]\nmissing = [name for name, p in must_exist if not Path(p).exists()]\nif missing:\n    raise FileNotFoundError(\"Missing required files: \" + \", \".join(missing))\n\n# opsional tapi penting untuk inference: test feature files\nopt_warn = []\nif not Path(PATHS[\"MATCH_FEAT_TEST\"]).exists():\n    opt_warn.append(\"match_features_test.csv (MATCH_FEAT_TEST)\")\nif not Path(PATHS[\"PRED_FEAT_TEST\"]).exists():\n    opt_warn.append(\"pred_features_test.csv (PRED_FEAT_TEST)\")\nif opt_warn:\n    print(\"WARNING: File opsional untuk inference belum ada:\")\n    for w in opt_warn:\n        print(\" -\", w)\n    print(\"Catatan: training masih aman (pakai *_train_all). Untuk inference gate ke test, file ini biasanya dibutuhkan.\")\n\n# DINO model dir opsional (warning saja)\nif not Path(PATHS[\"DINO_DIR\"]).exists():\n    print(f\"WARNING: DINO dir tidak ditemukan: {PATHS['DINO_DIR']}\")\n\n# ============================================================\n# 6) Print summary + export helpers\n# ============================================================\nSELECTED = {\n    \"ART_DIR\": str(ART_DIR),\n    \"CACHE_DIRS\": [str(x) for x in CACHE_DIRS],\n    \"MATCH_CFG_DIR\": str(MATCH_CFG_DIR),\n    \"PRED_CFG_DIR\": str(PRED_CFG_DIR),\n    \"DINO_CFG_DIR\": str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\",\n    \"DINO_DIR\": str(DINO_DIR),\n}\n\n# Training plan (dipakai stage berikutnya supaya model bagus):\n# - Group CV by case_id (anti leakage)\nTRAIN_PLAN = {\n    \"seed\": 2025,\n    \"group_col\": \"case_id\",\n    \"target_col\": \"y_forged\",\n    \"n_folds\": 5,\n    \"use_calibration\": True,\n    \"calibration\": \"isotonic\",  # biasanya kuat untuk tabular probs\n    \"tune_threshold_on_oof\": True,\n}\n\nprint(\"OK — Roots\")\nprint(\"  COMP_ROOT   :\", COMP_ROOT)\nprint(\"  OUT_DS_ROOT :\", OUT_DS_ROOT)\nprint(\"  OUT_ROOT    :\", OUT_ROOT)\nprint(\"  ART_DIR(use):\", ART_DIR)\nprint(\"  CACHE_DIRS  :\", [str(x) for x in CACHE_DIRS])\n\nprint(\"\\nOK — Selected CFG\")\nprint(\"  MATCH_CFG_DIR:\", MATCH_CFG_DIR.name)\nprint(\"  PRED_CFG_DIR :\", PRED_CFG_DIR.name)\nprint(\"  DINO_CFG_DIR :\", (DINO_CFG_DIR.name if DINO_CFG_DIR else \"(not found / optional)\"))\n\nprint(\"\\nOK — Key files (train)\")\nfor k in [\"DF_TRAIN_ALL\", \"CV_CASE_FOLDS\", \"MATCH_FEAT_TRAIN\", \"PRED_FEAT_TRAIN\", \"IMG_PROFILE_TRAIN\"]:\n    p = Path(PATHS[k])\n    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing/optional)'}\")\n\nprint(\"\\nOK — Key files (test/infer, optional)\")\nfor k in [\"MATCH_FEAT_TEST\", \"PRED_FEAT_TEST\", \"PRED_MAN_TEST\", \"PRED_SUMMARY\"]:\n    p = Path(PATHS[k])\n    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing)'}\")\n\nprint(\"\\nOK — DINO model dir\")\nprint(\"  DINO_DIR:\", DINO_DIR, \"(exists)\" if DINO_DIR.exists() else \"(missing)\")\n\n# export globals\nglobals().update({\n    \"MATCH_CFG_DIR\": MATCH_CFG_DIR,\n    \"PRED_CFG_DIR\": PRED_CFG_DIR,\n    \"DINO_CFG_DIR\": DINO_CFG_DIR,\n    \"CACHE_ROOTS\": CACHE_DIRS,\n    \"SELECTED\": SELECTED,\n    \"TRAIN_PLAN\": TRAIN_PLAN,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:57:47.290162Z","iopub.execute_input":"2026-01-04T13:57:47.290513Z","iopub.status.idle":"2026-01-04T13:57:47.751165Z","shell.execute_reply.started":"2026-01-04T13:57:47.290482Z","shell.execute_reply":"2026-01-04T13:57:47.750276Z"}},"outputs":[{"name":"stdout","text":"OK — Roots\n  COMP_ROOT   : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n  OUT_DS_ROOT : /kaggle/input/recod-ailuc-dinov2-base\n  OUT_ROOT    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc\n\nOK — Selected CFG\n  MATCH_CFG_DIR: match_base_cfg_f9f7ea3a65c5\n  PRED_CFG_DIR : pred_base_v3_v7_cfg_5dbf0aa165\n  DINO_CFG_DIR : cfg_3246fd54aab0\n\nOK — Key files\n  DF_TRAIN_ALL    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet  (exists)\n  CV_CASE_FOLDS   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv  (exists)\n  MATCH_FEAT_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv  (exists)\n  PRED_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv  (exists)\n  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet  (exists)\n\nOK — DINO model dir\n  DINO_LARGE_DIR: /kaggle/input/dinov2/pytorch/large/1 (exists)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Build Training Table (X, y, folds)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 2 — Build Training Table (X, y, folds) — REVISI FULL v2.1 (Tabular/Gate-ready, robust)\n#\n# Fokus:\n# - Bangun df_train_tabular + FEATURE_COLS dari pred_features (+match_features/+image_profile opsional)\n# - Split pakai cv_case_folds.csv (anti leakage by case_id)\n#\n# Output globals:\n# - df_train_tabular, FEATURE_COLS\n# - X_train, y_train, folds\n#\n# Saved:\n# - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n# - /kaggle/working/recodai_luc_gate_artifacts/feature_schema.json\n# - /kaggle/working/recodai_luc_gate_artifacts/df_train_tabular.parquet\n# ============================================================\n\nimport os, json, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# 0) Require PATHS\n# ----------------------------\nif \"PATHS\" not in globals() or not isinstance(PATHS, dict):\n    raise RuntimeError(\"Missing PATHS. Jalankan dulu STAGE 0 — Set Paths & Select Config (CFG).\")\n\n# ----------------------------\n# 1) Feature Engineering Config\n# ----------------------------\nFE_CFG = {\n    \"use_match_features\": True,\n    \"use_image_profile\": True,\n\n    # encode variant -> dummies (sering bantu score)\n    \"encode_variant_onehot\": True,\n    \"variant_min_count\": 1,          # keep semua variant; naikkan kalau mau buang yang super-rare\n\n    # transforms\n    \"add_log_features\": True,\n    \"add_sqrt_features\": False,      # opsional\n    \"add_interactions\": True,\n    \"add_missing_indicators\": True,\n\n    # outlier control\n    \"clip_by_quantile\": True,\n    \"clip_q\": 0.999,                 # p99.9 cap\n    \"clip_max_fallback\": 1e9,\n\n    # fill\n    \"fillna_value\": 0.0,\n\n    # prune\n    \"drop_constant_features\": True,\n\n    # dtype\n    \"cast_float32\": True,\n}\n\n# ----------------------------\n# 2) Prefer WORKING features if exist (kalau kamu regen di /kaggle/working)\n# ----------------------------\ndef _prefer_existing(*paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(str(p))\n        if p.exists():\n            return p\n    return Path(str(paths[0])) if paths and paths[0] is not None else Path(\"\")\n\nmatch_cfg_name = Path(PATHS.get(\"MATCH_CFG_DIR\", \"\")).name if PATHS.get(\"MATCH_CFG_DIR\") else \"\"\npred_cfg_name  = Path(PATHS.get(\"PRED_CFG_DIR\", \"\")).name  if PATHS.get(\"PRED_CFG_DIR\") else \"\"\n\nWORK_CACHE_ROOT = Path(\"/kaggle/working/recodai_luc/cache\")\nmatch_feat_work = (WORK_CACHE_ROOT / match_cfg_name / \"match_features_train_all.csv\") if match_cfg_name else None\npred_feat_work  = (WORK_CACHE_ROOT / pred_cfg_name  / \"pred_features_train_all.csv\")  if pred_cfg_name  else None\n\nPRED_FEAT_TRAIN  = _prefer_existing(pred_feat_work,  PATHS.get(\"PRED_FEAT_TRAIN\", \"\"))\nMATCH_FEAT_TRAIN = _prefer_existing(match_feat_work, PATHS.get(\"MATCH_FEAT_TRAIN\", \"\"))\n\nDF_TRAIN_ALL      = Path(PATHS[\"DF_TRAIN_ALL\"])\nCV_CASE_FOLDS     = Path(PATHS[\"CV_CASE_FOLDS\"])\nIMG_PROFILE_TRAIN = Path(PATHS.get(\"IMG_PROFILE_TRAIN\", \"\"))\n\nfor need_name, need_path in [\n    (\"df_train_all.parquet\", DF_TRAIN_ALL),\n    (\"cv_case_folds.csv\", CV_CASE_FOLDS),\n    (\"pred_features_train_all.csv\", PRED_FEAT_TRAIN),\n]:\n    if not Path(need_path).exists():\n        raise FileNotFoundError(f\"Missing required file: {need_name} -> {need_path}\")\n\nprint(\"Using:\")\nprint(\"  DF_TRAIN_ALL     :\", DF_TRAIN_ALL)\nprint(\"  CV_CASE_FOLDS    :\", CV_CASE_FOLDS)\nprint(\"  PRED_FEAT_TRAIN  :\", PRED_FEAT_TRAIN)\nprint(\"  MATCH_FEAT_TRAIN :\", MATCH_FEAT_TRAIN, \"(optional)\" if Path(MATCH_FEAT_TRAIN).exists() else \"(missing/skip)\")\nprint(\"  IMG_PROFILE_TRAIN:\", IMG_PROFILE_TRAIN, \"(optional)\" if Path(IMG_PROFILE_TRAIN).exists() else \"(missing/skip)\")\n\n# ----------------------------\n# 3) Load minimal inputs\n# ----------------------------\n# df_train_all: ambil kolom minimal supaya hemat memori\ndf_base = pd.read_parquet(\n    DF_TRAIN_ALL,\n    columns=[c for c in [\"sample_id\", \"uid\", \"case_id\", \"variant\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]]\n    if True else None\n)\n\ndf_cv   = pd.read_csv(CV_CASE_FOLDS)\ndf_pred = pd.read_csv(PRED_FEAT_TRAIN, low_memory=False)\n\ndf_match = None\nif FE_CFG[\"use_match_features\"] and Path(MATCH_FEAT_TRAIN).exists():\n    try:\n        df_match = pd.read_csv(MATCH_FEAT_TRAIN, low_memory=False)\n    except Exception:\n        df_match = None\n\ndf_prof = None\nif FE_CFG[\"use_image_profile\"] and Path(IMG_PROFILE_TRAIN).exists():\n    try:\n        df_prof = pd.read_parquet(IMG_PROFILE_TRAIN)\n    except Exception:\n        df_prof = None\n\n# ----------------------------\n# 4) Utilities: normalize ids and ensure columns\n# ----------------------------\ndef _to_str_series(s: pd.Series) -> pd.Series:\n    return s.astype(str).replace({\"nan\": \"\", \"None\": \"\"})\n\ndef _ensure_uid(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"uid\" not in df.columns:\n        for alt in [\"sample_id\", \"id\", \"key\"]:\n            if alt in df.columns:\n                df = df.rename(columns={alt: \"uid\"})\n                break\n    if \"uid\" not in df.columns:\n        raise ValueError(\"Cannot find uid column. Expected 'uid' or 'sample_id'.\")\n    df[\"uid\"] = _to_str_series(df[\"uid\"])\n    return df\n\ndef _parse_case_variant_from_uid(uid_s: pd.Series) -> pd.DataFrame:\n    uid = _to_str_series(uid_s)\n    # pattern: \"<case>__<variant>\" (utama)\n    case1 = uid.str.extract(r\"^(\\d+)__\")[0]\n    var1  = uid.str.extract(r\"__(.+)$\")[0]\n    # fallback: \"<case>_<variant>\"\n    case2 = uid.str.extract(r\"^(\\d+)_\")[0]\n    var2  = uid.str.extract(r\"_(\\w+)$\")[0]\n    case = case1.fillna(case2)\n    var  = var1.fillna(var2).fillna(\"unk\")\n    out = pd.DataFrame({\"case_id\": case, \"variant\": var})\n    return out\n\ndef _ensure_case_variant(df: pd.DataFrame, df_base_map: pd.DataFrame = None) -> pd.DataFrame:\n    df = _ensure_uid(df)\n\n    # kalau sudah ada case_id/variant, cukup rapikan\n    if \"case_id\" in df.columns and \"variant\" in df.columns:\n        df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n        if df[\"case_id\"].isna().any():\n            # coba fallback parse dari uid\n            pv = _parse_case_variant_from_uid(df[\"uid\"])\n            df[\"case_id\"] = df[\"case_id\"].fillna(pd.to_numeric(pv[\"case_id\"], errors=\"coerce\"))\n            if \"variant\" in df.columns:\n                df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n                df[\"variant\"] = df[\"variant\"].where(df[\"variant\"].str.len() > 0, pv[\"variant\"])\n        df[\"case_id\"] = df[\"case_id\"].astype(\"Int64\")\n        df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n        return df\n\n    # prioritas: merge dari df_train_all (lebih akurat daripada parse)\n    if df_base_map is not None and {\"uid\", \"case_id\", \"variant\"}.issubset(df_base_map.columns):\n        df = df.merge(df_base_map[[\"uid\", \"case_id\", \"variant\"]], on=\"uid\", how=\"left\")\n\n    if \"case_id\" not in df.columns or \"variant\" not in df.columns:\n        pv = _parse_case_variant_from_uid(df[\"uid\"])\n        if \"case_id\" not in df.columns:\n            df[\"case_id\"] = pd.to_numeric(pv[\"case_id\"], errors=\"coerce\")\n        if \"variant\" not in df.columns:\n            df[\"variant\"] = pv[\"variant\"]\n\n    df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n    df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n    return df\n\ndef _pick_label_col(df: pd.DataFrame) -> str:\n    for cand in [\"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]:\n        if cand in df.columns:\n            return cand\n    return \"\"\n\n# ----------------------------\n# 5) Prepare base mapping from df_train_all\n# ----------------------------\ndf_base = df_base.copy()\nif \"uid\" not in df_base.columns:\n    if \"sample_id\" in df_base.columns:\n        df_base = df_base.rename(columns={\"sample_id\": \"uid\"})\n    elif (\"case_id\" in df_base.columns and \"variant\" in df_base.columns):\n        df_base[\"uid\"] = _to_str_series(df_base[\"case_id\"]) + \"__\" + _to_str_series(df_base[\"variant\"])\ndf_base = _ensure_uid(df_base)\n\n# normalize base case/variant\nif \"case_id\" in df_base.columns:\n    df_base[\"case_id\"] = pd.to_numeric(df_base[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\nif \"variant\" in df_base.columns:\n    df_base[\"variant\"] = df_base[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n\nlabel_col = _pick_label_col(df_base)\nif not label_col:\n    raise ValueError(\"Cannot find label column in df_train_all (y_forged/has_mask/is_forged/forged).\")\n\n# drop duplicates: one row per uid\ndf_base_map = df_base.drop_duplicates(subset=[\"uid\"], keep=\"first\").copy()\n\n# ----------------------------\n# 6) Prepare folds\n# ----------------------------\nif \"case_id\" not in df_cv.columns or \"fold\" not in df_cv.columns:\n    raise ValueError(\"cv_case_folds.csv must contain columns: case_id, fold\")\n\ndf_cv = df_cv[[\"case_id\", \"fold\"]].copy()\ndf_cv[\"case_id\"] = pd.to_numeric(df_cv[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\ndf_cv[\"fold\"]    = pd.to_numeric(df_cv[\"fold\"], errors=\"coerce\").astype(\"Int64\")\ndf_cv = df_cv.dropna().astype({\"case_id\": int, \"fold\": int}).drop_duplicates(\"case_id\")\n\n# ----------------------------\n# 7) Start from df_pred (1 row per uid)\n# ----------------------------\ndf_pred = _ensure_case_variant(df_pred, df_base_map=df_base_map)\n\n# de-dup pred (kalau ada duplikat uid, keep first)\nif df_pred[\"uid\"].duplicated().any():\n    df_pred = df_pred.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n\ndf_train = df_pred.copy()\n\n# attach label from df_base_map (lebih aman daripada percaya pred_features)\nif \"y\" not in df_train.columns:\n    df_train = df_train.merge(\n        df_base_map[[\"uid\", label_col]].rename(columns={label_col: \"y\"}),\n        on=\"uid\", how=\"left\"\n    )\n\nif df_train[\"y\"].isna().any():\n    miss = int(df_train[\"y\"].isna().sum())\n    raise ValueError(f\"Label merge produced NaN in y: {miss} rows. Check df_train_all vs pred_features alignment.\")\n\ndf_train[\"y\"] = pd.to_numeric(df_train[\"y\"], errors=\"coerce\").astype(int)\n\n# attach folds by case_id\ndf_train = df_train.drop(columns=[\"fold\"], errors=\"ignore\").merge(df_cv, on=\"case_id\", how=\"left\")\nif df_train[\"fold\"].isna().any():\n    miss = int(df_train[\"fold\"].isna().sum())\n    raise ValueError(f\"Missing fold after merging cv_case_folds.csv: {miss} rows.\")\ndf_train[\"fold\"] = df_train[\"fold\"].astype(int)\n\n# ----------------------------\n# 8) Optional merge match features (new cols only)\n# ----------------------------\nif df_match is not None:\n    df_match = _ensure_case_variant(df_match, df_base_map=df_base_map)\n    if df_match[\"uid\"].duplicated().any():\n        df_match = df_match.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n\n    base_cols = set(df_train.columns)\n    new_cols = [c for c in df_match.columns if c not in base_cols and c not in [\"case_id\", \"variant\"]]\n    if new_cols:\n        df_train = df_train.merge(df_match[[\"uid\"] + new_cols], on=\"uid\", how=\"left\")\n\n# ----------------------------\n# 9) Optional merge image profile by case_id (numeric only, prefixed)\n# ----------------------------\nif df_prof is not None and \"case_id\" in df_prof.columns:\n    df_prof2 = df_prof.copy()\n    df_prof2[\"case_id\"] = pd.to_numeric(df_prof2[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n    df_prof2 = df_prof2.dropna(subset=[\"case_id\"]).astype({\"case_id\": int})\n    df_prof2 = df_prof2.drop_duplicates(\"case_id\")\n\n    # keep numeric columns only (+case_id)\n    prof_num = [\"case_id\"] + [c for c in df_prof2.columns if c != \"case_id\" and pd.api.types.is_numeric_dtype(df_prof2[c])]\n    df_prof2 = df_prof2[prof_num].copy()\n\n    # prefix (hindari clash)\n    ren = {c: f\"profile_{c}\" for c in df_prof2.columns if c != \"case_id\"}\n    df_prof2 = df_prof2.rename(columns=ren)\n    df_train = df_train.merge(df_prof2, on=\"case_id\", how=\"left\")\n\n# ----------------------------\n# 10) Feature engineering helpers\n# ----------------------------\ndef _num(s):\n    return pd.to_numeric(s, errors=\"coerce\")\n\ndef safe_log1p_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.log1p(x)\n\ndef safe_sqrt_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.sqrt(x)\n\ndef get_clip_cap(series: pd.Series, q: float, fallback: float):\n    s = _num(series).astype(float).replace([np.inf, -np.inf], np.nan).dropna()\n    if len(s) == 0:\n        return float(fallback)\n    # cap pakai abs untuk robust (kalau ada nilai negatif liar)\n    s = s[np.isfinite(s)]\n    cap = float(np.quantile(np.abs(s.values), q))\n    if (not np.isfinite(cap)) or (cap <= 0):\n        return float(fallback)\n    return float(cap)\n\n# ----------------------------\n# 11) Build candidate numeric feature list (pre-fill)\n# ----------------------------\n# Jangan masukkan target/split/ID numeric\nTARGET_COLS = {\"y\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"}\nSPLIT_COLS  = {\"fold\"}\nID_DROP_NUM = {\"case_id\"}  # jangan dipakai sebagai feature\n\n# Replace inf -> NaN for numeric cols\nfor c in df_train.columns:\n    if pd.api.types.is_numeric_dtype(df_train[c]):\n        df_train[c] = df_train[c].replace([np.inf, -np.inf], np.nan)\n\n# Missing indicators (sebelum fill)\nmissing_ind_cols = []\nif FE_CFG[\"add_missing_indicators\"]:\n    for c in df_train.columns:\n        if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n            continue\n        if pd.api.types.is_numeric_dtype(df_train[c]):\n            if df_train[c].isna().any():\n                ind = f\"isna_{c}\"\n                df_train[ind] = df_train[c].isna().astype(np.uint8)\n                missing_ind_cols.append(ind)\n\n# Heavy-tail columns (dinamis + beberapa umum)\nheavy_candidates = set([\n    \"peak_ratio\", \"best_weight\", \"best_count\", \"best_weight_frac\",\n    \"pair_count\", \"n_pairs_thr\", \"n_pairs_mnn\", \"overmask_tighten_steps\",\n    \"largest_comp\", \"n_comp\", \"grid_h\", \"grid_w\", \"patch\",\n    \"grid_area_frac\", \"area_frac\", \"inlier_ratio\",\n])\n# tambah otomatis: semua kolom yang mengandung kata kunci ini\nfor c in df_train.columns:\n    cl = c.lower()\n    if any(k in cl for k in [\"count\", \"pairs\", \"weight\", \"ratio\", \"area\", \"comp\"]):\n        if pd.api.types.is_numeric_dtype(df_train[c]):\n            heavy_candidates.add(c)\n\nclip_caps = {}\nif FE_CFG[\"clip_by_quantile\"]:\n    for c in sorted(list(heavy_candidates)):\n        if c in df_train.columns and pd.api.types.is_numeric_dtype(df_train[c]):\n            clip_caps[c] = get_clip_cap(df_train[c], FE_CFG[\"clip_q\"], FE_CFG[\"clip_max_fallback\"])\n\n# Clipped + log/sqrt transforms\nif FE_CFG[\"add_log_features\"] or FE_CFG[\"add_sqrt_features\"]:\n    for c, cap in clip_caps.items():\n        x = _num(df_train[c]).fillna(0.0).astype(float).values\n        x = np.clip(x, -cap, cap)  # symmetric cap, lebih aman\n        df_train[f\"{c}_cap\"] = x.astype(np.float32)\n\n        # log1p untuk magnitude (nonneg)\n        if FE_CFG[\"add_log_features\"]:\n            df_train[f\"logabs_{c}\"] = safe_log1p_nonneg(np.abs(x)).astype(np.float32)\n\n        if FE_CFG[\"add_sqrt_features\"]:\n            df_train[f\"sqrtabs_{c}\"] = safe_sqrt_nonneg(np.abs(x)).astype(np.float32)\n\n# Interactions (ringkas tapi sering berguna)\nif FE_CFG[\"add_interactions\"]:\n    def getf(col, default=0.0):\n        if col in df_train.columns:\n            return _num(df_train[col]).fillna(default).astype(float).values\n        return np.full(len(df_train), default, dtype=np.float64)\n\n    best_mean_sim = getf(\"best_mean_sim\", 0.0)\n    best_count    = getf(\"best_count\", 0.0)\n    peak_ratio    = getf(\"peak_ratio\", 0.0)\n    has_peak      = getf(\"has_peak\", 0.0)\n    grid_area     = getf(\"grid_area_frac\", 0.0)\n    area_frac     = getf(\"area_frac\", 0.0)\n    n_pairs_thr   = getf(\"n_pairs_thr\", 0.0)\n    n_pairs_mnn   = getf(\"n_pairs_mnn\", 0.0)\n    inlier_ratio  = getf(\"inlier_ratio\", 0.0)\n    gh = getf(\"grid_h\", 0.0)\n    gw = getf(\"grid_w\", 0.0)\n\n    gridN = np.clip(gh * gw, 0.0, None)\n\n    df_train[\"sim_x_count\"]      = (best_mean_sim * best_count).astype(np.float32)\n    df_train[\"peak_x_sim\"]       = (peak_ratio * best_mean_sim).astype(np.float32)\n    df_train[\"haspeak_x_sim\"]    = (has_peak * best_mean_sim).astype(np.float32)\n    df_train[\"area_x_sim\"]       = (grid_area * best_mean_sim).astype(np.float32)\n    df_train[\"area_x_count\"]     = (grid_area * best_count).astype(np.float32)\n    df_train[\"mask_grid_ratio\"]  = (area_frac / (1e-6 + grid_area)).astype(np.float32)\n    df_train[\"mnn_ratio\"]        = (n_pairs_mnn / (1.0 + n_pairs_thr)).astype(np.float32)\n    df_train[\"pairs_per_cell\"]   = (n_pairs_thr / (1.0 + gridN)).astype(np.float32)\n    df_train[\"inlier_x_pairs\"]   = (inlier_ratio * n_pairs_thr).astype(np.float32)\n\n# Fill NaN numeric -> 0\nnum_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\ndf_train[num_cols] = df_train[num_cols].fillna(FE_CFG[\"fillna_value\"])\n\n# ----------------------------\n# 12) Variant encoding (optional one-hot)\n# ----------------------------\nvariant_dummy_cols = []\nif FE_CFG[\"encode_variant_onehot\"]:\n    vc = df_train[\"variant\"].astype(str).fillna(\"unk\")\n    # (opsional) buang variant yang terlalu jarang\n    counts = vc.value_counts()\n    keep = set(counts[counts >= int(FE_CFG[\"variant_min_count\"])].index.tolist())\n    vc = vc.where(vc.isin(keep), other=\"rare\")\n\n    dummies = pd.get_dummies(vc, prefix=\"v\", dummy_na=False)\n    # pastikan dtype kecil\n    dummies = dummies.astype(np.uint8)\n    variant_dummy_cols = dummies.columns.tolist()\n    df_train = pd.concat([df_train, dummies], axis=1)\n\n# ----------------------------\n# 13) Select final feature columns (numeric only)\n# ----------------------------\n# Exclude target/split/id numeric. Variant dummies sudah numeric -> ikut.\nfeature_cols = []\nfor c in df_train.columns:\n    if not pd.api.types.is_numeric_dtype(df_train[c]):\n        continue\n    if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n        continue\n    feature_cols.append(c)\n\n# Drop constant features\ndropped_constant = []\nif FE_CFG[\"drop_constant_features\"]:\n    nun = df_train[feature_cols].nunique(dropna=False)\n    nonconst = nun[nun > 1].index.tolist()\n    dropped_constant = sorted(set(feature_cols) - set(nonconst))\n    feature_cols = nonconst\n\n# Cast float32 for numeric features (keep uint8 for dummies/indicators ok tapi boleh float32 juga)\nif FE_CFG[\"cast_float32\"]:\n    # pisahkan uint8 indicators agar tidak meledak memori; cast ke float32 biar konsisten training\n    df_train[feature_cols] = df_train[feature_cols].astype(np.float32)\n\nFEATURE_COLS = list(feature_cols)\n\n# ----------------------------\n# 14) Final outputs\n# ----------------------------\nbase_out_cols = [\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]\ndf_train_tabular = df_train[base_out_cols + FEATURE_COLS].copy()\n\nX_train = df_train_tabular[FEATURE_COLS]\ny_train = df_train_tabular[\"y\"].astype(int)\nfolds   = df_train_tabular[\"fold\"].astype(int)\n\nprint(\"\\nOK — Training table built\")\nprint(\"  df_train_tabular:\", df_train_tabular.shape)\nprint(\"  X_train:\", X_train.shape, \"| y pos%:\", float(y_train.mean()) * 100.0)\nprint(\"  folds:\", int(folds.nunique()), \"unique folds\")\nprint(\"  feature_cols:\", int(len(FEATURE_COLS)))\nif dropped_constant:\n    print(\"  dropped_constant_features:\", len(dropped_constant))\nif variant_dummy_cols:\n    print(\"  variant_dummies:\", len(variant_dummy_cols))\nif missing_ind_cols:\n    print(\"  missing_indicators:\", len(missing_ind_cols))\n\n# hard sanity\nif X_train.shape[0] != y_train.shape[0]:\n    raise RuntimeError(\"X_train and y_train row mismatch\")\nif y_train.isna().any():\n    raise RuntimeError(\"y_train contains NaN\")\nif folds.isna().any():\n    raise RuntimeError(\"folds contains NaN\")\n\nprint(\"\\nFeature head:\", FEATURE_COLS[:25])\nprint(\"Feature tail:\", FEATURE_COLS[-15:])\n\n# ----------------------------\n# 15) Save reproducible schema\n# ----------------------------\nOUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_ART.mkdir(parents=True, exist_ok=True)\n\nwith open(OUT_ART / \"feature_cols.json\", \"w\") as f:\n    json.dump(FEATURE_COLS, f, indent=2)\n\nschema = {\n    \"fe_cfg\": FE_CFG,\n    \"label_col_source\": label_col,\n    \"clip_caps\": clip_caps,\n    \"dropped_constant_features\": dropped_constant,\n    \"variant_dummy_cols\": variant_dummy_cols,\n    \"missing_indicator_cols\": missing_ind_cols,\n    \"n_features\": int(len(FEATURE_COLS)),\n    \"example_feature_head\": FEATURE_COLS[:30],\n}\nwith open(OUT_ART / \"feature_schema.json\", \"w\") as f:\n    json.dump(schema, f, indent=2)\n\ndf_train_tabular.to_parquet(OUT_ART / \"df_train_tabular.parquet\", index=False)\n\nprint(f\"\\nSaved -> {OUT_ART/'feature_cols.json'}\")\nprint(f\"Saved -> {OUT_ART/'feature_schema.json'}\")\nprint(f\"Saved -> {OUT_ART/'df_train_tabular.parquet'}\")\n\n# export globals\nglobals().update({\n    \"df_train_tabular\": df_train_tabular,\n    \"FEATURE_COLS\": FEATURE_COLS,\n    \"X_train\": X_train,\n    \"y_train\": y_train,\n    \"folds\": folds,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:57:47.752914Z","iopub.execute_input":"2026-01-04T13:57:47.753453Z","iopub.status.idle":"2026-01-04T13:57:48.398903Z","shell.execute_reply.started":"2026-01-04T13:57:47.753413Z","shell.execute_reply":"2026-01-04T13:57:48.398102Z"}},"outputs":[{"name":"stdout","text":"Using:\n  DF_TRAIN_ALL      : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet\n  CV_CASE_FOLDS     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv\n  PRED_FEAT_TRAIN   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv\n  MATCH_FEAT_TRAIN  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv (optional)\n  IMG_PROFILE_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet (optional)\n  DINO_LARGE_DIR    : /kaggle/input/dinov2/pytorch/large/1\n\nOK — Training table built\n  df_train_tabular: (5176, 67)\n  X_train: (5176, 62) | y pos%: 54.07650695517774\n  folds: 5 unique folds\n  feature_cols: 62\n  dropped_constant_features: 6\n\nFeature head: ['has_peak', 'peak_ratio', 'best_weight', 'best_count', 'best_mean_sim', 'n_pairs_thr', 'n_pairs_mnn', 'best_inlier_ratio', 'best_weight_frac', 'inlier_ratio', 'pair_count', 'uniq_src', 'uniq_dst', 'mean_sim', 'thr_used', 'cnt_thr_used', 'relaxed_used', 'min_pairs_used', 'area_frac', 'n_comp']\nFeature tail: ['log_largest_comp', 'grid_area_frac_cap', 'log_grid_area_frac', 'sim_x_count', 'area_x_sim', 'area_x_count', 'comp_density', 'comp_inv', 'mnn_ratio', 'has_peak_x_logpeak']\n\nSaved -> /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\nSaved -> /kaggle/working/recodai_luc_gate_artifacts/feature_schema.json\nSaved -> /kaggle/working/recodai_luc_gate_artifacts/df_train_tabular.parquet\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Train Baseline Model (Leakage-Safe CV)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 3 — Train Stronger Model (Leakage-Safe CV) — REVISI FULL v2.2\n# Upgrade utama:\n# - EMA evaluation + save best EMA weights\n# - Focal-BCE (optional) + label smoothing (optional)\n# - Feature token dropout + input noise (tabular regularization)\n# - Fix CFG name print bug + better FULL training epochs (median best epoch)\n# - Best threshold search per fold (for reporting)\n# ============================================================\n\nimport json, gc, math, time, os, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, log_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# 0) Require outputs from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nrequired_cols = {\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"}\nmissing_cols = [c for c in required_cols if c not in df_train_tabular.columns]\nif missing_cols:\n    raise ValueError(f\"df_train_tabular missing columns: {missing_cols}.\")\n\n# ----------------------------\n# 1) CFG (SAFE vs STRONG)\n# ----------------------------\nCFG_SAFE = {\n    \"seed\": 2025,\n\n    # mHC (PDF)\n    \"n_streams\": 4,\n    \"sinkhorn_tmax\": 20,\n    \"alpha_init\": 0.01,\n\n    # model capacity (SAFE)\n    \"d_model\": 256,\n    \"n_layers\": 6,\n    \"n_heads\": 8,\n    \"ffn_mult\": 4,\n    \"dropout\": 0.12,\n    \"attn_dropout\": 0.08,\n\n    # regularization for tabular\n    \"feat_token_drop_p\": 0.05,     # drop some feature-tokens (not CLS)\n    \"input_noise_std\": 0.01,       # small gaussian noise AFTER standardize\n    \"label_smoothing\": 0.00,       # optional\n    \"focal_gamma\": 1.5,            # 0.0 -> pure BCE. 1.5 cukup aman buat imbalance\n\n    # training\n    \"batch_size\": 256,\n    \"accum_steps\": 2,\n    \"epochs\": 60,\n\n    \"lr\": 3e-4,\n    \"betas\": (0.9, 0.95),\n    \"eps\": 1e-8,\n    \"weight_decay\": 5e-2,\n\n    \"warmup_frac\": 0.08,\n    \"lr_decay_milestones\": (0.80, 0.90),\n    \"lr_decay_values\": (0.316, 0.10),\n\n    \"grad_clip\": 1.0,\n\n    \"early_stop_patience\": 12,\n    \"early_stop_min_delta\": 1e-4,\n\n    # EMA\n    \"use_ema\": True,\n    \"ema_decay\": 0.999,\n\n    # reporting\n    \"report_thr\": 0.5,\n    \"search_best_thr\": True,\n    \"thr_grid\": 201,              # number of thresholds to test in [0,1]\n}\n\nCFG_STRONG = {\n    **CFG_SAFE,\n    \"d_model\": 384,\n    \"n_layers\": 8,\n    \"dropout\": 0.16,\n    \"epochs\": 80,\n    \"lr\": 2e-4,\n    \"weight_decay\": 7e-2,\n    \"batch_size\": 256,\n    \"accum_steps\": 2,\n    \"feat_token_drop_p\": 0.06,\n    \"input_noise_std\": 0.012,\n    \"focal_gamma\": 1.5,\n}\n\n# auto select\nCFG = dict(CFG_SAFE)\nCFG_NAME = \"SAFE\"\nif torch.cuda.is_available():\n    try:\n        mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n        if mem_gb >= 30:\n            CFG = dict(CFG_STRONG)\n            CFG_NAME = \"STRONG\"\n    except Exception:\n        pass\n\n# ----------------------------\n# 2) Seed + device\n# ----------------------------\ndef seed_everything(seed: int = 2025):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(int(CFG[\"seed\"]))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\nprint(\"Device:\", device, \"| AMP:\", use_amp, \"| CFG:\", CFG_NAME)\n\nif device.type == \"cuda\":\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\n# ----------------------------\n# 3) Build arrays + guard\n# ----------------------------\nX = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\nfolds = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\n\nif not np.isfinite(X).all():\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\nn = len(df_train_tabular)\nunique_folds = sorted(pd.Series(folds).unique().tolist())\nn_folds = len(unique_folds)\nn_features = X.shape[1]\n\nprint(\"Setup:\")\nprint(\"  rows      :\", n)\nprint(\"  folds     :\", n_folds, \"|\", unique_folds)\nprint(\"  pos%      :\", float(y.mean()) * 100.0)\nprint(\"  n_features:\", n_features)\n\n# ----------------------------\n# 4) Dataset\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\n# ----------------------------\n# 5) Normalization (leakage-safe)\n# ----------------------------\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 6) Metrics helpers\n# ----------------------------\ndef safe_auc(y_true, p):\n    y_true = np.asarray(y_true)\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-6, 1-1e-6)\n    return float(log_loss(y_true, p, labels=[0, 1]))\n\ndef find_best_threshold(y_true, p, n_grid=201):\n    # maximize F1 (bisa diganti Youden/accuracy kalau mau)\n    y_true = np.asarray(y_true).astype(int)\n    p = np.asarray(p).astype(np.float64)\n    best = {\"thr\": 0.5, \"f1\": -1.0, \"precision\": 0.0, \"recall\": 0.0}\n    for thr in np.linspace(0.0, 1.0, int(n_grid)):\n        yh = (p >= thr).astype(int)\n        f1 = f1_score(y_true, yh, zero_division=0)\n        if f1 > best[\"f1\"]:\n            best[\"thr\"] = float(thr)\n            best[\"f1\"] = float(f1)\n            best[\"precision\"] = float(precision_score(y_true, yh, zero_division=0))\n            best[\"recall\"] = float(recall_score(y_true, yh, zero_division=0))\n    return best\n\n# ----------------------------\n# 7) Sinkhorn projection\n# ----------------------------\ndef sinkhorn_doubly_stochastic(logits: torch.Tensor, tmax: int = 20, eps: float = 1e-6):\n    z = logits - logits.max()\n    M = torch.exp(z)\n    for _ in range(int(tmax)):\n        M = M / (M.sum(dim=-1, keepdim=True) + eps)\n        M = M / (M.sum(dim=-2, keepdim=True) + eps)\n    return M\n\n# ----------------------------\n# 8) RMSNorm\n# ----------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, d: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d))\n\n    def forward(self, x):\n        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()\n        return x * rms * self.weight\n\n# ----------------------------\n# 9) EMA helper\n# ----------------------------\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[name] = p.detach().clone()\n            p.copy_(self.shadow[name])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[name])\n        self.backup = {}\n\n# ----------------------------\n# 10) Model blocks (feature token dropout)\n# ----------------------------\nclass MHCAttnBlock(nn.Module):\n    def __init__(self, d_model, n_heads, ffn_mult, dropout, attn_dropout,\n                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01):\n        super().__init__()\n        self.n_streams = int(n_streams)\n        self.sinkhorn_tmax = int(sinkhorn_tmax)\n\n        self.norm1 = RMSNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=attn_dropout, batch_first=True)\n        self.drop1 = nn.Dropout(dropout)\n\n        self.norm2 = RMSNorm(d_model)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, ffn_mult * d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(ffn_mult * d_model, d_model),\n        )\n        self.drop2 = nn.Dropout(dropout)\n\n        self.h_logits = nn.Parameter(torch.zeros(self.n_streams, self.n_streams))\n        nn.init.zeros_(self.h_logits)\n\n        a0 = float(alpha_init)\n        a0 = min(max(a0, 1e-4), 1 - 1e-4)\n        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n\n    def forward(self, streams):\n        # streams: (B, n_streams, S, D)\n        B, nS, S, D = streams.shape\n\n        # update only stream 0\n        x = streams[:, 0]  # (B,S,D)\n\n        x0 = x\n        q = self.norm1(x)\n        attn_out, _ = self.attn(q, q, q, need_weights=False)\n        x = x0 + self.drop1(attn_out)\n\n        x1 = x\n        h = self.norm2(x)\n        h = self.ffn(h)\n        x = x1 + self.drop2(h)\n\n        streams = streams.clone()\n        streams[:, 0] = x\n\n        H = sinkhorn_doubly_stochastic(self.h_logits, tmax=self.sinkhorn_tmax)\n        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype)\n        I = torch.eye(self.n_streams, device=streams.device, dtype=streams.dtype)\n        Hres = (1.0 - alpha) * I + alpha * H.to(dtype=streams.dtype)\n\n        mixed = torch.einsum(\"ij,bjtd->bitd\", Hres, streams)\n        return mixed\n\nclass MHCFTTransformer(nn.Module):\n    def __init__(self, n_features,\n                 d_model=256, n_heads=8, n_layers=6, ffn_mult=4,\n                 dropout=0.12, attn_dropout=0.08,\n                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01,\n                 feat_token_drop_p=0.0):\n        super().__init__()\n        self.n_features = int(n_features)\n        self.d_model = int(d_model)\n        self.feat_token_drop_p = float(feat_token_drop_p)\n\n        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n        self.in_drop = nn.Dropout(dropout)\n\n        self.blocks = nn.ModuleList([\n            MHCAttnBlock(\n                d_model=self.d_model,\n                n_heads=n_heads,\n                ffn_mult=ffn_mult,\n                dropout=dropout,\n                attn_dropout=attn_dropout,\n                n_streams=n_streams,\n                sinkhorn_tmax=sinkhorn_tmax,\n                alpha_init=alpha_init\n            )\n            for _ in range(int(n_layers))\n        ])\n\n        self.out_norm = RMSNorm(self.d_model)\n        self.head = nn.Sequential(\n            nn.Linear(self.d_model, self.d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(self.d_model, 1),\n        )\n\n        self.n_streams = int(n_streams)\n\n    def forward(self, x):\n        # x: (B,F)\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)   # (B,F,D)\n        tok = tok + self.feat_emb.unsqueeze(0)\n\n        # feature-token dropout (regularizer), do not drop CLS\n        if self.training and self.feat_token_drop_p > 0:\n            B, F, D = tok.shape\n            keep = (torch.rand(B, F, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n            tok = tok * keep.unsqueeze(-1)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)                                    # (B,1,D)\n        seq = torch.cat([cls, tok], dim=1)                                  # (B,1+F,D)\n        seq = self.in_drop(seq)\n\n        streams = seq.unsqueeze(1).repeat(1, self.n_streams, 1, 1)          # (B,nS,S,D)\n        for blk in self.blocks:\n            streams = blk(streams)\n\n        z = streams[:, 0, 0]\n        z = self.out_norm(z)\n        logit = self.head(z).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 11) LR Scheduler\n# ----------------------------\ndef make_warmup_step_scheduler(optimizer, total_steps: int, warmup_steps: int,\n                              milestones_frac=(0.8, 0.9), decay_values=(0.316, 0.1)):\n    m1 = int(float(milestones_frac[0]) * total_steps)\n    m2 = int(float(milestones_frac[1]) * total_steps)\n    d1 = float(decay_values[0])\n    d2 = float(decay_values[1])\n\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        if step < m1:\n            return 1.0\n        elif step < m2:\n            return d1\n        else:\n            return d2\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# ----------------------------\n# 12) Loss: BCE / Focal-BCE with pos_weight + label smoothing\n# ----------------------------\ndef focal_bce_with_logits(logits, targets, pos_weight=None, gamma=0.0):\n    # targets in [0,1]\n    bce = F.binary_cross_entropy_with_logits(\n        logits, targets, reduction=\"none\", pos_weight=pos_weight\n    )\n    if gamma and gamma > 0:\n        p = torch.sigmoid(logits)\n        p_t = p * targets + (1 - p) * (1 - targets)\n        mod = (1.0 - p_t).clamp_min(0.0).pow(gamma)\n        bce = bce * mod\n    return bce.mean()\n\n# ----------------------------\n# 13) Predict helper (optionally with EMA)\n# ----------------------------\n@torch.no_grad()\ndef predict_proba(model, loader, ema: EMA = None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n    probs = []\n    for batch in loader:\n        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n        xb = xb.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            logits = model(xb)\n            p = torch.sigmoid(logits)\n        probs.append(p.detach().cpu().numpy())\n    out = np.concatenate(probs, axis=0).astype(np.float32)\n    if ema is not None:\n        ema.restore(model)\n    return out\n\n# ----------------------------\n# 14) Train one fold (EMA + focal + noise + best_state)\n# ----------------------------\ndef train_one_fold(X_tr, y_tr, X_va, y_va, cfg):\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n\n    model = MHCFTTransformer(\n        n_features=n_features,\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    # imbalance pos_weight\n    pos = int(y_tr.sum())\n    neg = int(len(y_tr) - pos)\n    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        betas=tuple(cfg[\"betas\"]),\n        eps=float(cfg[\"eps\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    optim_steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n    total_optim_steps = int(cfg[\"epochs\"]) * max(1, optim_steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt,\n        total_steps=total_optim_steps,\n        warmup_steps=warmup_steps,\n        milestones_frac=cfg[\"lr_decay_milestones\"],\n        decay_values=cfg[\"lr_decay_values\"],\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\n    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n\n    best = {\"val_logloss\": 1e9, \"val_auc\": None, \"epoch\": -1}\n    best_state = None\n    bad = 0\n\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n    label_smooth = float(cfg.get(\"label_smoothing\", 0.0))\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        t0 = time.time()\n        loss_sum = 0.0\n        n_sum = 0\n\n        opt.zero_grad(set_to_none=True)\n        micro_step = 0\n        optim_step_in_epoch = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            # input noise (after standardize)\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            # label smoothing for binary\n            if label_smooth and label_smooth > 0:\n                yb = yb * (1.0 - label_smooth) + 0.5 * label_smooth\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = focal_bce_with_logits(logits, yb, pos_weight=pos_weight, gamma=focal_gamma)\n                loss = loss / accum_steps\n\n            scaler.scale(loss).backward()\n            micro_step += 1\n\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro_step % accum_steps) == 0:\n                if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                optim_step_in_epoch += 1\n\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro_step % accum_steps) != 0:\n            if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            optim_step_in_epoch += 1\n            if ema is not None:\n                ema.update(model)\n\n        # validate (gunakan EMA kalau ada)\n        p_va = predict_proba(model, dl_va, ema=ema)\n        vll = safe_logloss(y_va, p_va)\n        vauc = safe_auc(y_va, p_va)\n\n        tr_loss = loss_sum / max(1, n_sum)\n        dt = time.time() - t0\n        print(f\"  epoch {epoch+1:03d}/{cfg['epochs']} | train_loss={tr_loss:.5f} | val_logloss={vll:.5f} | val_auc={vauc} | opt_steps={optim_step_in_epoch} | dt={dt:.1f}s\")\n\n        improved = (best[\"val_logloss\"] - vll) > float(cfg[\"early_stop_min_delta\"])\n        if improved:\n            best[\"val_logloss\"] = float(vll)\n            best[\"val_auc\"] = vauc\n            best[\"epoch\"] = int(epoch)\n            # simpan state (EMA weight kalau ada, supaya best benar-benar yang dieval)\n            if ema is not None:\n                ema.apply_shadow(model)\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n                ema.restore(model)\n            else:\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= int(cfg[\"early_stop_patience\"]):\n                print(f\"  early stop at epoch {epoch+1}, best_epoch={best['epoch']+1}, best_val_logloss={best['val_logloss']:.5f}\")\n                break\n\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n        gc.collect()\n\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    # final val preds (best)\n    p_va = predict_proba(model, dl_va, ema=None)  # model sudah di-load best_state\n\n    pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n        \"best\": best,\n    }\n    return pack, p_va, best\n\n# ----------------------------\n# 15) CV loop\n# ----------------------------\noof_pred = np.zeros(n, dtype=np.float32)\nfold_reports = []\nbest_epochs = []\n\nmodels_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts/baseline_mhc_transformer_folds\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\nfor f in unique_folds:\n    print(f\"\\n[Fold {f}]\")\n    tr_idx = np.where(folds != f)[0]\n    va_idx = np.where(folds == f)[0]\n\n    X_tr, y_tr = X[tr_idx], y[tr_idx]\n    X_va, y_va = X[va_idx], y[va_idx]\n\n    pack, p_va, best = train_one_fold(X_tr, y_tr, X_va, y_va, CFG)\n    oof_pred[va_idx] = p_va\n    best_epochs.append(int(best[\"epoch\"] + 1))\n\n    auc = safe_auc(y_va, p_va)\n    vll = safe_logloss(y_va, p_va)\n\n    # best threshold search (untuk report)\n    if bool(CFG.get(\"search_best_thr\", True)):\n        bt = find_best_threshold(y_va, p_va, n_grid=int(CFG.get(\"thr_grid\", 201)))\n        thr_use = float(bt[\"thr\"])\n        yhat = (p_va >= thr_use).astype(np.int32)\n        rep_thr = {\n            \"best_thr\": thr_use,\n            \"best_f1\": float(bt[\"f1\"]),\n            \"best_precision\": float(bt[\"precision\"]),\n            \"best_recall\": float(bt[\"recall\"]),\n        }\n    else:\n        thr_use = float(CFG[\"report_thr\"])\n        yhat = (p_va >= thr_use).astype(np.int32)\n        rep_thr = {\n            \"best_thr\": None,\n            \"best_f1\": None,\n            \"best_precision\": None,\n            \"best_recall\": None,\n        }\n\n    rep = {\n        \"fold\": int(f),\n        \"n_val\": int(len(va_idx)),\n        \"pos_val\": int(y_va.sum()),\n        \"auc\": auc,\n        \"logloss\": vll,\n        f\"f1@thr({thr_use:.3f})\": float(f1_score(y_va, yhat, zero_division=0)),\n        f\"precision@thr({thr_use:.3f})\": float(precision_score(y_va, yhat, zero_division=0)),\n        f\"recall@thr({thr_use:.3f})\": float(recall_score(y_va, yhat, zero_division=0)),\n        \"best_val_logloss\": float(best[\"val_logloss\"]),\n        \"best_val_auc\": best[\"val_auc\"],\n        \"best_epoch\": int(best[\"epoch\"] + 1),\n        **rep_thr,\n    }\n    fold_reports.append(rep)\n\n    torch.save(\n        {\"pack\": pack, \"feature_cols\": FEATURE_COLS},\n        models_dir / f\"baseline_mhc_transformer_fold_{f}.pt\"\n    )\n\n    if device.type == \"cuda\":\n        torch.cuda.empty_cache()\n    gc.collect()\n\n# ----------------------------\n# 16) Overall OOF metrics\n# ----------------------------\noof_auc = safe_auc(y, oof_pred)\noof_ll  = safe_logloss(y, oof_pred)\n\n# report with fixed thr=0.5 and also best thr on all OOF (optional)\nthr_fixed = float(CFG[\"report_thr\"])\noof_yhat_fixed = (oof_pred >= thr_fixed).astype(np.int32)\n\nbest_oof_thr = None\nbest_oof = None\nif bool(CFG.get(\"search_best_thr\", True)):\n    best_oof = find_best_threshold(y, oof_pred, n_grid=int(CFG.get(\"thr_grid\", 201)))\n    best_oof_thr = float(best_oof[\"thr\"])\n\noverall = {\n    \"rows\": int(n),\n    \"folds\": int(n_folds),\n    \"pos_total\": int(y.sum()),\n    \"pos_rate\": float(y.mean()),\n    \"oof_auc\": oof_auc,\n    \"oof_logloss\": oof_ll,\n    f\"oof_f1@{thr_fixed}\": float(f1_score(y, oof_yhat_fixed, zero_division=0)),\n    f\"oof_precision@{thr_fixed}\": float(precision_score(y, oof_yhat_fixed, zero_division=0)),\n    f\"oof_recall@{thr_fixed}\": float(recall_score(y, oof_yhat_fixed, zero_division=0)),\n    \"oof_best_thr\": best_oof_thr,\n    \"oof_best_f1\": (float(best_oof[\"f1\"]) if best_oof else None),\n}\n\ndf_rep = pd.DataFrame(fold_reports).sort_values(\"fold\").reset_index(drop=True)\nprint(\"\\nPer-fold report:\")\ndisplay(df_rep)\n\nprint(\"\\nOOF overall:\")\nprint(overall)\n\n# ----------------------------\n# 17) Train FULL model (epochs = median(best_epoch) * 1.15)\n# ----------------------------\ndef train_full_fixed(X_full_raw, y_full, cfg, epochs_full: int):\n    mu, sig = fit_standardizer(X_full_raw)\n    X_full = apply_standardizer(X_full_raw, mu, sig)\n\n    ds_full = TabDataset(X_full, y_full)\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n    dl_full = DataLoader(ds_full, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                         num_workers=nw, pin_memory=pin, drop_last=False,\n                         persistent_workers=(nw > 0))\n\n    model = MHCFTTransformer(\n        n_features=n_features,\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    pos = int(y_full.sum())\n    neg = int(len(y_full) - pos)\n    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        betas=tuple(cfg[\"betas\"]),\n        eps=float(cfg[\"eps\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    optim_steps_per_epoch = int(math.ceil(len(dl_full) / accum_steps))\n    total_optim_steps = int(epochs_full) * max(1, optim_steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt,\n        total_steps=total_optim_steps,\n        warmup_steps=warmup_steps,\n        milestones_frac=cfg[\"lr_decay_milestones\"],\n        decay_values=cfg[\"lr_decay_values\"],\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n    label_smooth = float(cfg.get(\"label_smoothing\", 0.0))\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n\n    print(f\"\\nTraining FULL mHC transformer for {epochs_full} epochs (median-best based)...\")\n\n    for epoch in range(int(epochs_full)):\n        model.train()\n        loss_sum = 0.0\n        n_sum = 0\n\n        opt.zero_grad(set_to_none=True)\n        micro_step = 0\n\n        for xb, yb in dl_full:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            if label_smooth and label_smooth > 0:\n                yb = yb * (1.0 - label_smooth) + 0.5 * label_smooth\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = focal_bce_with_logits(logits, yb, pos_weight=pos_weight, gamma=focal_gamma)\n                loss = loss / accum_steps\n\n            scaler.scale(loss).backward()\n            micro_step += 1\n\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro_step % accum_steps) == 0:\n                if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        if (micro_step % accum_steps) != 0:\n            if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        print(f\"  full epoch {epoch+1:03d}/{epochs_full} | loss={loss_sum/max(1,n_sum):.5f}\")\n\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n        gc.collect()\n\n    # save EMA weights if used\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    full_pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n        \"epochs_full\": int(epochs_full),\n        \"used_ema\": bool(ema is not None),\n    }\n\n    if ema is not None:\n        ema.restore(model)\n\n    return full_pack\n\n# decide epochs_full from CV best epochs\nmed_best = int(np.median(np.array(best_epochs, dtype=np.int32))) if len(best_epochs) else int(max(12, CFG[\"epochs\"] * 0.7))\nepochs_full = int(max(12, round(med_best * 1.15)))\nepochs_full = int(min(epochs_full, int(CFG[\"epochs\"])))  # jangan lebih dari max epochs config\n\nout_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\nfull_pack = train_full_fixed(X, y, CFG, epochs_full=epochs_full)\ntorch.save({\"pack\": full_pack, \"feature_cols\": FEATURE_COLS}, out_dir / \"baseline_mhc_transformer_model_full.pt\")\n\n# ----------------------------\n# 18) Save OOF + report\n# ----------------------------\ndf_oof = df_train_tabular[[\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]].copy()\ndf_oof[\"oof_pred_baseline_mhc_tf\"] = oof_pred\ndf_oof.to_csv(out_dir / \"oof_baseline_mhc_transformer.csv\", index=False)\n\nreport = {\n    \"model\": \"mHC-FTTransformer (numeric tabular) — upgraded v2.2 (EMA + focal + reg)\",\n    \"cfg_name\": CFG_NAME,\n    \"cfg\": CFG,\n    \"feature_count\": int(len(FEATURE_COLS)),\n    \"best_epochs\": best_epochs,\n    \"epochs_full\": int(epochs_full),\n    \"fold_reports\": fold_reports,\n    \"overall\": overall,\n}\nwith open(out_dir / \"baseline_mhc_transformer_cv_report.json\", \"w\") as f:\n    json.dump(report, f, indent=2)\n\nprint(\"\\nSaved artifacts:\")\nprint(\"  fold models  ->\", models_dir)\nprint(\"  full model   ->\", out_dir / \"baseline_mhc_transformer_model_full.pt\")\nprint(\"  oof preds    ->\", out_dir / \"oof_baseline_mhc_transformer.csv\")\nprint(\"  cv report    ->\", out_dir / \"baseline_mhc_transformer_cv_report.json\")\n\n# Export globals\nOOF_PRED_BASELINE_MHC_TF = oof_pred\nBASELINE_MHC_TF_OVERALL = overall\nBASELINE_MHC_TF_FOLD_REPORTS = fold_reports\nBASELINE_MHC_TF_BEST_EPOCHS = best_epochs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:57:48.400236Z","iopub.execute_input":"2026-01-04T13:57:48.400525Z"}},"outputs":[{"name":"stdout","text":"Device: cpu | AMP: False | CFG: SAFE\nSetup:\n  rows      : 5176\n  folds     : 5 | [0, 1, 2, 3, 4]\n  pos%      : 54.07650695517774\n  n_features: 62\n\n[Fold 0]\n  epoch 001/50 | train_loss=0.64418 | val_logloss=0.69617 | opt_steps=9 | dt=149.7s\n  epoch 002/50 | train_loss=0.64102 | val_logloss=0.70211 | opt_steps=9 | dt=151.1s\n  epoch 003/50 | train_loss=0.63783 | val_logloss=0.69920 | opt_steps=9 | dt=148.9s\n  epoch 004/50 | train_loss=0.63812 | val_logloss=0.69830 | opt_steps=9 | dt=148.2s\n  epoch 005/50 | train_loss=0.63792 | val_logloss=0.69034 | opt_steps=9 | dt=148.4s\n  epoch 006/50 | train_loss=0.63585 | val_logloss=0.68873 | opt_steps=9 | dt=149.4s\n  epoch 007/50 | train_loss=0.63943 | val_logloss=0.69627 | opt_steps=9 | dt=148.6s\n  epoch 008/50 | train_loss=0.63788 | val_logloss=0.70011 | opt_steps=9 | dt=146.9s\n  epoch 009/50 | train_loss=0.63812 | val_logloss=0.69200 | opt_steps=9 | dt=152.9s\n  epoch 010/50 | train_loss=0.63717 | val_logloss=0.68839 | opt_steps=9 | dt=148.1s\n  epoch 011/50 | train_loss=0.63815 | val_logloss=0.68804 | opt_steps=9 | dt=148.4s\n  epoch 012/50 | train_loss=0.63634 | val_logloss=0.69900 | opt_steps=9 | dt=148.4s\n  epoch 013/50 | train_loss=0.63481 | val_logloss=0.69700 | opt_steps=9 | dt=151.3s\n  epoch 014/50 | train_loss=0.63430 | val_logloss=0.68802 | opt_steps=9 | dt=149.2s\n  epoch 015/50 | train_loss=0.63462 | val_logloss=0.69267 | opt_steps=9 | dt=149.5s\n  epoch 016/50 | train_loss=0.63238 | val_logloss=0.69224 | opt_steps=9 | dt=150.9s\n  epoch 017/50 | train_loss=0.63293 | val_logloss=0.69818 | opt_steps=9 | dt=151.2s\n  epoch 018/50 | train_loss=0.63536 | val_logloss=0.71932 | opt_steps=9 | dt=150.9s\n  epoch 019/50 | train_loss=0.63754 | val_logloss=0.68946 | opt_steps=9 | dt=149.6s\n  epoch 020/50 | train_loss=0.63543 | val_logloss=0.69666 | opt_steps=9 | dt=150.9s\n  epoch 021/50 | train_loss=0.63532 | val_logloss=0.69896 | opt_steps=9 | dt=148.7s\n  early stop at epoch 21, best_epoch=11, best_val_logloss=0.68804\n\n[Fold 1]\n  epoch 001/50 | train_loss=0.63973 | val_logloss=0.69446 | opt_steps=9 | dt=148.1s\n  epoch 002/50 | train_loss=0.64153 | val_logloss=0.68947 | opt_steps=9 | dt=147.4s\n  epoch 003/50 | train_loss=0.63793 | val_logloss=0.70397 | opt_steps=9 | dt=149.5s\n  epoch 004/50 | train_loss=0.63881 | val_logloss=0.70304 | opt_steps=9 | dt=150.8s\n  epoch 005/50 | train_loss=0.64018 | val_logloss=0.69699 | opt_steps=9 | dt=148.3s\n  epoch 006/50 | train_loss=0.63572 | val_logloss=0.68996 | opt_steps=9 | dt=149.9s\n  epoch 007/50 | train_loss=0.63677 | val_logloss=0.69434 | opt_steps=9 | dt=150.0s\n  epoch 008/50 | train_loss=0.63597 | val_logloss=0.70030 | opt_steps=9 | dt=149.6s\n  epoch 009/50 | train_loss=0.63699 | val_logloss=0.69232 | opt_steps=9 | dt=150.8s\n  epoch 010/50 | train_loss=0.63551 | val_logloss=0.69131 | opt_steps=9 | dt=150.2s\n  epoch 011/50 | train_loss=0.63530 | val_logloss=0.69671 | opt_steps=9 | dt=148.6s\n  epoch 012/50 | train_loss=0.63566 | val_logloss=0.70779 | opt_steps=9 | dt=150.0s\n  early stop at epoch 12, best_epoch=2, best_val_logloss=0.68947\n\n[Fold 2]\n  epoch 001/50 | train_loss=0.64162 | val_logloss=0.69362 | opt_steps=9 | dt=148.3s\n  epoch 002/50 | train_loss=0.64020 | val_logloss=0.69274 | opt_steps=9 | dt=150.0s\n  epoch 003/50 | train_loss=0.63742 | val_logloss=0.70143 | opt_steps=9 | dt=150.4s\n  epoch 004/50 | train_loss=0.63665 | val_logloss=0.68881 | opt_steps=9 | dt=147.8s\n  epoch 005/50 | train_loss=0.63888 | val_logloss=0.71215 | opt_steps=9 | dt=151.3s\n  epoch 006/50 | train_loss=0.64169 | val_logloss=0.70402 | opt_steps=9 | dt=150.7s\n  epoch 007/50 | train_loss=0.64084 | val_logloss=0.68914 | opt_steps=9 | dt=152.4s\n  epoch 008/50 | train_loss=0.63786 | val_logloss=0.70115 | opt_steps=9 | dt=150.5s\n  epoch 009/50 | train_loss=0.63727 | val_logloss=0.69003 | opt_steps=9 | dt=152.3s\n  epoch 010/50 | train_loss=0.63697 | val_logloss=0.69168 | opt_steps=9 | dt=149.7s\n  epoch 011/50 | train_loss=0.63711 | val_logloss=0.69786 | opt_steps=9 | dt=150.5s\n  epoch 012/50 | train_loss=0.63723 | val_logloss=0.68948 | opt_steps=9 | dt=149.8s\n  epoch 013/50 | train_loss=0.63715 | val_logloss=0.68869 | opt_steps=9 | dt=150.7s\n  epoch 014/50 | train_loss=0.63682 | val_logloss=0.69041 | opt_steps=9 | dt=150.7s\n  epoch 015/50 | train_loss=0.63559 | val_logloss=0.69353 | opt_steps=9 | dt=151.0s\n  epoch 016/50 | train_loss=0.63493 | val_logloss=0.71144 | opt_steps=9 | dt=150.3s\n  epoch 017/50 | train_loss=0.63972 | val_logloss=0.68672 | opt_steps=9 | dt=149.8s\n  epoch 018/50 | train_loss=0.63500 | val_logloss=0.68673 | opt_steps=9 | dt=150.8s\n  epoch 019/50 | train_loss=0.64130 | val_logloss=0.69344 | opt_steps=9 | dt=146.7s\n  epoch 020/50 | train_loss=0.63651 | val_logloss=0.68865 | opt_steps=9 | dt=147.1s\n  epoch 021/50 | train_loss=0.63671 | val_logloss=0.69682 | opt_steps=9 | dt=150.2s\n  epoch 022/50 | train_loss=0.63606 | val_logloss=0.69269 | opt_steps=9 | dt=150.2s\n  epoch 023/50 | train_loss=0.63621 | val_logloss=0.69426 | opt_steps=9 | dt=152.3s\n  epoch 024/50 | train_loss=0.63487 | val_logloss=0.68869 | opt_steps=9 | dt=149.2s\n  epoch 025/50 | train_loss=0.63433 | val_logloss=0.69502 | opt_steps=9 | dt=149.8s\n  epoch 026/50 | train_loss=0.63528 | val_logloss=0.68932 | opt_steps=9 | dt=147.3s\n  epoch 027/50 | train_loss=0.63283 | val_logloss=0.68942 | opt_steps=9 | dt=150.8s\n  early stop at epoch 27, best_epoch=17, best_val_logloss=0.68672\n\n[Fold 3]\n  epoch 001/50 | train_loss=0.63900 | val_logloss=0.70236 | opt_steps=9 | dt=151.2s\n  epoch 002/50 | train_loss=0.63869 | val_logloss=0.69443 | opt_steps=9 | dt=151.6s\n  epoch 003/50 | train_loss=0.63772 | val_logloss=0.69172 | opt_steps=9 | dt=152.6s\n  epoch 004/50 | train_loss=0.64196 | val_logloss=0.69012 | opt_steps=9 | dt=150.3s\n  epoch 005/50 | train_loss=0.63748 | val_logloss=0.68974 | opt_steps=9 | dt=153.1s\n  epoch 006/50 | train_loss=0.63948 | val_logloss=0.69774 | opt_steps=9 | dt=151.0s\n  epoch 007/50 | train_loss=0.63645 | val_logloss=0.69058 | opt_steps=9 | dt=151.4s\n  epoch 008/50 | train_loss=0.63751 | val_logloss=0.70463 | opt_steps=9 | dt=149.0s\n  epoch 009/50 | train_loss=0.63762 | val_logloss=0.68887 | opt_steps=9 | dt=152.4s\n  epoch 010/50 | train_loss=0.63931 | val_logloss=0.69115 | opt_steps=9 | dt=153.6s\n  epoch 011/50 | train_loss=0.64062 | val_logloss=0.69158 | opt_steps=9 | dt=151.3s\n  epoch 012/50 | train_loss=0.63705 | val_logloss=0.69477 | opt_steps=9 | dt=151.0s\n  epoch 013/50 | train_loss=0.63638 | val_logloss=0.69087 | opt_steps=9 | dt=152.3s\n  epoch 014/50 | train_loss=0.63757 | val_logloss=0.69264 | opt_steps=9 | dt=152.1s\n  epoch 015/50 | train_loss=0.63634 | val_logloss=0.69153 | opt_steps=9 | dt=151.9s\n  epoch 016/50 | train_loss=0.63818 | val_logloss=0.69029 | opt_steps=9 | dt=150.0s\n  epoch 017/50 | train_loss=0.63804 | val_logloss=0.69577 | opt_steps=9 | dt=150.8s\n  epoch 018/50 | train_loss=0.63710 | val_logloss=0.69050 | opt_steps=9 | dt=149.7s\n  epoch 019/50 | train_loss=0.63746 | val_logloss=0.69136 | opt_steps=9 | dt=151.0s\n  early stop at epoch 19, best_epoch=9, best_val_logloss=0.68887\n\n[Fold 4]\n  epoch 001/50 | train_loss=0.64555 | val_logloss=0.69044 | opt_steps=9 | dt=152.1s\n  epoch 002/50 | train_loss=0.63785 | val_logloss=0.70024 | opt_steps=9 | dt=150.3s\n  epoch 003/50 | train_loss=0.64024 | val_logloss=0.71044 | opt_steps=9 | dt=151.6s\n  epoch 004/50 | train_loss=0.63926 | val_logloss=0.69752 | opt_steps=9 | dt=152.7s\n  epoch 005/50 | train_loss=0.64081 | val_logloss=0.69047 | opt_steps=9 | dt=151.3s\n  epoch 006/50 | train_loss=0.63811 | val_logloss=0.69094 | opt_steps=9 | dt=151.8s\n  epoch 007/50 | train_loss=0.63709 | val_logloss=0.70427 | opt_steps=9 | dt=150.9s\n  epoch 008/50 | train_loss=0.63918 | val_logloss=0.69013 | opt_steps=9 | dt=152.0s\n  epoch 009/50 | train_loss=0.63672 | val_logloss=0.69317 | opt_steps=9 | dt=152.2s\n  epoch 010/50 | train_loss=0.63731 | val_logloss=0.69221 | opt_steps=9 | dt=152.0s\n  epoch 011/50 | train_loss=0.63807 | val_logloss=0.69039 | opt_steps=9 | dt=152.5s\n  epoch 012/50 | train_loss=0.63698 | val_logloss=0.68891 | opt_steps=9 | dt=153.0s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Optimize Model & Hyperparameters (Iterative)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 4 — Optimize Model & Hyperparameters (Iterative) — TRANSFORMER ONLY\n# REVISI FULL v3.1 (mHC-lite PDF-inspired, differentiable + EMA + accum + reg)\n#\n# Fix penting:\n# - Sinkhorn-Knopp differentiable (H belajar beneran)\n# - EMA eval + save best (stabil)\n# - Grad accumulation + scheduler on optimizer-steps (bukan per batch)\n# - Feature-token drop + input noise (tabular regularization)\n# - 2-stage search: subset folds cepat -> full CV top-M\n# - Best model tidak retrain ulang: pakai fold_packs dari stage-2\n#\n# Primary score: OOF best F-beta (beta=0.5)\n#\n# Output:\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.json\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/oof_preds_<cfg_name>.csv (top configs)\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt\n# ============================================================\n\nimport os, json, gc, math, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# 0) Require data from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nX_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\nfolds_all = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\nuids_all = df_train_tabular[\"uid\"].astype(str).to_numpy()\n\nif not np.isfinite(X_all).all():\n    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n\nunique_folds = sorted(pd.Series(folds_all).unique().tolist())\nn = len(y_all)\npos_rate = float(y_all.mean())\nn_features = X_all.shape[1]\n\nprint(\"Optimize setup (Transformer-only, mHC-lite):\")\nprint(f\"  rows={n} | folds={len(unique_folds)} | pos%={pos_rate*100:.2f} | n_features={n_features}\")\n\n# ----------------------------\n# 1) Global settings\n# ----------------------------\nSEED = 2025\nBETA = 0.5\nTHR_GRID = 201\n\n# 2-stage runtime controls\nSTAGE1_FOLDS = min(3, len(unique_folds))\nSTAGE1_EPOCH_CAP = 35\nSTAGE1_PAT_CAP = 6\n\nSTAGE2_TOPM = 3\nREPORT_TOPK_OOF = 3\n\n# Optional time budget (0 = off)\nTIME_BUDGET_SEC = 0  # contoh: 2.5*60*60\n\ndef seed_everything(seed=2025):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\nprint(\"Device:\", device, \"| AMP:\", use_amp)\n\nif device.type == \"cuda\":\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\ndef get_mem_gb():\n    if not torch.cuda.is_available():\n        return 0.0\n    try:\n        return float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n    except Exception:\n        return 0.0\n\nMEM_GB = get_mem_gb()\nprint(\"GPU mem GB:\", MEM_GB)\n\n# ----------------------------\n# 2) Metrics helpers (F-beta primary)\n# ----------------------------\ndef best_fbeta_fast(y_true, p, beta=0.5, grid=201):\n    y = (np.asarray(y_true).astype(np.int32) == 1)\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1.0 - 1e-8)\n\n    thrs = np.linspace(0.01, 0.99, int(grid), dtype=np.float64)\n    pred = (p[:, None] >= thrs[None, :])\n\n    y1 = y[:, None]\n    tp = (pred & y1).sum(axis=0).astype(np.float64)\n    fp = (pred & (~y1)).sum(axis=0).astype(np.float64)\n    fn = (y.sum().astype(np.float64) - tp)\n\n    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n    recall    = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n\n    b2 = beta * beta\n    denom = (b2 * precision + recall)\n    fbeta = np.divide((1.0 + b2) * precision * recall, denom, out=np.zeros_like(precision), where=denom > 0)\n\n    j = int(np.argmax(fbeta))\n    return {\n        \"fbeta\": float(fbeta[j]),\n        \"thr\": float(thrs[j]),\n        \"precision\": float(precision[j]),\n        \"recall\": float(recall[j]),\n    }\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n    return float(log_loss(y_true, p, labels=[0, 1]))\n\n# ----------------------------\n# 3) Dataset + Standardizer (no leakage)\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n\n    def __len__(self): return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 4) RMSNorm + Differentiable Sinkhorn + EMA\n# ----------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, d, eps=1e-6):\n        super().__init__()\n        self.eps = float(eps)\n        self.weight = nn.Parameter(torch.ones(d))\n\n    def forward(self, x):\n        rms = torch.mean(x * x, dim=-1, keepdim=True)\n        x = x * torch.rsqrt(rms + self.eps)\n        return x * self.weight\n\ndef sinkhorn_knopp(P, tmax=20, eps=1e-6):\n    \"\"\"\n    Differentiable Sinkhorn-Knopp\n    P: (B,n,n) non-negative\n    \"\"\"\n    M = P.clamp_min(eps)\n    for _ in range(int(tmax)):\n        M = M / (M.sum(dim=-1, keepdim=True).clamp_min(eps))  # row\n        M = M / (M.sum(dim=-2, keepdim=True).clamp_min(eps))  # col\n    return M\n\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[name] = p.detach().clone()\n            p.copy_(self.shadow[name])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[name])\n        self.backup = {}\n\n# ----------------------------\n# 5) mHC-lite on CLS streams (stabil: residual-to-identity + learnable alpha)\n# ----------------------------\nclass MHCLite(nn.Module):\n    \"\"\"\n    Maintain n_streams for CLS only.\n    Build per-sample mixing matrix -> Sinkhorn -> residual blend with Identity.\n    \"\"\"\n    def __init__(self, d_model, n_streams=4, alpha_init=0.01, tmax=20, dropout=0.0):\n        super().__init__()\n        self.n = int(n_streams)\n        self.tmax = int(tmax)\n        self.drop = nn.Dropout(float(dropout))\n\n        self.norm = RMSNorm(d_model, eps=1e-6)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Linear(d_model, self.n * self.n),\n        )\n        self.softplus = nn.Softplus()\n\n        a0 = float(alpha_init)\n        a0 = min(max(a0, 1e-4), 1.0 - 1e-4)\n        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n\n    def forward(self, streams, cls_vec):\n        # streams: (B,n,D), cls_vec: (B,D)\n        B, n, D = streams.shape\n        h = self.norm(cls_vec)\n        logits = self.mlp(h).view(B, n, n)  # (B,n,n)\n\n        # non-negative\n        P = self.softplus(logits)\n\n        # doubly-stochastic\n        M = sinkhorn_knopp(P, tmax=self.tmax, eps=1e-6)  # (B,n,n)\n\n        # residual blend with Identity (stability)\n        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype, device=streams.device)\n        I = torch.eye(n, device=streams.device, dtype=streams.dtype).unsqueeze(0).expand(B, -1, -1)\n        H = (1.0 - alpha) * I + alpha * M\n\n        mixed = torch.einsum(\"bij,bjd->bid\", H, streams)\n        injected = mixed + cls_vec.unsqueeze(1)\n        return self.drop(injected)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n        super().__init__()\n        self.norm1 = RMSNorm(d_model, eps=1e-6)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=d_model, num_heads=int(n_heads),\n            dropout=float(attn_dropout), batch_first=True\n        )\n        self.drop1 = nn.Dropout(float(dropout))\n\n        self.norm2 = RMSNorm(d_model, eps=1e-6)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, int(ffn_mult) * d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(int(ffn_mult) * d_model, d_model),\n        )\n        self.drop2 = nn.Dropout(float(dropout))\n\n    def forward(self, x):\n        h = self.norm1(x)\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\n        x = x + self.drop1(attn_out)\n\n        h = self.norm2(x)\n        x = x + self.drop2(self.ffn(h))\n        return x\n\nclass FTTransformer_MHCLite(nn.Module):\n    \"\"\"\n    Numeric FT-Transformer + CLS-stream mHC-lite between blocks.\n    Regularizer: feature-token drop (zero some feature tokens, not CLS).\n    \"\"\"\n    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n                 dropout=0.2, attn_dropout=0.1,\n                 n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n                 feat_token_drop_p=0.0):\n        super().__init__()\n        self.n_features = int(n_features)\n        self.d_model = int(d_model)\n        self.n_layers = int(n_layers)\n        self.feat_token_drop_p = float(feat_token_drop_p)\n\n        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n        self.in_drop = nn.Dropout(float(dropout))\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                d_model=self.d_model,\n                n_heads=n_heads,\n                ffn_mult=ffn_mult,\n                dropout=dropout,\n                attn_dropout=attn_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.mhc = nn.ModuleList([\n            MHCLite(\n                d_model=self.d_model,\n                n_streams=n_streams,\n                alpha_init=alpha_init,\n                tmax=sinkhorn_tmax,\n                dropout=mhc_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.out_norm = RMSNorm(self.d_model, eps=1e-6)\n        self.head = nn.Sequential(\n            nn.Linear(self.d_model, self.d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(self.d_model, 1),\n        )\n\n    def forward(self, x):\n        # x: (B,F)\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)  # (B,F,D)\n        tok = tok + self.feat_emb.unsqueeze(0)\n\n        # feature-token drop\n        if self.training and self.feat_token_drop_p > 0:\n            B, F_, D = tok.shape\n            keep = (torch.rand(B, F_, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n            tok = tok * keep.unsqueeze(-1)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)  # (B,1,D)\n        seq = torch.cat([cls, tok], dim=1)  # (B,1+F,D)\n        seq = self.in_drop(seq)\n\n        # init CLS streams\n        nS = self.mhc[0].n\n        streams = seq[:, 0, :].unsqueeze(1).expand(B, nS, self.d_model).contiguous()\n\n        for l, blk in enumerate(self.blocks):\n            # avoid inplace on seq[:,0,:]\n            cls_in = streams.mean(dim=1).unsqueeze(1)  # (B,1,D)\n            seq = torch.cat([cls_in, seq[:, 1:, :]], dim=1)\n\n            seq = blk(seq)\n            cls_vec = seq[:, 0, :]\n\n            streams = self.mhc[l](streams, cls_vec)\n\n        out = self.out_norm(streams.mean(dim=1))\n        logit = self.head(out).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 6) Scheduler: warmup + step decay at ratios [0.8,0.9]\n# (scheduler on OPTIMIZER STEPS, bukan per batch)\n# ----------------------------\ndef make_warmup_step_scheduler(optimizer, total_steps, warmup_steps,\n                              r1=0.8, r2=0.9, d1=0.316, d2=0.1):\n    m1 = int(float(r1) * total_steps)\n    m2 = int(float(r2) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        mult = 1.0\n        if step >= m1:\n            mult *= float(d1)\n        if step >= m2:\n            mult *= float(d2)\n        return mult\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# ----------------------------\n# 7) Predict helper (robust batch)\n# ----------------------------\n@torch.no_grad()\ndef predict_proba(model, loader, ema: EMA = None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n    probs = []\n    for batch in loader:\n        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n        xb = xb.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            logits = model(xb)\n            p = torch.sigmoid(logits)\n        probs.append(p.detach().cpu().numpy())\n    out = np.concatenate(probs, axis=0).astype(np.float32)\n    if ema is not None:\n        ema.restore(model)\n    return out\n\n# ----------------------------\n# 8) Train one fold (AMP + accum + EMA + reg)\n# ----------------------------\ndef train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg):\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n\n    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n\n    model = FTTransformer_MHCLite(\n        n_features=n_features,\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    # imbalance: pos_weight\n    pos = int(y_tr.sum())\n    neg = int(len(y_tr) - pos)\n    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n\n    # optional focal gamma\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n    label_smoothing = float(cfg.get(\"label_smoothing\", 0.0))\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    def loss_fn(logits, targets):\n        # targets float [0,1]\n        if label_smoothing and label_smoothing > 0:\n            targets = targets * (1.0 - label_smoothing) + 0.5 * label_smoothing\n        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\", pos_weight=pos_weight)\n        if focal_gamma and focal_gamma > 0:\n            p = torch.sigmoid(logits)\n            p_t = p * targets + (1 - p) * (1 - targets)\n            mod = (1.0 - p_t).clamp_min(0.0).pow(focal_gamma)\n            bce = bce * mod\n        return bce.mean()\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n        eps=float(cfg[\"adam_eps\"]),\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n    total_steps = int(cfg[\"epochs\"]) * max(1, steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt,\n        total_steps=total_steps,\n        warmup_steps=warmup_steps,\n        r1=float(cfg[\"lr_decay_ratio1\"]),\n        r2=float(cfg[\"lr_decay_ratio2\"]),\n        d1=float(cfg[\"lr_decay_rate1\"]),\n        d2=float(cfg[\"lr_decay_rate2\"]),\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n\n    best_val = 1e18\n    best_state = None\n    best_epoch = -1\n    bad = 0\n    opt_step = 0\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum = 0.0\n        n_sum = 0\n        micro = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n                loss = loss / accum_steps\n\n            scaler.scale(loss).backward()\n            micro += 1\n\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro % accum_steps) == 0:\n                if float(cfg[\"grad_clip\"]) and float(cfg[\"grad_clip\"]) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                opt_step += 1\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro % accum_steps) != 0:\n            if float(cfg[\"grad_clip\"]) and float(cfg[\"grad_clip\"]) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            opt_step += 1\n            if ema is not None:\n                ema.update(model)\n\n        # validate with EMA (if enabled)\n        p_va = predict_proba(model, dl_va, ema=ema)\n        vll = safe_logloss(y_va, p_va)\n\n        improved = (best_val - vll) > float(cfg[\"min_delta\"])\n        if improved:\n            best_val = float(vll)\n            best_epoch = int(epoch)\n            if ema is not None:\n                ema.apply_shadow(model)\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n                ema.restore(model)\n            else:\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= int(cfg[\"patience\"]):\n                break\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    # final val preds (best)\n    p_va = predict_proba(model, dl_va, ema=None)\n\n    pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": dict(cfg),\n        \"best_epoch\": int(best_epoch + 1),\n        \"best_val_logloss\": float(best_val),\n    }\n    return pack, p_va\n\n# ----------------------------\n# 9) CV evaluator for a config\n# ----------------------------\ndef run_cv_config(cfg, cfg_name, folds_subset=None, beta=0.5, thr_grid=201):\n    oof = np.zeros(n, dtype=np.float32)\n    fold_rows = []\n    fold_packs = []\n\n    use_folds = unique_folds if folds_subset is None else list(folds_subset)\n\n    for f in use_folds:\n        tr = np.where(folds_all != f)[0]\n        va = np.where(folds_all == f)[0]\n\n        X_tr, y_tr = X_all[tr], y_all[tr]\n        X_va, y_va = X_all[va], y_all[va]\n\n        pack, p_va = train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg)\n        oof[va] = p_va\n\n        fold_auc = safe_auc(y_va, p_va)\n        fold_ll  = safe_logloss(y_va, p_va)\n        best_fold = best_fbeta_fast(y_va, p_va, beta=beta, grid=max(81, thr_grid//2))\n\n        fold_rows.append({\n            \"cfg\": cfg_name,\n            \"fold\": int(f),\n            \"n_val\": int(len(va)),\n            \"pos_val\": int(y_va.sum()),\n            \"auc\": fold_auc,\n            \"logloss\": fold_ll,\n            \"best_fbeta\": best_fold[\"fbeta\"],\n            \"best_thr\": best_fold[\"thr\"],\n            \"best_prec\": best_fold[\"precision\"],\n            \"best_rec\": best_fold[\"recall\"],\n            \"best_val_logloss\": float(pack[\"best_val_logloss\"]),\n            \"best_epoch\": int(pack[\"best_epoch\"]),\n        })\n\n        pack2 = dict(pack)\n        pack2[\"fold\"] = int(f)\n        fold_packs.append(pack2)\n\n        del pack\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if folds_subset is None:\n        idx_eval = np.arange(n)\n    else:\n        idx_eval = np.where(np.isin(folds_all, np.array(use_folds)))[0]\n\n    oof_eval = oof[idx_eval]\n    y_eval = y_all[idx_eval]\n\n    oof_auc = safe_auc(y_eval, oof_eval)\n    oof_ll  = safe_logloss(y_eval, oof_eval)\n    best_oof = best_fbeta_fast(y_eval, oof_eval, beta=beta, grid=thr_grid)\n\n    summary = {\n        \"cfg\": cfg_name,\n        \"stage\": \"full\" if folds_subset is None else f\"subset{len(use_folds)}\",\n        \"oof_auc\": oof_auc,\n        \"oof_logloss\": oof_ll,\n        \"oof_best_fbeta\": best_oof[\"fbeta\"],\n        \"oof_best_thr\": best_oof[\"thr\"],\n        \"oof_best_prec\": best_oof[\"precision\"],\n        \"oof_best_rec\": best_oof[\"recall\"],\n\n        # main arch\n        \"d_model\": cfg[\"d_model\"],\n        \"n_layers\": cfg[\"n_layers\"],\n        \"n_heads\": cfg[\"n_heads\"],\n        \"ffn_mult\": cfg[\"ffn_mult\"],\n        \"dropout\": cfg[\"dropout\"],\n        \"attn_dropout\": cfg[\"attn_dropout\"],\n\n        # mHC-lite\n        \"n_streams\": cfg[\"n_streams\"],\n        \"alpha_init\": cfg[\"alpha_init\"],\n        \"sinkhorn_tmax\": cfg[\"sinkhorn_tmax\"],\n        \"mhc_dropout\": cfg[\"mhc_dropout\"],\n\n        # reg\n        \"feat_token_drop_p\": cfg.get(\"feat_token_drop_p\", 0.0),\n        \"input_noise_std\": cfg.get(\"input_noise_std\", 0.0),\n        \"focal_gamma\": cfg.get(\"focal_gamma\", 0.0),\n        \"label_smoothing\": cfg.get(\"label_smoothing\", 0.0),\n\n        # train\n        \"batch_size\": cfg[\"batch_size\"],\n        \"accum_steps\": cfg.get(\"accum_steps\", 1),\n        \"epochs\": cfg[\"epochs\"],\n        \"lr\": cfg[\"lr\"],\n        \"weight_decay\": cfg[\"weight_decay\"],\n        \"warmup_frac\": cfg[\"warmup_frac\"],\n        \"beta1\": cfg[\"beta1\"],\n        \"beta2\": cfg[\"beta2\"],\n        \"adam_eps\": cfg[\"adam_eps\"],\n        \"lr_decay_ratio1\": cfg[\"lr_decay_ratio1\"],\n        \"lr_decay_ratio2\": cfg[\"lr_decay_ratio2\"],\n        \"lr_decay_rate1\": cfg[\"lr_decay_rate1\"],\n        \"lr_decay_rate2\": cfg[\"lr_decay_rate2\"],\n        \"patience\": cfg[\"patience\"],\n        \"min_delta\": cfg[\"min_delta\"],\n        \"grad_clip\": cfg[\"grad_clip\"],\n        \"use_ema\": cfg.get(\"use_ema\", True),\n        \"ema_decay\": cfg.get(\"ema_decay\", 0.999),\n    }\n    return summary, fold_rows, oof, fold_packs\n\n# ----------------------------\n# 10) Candidate configs (lebih efektif: kecil tapi tajam)\n# ----------------------------\ndef make_base():\n    # heuristic batch/accum\n    if device.type == \"cuda\":\n        if MEM_GB >= 30:\n            bs, acc = 512, 2\n        elif MEM_GB >= 16:\n            bs, acc = 384, 2\n        else:\n            bs, acc = 256, 2\n    else:\n        bs, acc = 256, 1\n\n    return dict(\n        batch_size=bs,\n        accum_steps=acc,\n        epochs=75 if device.type == \"cuda\" else 40,\n        lr=2e-4,\n        weight_decay=1.0e-2,\n        warmup_frac=0.10,\n        grad_clip=1.0,\n        patience=10,\n        min_delta=1e-4,\n\n        # optimizer (PDF-ish)\n        beta1=0.9,\n        beta2=0.95,\n        adam_eps=1e-8,\n\n        # schedule (PDF)\n        lr_decay_ratio1=0.8,\n        lr_decay_ratio2=0.9,\n        lr_decay_rate1=0.316,\n        lr_decay_rate2=0.1,\n\n        # mHC-lite\n        n_streams=4,\n        alpha_init=0.01,\n        sinkhorn_tmax=20,\n        mhc_dropout=0.00,\n\n        # regularization\n        feat_token_drop_p=0.05,\n        input_noise_std=0.01,\n        focal_gamma=1.5,\n        label_smoothing=0.00,\n\n        # EMA\n        use_ema=True,\n        ema_decay=0.999,\n    )\n\nBASE = make_base()\n\ncandidates = []\ncandidates.append((\"mhc_384x8_main\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=4, dropout=0.18, attn_dropout=0.10)))\ncandidates.append((\"mhc_384x10_reg\", dict(BASE, d_model=384, n_layers=10, n_heads=8,  ffn_mult=4, dropout=0.24, attn_dropout=0.12,\n                                         lr=1.6e-4, weight_decay=1.5e-2, patience=12,\n                                         feat_token_drop_p=0.06, input_noise_std=0.012)))\ncandidates.append((\"mhc_384x8_ffn2\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=2, dropout=0.16, attn_dropout=0.10,\n                                         lr=2.2e-4, weight_decay=8e-3)))\ncandidates.append((\"mhc_256x6_fast\", dict(BASE, d_model=256, n_layers=6,  n_heads=8,  ffn_mult=4, dropout=0.16, attn_dropout=0.08,\n                                         lr=3e-4, weight_decay=6e-3, epochs=min(BASE[\"epochs\"], 60), patience=9,\n                                         feat_token_drop_p=0.04, input_noise_std=0.010)))\n\nif device.type == \"cuda\" and MEM_GB >= 20:\n    candidates.append((\"mhc_512x10_big\", dict(BASE, d_model=512, n_layers=10, n_heads=16, ffn_mult=4, dropout=0.26, attn_dropout=0.14,\n                                             lr=1.2e-4, weight_decay=2.0e-2, epochs=max(BASE[\"epochs\"], 85), patience=12,\n                                             mhc_dropout=0.05, feat_token_drop_p=0.06)))\n\nprint(f\"\\nTotal Transformer candidates: {len(candidates)}\")\nprint(\"Primary score: OOF best F-beta (beta=0.5)\")\n\n# ----------------------------\n# 11) Run 2-stage search\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOPT_DIR = OUT_DIR / \"opt_search\"\nOPT_DIR.mkdir(parents=True, exist_ok=True)\n\n# stage-1 folds subset (evenly spaced)\nif STAGE1_FOLDS >= len(unique_folds):\n    folds_subset = unique_folds\nelse:\n    stepk = max(1, len(unique_folds) // STAGE1_FOLDS)\n    folds_subset = unique_folds[::stepk][:STAGE1_FOLDS]\n\nprint(\"\\nStage-1 folds subset:\", folds_subset)\n\nt0 = time.time()\nstage1_rows = []\nstage1_fold_rows = []\n\nfor i, (name, cfg) in enumerate(candidates, 1):\n    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n        print(\"Time budget reached. Stop search.\")\n        break\n\n    cfg1 = dict(cfg)\n    cfg1[\"epochs\"] = int(min(int(cfg1[\"epochs\"]), int(STAGE1_EPOCH_CAP)))\n    cfg1[\"patience\"] = int(min(int(cfg1[\"patience\"]), int(STAGE1_PAT_CAP)))\n\n    print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] CV(subset) -> {name}\")\n    summ, fold_rows, _, _ = run_cv_config(cfg1, name, folds_subset=folds_subset, beta=BETA, thr_grid=101)\n\n    stage1_rows.append(summ)\n    stage1_fold_rows.extend(fold_rows)\n\n    print(f\"  stage1 best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f} | logloss: {summ['oof_logloss']:.6f}\")\n\ndf_s1 = pd.DataFrame(stage1_rows).sort_values([\"oof_best_fbeta\",\"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\nprint(\"\\nStage-1 ranking (top):\")\ndisplay(df_s1.head(10))\n\ntopM = min(int(STAGE2_TOPM), len(df_s1))\nstage2_names = df_s1[\"cfg\"].head(topM).tolist()\nprint(\"\\nStage-2 will run full CV for:\", stage2_names)\n\nall_summaries = []\nall_fold_rows = []\noof_store = {}\npack_store = {}\n\nfor j, nm in enumerate(stage2_names, 1):\n    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n        print(\"Time budget reached. Stop stage-2.\")\n        break\n\n    cfg = None\n    for (nname, ccfg) in candidates:\n        if nname == nm:\n            cfg = ccfg\n            break\n    if cfg is None:\n        continue\n\n    print(f\"\\n[Stage-2 {j:02d}/{len(stage2_names)}] CV(full) -> {nm}\")\n    summ, fold_rows, oof, fold_packs = run_cv_config(cfg, nm, folds_subset=None, beta=BETA, thr_grid=THR_GRID)\n\n    all_summaries.append(summ)\n    all_fold_rows.extend(fold_rows)\n    oof_store[nm] = oof\n    pack_store[nm] = fold_packs\n\n    print(f\"  OOF best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f}\"\n          f\" | auc: {(summ['oof_auc'] if summ['oof_auc'] is not None else float('nan')):.6f}\"\n          f\" | logloss: {summ['oof_logloss']:.6f}\")\n\ndf_sum = pd.DataFrame(all_summaries)\ndf_fold = pd.DataFrame(all_fold_rows)\n\ndf_sum = df_sum.sort_values([\"oof_best_fbeta\", \"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n\nprint(\"\\nStage-2 top candidates (full CV):\")\ndisplay(df_sum)\n\n# save search results\ndf_sum.to_csv(OPT_DIR / \"opt_results.csv\", index=False)\nwith open(OPT_DIR / \"opt_results.json\", \"w\") as f:\n    json.dump(df_sum.to_dict(orient=\"records\"), f, indent=2)\ndf_fold.to_csv(OPT_DIR / \"opt_fold_details.csv\", index=False)\n\n# save OOF preds for top configs (debug)\ntop_names = df_sum[\"cfg\"].head(min(REPORT_TOPK_OOF, len(df_sum))).tolist()\nfor nm in top_names:\n    df_o = pd.DataFrame({\n        \"uid\": uids_all,\n        \"y\": y_all,\n        \"fold\": folds_all,\n        f\"oof_pred_{nm}\": oof_store[nm]\n    })\n    df_o.to_csv(OPT_DIR / f\"oof_preds_{nm}.csv\", index=False)\n\n# ----------------------------\n# 12) Choose best config + save BEST fold packs (tanpa retrain ulang)\n# ----------------------------\nif len(df_sum) == 0:\n    raise RuntimeError(\"Stage-2 produced no results. Turunkan kandidat/epochs atau cek device/VRAM.\")\n\nbest_single = df_sum.iloc[0].to_dict()\nbest_cfg_name = str(best_single[\"cfg\"])\n\nbest_cfg = None\nfor nm, cfg in candidates:\n    if nm == best_cfg_name:\n        best_cfg = cfg\n        break\nif best_cfg is None:\n    raise RuntimeError(\"Best cfg not found in candidates list (unexpected).\")\n\nbest_fold_packs = pack_store[best_cfg_name]\nbest_oof = oof_store[best_cfg_name]\nbest_oof_best = best_fbeta_fast(y_all, best_oof, beta=BETA, grid=THR_GRID)\n\nbest_model_path = OUT_DIR / \"best_gate_model.pt\"\ntorch.save(\n    {\n        \"type\": \"mhc_lite_ft_transformer_v3\",\n        \"feature_cols\": FEATURE_COLS,\n        \"fold_packs\": best_fold_packs,\n        \"cfg_name\": best_cfg_name,\n        \"cfg\": best_cfg,\n        \"seed\": SEED,\n    },\n    best_model_path\n)\n\nbest_bundle = {\n    \"type\": \"mhc_lite_ft_transformer_v3\",\n    \"model_name\": best_cfg_name,\n    \"members\": [best_cfg_name],\n    \"random_seed\": SEED,\n    \"beta_for_tuning\": BETA,\n\n    \"feature_cols\": FEATURE_COLS,\n    \"cfg\": best_cfg,\n\n    \"oof_best_thr\": best_oof_best[\"thr\"],\n    \"oof_best_fbeta\": best_oof_best[\"fbeta\"],\n    \"oof_best_prec\": best_oof_best[\"precision\"],\n    \"oof_best_rec\": best_oof_best[\"recall\"],\n    \"oof_auc\": safe_auc(y_all, best_oof),\n    \"oof_logloss\": safe_logloss(y_all, best_oof),\n\n    \"notes\": \"Best config from Step 4 (Transformer-only, mHC-lite differentiable + EMA + accum + reg). Step 5 should train FULL model or ensemble fold packs for inference.\",\n}\n\nwith open(OUT_DIR / \"best_gate_config.json\", \"w\") as f:\n    json.dump(best_bundle, f, indent=2)\n\nprint(\"\\nSaved best artifacts:\")\nprint(\"  best model (fold packs) ->\", best_model_path)\nprint(\"  best config             ->\", OUT_DIR / \"best_gate_config.json\")\nprint(\"  opt results             ->\", OPT_DIR / \"opt_results.csv\")\nprint(\"  fold detail             ->\", OPT_DIR / \"opt_fold_details.csv\")\n\n# Export globals for Step 5\nBEST_GATE_BUNDLE = best_bundle\nBEST_TF_CFG_NAME = best_cfg_name\nBEST_TF_CFG = best_cfg\nOPT_RESULTS_DF = df_sum\nBEST_TF_OOF = best_oof\nBEST_TF_OOF_METRIC = best_oof_best\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Training (Train on Full Data)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 5 — Final Training (Train on Full Data) — TRANSFORMER ONLY\n# REVISI FULL v4 (match Step 4 v3.1: mHC-lite differentiable + EMA + accum)\n#\n# Upgrade utama vs Step 5 kamu:\n# - Arsitektur disamakan dengan Step 4 best: FTTransformer_MHCLite (bukan FTTransformerBig)\n# - Sinkhorn differentiable + residual-to-Identity alpha (stabil)\n# - EMA eval + best saving (lebih stabil)\n# - Grad accumulation + scheduler dihitung per OPTIMIZER STEP (benar)\n# - Internal case-level val untuk cari best_epoch -> retrain full data dengan epoch itu\n# - OOM fallback otomatis\n#\n# Output:\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n# ============================================================\n\nimport os, json, gc, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import log_loss, roc_auc_score\n\n# ----------------------------\n# 0) REQUIRE\n# ----------------------------\nif \"df_train_tabular\" not in globals():\n    raise RuntimeError(\"Missing `df_train_tabular`. Jalankan Step 2 dulu.\")\nif \"FEATURE_COLS\" not in globals():\n    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 dulu.\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nneed_cols = {\"uid\", \"case_id\", \"y\"}\nmiss = [c for c in need_cols if c not in df_train_tabular.columns]\nif miss:\n    raise ValueError(f\"df_train_tabular missing columns: {miss}\")\n\nX_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n\nif not np.isfinite(X_all).all():\n    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Final training data:\")\nprint(f\"  rows={len(y_all)} | pos%={float(y_all.mean())*100:.2f} | n_features={X_all.shape[1]}\")\n\n# ----------------------------\n# 1) Load best cfg (Step 4 output)\n# ----------------------------\nbest_bundle = None\nsource = None\n\ncfg_path = OUT_DIR / \"best_gate_config.json\"\nbest_model_candidates = [\n    OUT_DIR / \"best_gate_model.pt\",\n    OUT_DIR / \"best_gate_model.pth\",\n]\n\nif \"BEST_GATE_BUNDLE\" in globals() and isinstance(BEST_GATE_BUNDLE, dict):\n    best_bundle = BEST_GATE_BUNDLE\n    source = \"memory(BEST_GATE_BUNDLE)\"\nelif cfg_path.exists():\n    best_bundle = json.loads(cfg_path.read_text())\n    source = str(cfg_path)\n\nif best_bundle is not None and isinstance(best_bundle, dict) and isinstance(best_bundle.get(\"cfg\", None), dict):\n    base_cfg = dict(best_bundle[\"cfg\"])\n    print(\"\\nLoaded cfg from:\", source)\nelse:\n    base_cfg = {}\n    print(\"\\nNo best_gate_config found. Using strong default cfg.\")\n\n# optional: load fold_packs dari best_gate_model.pt (biar final file bisa bawa ensemble fold juga)\nfold_packs_from_step4 = None\nbest_gate_model_path = None\nfor p in best_model_candidates:\n    if p.exists():\n        best_gate_model_path = p\n        break\n\nif best_gate_model_path is not None:\n    try:\n        obj = torch.load(best_gate_model_path, map_location=\"cpu\")\n        if isinstance(obj, dict) and isinstance(obj.get(\"fold_packs\", None), list):\n            fold_packs_from_step4 = obj[\"fold_packs\"]\n            print(\"Loaded fold_packs from:\", str(best_gate_model_path))\n    except Exception as e:\n        print(\"Warning: failed to load best_gate_model.pt fold_packs:\", repr(e))\n\n# ----------------------------\n# 2) Device + seed\n# ----------------------------\ndef seed_everything(seed: int):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nFINAL_SEED = 2025\nif isinstance(best_bundle, dict):\n    FINAL_SEED = int(best_bundle.get(\"seed\", best_bundle.get(\"random_seed\", 2025)))\n\nseed_everything(FINAL_SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\n\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\nvram_gb = None\nif device.type == \"cuda\":\n    vram_gb = float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n\nprint(\"\\nDevice:\", device, \"| AMP:\", use_amp, \"| VRAM_GB:\", (f\"{vram_gb:.1f}\" if vram_gb else \"CPU\"))\n\n# ----------------------------\n# 3) Training policy (stabil + kuat)\n# ----------------------------\nWANT_BIG_MODEL = True\nUSE_INTERNAL_VAL = True         # cari best_epoch dari val (case-level)\nVAL_FRAC_CASE = 0.08            # 8% case untuk val\nEARLY_STOP = True\n\n# runtime: 1 seed default\nN_SEEDS = 1\n\n# target effective batch\nTARGET_EFF_BATCH = 1024 if device.type == \"cuda\" else 256\n\n# ----------------------------\n# 4) Dataset + Standardizer\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 5) Metrics helpers\n# ----------------------------\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n    return float(log_loss(y_true, p, labels=[0, 1]))\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\n# ----------------------------\n# 6) Model: FTTransformer_MHCLite (MATCH Step 4 v3.1)\n# ----------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, d, eps=1e-6):\n        super().__init__()\n        self.eps = float(eps)\n        self.weight = nn.Parameter(torch.ones(d))\n    def forward(self, x):\n        rms = torch.mean(x * x, dim=-1, keepdim=True)\n        x = x * torch.rsqrt(rms + self.eps)\n        return x * self.weight\n\ndef sinkhorn_knopp(P, tmax=20, eps=1e-6):\n    M = P.clamp_min(eps)\n    for _ in range(int(tmax)):\n        M = M / (M.sum(dim=-1, keepdim=True).clamp_min(eps))  # row\n        M = M / (M.sum(dim=-2, keepdim=True).clamp_min(eps))  # col\n    return M\n\nclass MHCLite(nn.Module):\n    def __init__(self, d_model, n_streams=4, alpha_init=0.01, tmax=20, dropout=0.0):\n        super().__init__()\n        self.n = int(n_streams)\n        self.tmax = int(tmax)\n        self.drop = nn.Dropout(float(dropout))\n\n        self.norm = RMSNorm(d_model, eps=1e-6)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Linear(d_model, self.n * self.n),\n        )\n        self.softplus = nn.Softplus()\n\n        a0 = float(alpha_init)\n        a0 = min(max(a0, 1e-4), 1.0 - 1e-4)\n        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n\n    def forward(self, streams, cls_vec):\n        # streams: (B,n,D), cls_vec: (B,D)\n        B, n, D = streams.shape\n        h = self.norm(cls_vec)\n        logits = self.mlp(h).view(B, n, n)\n        P = self.softplus(logits)\n        M = sinkhorn_knopp(P, tmax=self.tmax, eps=1e-6)\n\n        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype, device=streams.device)\n        I = torch.eye(n, device=streams.device, dtype=streams.dtype).unsqueeze(0).expand(B, -1, -1)\n        H = (1.0 - alpha) * I + alpha * M\n\n        mixed = torch.einsum(\"bij,bjd->bid\", H, streams)\n        injected = mixed + cls_vec.unsqueeze(1)\n        return self.drop(injected)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n        super().__init__()\n        self.norm1 = RMSNorm(d_model, eps=1e-6)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=d_model, num_heads=int(n_heads),\n            dropout=float(attn_dropout), batch_first=True\n        )\n        self.drop1 = nn.Dropout(float(dropout))\n\n        self.norm2 = RMSNorm(d_model, eps=1e-6)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, int(ffn_mult) * d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(int(ffn_mult) * d_model, d_model),\n        )\n        self.drop2 = nn.Dropout(float(dropout))\n\n    def forward(self, x):\n        h = self.norm1(x)\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\n        x = x + self.drop1(attn_out)\n        h = self.norm2(x)\n        x = x + self.drop2(self.ffn(h))\n        return x\n\nclass FTTransformer_MHCLite(nn.Module):\n    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n                 dropout=0.2, attn_dropout=0.1,\n                 n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n                 feat_token_drop_p=0.0):\n        super().__init__()\n        self.n_features = int(n_features)\n        self.d_model = int(d_model)\n        self.n_layers = int(n_layers)\n        self.feat_token_drop_p = float(feat_token_drop_p)\n\n        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n        self.in_drop = nn.Dropout(float(dropout))\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                d_model=self.d_model,\n                n_heads=n_heads,\n                ffn_mult=ffn_mult,\n                dropout=dropout,\n                attn_dropout=attn_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.mhc = nn.ModuleList([\n            MHCLite(\n                d_model=self.d_model,\n                n_streams=n_streams,\n                alpha_init=alpha_init,\n                tmax=sinkhorn_tmax,\n                dropout=mhc_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.out_norm = RMSNorm(self.d_model, eps=1e-6)\n        self.head = nn.Sequential(\n            nn.Linear(self.d_model, self.d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(self.d_model, 1),\n        )\n\n    def forward(self, x):\n        # x: (B,F)\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)  # (B,F,D)\n        tok = tok + self.feat_emb.unsqueeze(0)\n\n        # feature-token drop\n        if self.training and self.feat_token_drop_p > 0:\n            B, F_, D = tok.shape\n            keep = (torch.rand(B, F_, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n            tok = tok * keep.unsqueeze(-1)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)  # (B,1,D)\n        seq = torch.cat([cls, tok], dim=1)  # (B,1+F,D)\n        seq = self.in_drop(seq)\n\n        nS = self.mhc[0].n\n        streams = seq[:, 0, :].unsqueeze(1).expand(B, nS, self.d_model).contiguous()\n\n        for l, blk in enumerate(self.blocks):\n            cls_in = streams.mean(dim=1).unsqueeze(1)  # (B,1,D)\n            seq = torch.cat([cls_in, seq[:, 1:, :]], dim=1)\n            seq = blk(seq)\n            cls_vec = seq[:, 0, :]\n            streams = self.mhc[l](streams, cls_vec)\n\n        out = self.out_norm(streams.mean(dim=1))\n        logit = self.head(out).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 7) EMA + scheduler (optimizer-steps)\n# ----------------------------\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[name] = p.detach().clone()\n            p.copy_(self.shadow[name])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[name])\n        self.backup = {}\n\ndef make_warmup_step_scheduler(optimizer, total_steps, warmup_steps,\n                              r1=0.8, r2=0.9, d1=0.316, d2=0.1):\n    m1 = int(float(r1) * total_steps)\n    m2 = int(float(r2) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        mult = 1.0\n        if step >= m1:\n            mult *= float(d1)\n        if step >= m2:\n            mult *= float(d2)\n        return mult\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n@torch.no_grad()\ndef predict_proba(model, loader, ema: EMA = None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n    ps = []\n    for batch in loader:\n        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n        xb = xb.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            logits = model(xb)\n            p = torch.sigmoid(logits)\n        ps.append(p.detach().cpu().numpy())\n    out = np.concatenate(ps, axis=0).astype(np.float32)\n    if ema is not None:\n        ema.restore(model)\n    return out\n\n# ----------------------------\n# 8) CFG merge + autoscale\n# ----------------------------\n# default fallback cfg (kalau best_bundle tidak ada)\nCFG = dict(\n    # arch\n    d_model=384, n_layers=8, n_heads=8, ffn_mult=4,\n    dropout=0.20, attn_dropout=0.10,\n\n    # mHC-lite\n    n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n\n    # regularization\n    feat_token_drop_p=0.05,\n    input_noise_std=0.01,\n    focal_gamma=1.5,\n    label_smoothing=0.00,\n\n    # optim\n    lr=2e-4,\n    weight_decay=1.0e-2,\n    beta1=0.9,\n    beta2=0.95,\n    adam_eps=1e-8,\n\n    # sched\n    warmup_frac=0.10,\n    lr_decay_ratio1=0.8,\n    lr_decay_ratio2=0.9,\n    lr_decay_rate1=0.316,\n    lr_decay_rate2=0.1,\n\n    # train\n    batch_size=512 if device.type == \"cuda\" else 256,\n    epochs=75 if device.type == \"cuda\" else 35,\n    accum_steps=2 if device.type == \"cuda\" else 1,\n    grad_clip=1.0,\n    patience=10,\n    min_delta=1e-4,\n\n    # EMA\n    use_ema=True,\n    ema_decay=0.999,\n)\n\n# merge best cfg -> CFG\nfor k, v in base_cfg.items():\n    if k in CFG:\n        CFG[k] = v\n\ndef autoscale_cfg(cfg: dict):\n    cfg = dict(cfg)\n    if device.type != \"cuda\":\n        cfg[\"d_model\"] = min(int(cfg[\"d_model\"]), 256)\n        cfg[\"n_layers\"] = min(int(cfg[\"n_layers\"]), 6)\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 256)\n        cfg[\"epochs\"] = min(int(cfg[\"epochs\"]), 35)\n        cfg[\"accum_steps\"] = 1\n        return cfg\n\n    # CUDA scaling “besar tapi aman”\n    if not WANT_BIG_MODEL or vram_gb is None:\n        return cfg\n\n    if vram_gb >= 24:\n        cfg[\"d_model\"] = max(int(cfg[\"d_model\"]), 512)\n        cfg[\"n_layers\"] = max(int(cfg[\"n_layers\"]), 10)\n        cfg[\"n_heads\"]  = max(int(cfg[\"n_heads\"]), 16)\n        cfg[\"dropout\"] = max(float(cfg[\"dropout\"]), 0.24)\n        cfg[\"attn_dropout\"] = max(float(cfg[\"attn_dropout\"]), 0.12)\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 512)\n        cfg[\"epochs\"] = max(int(cfg[\"epochs\"]), 85)\n        cfg[\"weight_decay\"] = max(float(cfg[\"weight_decay\"]), 2e-2)\n        cfg[\"lr\"] = min(float(cfg[\"lr\"]), 1.6e-4)\n        cfg[\"accum_steps\"] = max(int(cfg.get(\"accum_steps\", 2)), 2)\n    elif vram_gb >= 16:\n        cfg[\"d_model\"] = max(int(cfg[\"d_model\"]), 448)\n        cfg[\"n_layers\"] = max(int(cfg[\"n_layers\"]), 9)\n        cfg[\"dropout\"] = max(float(cfg[\"dropout\"]), 0.22)\n        cfg[\"attn_dropout\"] = max(float(cfg[\"attn_dropout\"]), 0.12)\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 512)\n        cfg[\"epochs\"] = max(int(cfg[\"epochs\"]), 75)\n        cfg[\"weight_decay\"] = max(float(cfg[\"weight_decay\"]), 1.5e-2)\n        cfg[\"lr\"] = min(float(cfg[\"lr\"]), 1.8e-4)\n        cfg[\"accum_steps\"] = max(int(cfg.get(\"accum_steps\", 2)), 2)\n    else:\n        cfg[\"d_model\"] = max(int(cfg[\"d_model\"]), 384)\n        cfg[\"n_layers\"] = max(int(cfg[\"n_layers\"]), 8)\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 384)\n        cfg[\"epochs\"] = max(int(cfg[\"epochs\"]), 70)\n        cfg[\"accum_steps\"] = max(int(cfg.get(\"accum_steps\", 2)), 2)\n    return cfg\n\nCFG = autoscale_cfg(CFG)\n\n# ensure effective batch near target\nCFG[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(CFG[\"batch_size\"]))))\n\nprint(\"\\nCFG (final):\")\nfor k in [\n    \"d_model\",\"n_layers\",\"n_heads\",\"ffn_mult\",\"dropout\",\"attn_dropout\",\n    \"n_streams\",\"alpha_init\",\"sinkhorn_tmax\",\"mhc_dropout\",\n    \"batch_size\",\"accum_steps\",\"epochs\",\"lr\",\"weight_decay\",\"warmup_frac\",\"patience\"\n]:\n    print(f\"  {k}: {CFG[k]}\")\n\n# ----------------------------\n# 9) Internal val split (case_id-safe)\n# ----------------------------\ndef make_case_split(df: pd.DataFrame, val_frac=0.08, seed=2025):\n    g = df.groupby(\"case_id\")[\"y\"].max().reset_index().rename(columns={\"y\": \"case_y\"})\n    pos_cases = g.loc[g[\"case_y\"] == 1, \"case_id\"].to_numpy()\n    neg_cases = g.loc[g[\"case_y\"] == 0, \"case_id\"].to_numpy()\n\n    rng = np.random.RandomState(int(seed))\n    rng.shuffle(pos_cases)\n    rng.shuffle(neg_cases)\n\n    n_val_pos = max(1, int(len(pos_cases) * float(val_frac))) if len(pos_cases) else 0\n    n_val_neg = max(1, int(len(neg_cases) * float(val_frac))) if len(neg_cases) else 0\n\n    val_cases = np.concatenate([pos_cases[:n_val_pos], neg_cases[:n_val_neg]])\n    val_set = set(map(str, val_cases.tolist()))\n    is_val = df[\"case_id\"].astype(str).map(lambda x: str(x) in val_set).to_numpy(dtype=bool)\n    return is_val\n\n# ----------------------------\n# 10) Train with internal val to get best_epoch\n# ----------------------------\ndef train_with_internal_val_get_best_epoch(X_raw, y_raw, cfg, seed=2025):\n    seed_everything(int(seed))\n\n    is_val = make_case_split(df_train_tabular, val_frac=float(VAL_FRAC_CASE), seed=int(seed))\n    tr_idx = np.where(~is_val)[0]\n    va_idx = np.where(is_val)[0]\n\n    X_tr, y_tr = X_raw[tr_idx], y_raw[tr_idx]\n    X_va, y_va = X_raw[va_idx], y_raw[va_idx]\n\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                       num_workers=2, pin_memory=(device.type==\"cuda\"), drop_last=False)\n    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                       num_workers=2, pin_memory=(device.type==\"cuda\"), drop_last=False)\n\n    model = FTTransformer_MHCLite(\n        n_features=X_raw.shape[1],\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    # pos_weight\n    pos = float(y_tr.sum())\n    neg = float(len(y_tr) - pos)\n    pos_weight = torch.tensor([neg / max(1.0, pos)], device=device, dtype=torch.float32)\n\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n    label_smoothing = float(cfg.get(\"label_smoothing\", 0.0))\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    def loss_fn(logits, targets):\n        if label_smoothing and label_smoothing > 0:\n            targets = targets * (1.0 - label_smoothing) + 0.5 * label_smoothing\n        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\", pos_weight=pos_weight)\n        if focal_gamma and focal_gamma > 0:\n            p = torch.sigmoid(logits)\n            p_t = p * targets + (1 - p) * (1 - targets)\n            mod = (1.0 - p_t).clamp_min(0.0).pow(focal_gamma)\n            bce = bce * mod\n        return bce.mean()\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n        eps=float(cfg[\"adam_eps\"]),\n    )\n\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl_tr) / accum))\n    total_steps = int(cfg[\"epochs\"]) * max(1, steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt, total_steps=total_steps, warmup_steps=warmup_steps,\n        r1=float(cfg[\"lr_decay_ratio1\"]), r2=float(cfg[\"lr_decay_ratio2\"]),\n        d1=float(cfg[\"lr_decay_rate1\"]), d2=float(cfg[\"lr_decay_rate2\"])\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n\n    best_val = 1e18\n    best_epoch = -1\n    bad = 0\n\n    print(f\"\\nInternal val split: train={len(tr_idx)} | val={len(va_idx)} | val_pos%={float(y_va.mean())*100:.2f}\")\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n        loss_sum = 0.0\n        n_sum = 0\n        micro = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n                loss = loss / float(accum)\n\n            scaler.scale(loss).backward()\n            micro += 1\n\n            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n            n_sum += xb.size(0)\n\n            if (micro % accum) == 0:\n                if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro % accum) != 0:\n            if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        p_va = predict_proba(model, dl_va, ema=ema)\n        vll = safe_logloss(y_va, p_va)\n        vauc = safe_auc(y_va, p_va)\n\n        improved = (best_val - vll) > float(cfg[\"min_delta\"])\n        if improved:\n            best_val = float(vll)\n            best_epoch = int(epoch) + 1\n            bad = 0\n        else:\n            bad += 1\n\n        print(f\"  epoch {epoch+1:03d}/{int(cfg['epochs'])} | tr_loss={loss_sum/max(1,n_sum):.5f} | val_ll={vll:.5f} | val_auc={(vauc if vauc is not None else float('nan')):.5f} | bad={bad}\")\n\n        if EARLY_STOP and bad >= int(cfg[\"patience\"]):\n            break\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    # best_epoch fallback\n    if best_epoch < 1:\n        best_epoch = max(12, int(cfg[\"epochs\"] * 0.6))\n\n    return {\n        \"best_epoch\": int(best_epoch),\n        \"best_val_logloss\": float(best_val) if best_val < 1e18 else None,\n    }\n\n# ----------------------------\n# 11) Train FULL data for fixed epochs (best_epoch)\n# ----------------------------\ndef train_full_fixed_epochs(X_raw, y_raw, cfg, epochs_fixed, seed=2025):\n    seed_everything(int(seed))\n\n    mu, sig = fit_standardizer(X_raw)\n    Xn = apply_standardizer(X_raw, mu, sig)\n\n    ds = TabDataset(Xn, y_raw)\n    dl = DataLoader(ds, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                    num_workers=2, pin_memory=(device.type==\"cuda\"), drop_last=False)\n\n    model = FTTransformer_MHCLite(\n        n_features=X_raw.shape[1],\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    pos = float(y_raw.sum())\n    neg = float(len(y_raw) - pos)\n    pos_weight = torch.tensor([neg / max(1.0, pos)], device=device, dtype=torch.float32)\n\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n    label_smoothing = float(cfg.get(\"label_smoothing\", 0.0))\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    def loss_fn(logits, targets):\n        if label_smoothing and label_smoothing > 0:\n            targets = targets * (1.0 - label_smoothing) + 0.5 * label_smoothing\n        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\", pos_weight=pos_weight)\n        if focal_gamma and focal_gamma > 0:\n            p = torch.sigmoid(logits)\n            p_t = p * targets + (1 - p) * (1 - targets)\n            mod = (1.0 - p_t).clamp_min(0.0).pow(focal_gamma)\n            bce = bce * mod\n        return bce.mean()\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n        eps=float(cfg[\"adam_eps\"]),\n    )\n\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl) / accum))\n    total_steps = int(epochs_fixed) * max(1, steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt, total_steps=total_steps, warmup_steps=warmup_steps,\n        r1=float(cfg[\"lr_decay_ratio1\"]), r2=float(cfg[\"lr_decay_ratio2\"]),\n        d1=float(cfg[\"lr_decay_rate1\"]), d2=float(cfg[\"lr_decay_rate2\"])\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n\n    t0 = time.time()\n    for epoch in range(int(epochs_fixed)):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n        loss_sum = 0.0\n        n_sum = 0\n        micro = 0\n\n        for xb, yb in dl:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n                loss = loss / float(accum)\n\n            scaler.scale(loss).backward()\n            micro += 1\n\n            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n            n_sum += xb.size(0)\n\n            if (micro % accum) == 0:\n                if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro % accum) != 0:\n            if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        print(f\"  full epoch {epoch+1:03d}/{int(epochs_fixed)} | loss={loss_sum/max(1,n_sum):.5f}\")\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    # save EMA weights if enabled (usually better)\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    pack = {\n        \"type\": \"mhc_lite_ft_transformer_full_v4\",\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": dict(cfg),\n        \"seed\": int(seed),\n        \"train_rows\": int(len(y_raw)),\n        \"pos_rate\": float(y_raw.mean()),\n        \"epochs_fixed\": int(epochs_fixed),\n        \"accum_steps\": int(accum),\n        \"train_time_s\": float(time.time() - t0),\n        \"used_ema_weights\": bool(ema is not None),\n    }\n    return pack\n\n# ----------------------------\n# 12) Train final (OOM-safe fallback)\n# ----------------------------\nfinal_full_packs = []\n\nfor s in range(int(N_SEEDS)):\n    print(f\"\\n[Final Train v4] seed_offset={s}\")\n\n    try:\n        # Phase A: get best_epoch from internal val\n        if USE_INTERNAL_VAL:\n            info = train_with_internal_val_get_best_epoch(X_all, y_all, CFG, seed=FINAL_SEED + s)\n            best_epoch = int(info[\"best_epoch\"])\n            # retrain on full data (slightly extend by 5% for stability)\n            E_FULL = int(min(int(CFG[\"epochs\"]), max(12, round(best_epoch * 1.05))))\n            print(f\"\\nBest_epoch(from internal val)={best_epoch} -> Retrain FULL for E_FULL={E_FULL}\")\n        else:\n            E_FULL = int(CFG[\"epochs\"])\n\n        full_pack = train_full_fixed_epochs(X_all, y_all, CFG, epochs_fixed=E_FULL, seed=FINAL_SEED + s)\n\n    except RuntimeError as e:\n        msg = str(e).lower()\n        if (\"out of memory\" in msg) and device.type == \"cuda\":\n            print(\"  OOM detected. Applying fallback (batch_size/width/depth downshift).\")\n            torch.cuda.empty_cache()\n\n            CFG[\"batch_size\"] = max(128, int(CFG[\"batch_size\"]) // 2)\n            CFG[\"d_model\"] = max(256, int(CFG[\"d_model\"]) - 64)\n            CFG[\"n_layers\"] = max(6, int(CFG[\"n_layers\"]) - 2)\n            CFG[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(CFG[\"batch_size\"]))))\n\n            print(\"  New CFG after fallback:\")\n            for k in [\"d_model\",\"n_layers\",\"n_heads\",\"batch_size\",\"accum_steps\",\"epochs\"]:\n                print(f\"    {k}: {CFG[k]}\")\n\n            if USE_INTERNAL_VAL:\n                info = train_with_internal_val_get_best_epoch(X_all, y_all, CFG, seed=FINAL_SEED + s)\n                best_epoch = int(info[\"best_epoch\"])\n                E_FULL = int(min(int(CFG[\"epochs\"]), max(12, round(best_epoch * 1.05))))\n                print(f\"\\nBest_epoch={best_epoch} -> Retrain FULL for E_FULL={E_FULL}\")\n            else:\n                E_FULL = int(CFG[\"epochs\"])\n\n            full_pack = train_full_fixed_epochs(X_all, y_all, CFG, epochs_fixed=E_FULL, seed=FINAL_SEED + s)\n        else:\n            raise\n\n    final_full_packs.append(full_pack)\n    gc.collect()\n    if device.type == \"cuda\":\n        torch.cuda.empty_cache()\n\n# ----------------------------\n# 13) Save artifacts\n# ----------------------------\nfinal_model_path = OUT_DIR / \"final_gate_model.pt\"\n\n# threshold recommended from Step 4 (OOF)\nbest_thr = None\nif isinstance(best_bundle, dict):\n    best_thr = best_bundle.get(\"oof_best_thr\", None)\n\ntorch.save(\n    {\n        \"type\": \"final_gate_v4\",\n        \"feature_cols\": FEATURE_COLS,\n\n        # keep fold ensemble from Step 4 if available (strong option for inference)\n        \"fold_packs\": fold_packs_from_step4,\n\n        # full-data trained packs (list; usually 1 seed)\n        \"full_packs\": final_full_packs,\n\n        \"recommended_thr\": best_thr,\n        \"bundle_source\": source,\n    },\n    final_model_path\n)\n\nfinal_bundle = {\n    \"type\": \"final_gate_v4\",\n    \"feature_cols\": FEATURE_COLS,\n    \"n_seeds\": int(N_SEEDS),\n    \"seeds\": [int(p[\"seed\"]) for p in final_full_packs],\n    \"cfg\": dict(CFG),\n\n    \"use_internal_val\": bool(USE_INTERNAL_VAL),\n    \"val_frac_case\": float(VAL_FRAC_CASE) if USE_INTERNAL_VAL else 0.0,\n    \"early_stop\": bool(EARLY_STOP) if USE_INTERNAL_VAL else False,\n\n    \"train_rows\": int(len(y_all)),\n    \"pos_rate\": float(y_all.mean()),\n\n    \"recommended_thr\": best_thr,\n    \"has_fold_packs_from_step4\": bool(fold_packs_from_step4 is not None),\n    \"notes\": \"Final model uses Step4-matched mHC-lite FTTransformer with EMA+accum and full-data retrain using best_epoch from internal val.\",\n}\n\nfinal_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\nfinal_bundle_path.write_text(json.dumps(final_bundle, indent=2))\n\nprint(\"\\nSaved final training artifacts:\")\nprint(\"  model  ->\", final_model_path)\nprint(\"  bundle ->\", final_bundle_path)\n\n# Export globals\nFINAL_GATE_MODEL_PT = str(final_model_path)\nFINAL_GATE_BUNDLE = final_bundle\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finalize & Save Model Bundle (Reproducible)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 6 — Finalize & Save Model Bundle (Reproducible) — REVISI FULL v4 (TRANSFORMER COMPAT)\n# - Compatible with Step 5 v4: final_gate_model.pt contains {fold_packs, full_packs, recommended_thr}\n# - Bundle artifacts + thresholds + manifest + ZIP portable (no submission)\n#\n# REQUIRE:\n# - Step 2: feature_cols.json (atau feature_cols.json alternative path)\n# - Step 5: final_gate_model.pt + final_gate_bundle.json\n# ============================================================\n\nimport os, json, time, platform, warnings, zipfile, hashlib\nfrom pathlib import Path\n\nwarnings.filterwarnings(\"ignore\")\n\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef read_json_safe(p: Path, default=None):\n    try:\n        return json.loads(Path(p).read_text())\n    except Exception:\n        return default\n\ndef pick_first_existing(paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(p)\n        if p.exists() and p.is_file():\n            return p\n    return None\n\ndef safe_add(zf: zipfile.ZipFile, p: Path, arcname: str):\n    if p is None:\n        return\n    p = Path(p)\n    if p.exists() and p.is_file():\n        zf.write(p, arcname=arcname)\n\ndef sha256_file(p: Path, chunk=1024 * 1024):\n    p = Path(p)\n    h = hashlib.sha256()\n    with p.open(\"rb\") as f:\n        while True:\n            b = f.read(chunk)\n            if not b:\n                break\n            h.update(b)\n    return h.hexdigest()\n\ndef file_meta(p: Path):\n    p = Path(p)\n    if not p.exists() or not p.is_file():\n        return None\n    return {\n        \"path\": str(p),\n        \"name\": p.name,\n        \"bytes\": int(p.stat().st_size),\n        \"sha256\": sha256_file(p),\n        \"mtime_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(p.stat().st_mtime)),\n    }\n\n# ----------------------------\n# 0) Locate required artifacts\n# ----------------------------\nfinal_model_pt   = OUT_DIR / \"final_gate_model.pt\"\nfinal_bundle_json = OUT_DIR / \"final_gate_bundle.json\"\n\n# feature_cols: Step 2 biasanya menyimpan ini\nfeature_cols_path = pick_first_existing([\n    OUT_DIR / \"feature_cols.json\",\n    OUT_DIR / \"model_bundle_pack_feature_cols.json\",  # (optional legacy)\n])\n\nif feature_cols_path is None:\n    raise FileNotFoundError(\"Missing feature_cols.json (jalankan Step 2 dulu).\")\nif not final_model_pt.exists():\n    raise FileNotFoundError(f\"Missing final model: {final_model_pt} (jalankan Step 5 dulu).\")\n\nmodel_format = \"torch_pt\"\nfinal_model_path = final_model_pt\n\n# Optional / extras\nbaseline_report_path = pick_first_existing([\n    OUT_DIR / \"baseline_mhc_transformer_cv_report.json\",   # Step 3 (yang paling mungkin)\n    OUT_DIR / \"baseline_mhc_transformer_cv_report_v2.json\",\n    OUT_DIR / \"baseline_transformer_cv_report.json\",\n    OUT_DIR / \"baseline_cv_report.json\",\n])\n\nbest_gate_config_path = pick_first_existing([\n    OUT_DIR / \"best_gate_config.json\",  # Step 4\n])\n\nbest_gate_model_path = pick_first_existing([\n    OUT_DIR / \"best_gate_model.pt\",     # Step 4\n])\n\nopt_results_csv = pick_first_existing([\n    OUT_DIR / \"opt_search\" / \"opt_results.csv\",\n])\n\nopt_fold_csv = pick_first_existing([\n    OUT_DIR / \"opt_search\" / \"opt_fold_details.csv\",\n])\n\n# OOF optional\noof_baseline_csv = pick_first_existing([\n    OUT_DIR / \"oof_baseline_mhc_transformer.csv\",\n    OUT_DIR / \"oof_baseline_transformer.csv\",\n    OUT_DIR / \"oof_baseline.csv\",\n])\n\nprint(\"Found artifacts:\")\nprint(\"  final_model        :\", final_model_path, f\"(format={model_format})\")\nprint(\"  final_bundle       :\", final_bundle_json if final_bundle_json.exists() else \"(missing/skip)\")\nprint(\"  feature_cols       :\", feature_cols_path)\nprint(\"  best_gate_config   :\", best_gate_config_path if best_gate_config_path else \"(missing/skip)\")\nprint(\"  best_gate_model    :\", best_gate_model_path if best_gate_model_path else \"(missing/skip)\")\nprint(\"  baseline_report    :\", baseline_report_path if baseline_report_path else \"(missing/skip)\")\nprint(\"  opt_results_csv    :\", opt_results_csv if opt_results_csv else \"(missing/skip)\")\nprint(\"  opt_fold_csv       :\", opt_fold_csv if opt_fold_csv else \"(missing/skip)\")\nprint(\"  oof_baseline_csv   :\", oof_baseline_csv if oof_baseline_csv else \"(missing/skip)\")\n\n# ----------------------------\n# 1) Load metadata\n# ----------------------------\nfeature_cols = read_json_safe(feature_cols_path, default=[])\nif not isinstance(feature_cols, list) or len(feature_cols) == 0:\n    raise ValueError(f\"feature_cols invalid/empty: {feature_cols_path}\")\n\nfinal_bundle = read_json_safe(final_bundle_json, default={}) if final_bundle_json.exists() else {}\nbaseline_report = read_json_safe(baseline_report_path, default=None) if baseline_report_path else None\nbest_gate_config = read_json_safe(best_gate_config_path, default=None) if best_gate_config_path else None\n\n# Try read recommended_thr from final_gate_model.pt (Step 5 v4 puts it there)\nrecommended_thr_from_pt = None\ntry:\n    import torch\n    obj = torch.load(final_model_pt, map_location=\"cpu\")\n    if isinstance(obj, dict):\n        recommended_thr_from_pt = obj.get(\"recommended_thr\", None)\nexcept Exception as e:\n    print(\"Warning: failed to read final_gate_model.pt for recommended_thr:\", repr(e))\n\n# ----------------------------\n# 2) Thresholds (robust priority)\n# Priority:\n#   (a) thresholds.json (if exists)\n#   (b) final_gate_bundle.json -> recommended_thr\n#   (c) final_gate_model.pt -> recommended_thr\n#   (d) best_gate_config.json -> oof_best_thr (new) OR selection.oof_best_thr (legacy)\n#   (e) fallback 0.5\n# ----------------------------\nthresholds_path = OUT_DIR / \"thresholds.json\"\n\ndef extract_thr_from_best_gate_config(cfg: dict):\n    if not isinstance(cfg, dict):\n        return None\n    # new style\n    if \"oof_best_thr\" in cfg:\n        try:\n            return float(cfg[\"oof_best_thr\"])\n        except Exception:\n            pass\n    # legacy: selection.oof_best_thr\n    sel = cfg.get(\"selection\", None)\n    if isinstance(sel, dict) and (\"oof_best_thr\" in sel):\n        try:\n            return float(sel[\"oof_best_thr\"])\n        except Exception:\n            pass\n    return None\n\nif thresholds_path.exists():\n    thresholds = read_json_safe(thresholds_path, default={})\n    if not isinstance(thresholds, dict) or (\"T_gate\" not in thresholds):\n        thresholds = {}\nelse:\n    thresholds = {}\n\nT_gate = None\n# (a) existing thresholds.json\nif isinstance(thresholds, dict):\n    try:\n        if thresholds.get(\"T_gate\", None) is not None:\n            T_gate = float(thresholds[\"T_gate\"])\n    except Exception:\n        T_gate = None\n\n# (b) final_bundle recommended_thr\nif T_gate is None and isinstance(final_bundle, dict):\n    try:\n        if final_bundle.get(\"recommended_thr\", None) is not None:\n            T_gate = float(final_bundle[\"recommended_thr\"])\n    except Exception:\n        T_gate = None\n\n# (c) final_model.pt recommended_thr\nif T_gate is None and recommended_thr_from_pt is not None:\n    try:\n        T_gate = float(recommended_thr_from_pt)\n    except Exception:\n        T_gate = None\n\n# (d) best_gate_config oof_best_thr\nif T_gate is None and isinstance(best_gate_config, dict):\n    T_gate = extract_thr_from_best_gate_config(best_gate_config)\n\n# (e) fallback\nif T_gate is None:\n    T_gate = 0.5\n\n# write thresholds.json in a stable schema\nthresholds = {\n    \"T_gate\": float(T_gate),\n    \"beta_for_tuning\": float(best_gate_config.get(\"beta_for_tuning\", 0.5)) if isinstance(best_gate_config, dict) else 0.5,\n    \"guards\": {\n        \"min_area_frac\": None,\n        \"max_area_frac\": None,\n        \"max_components\": None,\n    },\n    \"source_priority\": [\n        \"thresholds.json (existing)\",\n        \"final_gate_bundle.json.recommended_thr\",\n        \"final_gate_model.pt.recommended_thr\",\n        \"best_gate_config.json.oof_best_thr (or selection.oof_best_thr legacy)\",\n        \"fallback 0.5\",\n    ],\n    \"notes\": \"Gate threshold used for binary decision. Update after calibration/OOF tuning if needed.\",\n}\nthresholds_path.write_text(json.dumps(thresholds, indent=2))\n\n# ----------------------------\n# 3) Capture dataset/cfg metadata (if available)\n# ----------------------------\ncfg_meta = {}\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    cfg_meta = {\n        \"COMP_ROOT\": PATHS.get(\"COMP_ROOT\", None),\n        \"OUT_DS_ROOT\": PATHS.get(\"OUT_DS_ROOT\", None),\n        \"OUT_ROOT\": PATHS.get(\"OUT_ROOT\", None),\n        \"MATCH_CFG_DIR\": PATHS.get(\"MATCH_CFG_DIR\", None),\n        \"PRED_CFG_DIR\": PATHS.get(\"PRED_CFG_DIR\", None),\n        \"DINO_CFG_DIR\": PATHS.get(\"DINO_CFG_DIR\", None),\n        \"DINO_LARGE_DIR\": PATHS.get(\"DINO_LARGE_DIR\", None),\n        \"PRED_FEAT_TRAIN\": PATHS.get(\"PRED_FEAT_TRAIN\", None),\n        \"MATCH_FEAT_TRAIN\": PATHS.get(\"MATCH_FEAT_TRAIN\", None),\n        \"DF_TRAIN_ALL\": PATHS.get(\"DF_TRAIN_ALL\", None),\n        \"CV_CASE_FOLDS\": PATHS.get(\"CV_CASE_FOLDS\", None),\n        \"IMG_PROFILE_TRAIN\": PATHS.get(\"IMG_PROFILE_TRAIN\", None),\n    }\n\n# ----------------------------\n# 4) Manifest (reproducible)\n# ----------------------------\ntask_str = \"Recod.ai/LUC — Gate Model — DINOv2 features + Transformer gate (.pt)\"\nbundle_version = \"v4\"\n\n# artifact list for hashing\nartifact_paths = [\n    final_model_path,\n    final_bundle_json if final_bundle_json.exists() else None,\n    feature_cols_path,\n    thresholds_path,\n    baseline_report_path,\n    best_gate_config_path,\n    best_gate_model_path,\n    opt_results_csv,\n    opt_fold_csv,\n    oof_baseline_csv,\n]\nartifact_paths = [p for p in artifact_paths if p is not None]\n\nartifacts_meta = {}\nfor p in artifact_paths:\n    m = file_meta(p)\n    if m is not None:\n        artifacts_meta[p.name] = m\n\n# summaries (robust: handle new/old keys)\nopt_summary = None\nif isinstance(best_gate_config, dict):\n    opt_summary = best_gate_config.get(\"selection\", None)\n    if opt_summary is None:\n        # new style: keys directly on dict\n        opt_summary = {\n            \"model_name\": best_gate_config.get(\"model_name\", None),\n            \"oof_best_thr\": best_gate_config.get(\"oof_best_thr\", None),\n            \"oof_best_fbeta\": best_gate_config.get(\"oof_best_fbeta\", None),\n            \"oof_auc\": best_gate_config.get(\"oof_auc\", None),\n            \"oof_logloss\": best_gate_config.get(\"oof_logloss\", None),\n        }\n\nmanifest = {\n    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"python\": platform.python_version(),\n    \"platform\": platform.platform(),\n    \"bundle_version\": bundle_version,\n    \"task\": task_str,\n    \"model_format\": model_format,\n    \"artifacts_index\": artifacts_meta,\n    \"cfg_meta\": cfg_meta,\n    \"model_summary\": {\n        \"type\": (final_bundle.get(\"type\") if isinstance(final_bundle, dict) else None),\n        \"n_seeds\": (final_bundle.get(\"n_seeds\") if isinstance(final_bundle, dict) else None),\n        \"seeds\": (final_bundle.get(\"seeds\") if isinstance(final_bundle, dict) else None),\n        \"train_rows\": (final_bundle.get(\"train_rows\") if isinstance(final_bundle, dict) else None),\n        \"pos_rate\": (final_bundle.get(\"pos_rate\") if isinstance(final_bundle, dict) else None),\n        \"feature_count\": int(len(feature_cols)),\n        \"T_gate\": float(thresholds.get(\"T_gate\", 0.5)),\n        \"recommended_thr_from_pt\": recommended_thr_from_pt,\n    },\n    \"baseline_summary\": (baseline_report.get(\"overall\") if isinstance(baseline_report, dict) else None),\n    \"opt_summary\": opt_summary,\n}\n\nmanifest_path = OUT_DIR / \"model_bundle_manifest.json\"\nmanifest_path.write_text(json.dumps(manifest, indent=2))\n\n# ----------------------------\n# 5) Bundle pack (portable JSON) + optional joblib\n# ----------------------------\nbundle_pack = {\n    \"bundle_version\": bundle_version,\n    \"model_format\": model_format,\n    \"final_model_path\": str(final_model_path),\n    \"final_bundle\": final_bundle,\n    \"feature_cols\": feature_cols,\n    \"thresholds\": thresholds,\n    \"cfg_meta\": cfg_meta,\n    \"manifest\": manifest,\n}\n\nbundle_pack_json = OUT_DIR / \"model_bundle_pack.json\"\nbundle_pack_json.write_text(json.dumps(bundle_pack, indent=2))\n\nbundle_pack_joblib = OUT_DIR / \"model_bundle_pack.joblib\"\njoblib_ok = False\ntry:\n    import joblib\n    joblib.dump(bundle_pack, bundle_pack_joblib)\n    joblib_ok = True\nexcept Exception:\n    joblib_ok = False\n\n# ----------------------------\n# 6) Create portable ZIP\n# ----------------------------\nzip_path = OUT_DIR / \"model_bundle_v4.zip\"\n\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n    # core\n    safe_add(zf, final_model_path, final_model_path.name)\n    safe_add(zf, final_bundle_json if final_bundle_json.exists() else None, final_bundle_json.name)\n    safe_add(zf, feature_cols_path, feature_cols_path.name)\n    safe_add(zf, thresholds_path, thresholds_path.name)\n    safe_add(zf, manifest_path, manifest_path.name)\n    safe_add(zf, bundle_pack_json, bundle_pack_json.name)\n    if joblib_ok:\n        safe_add(zf, bundle_pack_joblib, bundle_pack_joblib.name)\n\n    # extras\n    safe_add(zf, baseline_report_path, baseline_report_path.name if baseline_report_path else \"baseline_report.json\")\n    safe_add(zf, best_gate_config_path, best_gate_config_path.name if best_gate_config_path else \"best_gate_config.json\")\n    safe_add(zf, best_gate_model_path, best_gate_model_path.name if best_gate_model_path else \"best_gate_model.pt\")\n\n    if opt_results_csv:\n        safe_add(zf, opt_results_csv, f\"opt_search/{opt_results_csv.name}\")\n    if opt_fold_csv:\n        safe_add(zf, opt_fold_csv, f\"opt_search/{opt_fold_csv.name}\")\n\n    if oof_baseline_csv:\n        safe_add(zf, oof_baseline_csv, f\"oof/{oof_baseline_csv.name}\")\n\nprint(\"\\nOK — Model bundle finalized\")\nprint(\"  manifest     ->\", manifest_path)\nprint(\"  pack (json)  ->\", bundle_pack_json)\nprint(\"  pack (joblib)->\", (bundle_pack_joblib if joblib_ok else \"(skip; joblib not available)\"))\nprint(\"  thresholds   ->\", thresholds_path)\nprint(\"  zip          ->\", zip_path)\n\nprint(\"\\nBundle summary:\")\nprint(\"  bundle_version:\", bundle_version)\nprint(\"  model_format  :\", model_format)\nprint(\"  feature_cnt   :\", len(feature_cols))\nprint(\"  T_gate        :\", thresholds.get(\"T_gate\"))\nprint(\"  task          :\", task_str)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}