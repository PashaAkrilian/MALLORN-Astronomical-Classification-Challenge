{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113558,"databundleVersionId":14878066,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14387840,"sourceType":"datasetVersion","datasetId":9171323},{"sourceId":4537,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3329,"modelId":986}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Paths & Select Config (CFG)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Set Paths & Select Config (CFG) (Kaggle-ready, offline)\n# - Root kompetisi: /kaggle/input/recodai-luc-scientific-image-forgery-detection\n# - Root dataset output: /kaggle/input/recod-ailuc-dinov2-base  (atau variasi nama lain)\n# - Auto-detect jika nama folder beda\n# - Auto-pilih CFG terbaik untuk MATCH + PRED berdasarkan coverage fitur train\n#\n# Output globals:\n# - COMP_ROOT, OUT_DS_ROOT, OUT_ROOT\n# - PATHS (dict jalur penting)\n# - MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR (opsional)\n# ============================================================\n\nimport os, re, json\nfrom pathlib import Path\nimport pandas as pd\n\n# ----------------------------\n# Helper: find competition root\n# ----------------------------\ndef find_comp_root(preferred: str = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\") -> Path:\n    p = Path(preferred)\n    if p.exists():\n        return p\n    # fallback: scan /kaggle/input for a dir that looks like the competition\n    base = Path(\"/kaggle/input\")\n    if not base.exists():\n        raise FileNotFoundError(\"/kaggle/input not found (are you in Kaggle notebook?)\")\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        # heuristic: must have sample_submission.csv and test_images/train_images\n        if (d / \"sample_submission.csv\").exists() and ((d / \"train_images\").exists() or (d / \"test_images\").exists()):\n            cands.append(d)\n    if not cands:\n        raise FileNotFoundError(\n            \"Competition root not found. Expected folder with sample_submission.csv and train_images/test_images.\"\n        )\n    # prefer one containing 'recodai' and 'forgery'\n    cands.sort(key=lambda x: ((\"recodai\" not in x.name.lower()), (\"forgery\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: find output dataset root\n# ----------------------------\ndef find_output_dataset_root(preferred_names=(\n    \"recod-ailuc-dinov2-base\",\n    \"recod-ai-luc-dinov2-base\",\n    \"recodai-luc-dinov2-base\",\n    \"recodai-luc-dinov2\",\n)) -> Path:\n    base = Path(\"/kaggle/input\")\n    # try preferred direct hits first\n    for nm in preferred_names:\n        p = base / nm\n        if p.exists():\n            return p\n\n    # fallback: scan for a dataset that contains recodai_luc/artifacts\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        # either directly has recodai_luc, or nested one-level\n        if (d / \"recodai_luc\" / \"artifacts\").exists():\n            cands.append(d)\n            continue\n        # some datasets wrap inside one folder\n        inner = list(d.glob(\"*/recodai_luc/artifacts\"))\n        if inner:\n            cands.append(d)\n    if not cands:\n        raise FileNotFoundError(\n            \"Output dataset root not found. Expected something like /kaggle/input/<...>/recodai_luc/artifacts/\"\n        )\n    # prefer those containing 'dinov2' in name\n    cands.sort(key=lambda x: ((\"dinov2\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: resolve OUT_ROOT = <dataset>/recodai_luc\n# ----------------------------\ndef resolve_out_root(out_ds_root: Path) -> Path:\n    direct = out_ds_root / \"recodai_luc\"\n    if direct.exists():\n        return direct\n    # else find nested\n    hits = list(out_ds_root.glob(\"*/recodai_luc\"))\n    if hits:\n        return hits[0]\n    raise FileNotFoundError(f\"Could not locate recodai_luc folder under {out_ds_root}\")\n\n# ----------------------------\n# Helper: pick best cfg directory by coverage of train feature csv\n# ----------------------------\ndef pick_best_cfg(cache_root: Path, prefix: str, feat_train_pattern: str) -> Path:\n    \"\"\"\n    prefix: e.g. 'match_base_cfg_' or 'pred_base'\n    feat_train_pattern: glob pattern for feature csv inside cfg dir\n    \"\"\"\n    cands = []\n    for d in cache_root.iterdir():\n        if not d.is_dir():\n            continue\n        if not d.name.startswith(prefix):\n            continue\n        # find feature file\n        feat_files = list(d.glob(feat_train_pattern))\n        if not feat_files:\n            continue\n        feat_path = feat_files[0]\n        # score by row count (coverage)\n        try:\n            n = sum(1 for _ in open(feat_path, \"r\", encoding=\"utf-8\", errors=\"ignore\")) - 1\n        except Exception:\n            n = -1\n        cands.append((n, d, feat_path))\n\n    if not cands:\n        raise FileNotFoundError(f\"No cfg folders found under {cache_root} with prefix={prefix} and {feat_train_pattern}\")\n\n    # choose max rows, tie-break by name\n    cands.sort(key=lambda x: (-x[0], x[1].name))\n    best_n, best_dir, best_feat = cands[0]\n    return best_dir\n\n# ----------------------------\n# 0) Locate roots\n# ----------------------------\nCOMP_ROOT = find_comp_root(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\nOUT_DS_ROOT = find_output_dataset_root()\n\nOUT_ROOT = resolve_out_root(OUT_DS_ROOT)  # .../recodai_luc\nART_DIR = OUT_ROOT / \"artifacts\"\nCACHE_DIR = OUT_ROOT / \"cache\"\n\n# ----------------------------\n# 1) Competition paths (raw images/masks)\n# ----------------------------\nPATHS = {}\nPATHS[\"COMP_ROOT\"] = str(COMP_ROOT)\nPATHS[\"SAMPLE_SUB\"] = str(COMP_ROOT / \"sample_submission.csv\")\n\n# common competition layout (handle if nested)\nPATHS[\"TRAIN_IMAGES\"] = str(COMP_ROOT / \"train_images\")\nPATHS[\"TEST_IMAGES\"]  = str(COMP_ROOT / \"test_images\")\nPATHS[\"TRAIN_MASKS\"]  = str(COMP_ROOT / \"train_masks\")\nPATHS[\"SUPP_IMAGES\"]  = str(COMP_ROOT / \"supplemental_images\")\nPATHS[\"SUPP_MASKS\"]   = str(COMP_ROOT / \"supplemental_masks\")\n\n# optional subfolders inside train_images\nPATHS[\"TRAIN_AUTH_DIR\"] = str(COMP_ROOT / \"train_images\" / \"authentic\")\nPATHS[\"TRAIN_FORG_DIR\"] = str(COMP_ROOT / \"train_images\" / \"forged\")\n\n# ----------------------------\n# 2) Output dataset paths (clean artifacts + cache)\n# ----------------------------\nPATHS[\"OUT_DS_ROOT\"] = str(OUT_DS_ROOT)\nPATHS[\"OUT_ROOT\"] = str(OUT_ROOT)\nPATHS[\"ART_DIR\"] = str(ART_DIR)\nPATHS[\"CACHE_DIR\"] = str(CACHE_DIR)\n\n# artifacts (train tables / folds / profiles)\nPATHS[\"DF_TRAIN_ALL\"] = str(ART_DIR / \"df_train_all.parquet\")\nPATHS[\"DF_TRAIN_CLS\"] = str(ART_DIR / \"df_train_cls.parquet\")\nPATHS[\"DF_TRAIN_SEG\"] = str(ART_DIR / \"df_train_seg.parquet\")\nPATHS[\"DF_TEST\"]      = str(ART_DIR / \"df_test.parquet\")\nPATHS[\"CV_CASE_FOLDS\"]   = str(ART_DIR / \"cv_case_folds.csv\")\nPATHS[\"CV_SAMPLE_FOLDS\"] = str(ART_DIR / \"cv_sample_folds.csv\")\nPATHS[\"IMG_PROFILE_TRAIN\"] = str(ART_DIR / \"image_profile_train.parquet\")\nPATHS[\"IMG_PROFILE_TEST\"]  = str(ART_DIR / \"image_profile_test.parquet\")\nPATHS[\"MASK_PROFILE\"]      = str(ART_DIR / \"mask_profile.parquet\")\nPATHS[\"CASE_SUMMARY\"]      = str(ART_DIR / \"case_summary.parquet\")\n\n# ----------------------------\n# 3) Select best MATCH/PRED CFG dirs automatically\n# ----------------------------\nif not CACHE_DIR.exists():\n    raise FileNotFoundError(f\"CACHE_DIR not found: {CACHE_DIR}\")\n\n# Match cfg dirs look like: match_base_cfg_<hash>\nMATCH_CFG_DIR = pick_best_cfg(\n    CACHE_DIR,\n    prefix=\"match_base_cfg_\",\n    feat_train_pattern=\"match_features_train_all.csv\"\n)\n# Pred cfg dirs look like: pred_base_v3_v7_cfg_<hash> (name may vary; use startswith 'pred_base')\n# We'll scan by startswith 'pred_base' and require pred_features_train_all.csv\nPRED_CFG_DIR = pick_best_cfg(\n    CACHE_DIR,\n    prefix=\"pred_base\",\n    feat_train_pattern=\"pred_features_train_all.csv\"\n)\n\n# DINO cache cfg (optional)\nDINO_CFG_DIR = None\ndino_root = CACHE_DIR / \"dino_v2_large\"\nif dino_root.exists():\n    # choose any cfg_* that has manifest_train_all.csv\n    dino_cands = []\n    for d in dino_root.iterdir():\n        if d.is_dir() and d.name.startswith(\"cfg_\") and (d / \"manifest_train_all.csv\").exists():\n            dino_cands.append(d)\n    if dino_cands:\n        dino_cands.sort(key=lambda x: x.name)\n        DINO_CFG_DIR = dino_cands[0]\n\n# attach feature file paths\nPATHS[\"MATCH_CFG_DIR\"] = str(MATCH_CFG_DIR)\nPATHS[\"PRED_CFG_DIR\"]  = str(PRED_CFG_DIR)\nPATHS[\"DINO_CFG_DIR\"]  = str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\"\n\nPATHS[\"MATCH_FEAT_TRAIN\"] = str(MATCH_CFG_DIR / \"match_features_train_all.csv\")\nPATHS[\"MATCH_FEAT_TEST\"]  = str(MATCH_CFG_DIR / \"match_features_test.csv\")\nPATHS[\"PRED_FEAT_TRAIN\"]  = str(PRED_CFG_DIR / \"pred_features_train_all.csv\")\nPATHS[\"PRED_FEAT_TEST\"]   = str(PRED_CFG_DIR / \"pred_features_test.csv\")\n\n# ----------------------------\n# 4) Sanity checks (no hard fail for optional files)\n# ----------------------------\nmust_exist = [\n    (\"sample_submission.csv\", PATHS[\"SAMPLE_SUB\"]),\n    (\"df_train_all.parquet\",  PATHS[\"DF_TRAIN_ALL\"]),\n    (\"cv_case_folds.csv\",     PATHS[\"CV_CASE_FOLDS\"]),\n    (\"match_features_train_all.csv\", PATHS[\"MATCH_FEAT_TRAIN\"]),\n    (\"pred_features_train_all.csv\",  PATHS[\"PRED_FEAT_TRAIN\"]),\n]\nmissing = [name for name, p in must_exist if not Path(p).exists()]\nif missing:\n    raise FileNotFoundError(\"Missing required files: \" + \", \".join(missing))\n\nprint(\"OK — Roots\")\nprint(\"  COMP_ROOT   :\", COMP_ROOT)\nprint(\"  OUT_DS_ROOT :\", OUT_DS_ROOT)\nprint(\"  OUT_ROOT    :\", OUT_ROOT)\nprint(\"\\nOK — Selected CFG\")\nprint(\"  MATCH_CFG_DIR:\", MATCH_CFG_DIR.name)\nprint(\"  PRED_CFG_DIR :\", PRED_CFG_DIR.name)\nprint(\"  DINO_CFG_DIR :\", (DINO_CFG_DIR.name if DINO_CFG_DIR else \"(not found / optional)\"))\n\nprint(\"\\nOK — Key files\")\nfor k in [\"DF_TRAIN_ALL\",\"CV_CASE_FOLDS\",\"MATCH_FEAT_TRAIN\",\"PRED_FEAT_TRAIN\",\"IMG_PROFILE_TRAIN\"]:\n    p = Path(PATHS[k])\n    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing/optional)'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:28:42.158655Z","iopub.execute_input":"2026-01-04T08:28:42.159779Z","iopub.status.idle":"2026-01-04T08:28:42.209634Z","shell.execute_reply.started":"2026-01-04T08:28:42.159743Z","shell.execute_reply":"2026-01-04T08:28:42.208584Z"}},"outputs":[{"name":"stdout","text":"OK — Roots\n  COMP_ROOT   : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n  OUT_DS_ROOT : /kaggle/input/recod-ailuc-dinov2-base\n  OUT_ROOT    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc\n\nOK — Selected CFG\n  MATCH_CFG_DIR: match_base_cfg_f9f7ea3a65c5\n  PRED_CFG_DIR : pred_base_v3_v7_cfg_5dbf0aa165\n  DINO_CFG_DIR : cfg_3246fd54aab0\n\nOK — Key files\n  DF_TRAIN_ALL    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet  (exists)\n  CV_CASE_FOLDS   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv  (exists)\n  MATCH_FEAT_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv  (exists)\n  PRED_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv  (exists)\n  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet  (exists)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Build Training Table (X, y, folds)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 2 — Build Training Table (X, y, folds) (DINOv2-LARGE pipeline)\n# - Fokus: siapkan df_train_tabular + (X_train, y_train, folds, feature_cols)\n# - Sumber utama: output dataset (pred_features + (opsional) match_features + image_profile)\n# - Split: gunakan cv_case_folds.csv (anti leakage, by case_id)\n# - Tidak ada submission di sini\n#\n# Catatan:\n# - DINOv2 Large model path (offline): /kaggle/input/dinov2/pytorch/large/1\n#   (Di step ini hanya dicek exist; ekstraksi DINO tidak dilakukan di step ini.)\n#\n# REQUIRE: Jalankan STAGE 0 (Set Paths & Select Config) dulu => PATHS ada.\n# ============================================================\n\nimport os, json, math, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# 0) Require PATHS\n# ----------------------------\nif \"PATHS\" not in globals() or not isinstance(PATHS, dict):\n    raise RuntimeError(\"Missing PATHS. Jalankan dulu STAGE 0 — Set Paths & Select Config (CFG).\")\n\n# ----------------------------\n# 1) Check DINOv2 Large local path (offline)\n# ----------------------------\nDINO_LARGE_DIR = Path(\"/kaggle/input/dinov2/pytorch/large/1\")\nif not DINO_LARGE_DIR.exists():\n    raise FileNotFoundError(f\"DINOv2-Large path not found: {DINO_LARGE_DIR}\")\nPATHS[\"DINO_LARGE_DIR\"] = str(DINO_LARGE_DIR)\n\n# ----------------------------\n# 2) Prefer WORKING features if exist (because you may have re-generated there)\n# ----------------------------\ndef prefer_working(input_path: str, working_candidate: str | None = None) -> Path:\n    p_in = Path(input_path)\n    if working_candidate is not None:\n        p_w = Path(working_candidate)\n        if p_w.exists():\n            return p_w\n    return p_in\n\n# Build working candidates from selected cfg dir names\nmatch_cfg_name = Path(PATHS[\"MATCH_CFG_DIR\"]).name\npred_cfg_name  = Path(PATHS[\"PRED_CFG_DIR\"]).name\n\nWORK_ROOT = Path(\"/kaggle/working/recodai_luc/cache\")\nmatch_feat_work = WORK_ROOT / match_cfg_name / \"match_features_train_all.csv\"\npred_feat_work  = WORK_ROOT / pred_cfg_name  / \"pred_features_train_all.csv\"\n\n# Resolve final feature paths\nPRED_FEAT_TRAIN = prefer_working(PATHS[\"PRED_FEAT_TRAIN\"], str(pred_feat_work))\nMATCH_FEAT_TRAIN = prefer_working(PATHS[\"MATCH_FEAT_TRAIN\"], str(match_feat_work))\n\n# Base artifacts\nDF_TRAIN_ALL = Path(PATHS[\"DF_TRAIN_ALL\"])\nCV_CASE_FOLDS = Path(PATHS[\"CV_CASE_FOLDS\"])\nIMG_PROFILE_TRAIN = Path(PATHS.get(\"IMG_PROFILE_TRAIN\", \"\"))\n\nfor need_name, need_path in [\n    (\"df_train_all.parquet\", DF_TRAIN_ALL),\n    (\"cv_case_folds.csv\", CV_CASE_FOLDS),\n    (\"pred_features_train_all.csv\", PRED_FEAT_TRAIN),\n]:\n    if not need_path.exists():\n        raise FileNotFoundError(f\"Missing required file: {need_name} -> {need_path}\")\n\nprint(\"Using:\")\nprint(\"  DF_TRAIN_ALL     :\", DF_TRAIN_ALL)\nprint(\"  CV_CASE_FOLDS    :\", CV_CASE_FOLDS)\nprint(\"  PRED_FEAT_TRAIN  :\", PRED_FEAT_TRAIN)\nprint(\"  MATCH_FEAT_TRAIN :\", MATCH_FEAT_TRAIN, \"(optional)\" if MATCH_FEAT_TRAIN.exists() else \"(missing/skip)\")\nprint(\"  IMG_PROFILE_TRAIN:\", IMG_PROFILE_TRAIN, \"(optional)\" if IMG_PROFILE_TRAIN.exists() else \"(missing/skip)\")\nprint(\"  DINO_LARGE_DIR   :\", DINO_LARGE_DIR)\n\n# ----------------------------\n# 3) Load minimal inputs\n# ----------------------------\ndf_base = pd.read_parquet(DF_TRAIN_ALL)\ndf_cv   = pd.read_csv(CV_CASE_FOLDS)\n\ndf_pred = pd.read_csv(PRED_FEAT_TRAIN)\n\n# Optional match_features: only used if pred_features does NOT already contain match cols you want\ndf_match = None\nif MATCH_FEAT_TRAIN.exists():\n    try:\n        df_match = pd.read_csv(MATCH_FEAT_TRAIN)\n    except Exception:\n        df_match = None\n\ndf_prof = None\nif IMG_PROFILE_TRAIN.exists():\n    try:\n        df_prof = pd.read_parquet(IMG_PROFILE_TRAIN)\n    except Exception:\n        df_prof = None\n\n# ----------------------------\n# 4) Normalize keys: uid/sample_id, case_id, variant\n# ----------------------------\ndef ensure_uid_case_variant(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # uid column name variants\n    if \"uid\" not in df.columns:\n        for alt in [\"sample_id\", \"id\", \"key\"]:\n            if alt in df.columns:\n                df = df.rename(columns={alt: \"uid\"})\n                break\n\n    if \"uid\" not in df.columns:\n        raise ValueError(\"Cannot find uid/sample_id column in pred_features. Expected column 'uid' or 'sample_id'.\")\n\n    # case_id + variant may already exist\n    if \"case_id\" not in df.columns or \"variant\" not in df.columns:\n        # parse from uid patterns:\n        # - \"10015__auth\" or \"10015__forg\"\n        # - \"10015_auth\"  or \"10015_forg\"\n        uid = df[\"uid\"].astype(str)\n        if \"case_id\" not in df.columns:\n            # take leading digits\n            df[\"case_id\"] = uid.str.extract(r\"^(\\d+)\")[0].astype(\"Int64\")\n        if \"variant\" not in df.columns:\n            # try __variant first else _variant\n            v = uid.str.extract(r\"__(\\w+)$\")[0]\n            v2 = uid.str.extract(r\"_(\\w+)$\")[0]\n            df[\"variant\"] = v.fillna(v2).fillna(\"unk\")\n\n    # enforce dtypes\n    df[\"case_id\"] = df[\"case_id\"].astype(int)\n    df[\"variant\"] = df[\"variant\"].astype(str)\n    df[\"uid\"] = df[\"uid\"].astype(str)\n    return df\n\ndf_pred = ensure_uid_case_variant(df_pred)\n\n# Base label table: ensure has y_forged and uid key if needed\ndf_base2 = df_base.copy()\nif \"uid\" not in df_base2.columns:\n    # common in your pipeline: sample_id\n    if \"sample_id\" in df_base2.columns:\n        df_base2 = df_base2.rename(columns={\"sample_id\": \"uid\"})\n    elif \"uid\" not in df_base2.columns:\n        # some tables use case_id+variant; build uid like pred does if possible\n        if \"case_id\" in df_base2.columns and \"variant\" in df_base2.columns:\n            df_base2[\"uid\"] = df_base2[\"case_id\"].astype(str) + \"__\" + df_base2[\"variant\"].astype(str)\n        else:\n            # fallback: no uid; we can still merge by (case_id, variant) if available later\n            pass\n\n# Ensure label exists\nlabel_col = None\nfor cand in [\"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]:\n    if cand in df_base2.columns:\n        label_col = cand\n        break\n\nif label_col is None and \"y_forged\" in df_pred.columns:\n    label_col = \"y_forged\"\n\nif label_col is None:\n    raise ValueError(\"Cannot find label column (y_forged/has_mask/etc) in df_train_all or pred_features.\")\n\n# CV folds: must have case_id and fold\nif \"case_id\" not in df_cv.columns or \"fold\" not in df_cv.columns:\n    raise ValueError(\"cv_case_folds.csv must contain columns: case_id, fold\")\n\ndf_cv[\"case_id\"] = df_cv[\"case_id\"].astype(int)\ndf_cv[\"fold\"] = df_cv[\"fold\"].astype(int)\n\n# ----------------------------\n# 5) Merge: start from df_pred (already 1 row per uid)\n# ----------------------------\ndf_train = df_pred.copy()\n\n# attach label (prefer df_pred if already has y_forged)\nif \"y_forged\" in df_train.columns:\n    df_train[\"y\"] = df_train[\"y_forged\"].astype(int)\nelse:\n    # merge from df_base2\n    if \"uid\" in df_base2.columns:\n        df_train = df_train.merge(df_base2[[\"uid\", label_col]].rename(columns={label_col: \"y\"}), on=\"uid\", how=\"left\")\n    else:\n        # merge by case_id+variant\n        if {\"case_id\",\"variant\",label_col}.issubset(df_base2.columns):\n            df_train = df_train.merge(df_base2[[\"case_id\",\"variant\",label_col]].rename(columns={label_col:\"y\"}),\n                                      on=[\"case_id\",\"variant\"], how=\"left\")\n        else:\n            raise ValueError(\"Could not merge label from df_train_all (missing uid or case_id+variant).\")\n    df_train[\"y\"] = df_train[\"y\"].astype(int)\n\n# attach folds (override any fold column that might exist)\ndf_train = df_train.drop(columns=[\"fold\"], errors=\"ignore\").merge(df_cv[[\"case_id\",\"fold\"]], on=\"case_id\", how=\"left\")\nif df_train[\"fold\"].isna().any():\n    miss = int(df_train[\"fold\"].isna().sum())\n    raise ValueError(f\"Some rows missing fold assignment after merging cv_case_folds.csv: {miss} rows\")\n\ndf_train[\"fold\"] = df_train[\"fold\"].astype(int)\n\n# optional: merge match_features if you want extra columns not already present\nif df_match is not None:\n    df_match = ensure_uid_case_variant(df_match)\n    # only bring new numeric columns\n    base_cols = set(df_train.columns)\n    new_cols = [c for c in df_match.columns if c not in base_cols]\n    # keep id columns for merge\n    keep_cols = [\"uid\"] + [c for c in new_cols if c not in [\"case_id\",\"variant\"]]\n    # merge\n    df_train = df_train.merge(df_match[keep_cols], on=\"uid\", how=\"left\")\n\n# optional: merge image profile\nif df_prof is not None:\n    # prefer merge by case_id\n    if \"case_id\" in df_prof.columns:\n        df_prof2 = df_prof.copy()\n        df_prof2[\"case_id\"] = df_prof2[\"case_id\"].astype(int)\n        # drop duplicates per case_id if any (keep first)\n        df_prof2 = df_prof2.drop_duplicates(\"case_id\")\n        # avoid column clashes\n        clash = set(df_prof2.columns).intersection(df_train.columns)\n        clash -= {\"case_id\"}\n        if clash:\n            df_prof2 = df_prof2.rename(columns={c: f\"profile_{c}\" for c in clash})\n        df_train = df_train.merge(df_prof2, on=\"case_id\", how=\"left\")\n\n# ----------------------------\n# 6) Feature engineering (safe transforms + caps)\n# ----------------------------\ndef safe_log1p(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.log1p(x)\n\n# Peak ratio can be 1e9 sentinel; cap + log transform\nif \"peak_ratio\" in df_train.columns:\n    df_train[\"peak_ratio_cap\"] = np.clip(df_train[\"peak_ratio\"].astype(float).fillna(0.0), 0.0, 1e6)\n    df_train[\"log_peak_ratio\"] = safe_log1p(df_train[\"peak_ratio_cap\"])\nelse:\n    df_train[\"peak_ratio_cap\"] = 0.0\n    df_train[\"log_peak_ratio\"] = 0.0\n\n# Best weight also can be huge; cap + log\nif \"best_weight\" in df_train.columns:\n    df_train[\"best_weight_cap\"] = np.clip(df_train[\"best_weight\"].astype(float).fillna(0.0), 0.0, 1e9)\n    df_train[\"log_best_weight\"] = safe_log1p(df_train[\"best_weight_cap\"])\nelse:\n    df_train[\"best_weight_cap\"] = 0.0\n    df_train[\"log_best_weight\"] = 0.0\n\n# Replace inf/nan for all numeric columns (later selection will ensure only numeric)\nfor c in df_train.columns:\n    if pd.api.types.is_numeric_dtype(df_train[c]):\n        df_train[c] = df_train[c].replace([np.inf, -np.inf], np.nan)\n\n# ----------------------------\n# 7) Select feature columns (numeric only, exclude identifiers/labels)\n# ----------------------------\nID_COLS = {\"uid\",\"case_id\",\"variant\"}\nTARGET_COLS = {\"y\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"}\nSPLIT_COLS = {\"fold\"}\n\n# numeric columns only\nnum_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n\n# exclude label/split + optionally exclude raw id-like numeric (case_id)\nfeature_cols = [c for c in num_cols if c not in TARGET_COLS and c not in SPLIT_COLS and c not in [\"case_id\"]]\n\n# ensure engineered columns are included\nfor must in [\"log_peak_ratio\", \"log_best_weight\"]:\n    if must in df_train.columns and must not in feature_cols:\n        feature_cols.append(must)\n\n# fill NaN for feature columns\ndf_train[feature_cols] = df_train[feature_cols].fillna(0.0)\n\n# cast to float32 for memory\ndf_train[feature_cols] = df_train[feature_cols].astype(np.float32)\n\n# ----------------------------\n# 8) Final outputs\n# ----------------------------\ndf_train_tabular = df_train[[\"uid\",\"case_id\",\"variant\",\"fold\",\"y\"] + feature_cols].copy()\n\nX_train = df_train_tabular[feature_cols]\ny_train = df_train_tabular[\"y\"].astype(int)\nfolds   = df_train_tabular[\"fold\"].astype(int)\n\nprint(\"\\nOK — Training table built\")\nprint(\"  df_train_tabular:\", df_train_tabular.shape)\nprint(\"  X_train:\", X_train.shape, \"| y pos%:\", float(y_train.mean())*100.0)\nprint(\"  folds:\", folds.nunique(), \"unique folds\")\nprint(\"  feature_cols:\", len(feature_cols))\n\n# quick sanity\nif X_train.shape[0] != y_train.shape[0]:\n    raise RuntimeError(\"X_train and y_train row mismatch\")\nif y_train.isna().any():\n    raise RuntimeError(\"y_train contains NaN\")\nif folds.isna().any():\n    raise RuntimeError(\"folds contains NaN\")\n\n# export globals for next steps\nFEATURE_COLS = feature_cols\nprint(\"\\nFeature head:\", FEATURE_COLS[:20])\nprint(\"Feature tail:\", FEATURE_COLS[-10:])\n\n# Optional: save the feature list now (handy for consistency)\nOUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_ART.mkdir(parents=True, exist_ok=True)\nwith open(OUT_ART / \"feature_cols.json\", \"w\") as f:\n    json.dump(FEATURE_COLS, f, indent=2)\n\nprint(f\"\\nSaved feature list -> {OUT_ART/'feature_cols.json'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:28:42.345075Z","iopub.execute_input":"2026-01-04T08:28:42.345713Z","iopub.status.idle":"2026-01-04T08:28:42.553431Z","shell.execute_reply.started":"2026-01-04T08:28:42.345675Z","shell.execute_reply":"2026-01-04T08:28:42.552438Z"}},"outputs":[{"name":"stdout","text":"Using:\n  DF_TRAIN_ALL     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet\n  CV_CASE_FOLDS    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv\n  PRED_FEAT_TRAIN  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv\n  MATCH_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv (optional)\n  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet (optional)\n  DINO_LARGE_DIR   : /kaggle/input/dinov2/pytorch/large/1\n\nOK — Training table built\n  df_train_tabular: (5176, 52)\n  X_train: (5176, 47) | y pos%: 54.07650695517774\n  folds: 5 unique folds\n  feature_cols: 47\n\nFeature head: ['feat_exists', 'match_exists', 'has_peak', 'peak_ratio', 'best_weight', 'best_count', 'best_mean_sim', 'n_pairs_thr', 'n_pairs_mnn', 'best_inlier_ratio', 'best_weight_frac', 'inlier_ratio', 'pair_count', 'uniq_src', 'uniq_dst', 'mean_sim', 'thr_used', 'cnt_thr_used', 'relaxed_used', 'min_pairs_used']\nFeature tail: ['roi_x0', 'roi_y0', 'roi_x1', 'roi_y1', 'roi_area_frac', 'edge_density', 'peak_ratio_cap', 'log_peak_ratio', 'best_weight_cap', 'log_best_weight']\n\nSaved feature list -> /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Train Baseline Model (Leakage-Safe CV)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 3 — Train Baseline Model (Leakage-Safe CV) — REVISI FULL\n# - Baseline: StandardScaler + LogisticRegression\n# - CV: pakai kolom `fold` (dibuat by case_id dari cv_case_folds.csv)\n# - Output:\n#   * OOF probabilities (untuk calibration/threshold di step berikutnya)\n#   * CV report (AUC, F1, Precision, Recall, LogLoss)\n#   * Simpan model per fold + model_full\n#\n# Catatan penting revisi:\n# - Pakai sklearn.clone (lebih aman daripada joblib dumps/loads untuk deepcopy estimator)\n# - log_loss selalu pakai labels=[0,1] (biar aman kalau suatu fold cuma 1 kelas)\n# - AUC aman jika 1 kelas (return None)\n# - Semua NaN/inf di X sudah harus ditangani di Step 2; tetap ada guard kecil\n# ============================================================\n\nimport json, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, log_loss\nimport joblib\n\n# ----------------------------\n# 0) Require outputs from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nrequired_cols = {\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"}\nmissing_cols = [c for c in required_cols if c not in df_train_tabular.columns]\nif missing_cols:\n    raise ValueError(f\"df_train_tabular missing columns: {missing_cols}. Pastikan Step 2 berhasil.\")\n\n# ----------------------------\n# 1) Build arrays + sanity\n# ----------------------------\nX = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny = df_train_tabular[\"y\"].to_numpy(dtype=np.int32, copy=True)\nfolds = df_train_tabular[\"fold\"].to_numpy(dtype=np.int32, copy=True)\n\n# extra guard (harusnya sudah bersih di Step 2)\nif not np.isfinite(X).all():\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\nn = len(df_train_tabular)\nunique_folds = sorted(pd.Series(folds).unique().tolist())\nn_folds = len(unique_folds)\n\nprint(\"Baseline setup:\")\nprint(\"  rows      :\", n)\nprint(\"  folds     :\", n_folds, \"|\", unique_folds)\nprint(\"  pos%      :\", float(y.mean()) * 100.0)\nprint(\"  n_features:\", X.shape[1])\n\n# ----------------------------\n# 2) Define baseline model\n# ----------------------------\n# Kamu bisa tweak parameter baseline di sini kalau perlu (tetap baseline sederhana):\nBASELINE_PARAMS = dict(\n    solver=\"lbfgs\",\n    max_iter=4000,\n    C=1.0,\n    class_weight=\"balanced\",\n)\n\nbaseline = Pipeline(steps=[\n    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n    (\"clf\", LogisticRegression(**BASELINE_PARAMS)),\n])\n\n# ----------------------------\n# 3) CV training (Leakage-safe: fold sudah by case_id)\n# ----------------------------\noof_pred = np.zeros(n, dtype=np.float32)\nfold_reports = []\n\nmodels_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts/baseline_folds\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\nfor f in unique_folds:\n    tr_idx = np.where(folds != f)[0]\n    va_idx = np.where(folds == f)[0]\n\n    X_tr, y_tr = X[tr_idx], y[tr_idx]\n    X_va, y_va = X[va_idx], y[va_idx]\n\n    model_f = clone(baseline)\n    model_f.fit(X_tr, y_tr)\n\n    p_va = model_f.predict_proba(X_va)[:, 1].astype(np.float32)\n    oof_pred[va_idx] = p_va\n\n    # Metrics (fold-safe)\n    auc = None\n    if len(np.unique(y_va)) > 1:\n        auc = float(roc_auc_score(y_va, p_va))\n\n    yhat = (p_va >= 0.5).astype(np.int32)\n\n    rep = {\n        \"fold\": int(f),\n        \"n_val\": int(len(va_idx)),\n        \"pos_val\": int(y_va.sum()),\n        \"auc\": auc,\n        \"f1@0.5\": float(f1_score(y_va, yhat, zero_division=0)),\n        \"precision@0.5\": float(precision_score(y_va, yhat, zero_division=0)),\n        \"recall@0.5\": float(recall_score(y_va, yhat, zero_division=0)),\n        \"logloss\": float(log_loss(y_va, np.clip(p_va, 1e-6, 1 - 1e-6), labels=[0, 1])),\n    }\n    fold_reports.append(rep)\n\n    # save fold model\n    joblib.dump(model_f, models_dir / f\"baseline_fold_{f}.joblib\")\n\n    del model_f\n    gc.collect()\n\n# ----------------------------\n# 4) Overall OOF metrics\n# ----------------------------\noof_auc = None\nif len(np.unique(y)) > 1:\n    oof_auc = float(roc_auc_score(y, oof_pred))\n\noof_yhat = (oof_pred >= 0.5).astype(np.int32)\n\noverall = {\n    \"rows\": int(n),\n    \"folds\": int(n_folds),\n    \"pos_total\": int(y.sum()),\n    \"pos_rate\": float(y.mean()),\n    \"oof_auc\": oof_auc,\n    \"oof_f1@0.5\": float(f1_score(y, oof_yhat, zero_division=0)),\n    \"oof_precision@0.5\": float(precision_score(y, oof_yhat, zero_division=0)),\n    \"oof_recall@0.5\": float(recall_score(y, oof_yhat, zero_division=0)),\n    \"oof_logloss\": float(log_loss(y, np.clip(oof_pred, 1e-6, 1 - 1e-6), labels=[0, 1])),\n}\n\ndf_rep = pd.DataFrame(fold_reports).sort_values(\"fold\").reset_index(drop=True)\nprint(\"\\nPer-fold report:\")\ndisplay(df_rep)\n\nprint(\"\\nOOF overall:\")\nprint(overall)\n\n# ----------------------------\n# 5) Train baseline on full data & save artifacts\n# ----------------------------\nout_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\nmodel_full = clone(baseline)\nmodel_full.fit(X, y)\njoblib.dump(model_full, out_dir / \"baseline_model_full.joblib\")\n\n# Save OOF predictions table\ndf_oof = df_train_tabular[[\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]].copy()\ndf_oof[\"oof_pred_baseline\"] = oof_pred\ndf_oof.to_csv(out_dir / \"oof_baseline.csv\", index=False)\n\n# Save report JSON\nreport = {\n    \"model\": \"LogisticRegression + StandardScaler (class_weight=balanced)\",\n    \"params\": BASELINE_PARAMS,\n    \"feature_count\": int(len(FEATURE_COLS)),\n    \"fold_reports\": fold_reports,\n    \"overall\": overall,\n}\nwith open(out_dir / \"baseline_cv_report.json\", \"w\") as f:\n    json.dump(report, f, indent=2)\n\nprint(\"\\nSaved artifacts:\")\nprint(\"  fold models  ->\", models_dir)\nprint(\"  full model   ->\", out_dir / \"baseline_model_full.joblib\")\nprint(\"  oof preds    ->\", out_dir / \"oof_baseline.csv\")\nprint(\"  cv report    ->\", out_dir / \"baseline_cv_report.json\")\n\n# Export globals for next steps\nOOF_PRED_BASELINE = oof_pred\nBASELINE_OVERALL = overall\nBASELINE_FOLD_REPORTS = fold_reports\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:28:42.555376Z","iopub.execute_input":"2026-01-04T08:28:42.556249Z","iopub.status.idle":"2026-01-04T08:28:43.918028Z","shell.execute_reply.started":"2026-01-04T08:28:42.556221Z","shell.execute_reply":"2026-01-04T08:28:43.916861Z"}},"outputs":[{"name":"stdout","text":"Baseline setup:\n  rows      : 5176\n  folds     : 5 | [0, 1, 2, 3, 4]\n  pos%      : 54.07650695517774\n  n_features: 47\n\nPer-fold report:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   fold  n_val  pos_val       auc    f1@0.5  precision@0.5  recall@0.5  \\\n0     0   1034      559  0.533153  0.560430       0.560932    0.559928   \n1     1   1041      561  0.543366  0.581694       0.559211    0.606061   \n2     2   1032      559  0.532439  0.569456       0.550000    0.590340   \n3     3   1035      560  0.534414  0.585654       0.555200    0.619643   \n4     4   1034      560  0.541281  0.574188       0.564767    0.583929   \n\n    logloss  \n0  0.696173  \n1  0.688935  \n2  0.688766  \n3  0.692037  \n4  0.686037  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold</th>\n      <th>n_val</th>\n      <th>pos_val</th>\n      <th>auc</th>\n      <th>f1@0.5</th>\n      <th>precision@0.5</th>\n      <th>recall@0.5</th>\n      <th>logloss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1034</td>\n      <td>559</td>\n      <td>0.533153</td>\n      <td>0.560430</td>\n      <td>0.560932</td>\n      <td>0.559928</td>\n      <td>0.696173</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1041</td>\n      <td>561</td>\n      <td>0.543366</td>\n      <td>0.581694</td>\n      <td>0.559211</td>\n      <td>0.606061</td>\n      <td>0.688935</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1032</td>\n      <td>559</td>\n      <td>0.532439</td>\n      <td>0.569456</td>\n      <td>0.550000</td>\n      <td>0.590340</td>\n      <td>0.688766</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1035</td>\n      <td>560</td>\n      <td>0.534414</td>\n      <td>0.585654</td>\n      <td>0.555200</td>\n      <td>0.619643</td>\n      <td>0.692037</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1034</td>\n      <td>560</td>\n      <td>0.541281</td>\n      <td>0.574188</td>\n      <td>0.564767</td>\n      <td>0.583929</td>\n      <td>0.686037</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nOOF overall:\n{'rows': 5176, 'folds': 5, 'pos_total': 2799, 'pos_rate': 0.5407650695517774, 'oof_auc': 0.5366708736502595, 'oof_f1@0.5': 0.5744496446524527, 'oof_precision@0.5': 0.5579124579124579, 'oof_recall@0.5': 0.5919971418363701, 'oof_logloss': 0.6903884325713943}\n\nSaved artifacts:\n  fold models  -> /kaggle/working/recodai_luc_gate_artifacts/baseline_folds\n  full model   -> /kaggle/working/recodai_luc_gate_artifacts/baseline_model_full.joblib\n  oof preds    -> /kaggle/working/recodai_luc_gate_artifacts/oof_baseline.csv\n  cv report    -> /kaggle/working/recodai_luc_gate_artifacts/baseline_cv_report.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Optimize Model & Hyperparameters (Iterative)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 4 — Optimize Model & Hyperparameters (Iterative)\n# - Fokus: iterasi model gate tabular (tanpa submission)\n# - Model kandidat: LogisticRegression, HistGradientBoosting, ExtraTrees\n# - Validasi: Leakage-safe CV pakai kolom `fold` (by case_id)\n# - Skor utama untuk compare: Best F-beta (beta=0.5) dari OOF (lebih anti-FP)\n#   + tetap log AUC & LogLoss sebagai sanity metric.\n#\n# Output:\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.json\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/oof_preds_<model>.csv (top models)\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.joblib\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n#\n# REQUIRE:\n# - Step 2 sudah jalan: df_train_tabular, FEATURE_COLS\n# ============================================================\n\nimport os, json, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier, HistGradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score, log_loss, fbeta_score, precision_score, recall_score\nimport joblib\n\n# ----------------------------\n# 0) Require data from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nX = df_train_tabular[FEATURE_COLS].values.astype(np.float32)\ny = df_train_tabular[\"y\"].values.astype(int)\nfolds = df_train_tabular[\"fold\"].values.astype(int)\nuids = df_train_tabular[\"uid\"].astype(str).values\n\nunique_folds = sorted(pd.Series(folds).unique().tolist())\nn = len(y)\npos_rate = float(y.mean())\n\nprint(\"Optimize setup:\")\nprint(f\"  rows={n} | folds={len(unique_folds)} | pos%={pos_rate*100:.2f} | n_features={X.shape[1]}\")\n\n# ----------------------------\n# 1) Helpers: metrics & threshold search\n# ----------------------------\ndef compute_best_fbeta(y_true, p, beta=0.5, grid=201):\n    \"\"\"\n    Cari threshold terbaik untuk F-beta dari probabilitas p.\n    beta<1 menekankan precision (anti false-positive), cocok untuk gate.\n    \"\"\"\n    p = np.asarray(p, dtype=np.float64)\n    p = np.clip(p, 1e-8, 1-1e-8)\n\n    best = {\"fbeta\": -1.0, \"thr\": 0.5, \"precision\": 0.0, \"recall\": 0.0}\n    for thr in np.linspace(0.01, 0.99, grid):\n        yh = (p >= thr).astype(int)\n        f = fbeta_score(y_true, yh, beta=beta, zero_division=0)\n        if f > best[\"fbeta\"]:\n            best[\"fbeta\"] = float(f)\n            best[\"thr\"] = float(thr)\n            best[\"precision\"] = float(precision_score(y_true, yh, zero_division=0))\n            best[\"recall\"] = float(recall_score(y_true, yh, zero_division=0))\n    return best\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1-1e-8)\n    return float(log_loss(y_true, p))\n\n# ----------------------------\n# 2) CV runner (leakage-safe)\n# ----------------------------\ndef run_cv(model_builder, model_name, beta=0.5, save_oof=False, out_dir=None):\n    \"\"\"\n    model_builder(fold_idx) -> sklearn estimator (sudah siap fit).\n    Untuk HGB: kita pakai sample_weight agar balance.\n    \"\"\"\n    oof = np.zeros(n, dtype=np.float32)\n    fold_rows = []\n\n    for f in unique_folds:\n        tr = np.where(folds != f)[0]\n        va = np.where(folds == f)[0]\n\n        X_tr, y_tr = X[tr], y[tr]\n        X_va, y_va = X[va], y[va]\n\n        est = model_builder(f)\n\n        # sample_weight untuk model yang tidak punya class_weight (HGB)\n        fit_kwargs = {}\n        if isinstance(est, HistGradientBoostingClassifier):\n            # weight_pos = neg/pos (balanced)\n            pos = max(1, int(y_tr.sum()))\n            neg = max(1, int(len(y_tr) - y_tr.sum()))\n            w_pos = neg / pos\n            sw = np.where(y_tr == 1, w_pos, 1.0).astype(np.float32)\n            fit_kwargs[\"sample_weight\"] = sw\n\n        est.fit(X_tr, y_tr, **fit_kwargs)\n\n        if hasattr(est, \"predict_proba\"):\n            p_va = est.predict_proba(X_va)[:, 1].astype(np.float32)\n        else:\n            # fallback (harusnya tidak kejadian)\n            p_va = est.predict(X_va).astype(np.float32)\n\n        oof[va] = p_va\n\n        fold_auc = safe_auc(y_va, p_va)\n        fold_ll  = safe_logloss(y_va, p_va)\n        best_fold = compute_best_fbeta(y_va, p_va, beta=beta, grid=101)\n\n        fold_rows.append({\n            \"model\": model_name,\n            \"fold\": int(f),\n            \"n_val\": int(len(va)),\n            \"pos_val\": int(y_va.sum()),\n            \"auc\": fold_auc,\n            \"logloss\": fold_ll,\n            \"best_fbeta\": best_fold[\"fbeta\"],\n            \"best_thr\": best_fold[\"thr\"],\n            \"best_prec\": best_fold[\"precision\"],\n            \"best_rec\": best_fold[\"recall\"],\n        })\n\n        del est\n        gc.collect()\n\n    # overall OOF\n    oof_auc = safe_auc(y, oof)\n    oof_ll  = safe_logloss(y, oof)\n    best_oof = compute_best_fbeta(y, oof, beta=beta, grid=201)\n\n    summary = {\n        \"model\": model_name,\n        \"oof_auc\": oof_auc,\n        \"oof_logloss\": oof_ll,\n        \"oof_best_fbeta\": best_oof[\"fbeta\"],\n        \"oof_best_thr\": best_oof[\"thr\"],\n        \"oof_best_prec\": best_oof[\"precision\"],\n        \"oof_best_rec\": best_oof[\"recall\"],\n    }\n\n    if save_oof and out_dir is not None:\n        out_dir = Path(out_dir)\n        out_dir.mkdir(parents=True, exist_ok=True)\n        df_o = pd.DataFrame({\n            \"uid\": uids,\n            \"y\": y,\n            \"fold\": folds,\n            f\"oof_pred_{model_name}\": oof\n        })\n        df_o.to_csv(out_dir / f\"oof_preds_{model_name}.csv\", index=False)\n\n    return summary, fold_rows, oof\n\n# ----------------------------\n# 3) Define candidate configs (kamu bisa tambah/ubah parameter di sini)\n# ----------------------------\n# Penjelasan singkat (ID):\n# - Logistic: baseline kuat, probabilitas relatif stabil\n# - HGB: non-linear cepat, bagus untuk interaksi fitur\n# - ExtraTrees: robust untuk noise, sering bagus di tabular feature hasil match/pred\nBETA = 0.5  # beta<1 => fokus precision (anti false-positive)\nRANDOM_SEED = 2025\n\ncandidates = []\n\n# (A) Logistic Regression grid kecil\nfor C in [0.25, 0.5, 1.0, 2.0, 4.0]:\n    name = f\"logreg_C{C:g}\"\n    def builder_factory(Cval):\n        def _builder(_fold):\n            return Pipeline(steps=[\n                (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n                (\"clf\", LogisticRegression(\n                    solver=\"lbfgs\",\n                    max_iter=3000,\n                    C=Cval,\n                    class_weight=\"balanced\",\n                ))\n            ])\n        return _builder\n    candidates.append((name, builder_factory(C)))\n\n# (B) HistGradientBoosting grid kecil\nfor lr in [0.05, 0.08, 0.12]:\n    for md in [3, 4]:\n        for leaf in [31, 63]:\n            name = f\"hgb_lr{lr}_md{md}_leaf{leaf}\"\n            def builder_factory(lrval, mdval, leafval):\n                def _builder(_fold):\n                    return HistGradientBoostingClassifier(\n                        learning_rate=lrval,\n                        max_depth=mdval,\n                        max_leaf_nodes=leafval,\n                        max_iter=250,\n                        early_stopping=False,\n                        random_state=RANDOM_SEED,\n                    )\n                return _builder\n            candidates.append((name, builder_factory(lr, md, leaf)))\n\n# (C) ExtraTrees grid kecil\nfor n_est in [400, 800]:\n    for md in [None, 24]:\n        for msl in [1, 3]:\n            name = f\"et_n{n_est}_md{md if md is not None else 'None'}_msl{msl}\"\n            def builder_factory(n_est_val, md_val, msl_val):\n                def _builder(_fold):\n                    return ExtraTreesClassifier(\n                        n_estimators=n_est_val,\n                        max_depth=md_val,\n                        min_samples_leaf=msl_val,\n                        max_features=\"sqrt\",\n                        class_weight=\"balanced\",\n                        n_jobs=-1,\n                        random_state=RANDOM_SEED,\n                    )\n                return _builder\n            candidates.append((name, builder_factory(n_est, md, msl)))\n\nprint(f\"\\nTotal candidates: {len(candidates)}\")\nprint(\"Scoring: OOF best F-beta (beta=0.5) is primary\")\n\n# ----------------------------\n# 4) Run iterative search\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOPT_DIR = OUT_DIR / \"opt_search\"\nOPT_DIR.mkdir(parents=True, exist_ok=True)\n\nall_summaries = []\nall_fold_rows = []\noof_store = {}  # store OOF preds for possible ensemble\n\nfor i, (name, builder) in enumerate(candidates, 1):\n    print(f\"[{i:03d}/{len(candidates)}] CV -> {name}\")\n    summ, fold_rows, oof = run_cv(builder, name, beta=BETA, save_oof=False)\n    all_summaries.append(summ)\n    all_fold_rows.extend(fold_rows)\n    oof_store[name] = oof\n    print(\"  oof_best_fbeta:\", f\"{summ['oof_best_fbeta']:.5f}\",\n          \"| oof_auc:\", f\"{(summ['oof_auc'] if summ['oof_auc'] is not None else float('nan')):.5f}\",\n          \"| oof_logloss:\", f\"{summ['oof_logloss']:.5f}\",\n          \"| thr:\", f\"{summ['oof_best_thr']:.3f}\")\n\ndf_sum = pd.DataFrame(all_summaries)\ndf_fold = pd.DataFrame(all_fold_rows)\n\n# rank by primary metric (oof_best_fbeta), tie-break by logloss (lower better)\ndf_sum[\"rank_key\"] = list(zip(-df_sum[\"oof_best_fbeta\"], df_sum[\"oof_logloss\"]))\ndf_sum = df_sum.sort_values([\"oof_best_fbeta\", \"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n\nprint(\"\\nTop 10 candidates:\")\ndisplay(df_sum.head(10))\n\n# save search results\ndf_sum.drop(columns=[\"rank_key\"], errors=\"ignore\").to_csv(OPT_DIR / \"opt_results.csv\", index=False)\nwith open(OPT_DIR / \"opt_results.json\", \"w\") as f:\n    json.dump(df_sum.drop(columns=[\"rank_key\"], errors=\"ignore\").to_dict(orient=\"records\"), f, indent=2)\n\n# ----------------------------\n# 5) Optional: simple ensemble of top-K (avg OOF)\n# ----------------------------\nTOPK = 3\ntop_names = df_sum[\"model\"].head(TOPK).tolist()\nif len(top_names) >= 2:\n    oof_ens = np.mean([oof_store[nm] for nm in top_names], axis=0).astype(np.float32)\n    ens_best = compute_best_fbeta(y, oof_ens, beta=BETA, grid=201)\n    ens_auc  = safe_auc(y, oof_ens)\n    ens_ll   = safe_logloss(y, oof_ens)\n\n    ens_summary = {\n        \"model\": f\"ensemble_avg_top{TOPK}\",\n        \"members\": top_names,\n        \"oof_auc\": ens_auc,\n        \"oof_logloss\": ens_ll,\n        \"oof_best_fbeta\": ens_best[\"fbeta\"],\n        \"oof_best_thr\": ens_best[\"thr\"],\n        \"oof_best_prec\": ens_best[\"precision\"],\n        \"oof_best_rec\": ens_best[\"recall\"],\n    }\n    print(\"\\nEnsemble OOF (avg) result:\")\n    print(ens_summary)\n\n    # append to summary table (for visibility)\n    df_sum2 = pd.concat([df_sum.drop(columns=[\"rank_key\"], errors=\"ignore\"),\n                         pd.DataFrame([ens_summary])], ignore_index=True)\nelse:\n    ens_summary = None\n    df_sum2 = df_sum.drop(columns=[\"rank_key\"], errors=\"ignore\")\n\n# save fold detail\ndf_fold.to_csv(OPT_DIR / \"opt_fold_details.csv\", index=False)\n\n# ----------------------------\n# 6) Select best model (compare best single vs ensemble)\n# ----------------------------\nbest_single = df_sum.iloc[0].to_dict()\nbest_choice = {\"type\": \"single\", \"model\": best_single[\"model\"], \"summary\": best_single}\n\nif ens_summary is not None and ens_summary[\"oof_best_fbeta\"] >= best_single[\"oof_best_fbeta\"]:\n    best_choice = {\"type\": \"ensemble\", \"model\": ens_summary[\"model\"], \"summary\": ens_summary}\n\nprint(\"\\nBest choice:\")\nprint(best_choice)\n\n# ----------------------------\n# 7) Train final model(s) on FULL DATA and save bundle\n# ----------------------------\ndef build_estimator_by_name(name: str):\n    # rebuild the estimator from the name string we used above\n    if name.startswith(\"logreg_C\"):\n        C = float(name.split(\"logreg_C\", 1)[1])\n        return Pipeline(steps=[\n            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n            (\"clf\", LogisticRegression(\n                solver=\"lbfgs\",\n                max_iter=3000,\n                C=C,\n                class_weight=\"balanced\",\n            ))\n        ])\n\n    if name.startswith(\"hgb_\"):\n        # parse: hgb_lr{lr}_md{md}_leaf{leaf}\n        parts = name.split(\"_\")\n        lr = float(parts[1].replace(\"lr\", \"\"))\n        md = int(parts[2].replace(\"md\", \"\"))\n        leaf = int(parts[3].replace(\"leaf\", \"\"))\n        return HistGradientBoostingClassifier(\n            learning_rate=lr,\n            max_depth=md,\n            max_leaf_nodes=leaf,\n            max_iter=250,\n            early_stopping=False,\n            random_state=RANDOM_SEED,\n        )\n\n    if name.startswith(\"et_\"):\n        # parse: et_n{n}_md{md}_msl{msl}\n        parts = name.split(\"_\")\n        n_est = int(parts[1].replace(\"n\", \"\"))\n        md_str = parts[2].replace(\"md\", \"\")\n        md = None if md_str == \"None\" else int(md_str)\n        msl = int(parts[3].replace(\"msl\", \"\"))\n        return ExtraTreesClassifier(\n            n_estimators=n_est,\n            max_depth=md,\n            min_samples_leaf=msl,\n            max_features=\"sqrt\",\n            class_weight=\"balanced\",\n            n_jobs=-1,\n            random_state=RANDOM_SEED,\n        )\n\n    raise ValueError(f\"Unknown model name pattern: {name}\")\n\nbundle = {\n    \"type\": best_choice[\"type\"],\n    \"beta\": BETA,\n    \"feature_cols\": FEATURE_COLS,\n}\n\n# save top-K OOF csv for debugging (optional)\nfor nm in top_names[:3]:\n    df_o = pd.DataFrame({\"uid\": uids, \"y\": y, \"fold\": folds, \"oof_pred\": oof_store[nm]})\n    df_o.to_csv(OPT_DIR / f\"oof_preds_{nm}.csv\", index=False)\n\nif best_choice[\"type\"] == \"single\":\n    best_name = best_choice[\"model\"]\n    est = build_estimator_by_name(best_name)\n\n    fit_kwargs = {}\n    if isinstance(est, HistGradientBoostingClassifier):\n        pos = max(1, int(y.sum()))\n        neg = max(1, int(len(y) - y.sum()))\n        w_pos = neg / pos\n        sw = np.where(y == 1, w_pos, 1.0).astype(np.float32)\n        fit_kwargs[\"sample_weight\"] = sw\n\n    est.fit(X, y, **fit_kwargs)\n    joblib.dump(est, OUT_DIR / \"best_gate_model.joblib\")\n\n    bundle[\"model_name\"] = best_name\n    bundle[\"members\"] = [best_name]\n    bundle[\"oof_best_thr\"] = best_choice[\"summary\"][\"oof_best_thr\"]\n    bundle[\"oof_best_fbeta\"] = best_choice[\"summary\"][\"oof_best_fbeta\"]\n\nelse:\n    # ensemble: train each member on full data and save list\n    members = best_choice[\"summary\"][\"members\"]\n    models = []\n    for nm in members:\n        est = build_estimator_by_name(nm)\n        fit_kwargs = {}\n        if isinstance(est, HistGradientBoostingClassifier):\n            pos = max(1, int(y.sum()))\n            neg = max(1, int(len(y) - y.sum()))\n            w_pos = neg / pos\n            sw = np.where(y == 1, w_pos, 1.0).astype(np.float32)\n            fit_kwargs[\"sample_weight\"] = sw\n        est.fit(X, y, **fit_kwargs)\n        models.append(est)\n\n    joblib.dump(models, OUT_DIR / \"best_gate_model.joblib\")  # list of estimators\n\n    bundle[\"model_name\"] = best_choice[\"model\"]\n    bundle[\"members\"] = members\n    bundle[\"oof_best_thr\"] = best_choice[\"summary\"][\"oof_best_thr\"]\n    bundle[\"oof_best_fbeta\"] = best_choice[\"summary\"][\"oof_best_fbeta\"]\n\nwith open(OUT_DIR / \"best_gate_config.json\", \"w\") as f:\n    json.dump(bundle, f, indent=2)\n\nprint(\"\\nSaved best artifacts:\")\nprint(\"  best model  ->\", OUT_DIR / \"best_gate_model.joblib\")\nprint(\"  best config ->\", OUT_DIR / \"best_gate_config.json\")\nprint(\"  opt results ->\", OPT_DIR / \"opt_results.csv\")\nprint(\"  fold detail ->\", OPT_DIR / \"opt_fold_details.csv\")\n\n# Export globals (untuk step berikutnya seperti calibration/threshold lebih lanjut jika mau)\nBEST_GATE_BUNDLE = bundle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T08:28:43.919125Z","iopub.execute_input":"2026-01-04T08:28:43.919409Z"}},"outputs":[{"name":"stdout","text":"Optimize setup:\n  rows=5176 | folds=5 | pos%=54.08 | n_features=47\n\nTotal candidates: 25\nScoring: OOF best F-beta (beta=0.5) is primary\n[001/25] CV -> logreg_C0.25\n  oof_best_fbeta: 0.59989 | oof_auc: 0.53703 | oof_logloss: 0.68981 | thr: 0.441\n[002/25] CV -> logreg_C0.5\n  oof_best_fbeta: 0.59997 | oof_auc: 0.53658 | oof_logloss: 0.69011 | thr: 0.426\n[003/25] CV -> logreg_C1\n  oof_best_fbeta: 0.59962 | oof_auc: 0.53667 | oof_logloss: 0.69039 | thr: 0.426\n[004/25] CV -> logreg_C2\n  oof_best_fbeta: 0.59984 | oof_auc: 0.53669 | oof_logloss: 0.69067 | thr: 0.426\n[005/25] CV -> logreg_C4\n  oof_best_fbeta: 0.60035 | oof_auc: 0.53695 | oof_logloss: 0.69095 | thr: 0.426\n[006/25] CV -> hgb_lr0.05_md3_leaf31\n  oof_best_fbeta: 0.60125 | oof_auc: 0.53916 | oof_logloss: 0.69015 | thr: 0.387\n[007/25] CV -> hgb_lr0.05_md3_leaf63\n  oof_best_fbeta: 0.60125 | oof_auc: 0.53916 | oof_logloss: 0.69015 | thr: 0.387\n[008/25] CV -> hgb_lr0.05_md4_leaf31\n  oof_best_fbeta: 0.59902 | oof_auc: 0.52809 | oof_logloss: 0.69525 | thr: 0.392\n[009/25] CV -> hgb_lr0.05_md4_leaf63\n  oof_best_fbeta: 0.59902 | oof_auc: 0.52809 | oof_logloss: 0.69525 | thr: 0.392\n[010/25] CV -> hgb_lr0.08_md3_leaf31\n  oof_best_fbeta: 0.60037 | oof_auc: 0.53251 | oof_logloss: 0.69384 | thr: 0.368\n[011/25] CV -> hgb_lr0.08_md3_leaf63\n  oof_best_fbeta: 0.60037 | oof_auc: 0.53251 | oof_logloss: 0.69384 | thr: 0.368\n[012/25] CV -> hgb_lr0.08_md4_leaf31\n  oof_best_fbeta: 0.59884 | oof_auc: 0.52537 | oof_logloss: 0.70148 | thr: 0.397\n[013/25] CV -> hgb_lr0.08_md4_leaf63\n  oof_best_fbeta: 0.59884 | oof_auc: 0.52537 | oof_logloss: 0.70148 | thr: 0.397\n[014/25] CV -> hgb_lr0.12_md3_leaf31\n  oof_best_fbeta: 0.59888 | oof_auc: 0.53315 | oof_logloss: 0.70012 | thr: 0.373\n[015/25] CV -> hgb_lr0.12_md3_leaf63\n  oof_best_fbeta: 0.59888 | oof_auc: 0.53315 | oof_logloss: 0.70012 | thr: 0.373\n[016/25] CV -> hgb_lr0.12_md4_leaf31\n  oof_best_fbeta: 0.59719 | oof_auc: 0.51695 | oof_logloss: 0.71257 | thr: 0.231\n[017/25] CV -> hgb_lr0.12_md4_leaf63\n  oof_best_fbeta: 0.59719 | oof_auc: 0.51695 | oof_logloss: 0.71257 | thr: 0.231\n[018/25] CV -> et_n400_mdNone_msl1\n  oof_best_fbeta: 0.59511 | oof_auc: 0.52578 | oof_logloss: 0.78693 | thr: 0.010\n[019/25] CV -> et_n400_mdNone_msl3\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Final Training (Train on Full Data)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 5 — Final Training (Train on Full Data)\n# - Tujuan: latih ulang model terbaik pada seluruh data train (full data)\n# - Input: hasil Step 2 (df_train_tabular + FEATURE_COLS)\n# - Ambil konfigurasi terbaik dari:\n#     /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n#   atau dari variabel BEST_GATE_BUNDLE (jika masih ada di memory)\n# - Output:\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.joblib\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n#\n# Catatan:\n# - Tidak ada submission di sini\n# - Untuk HGB: pakai sample_weight (balanced) karena tidak punya class_weight\n# ============================================================\n\nimport json, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier, HistGradientBoostingClassifier\nimport joblib\n\n# ----------------------------\n# 0) Require outputs from Step 2\n# ----------------------------\nif \"df_train_tabular\" not in globals():\n    raise RuntimeError(\"Missing `df_train_tabular`. Jalankan Step 2 — Build Training Table dulu.\")\nif \"FEATURE_COLS\" not in globals():\n    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 — Build Training Table dulu.\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nX = df_train_tabular[FEATURE_COLS].values.astype(np.float32)\ny = df_train_tabular[\"y\"].values.astype(int)\n\nprint(\"Final training data:\")\nprint(\"  rows:\", len(y), \"| pos%:\", float(y.mean()) * 100.0, \"| n_features:\", X.shape[1])\n\n# ----------------------------\n# 1) Load best config (from disk or memory)\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\ncfg_path = OUT_DIR / \"best_gate_config.json\"\n\nif \"BEST_GATE_BUNDLE\" in globals() and isinstance(BEST_GATE_BUNDLE, dict):\n    best_cfg = BEST_GATE_BUNDLE\n    source = \"memory(BEST_GATE_BUNDLE)\"\nelif cfg_path.exists():\n    best_cfg = json.loads(cfg_path.read_text())\n    source = str(cfg_path)\nelse:\n    raise FileNotFoundError(\n        \"Best config not found. Jalankan Step 4 — Optimize Model & Hyperparameters dulu \"\n        \"atau pastikan best_gate_config.json ada.\"\n    )\n\nprint(\"\\nLoaded best config from:\", source)\nprint(\"  type      :\", best_cfg.get(\"type\"))\nprint(\"  model_name :\", best_cfg.get(\"model_name\"))\nprint(\"  members   :\", best_cfg.get(\"members\"))\n\n# Optional: override if you want manual final choice\n# FORCE_MEMBERS = None  # e.g. [\"logreg_C1\", \"et_n800_mdNone_msl1\"]\nFORCE_MEMBERS = None\n\nmembers = best_cfg.get(\"members\", [])\nif FORCE_MEMBERS is not None:\n    members = list(FORCE_MEMBERS)\n\nif not members:\n    raise ValueError(\"No members found in best config. Expected 'members' list.\")\n\n# Use stored seed if present\nRANDOM_SEED = int(best_cfg.get(\"random_seed\", 2025))\n\n# ----------------------------\n# 2) Rebuild estimator from name (must match naming in Step 4)\n# ----------------------------\ndef build_estimator_by_name(name: str):\n    # Logistic Regression pattern: logreg_C{C}\n    if name.startswith(\"logreg_C\"):\n        C = float(name.split(\"logreg_C\", 1)[1])\n        return Pipeline(steps=[\n            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n            (\"clf\", LogisticRegression(\n                solver=\"lbfgs\",\n                max_iter=3000,\n                C=C,\n                class_weight=\"balanced\",\n            ))\n        ])\n\n    # HistGradientBoosting pattern: hgb_lr{lr}_md{md}_leaf{leaf}\n    if name.startswith(\"hgb_\"):\n        parts = name.split(\"_\")\n        lr = float(parts[1].replace(\"lr\", \"\"))\n        md = int(parts[2].replace(\"md\", \"\"))\n        leaf = int(parts[3].replace(\"leaf\", \"\"))\n        return HistGradientBoostingClassifier(\n            learning_rate=lr,\n            max_depth=md,\n            max_leaf_nodes=leaf,\n            max_iter=250,\n            early_stopping=False,\n            random_state=RANDOM_SEED,\n        )\n\n    # ExtraTrees pattern: et_n{n}_md{md}_msl{msl}\n    if name.startswith(\"et_\"):\n        parts = name.split(\"_\")\n        n_est = int(parts[1].replace(\"n\", \"\"))\n        md_str = parts[2].replace(\"md\", \"\")\n        md = None if md_str == \"None\" else int(md_str)\n        msl = int(parts[3].replace(\"msl\", \"\"))\n        return ExtraTreesClassifier(\n            n_estimators=n_est,\n            max_depth=md,\n            min_samples_leaf=msl,\n            max_features=\"sqrt\",\n            class_weight=\"balanced\",\n            n_jobs=-1,\n            random_state=RANDOM_SEED,\n        )\n\n    raise ValueError(f\"Unknown model name pattern: {name}\")\n\ndef fit_with_optional_sample_weight(est, X, y):\n    fit_kwargs = {}\n    # HGB tidak punya class_weight -> pakai sample_weight balanced\n    if isinstance(est, HistGradientBoostingClassifier):\n        pos = max(1, int(y.sum()))\n        neg = max(1, int(len(y) - y.sum()))\n        w_pos = neg / pos\n        sw = np.where(y == 1, w_pos, 1.0).astype(np.float32)\n        fit_kwargs[\"sample_weight\"] = sw\n    est.fit(X, y, **fit_kwargs)\n    return est\n\n# ----------------------------\n# 3) Train final model(s) on full data\n# ----------------------------\ntrained_models = []\nfor i, nm in enumerate(members, 1):\n    print(f\"[{i}/{len(members)}] Training member: {nm}\")\n    est = build_estimator_by_name(nm)\n    est = fit_with_optional_sample_weight(est, X, y)\n    trained_models.append(est)\n    gc.collect()\n\n# If only one member, save single estimator; else save list (ensemble)\nfinal_obj = trained_models[0] if len(trained_models) == 1 else trained_models\n\n# ----------------------------\n# 4) Save final artifacts\n# ----------------------------\nfinal_model_path = OUT_DIR / \"final_gate_model.joblib\"\njoblib.dump(final_obj, final_model_path)\n\nfinal_bundle = {\n    \"type\": \"single\" if len(trained_models) == 1 else \"ensemble\",\n    \"model_name\": \"final_gate_model\",\n    \"members\": members,\n    \"random_seed\": RANDOM_SEED,\n    \"feature_cols\": FEATURE_COLS,\n    # (Opsional) simpan threshold OOF terbaik dari step sebelumnya untuk referensi\n    \"oof_best_thr\": best_cfg.get(\"oof_best_thr\", None),\n    \"oof_best_fbeta\": best_cfg.get(\"oof_best_fbeta\", None),\n    \"train_rows\": int(len(y)),\n    \"pos_rate\": float(y.mean()),\n}\n\nfinal_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\nfinal_bundle_path.write_text(json.dumps(final_bundle, indent=2))\n\nprint(\"\\nSaved final training artifacts:\")\nprint(\"  model  ->\", final_model_path)\nprint(\"  bundle ->\", final_bundle_path)\n\n# Export globals (untuk step berikutnya jika mau calibration/threshold lanjut)\nFINAL_GATE_MODEL = final_obj\nFINAL_GATE_BUNDLE = final_bundle\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finalize & Save Model Bundle (Reproducible)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 6 — Finalize & Save Model Bundle (Reproducible)\n# - Tujuan: satukan semua artefak penting menjadi 1 bundle yang mudah di-load ulang\n# - Tidak ada submission di sini\n#\n# Bundle berisi:\n#  1) final model (final_gate_model.joblib)\n#  2) feature_cols.json\n#  3) thresholds.json (placeholder / bisa diisi dari step tuning berikutnya)\n#  4) training report (baseline + optimize + final summary)\n#  5) metadata cfg (MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR, roots)\n#\n# REQUIRE:\n# - Step 2 (feature_cols.json sudah dibuat) dan Step 5 (final model sudah ada)\n# ============================================================\n\nimport os, json, time, platform, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport joblib\n\nwarnings.filterwarnings(\"ignore\")\n\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 0) Locate required artifacts\n# ----------------------------\nfinal_model_path = OUT_DIR / \"final_gate_model.joblib\"\nfinal_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\nfeature_cols_path = OUT_DIR / \"feature_cols.json\"\n\nif not final_model_path.exists():\n    raise FileNotFoundError(f\"Missing final model: {final_model_path} (jalankan Step 5 dulu)\")\nif not feature_cols_path.exists():\n    raise FileNotFoundError(f\"Missing feature_cols: {feature_cols_path} (jalankan Step 2 dulu)\")\n\n# Optional artifacts\nbaseline_report_path = OUT_DIR / \"baseline_cv_report.json\"\nopt_config_path = OUT_DIR / \"best_gate_config.json\"\nopt_results_csv = OUT_DIR / \"opt_search\" / \"opt_results.csv\"\nopt_fold_csv = OUT_DIR / \"opt_search\" / \"opt_fold_details.csv\"\noof_baseline_csv = OUT_DIR / \"oof_baseline.csv\"\n\n# ----------------------------\n# 1) Load minimal metadata\n# ----------------------------\nfeature_cols = json.loads(feature_cols_path.read_text())\n\nfinal_bundle = {}\nif final_bundle_path.exists():\n    final_bundle = json.loads(final_bundle_path.read_text())\n\nbaseline_report = None\nif baseline_report_path.exists():\n    try:\n        baseline_report = json.loads(baseline_report_path.read_text())\n    except Exception:\n        baseline_report = None\n\nopt_config = None\nif opt_config_path.exists():\n    try:\n        opt_config = json.loads(opt_config_path.read_text())\n    except Exception:\n        opt_config = None\n\n# ----------------------------\n# 2) Threshold placeholders (can be updated later)\n# ----------------------------\nthresholds_path = OUT_DIR / \"thresholds.json\"\nif thresholds_path.exists():\n    thresholds = json.loads(thresholds_path.read_text())\nelse:\n    # Placeholder: nanti bisa di-update dari step tuning threshold/guard\n    thresholds = {\n        \"T_gate\": final_bundle.get(\"oof_best_thr\", 0.5),\n        \"beta_for_tuning\": 0.5,\n        \"guards\": {\n            \"min_area_frac\": None,\n            \"max_area_frac\": None,\n            \"max_components\": None\n        },\n        \"notes\": \"Placeholder. Update after calibration/threshold tuning.\"\n    }\n    thresholds_path.write_text(json.dumps(thresholds, indent=2))\n\n# ----------------------------\n# 3) Capture dataset/cfg metadata (if available)\n# ----------------------------\ncfg_meta = {}\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    cfg_meta = {\n        \"COMP_ROOT\": PATHS.get(\"COMP_ROOT\", None),\n        \"OUT_DS_ROOT\": PATHS.get(\"OUT_DS_ROOT\", None),\n        \"OUT_ROOT\": PATHS.get(\"OUT_ROOT\", None),\n        \"MATCH_CFG_DIR\": PATHS.get(\"MATCH_CFG_DIR\", None),\n        \"PRED_CFG_DIR\": PATHS.get(\"PRED_CFG_DIR\", None),\n        \"DINO_CFG_DIR\": PATHS.get(\"DINO_CFG_DIR\", None),\n        \"DINO_LARGE_DIR\": PATHS.get(\"DINO_LARGE_DIR\", None),\n        \"PRED_FEAT_TRAIN\": PATHS.get(\"PRED_FEAT_TRAIN\", None),\n        \"MATCH_FEAT_TRAIN\": PATHS.get(\"MATCH_FEAT_TRAIN\", None),\n        \"DF_TRAIN_ALL\": PATHS.get(\"DF_TRAIN_ALL\", None),\n        \"CV_CASE_FOLDS\": PATHS.get(\"CV_CASE_FOLDS\", None),\n        \"IMG_PROFILE_TRAIN\": PATHS.get(\"IMG_PROFILE_TRAIN\", None),\n    }\n\n# ----------------------------\n# 4) Build reproducible manifest\n# ----------------------------\nmanifest = {\n    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"python\": platform.python_version(),\n    \"platform\": platform.platform(),\n    \"bundle_version\": \"v1\",\n    \"task\": \"Recod.ai/LUC — Gate Model (authentic vs forged) — DINOv2 Large pipeline\",\n    \"artifacts\": {\n        \"final_model\": str(final_model_path),\n        \"final_bundle\": str(final_bundle_path) if final_bundle_path.exists() else None,\n        \"feature_cols\": str(feature_cols_path),\n        \"thresholds\": str(thresholds_path),\n        \"baseline_report\": str(baseline_report_path) if baseline_report_path.exists() else None,\n        \"opt_config\": str(opt_config_path) if opt_config_path.exists() else None,\n        \"opt_results_csv\": str(opt_results_csv) if opt_results_csv.exists() else None,\n        \"opt_fold_details_csv\": str(opt_fold_csv) if opt_fold_csv.exists() else None,\n        \"oof_baseline_csv\": str(oof_baseline_csv) if oof_baseline_csv.exists() else None,\n    },\n    \"cfg_meta\": cfg_meta,\n    \"model_summary\": {\n        \"type\": final_bundle.get(\"type\", None),\n        \"members\": final_bundle.get(\"members\", None),\n        \"train_rows\": final_bundle.get(\"train_rows\", None),\n        \"pos_rate\": final_bundle.get(\"pos_rate\", None),\n        \"feature_count\": len(feature_cols),\n    },\n    \"baseline_summary\": (baseline_report.get(\"overall\") if isinstance(baseline_report, dict) else None),\n    \"opt_summary\": (opt_config if isinstance(opt_config, dict) else None),\n}\n\nmanifest_path = OUT_DIR / \"model_bundle_manifest.json\"\nmanifest_path.write_text(json.dumps(manifest, indent=2))\n\n# ----------------------------\n# 5) Create a single \"bundle pack\" (joblib) for easy reload\n# ----------------------------\n# Note: we store model object bytes only via joblib file path to avoid duplication.\n# Bundle pack contains pointers + key data.\nbundle_pack = {\n    \"final_model_path\": str(final_model_path),\n    \"final_bundle\": final_bundle,\n    \"feature_cols\": feature_cols,\n    \"thresholds\": thresholds,\n    \"cfg_meta\": cfg_meta,\n    \"manifest\": manifest,\n}\n\nbundle_pack_path = OUT_DIR / \"model_bundle_pack.joblib\"\njoblib.dump(bundle_pack, bundle_pack_path)\n\nprint(\"OK — Model bundle finalized\")\nprint(\"  manifest  ->\", manifest_path)\nprint(\"  pack      ->\", bundle_pack_path)\nprint(\"  thresholds->\", thresholds_path)\n\nprint(\"\\nBundle summary:\")\nprint(\"  type        :\", manifest[\"model_summary\"][\"type\"])\nprint(\"  members     :\", manifest[\"model_summary\"][\"members\"])\nprint(\"  feature_cnt :\", manifest[\"model_summary\"][\"feature_count\"])\nprint(\"  T_gate      :\", thresholds.get(\"T_gate\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}