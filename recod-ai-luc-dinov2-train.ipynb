{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83af7c1",
   "metadata": {
    "papermill": {
     "duration": 0.007726,
     "end_time": "2026-01-08T11:34:38.708837",
     "exception": false,
     "start_time": "2026-01-08T11:34:38.701111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set Paths & Select Config (CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe95a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T11:34:38.722468Z",
     "iopub.status.busy": "2026-01-08T11:34:38.722208Z",
     "iopub.status.idle": "2026-01-08T11:34:40.385259Z",
     "shell.execute_reply": "2026-01-08T11:34:40.384426Z"
    },
    "papermill": {
     "duration": 1.672266,
     "end_time": "2026-01-08T11:34:40.386925",
     "exception": false,
     "start_time": "2026-01-08T11:34:38.714659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK — Roots\n",
      "  COMP_ROOT   : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n",
      "  OUT_DS_ROOT : /kaggle/input/recod-ailuc-dinov2-base\n",
      "  OUT_ROOT    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc\n",
      "  ART_DIR(use): /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts\n",
      "  CACHE_DIRS  : ['/kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache']\n",
      "\n",
      "OK — Selected CFG (PRIMARY)\n",
      "  MATCH_CFG_DIR: match_base_cfg_f9f7ea3a65c5\n",
      "  PRED_CFG_DIR : pred_base_v3_v7_cfg_5dbf0aa165\n",
      "  DINO_CFG_DIR : cfg_3246fd54aab0\n",
      "\n",
      "OK — Top candidates (for MULTI-CFG training)\n",
      "MATCH CFG TOP:\n",
      "  #01 match_base_cfg_f9f7ea3a65c5 | score=3007202.8 | train_rows=5176 | prefer_hits=3 | dir_hits=0\n",
      "PRED  CFG TOP:\n",
      "  #01 pred_base_v3_v7_cfg_5dbf0aa165 | score=4407206.0 | train_rows=5176 | prefer_hits=4 | dir_hits=2\n",
      "\n",
      "OK — Key files (train)\n",
      "  DF_TRAIN_ALL    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet  (exists)\n",
      "  CV_CASE_FOLDS   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv  (exists)\n",
      "  MATCH_FEAT_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv  (exists)\n",
      "  PRED_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv  (exists)\n",
      "  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet  (exists)\n",
      "\n",
      "OK — Key files (test/infer, optional)\n",
      "  MATCH_FEAT_TEST : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_test.csv  (exists)\n",
      "  PRED_FEAT_TEST  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_test.csv  (exists)\n",
      "  PRED_MAN_TEST   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/manifest_pred_test.csv  (exists)\n",
      "  PRED_SUMMARY    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_summary.json  (exists)\n",
      "\n",
      "OK — DINO model dir\n",
      "  DINO_DIR: /kaggle/input/dinov2/pytorch/large/1 (exists)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 0 — Set Paths & Select Config (CFG) (Kaggle-ready, offline)\n",
    "# REVISI FULL v3.0 (lebih kuat + siap MULTI-CFG + lebih aman anti-error)\n",
    "#\n",
    "# Fokus upgrade v3.0 (sesuai strategi naik score):\n",
    "# - Multi-CFG support: pilih TOP-K CFG kandidat (bukan cuma 1) untuk MATCH & PRED\n",
    "#   -> enabling: stacking/selector di stage training (anti overfit satu CFG)\n",
    "# - Scoring CFG lebih kaya:\n",
    "#   * wajib: feat_train ada & rows > 0\n",
    "#   * prefer: feat_test ada\n",
    "#   * prefer: manifest_test ada, pred_summary.json ada\n",
    "#   * prefer: ada folder npz test/train_all (untuk submission / mask features)\n",
    "#   * tie-break: rows train/test + modified time\n",
    "# - Cache/artifacts root independen:\n",
    "#   * artifacts bisa dari input, cache bisa dari working (atau sebaliknya)\n",
    "# - DINO cache cfg autodetect multi-backbone (large/giant/base) dari cache\n",
    "# - Sanity guard lebih informatif, file opsional tidak bikin crash\n",
    "#\n",
    "# Output globals (TETAP, JANGAN diganti):\n",
    "# - COMP_ROOT, OUT_DS_ROOT, OUT_ROOT\n",
    "# - PATHS (dict)\n",
    "# - MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR\n",
    "#\n",
    "# Extra globals (aman, membantu training lanjutan):\n",
    "# - MATCH_CFG_DIRS, PRED_CFG_DIRS (list[Path] TOP-K)\n",
    "# - MATCH_CFG_INFO, PRED_CFG_INFO (list[dict] detail skor)\n",
    "# - CACHE_ROOTS (list[Path]), SELECTED (dict), TRAIN_PLAN (dict)\n",
    "# ============================================================\n",
    "\n",
    "import os, re, json, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Config knobs (boleh diubah)\n",
    "# ----------------------------\n",
    "TOPK_MATCH_CFGS = int(os.environ.get(\"TOPK_MATCH_CFGS\", \"5\"))\n",
    "TOPK_PRED_CFGS  = int(os.environ.get(\"TOPK_PRED_CFGS\",  \"8\"))\n",
    "\n",
    "# Kalau kamu mau paksa pakai dataset input tertentu:\n",
    "# export LUC_OUT_DS_ROOT=\"/kaggle/input/<nama_dataset_output>\"\n",
    "ENV_OUT_DS_ROOT = os.environ.get(\"LUC_OUT_DS_ROOT\", \"\").strip()\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: fast count CSV rows (binary newline count)\n",
    "# ----------------------------\n",
    "def _fast_count_rows_csv(path: Path, assume_header: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Count rows quickly by counting '\\n' in binary.\n",
    "    Returns -1 if error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not path.exists() or not path.is_file():\n",
    "            return -1\n",
    "        # count newlines\n",
    "        nl = 0\n",
    "        with path.open(\"rb\") as f:\n",
    "            while True:\n",
    "                b = f.read(1024 * 1024)\n",
    "                if not b:\n",
    "                    break\n",
    "                nl += b.count(b\"\\n\")\n",
    "        # if file ends without newline, nl still ok for line count approximation\n",
    "        # rows = lines - header\n",
    "        rows = nl - (1 if assume_header else 0)\n",
    "        return int(max(rows, 0))\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_mtime(p: Path) -> float:\n",
    "    try:\n",
    "        return float(p.stat().st_mtime)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def _is_nonempty_file(p: Path) -> bool:\n",
    "    try:\n",
    "        return p is not None and p.exists() and p.is_file() and p.stat().st_size > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _dir_has_any_npz(d: Path) -> bool:\n",
    "    try:\n",
    "        if d is None or (not d.exists()) or (not d.is_dir()):\n",
    "            return False\n",
    "        # cepat: cek 1 file npz saja\n",
    "        for _ in d.glob(\"*.npz\"):\n",
    "            return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: find competition root\n",
    "# ----------------------------\n",
    "def find_comp_root(preferred: str = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\") -> Path:\n",
    "    p = Path(preferred)\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(\"/kaggle/input tidak ditemukan (pastikan kamu di Kaggle Notebook).\")\n",
    "\n",
    "    cands = []\n",
    "    for d in base.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if (d / \"sample_submission.csv\").exists() and ((d / \"train_images\").exists() or (d / \"test_images\").exists()):\n",
    "            cands.append(d)\n",
    "\n",
    "    if not cands:\n",
    "        for d in base.iterdir():\n",
    "            if not d.is_dir():\n",
    "                continue\n",
    "            for x in d.iterdir():\n",
    "                if not x.is_dir():\n",
    "                    continue\n",
    "                if (x / \"sample_submission.csv\").exists() and ((x / \"train_images\").exists() or (x / \"test_images\").exists()):\n",
    "                    cands.append(x)\n",
    "\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\n",
    "            \"COMP_ROOT tidak ditemukan. Harus ada folder yang memuat sample_submission.csv dan train_images/test_images.\"\n",
    "        )\n",
    "\n",
    "    cands.sort(key=lambda x: ((\"recodai\" not in x.name.lower()),\n",
    "                              (\"forgery\" not in x.name.lower()),\n",
    "                              x.name))\n",
    "    return cands[0]\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: find output dataset root (hasil PREP)\n",
    "# ----------------------------\n",
    "def find_output_dataset_root(preferred_names=(\n",
    "    \"recod-ailuc-dinov2-base\",\n",
    "    \"recod-ai-luc-dinov2-base\",\n",
    "    \"recodai-luc-dinov2-base\",\n",
    "    \"recodai-luc-dinov2\",\n",
    "    \"recodai-luc-dinov2-prep\",\n",
    ")) -> Path:\n",
    "    base = Path(\"/kaggle/input\")\n",
    "\n",
    "    # env override\n",
    "    if ENV_OUT_DS_ROOT:\n",
    "        p = Path(ENV_OUT_DS_ROOT)\n",
    "        if p.exists():\n",
    "            return p\n",
    "        else:\n",
    "            print(f\"WARNING: ENV LUC_OUT_DS_ROOT tidak ditemukan: {p}\")\n",
    "\n",
    "    for nm in preferred_names:\n",
    "        p = base / nm\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    cands = []\n",
    "    for d in base.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if (d / \"recodai_luc\" / \"artifacts\").exists() or (d / \"recodai_luc\" / \"cache\").exists():\n",
    "            cands.append(d)\n",
    "            continue\n",
    "        inner = list(d.glob(\"*/recodai_luc/artifacts\"))\n",
    "        if inner:\n",
    "            cands.append(d)\n",
    "\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"OUT_DS_ROOT tidak ditemukan. Harus ada /kaggle/input/<...>/recodai_luc/(artifacts|cache)/\")\n",
    "\n",
    "    cands.sort(key=lambda x: ((\"dinov2\" not in x.name.lower()), x.name))\n",
    "    return cands[0]\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: resolve OUT_ROOT = <dataset>/recodai_luc\n",
    "# ----------------------------\n",
    "def resolve_out_root(out_ds_root: Path) -> Path:\n",
    "    direct = out_ds_root / \"recodai_luc\"\n",
    "    if direct.exists():\n",
    "        return direct\n",
    "    hits = list(out_ds_root.glob(\"*/recodai_luc\"))\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "    raise FileNotFoundError(f\"Folder recodai_luc tidak ditemukan di bawah {out_ds_root}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: pick TOP-K cfg directories by multi-criteria scoring\n",
    "# ----------------------------\n",
    "def pick_top_cfgs(\n",
    "    cache_roots,\n",
    "    prefixes,\n",
    "    required_train_file: str,\n",
    "    prefer_files=(),\n",
    "    extra_prefer_dirs=(),\n",
    "    max_return: int = 5,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Return list of candidate dicts sorted by score desc.\n",
    "    Each candidate dict: {dir, score, train_rows, prefer_hits, mt, root, debug...}\n",
    "    \"\"\"\n",
    "    if isinstance(prefixes, str):\n",
    "        prefixes = [prefixes]\n",
    "    prefixes = list(prefixes)\n",
    "\n",
    "    prefer_files = list(prefer_files or [])\n",
    "    extra_prefer_dirs = list(extra_prefer_dirs or [])\n",
    "\n",
    "    cands = []\n",
    "    for root in cache_roots:\n",
    "        root = Path(root)\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        for d in root.iterdir():\n",
    "            if not d.is_dir():\n",
    "                continue\n",
    "            if not any(d.name.startswith(px) for px in prefixes):\n",
    "                continue\n",
    "\n",
    "            train_fp = d / required_train_file\n",
    "            if not train_fp.exists():\n",
    "                continue\n",
    "\n",
    "            train_n = _fast_count_rows_csv(train_fp)\n",
    "            if train_n <= 0:\n",
    "                continue\n",
    "\n",
    "            # prefer files\n",
    "            pref_hit = 0\n",
    "            pref_rows_sum = 0\n",
    "            pref_detail = {}\n",
    "            for fn in prefer_files:\n",
    "                fp = d / fn\n",
    "                ok = _is_nonempty_file(fp)\n",
    "                pref_detail[fn] = bool(ok)\n",
    "                if ok:\n",
    "                    pref_hit += 1\n",
    "                    pref_rows_sum += max(_fast_count_rows_csv(fp), 0)\n",
    "\n",
    "            # prefer dirs (npz availability or other dirs)\n",
    "            dir_hit = 0\n",
    "            dir_detail = {}\n",
    "            for dn in extra_prefer_dirs:\n",
    "                dd = d / dn\n",
    "                ok = dd.exists() and dd.is_dir()\n",
    "                # special: if expecting npz, check any npz\n",
    "                if ok and dn in (\"test\", \"train_all\"):\n",
    "                    ok = _dir_has_any_npz(dd)\n",
    "                dir_detail[dn] = bool(ok)\n",
    "                if ok:\n",
    "                    dir_hit += 1\n",
    "\n",
    "            # mtime: consider dir + train file + any prefer file\n",
    "            mt = max(_safe_mtime(d), _safe_mtime(train_fp))\n",
    "            for fn in prefer_files:\n",
    "                mt = max(mt, _safe_mtime(d / fn))\n",
    "\n",
    "            # score design:\n",
    "            # - strong prefer: prefer files hits\n",
    "            # - prefer dirs hits (mask availability)\n",
    "            # - then train rows, then prefer rows\n",
    "            # - then newest mtime\n",
    "            score = 0.0\n",
    "            score += 1e6 * float(pref_hit)            # prefer having test/manifest/summary\n",
    "            score += 2e5 * float(dir_hit)             # prefer having npz dirs\n",
    "            score += 1.0  * float(train_n)             # size of train features\n",
    "            score += 0.05 * float(pref_rows_sum)       # size of prefer files\n",
    "            score += 1e-6 * float(mt)                  # newest\n",
    "\n",
    "            cands.append({\n",
    "                \"dir\": d,\n",
    "                \"root\": root,\n",
    "                \"score\": score,\n",
    "                \"train_file\": str(train_fp),\n",
    "                \"train_rows\": int(train_n),\n",
    "                \"prefer_hits\": int(pref_hit),\n",
    "                \"prefer_rows_sum\": int(pref_rows_sum),\n",
    "                \"prefer_detail\": pref_detail,\n",
    "                \"dir_hits\": int(dir_hit),\n",
    "                \"dir_detail\": dir_detail,\n",
    "                \"mtime\": float(mt),\n",
    "            })\n",
    "\n",
    "    cands.sort(key=lambda x: (-x[\"score\"], x[\"dir\"].name))\n",
    "    return cands[:max_return]\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: detect DINO model dir (offline)\n",
    "# ----------------------------\n",
    "def detect_dino_dir() -> Path:\n",
    "    # try standard Kaggle dinov2 dataset structure\n",
    "    base = Path(\"/kaggle/input/dinov2/pytorch\")\n",
    "    if base.exists():\n",
    "        # prefer large -> giant -> base\n",
    "        for name in [\"large\", \"giant\", \"base\"]:\n",
    "            p = base / name / \"1\"\n",
    "            if p.exists():\n",
    "                return p\n",
    "    # fallback (might be missing; warning only)\n",
    "    return Path(\"/kaggle/input/dinov2/pytorch/large/1\")\n",
    "\n",
    "def detect_dino_cache_cfg(cache_dirs: list) -> Path:\n",
    "    \"\"\"\n",
    "    Find best cfg under cache/dino_v2_*/cfg_*/manifest_train_all.csv\n",
    "    Prefer large then giant then base, but choose whichever has best manifest size.\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    best_key = None  # tuple for sorting\n",
    "\n",
    "    # priority map (lower is better)\n",
    "    prio = {\"dino_v2_large\": 0, \"dino_v2_giant\": 1, \"dino_v2_base\": 2}\n",
    "\n",
    "    for root in cache_dirs:\n",
    "        root = Path(root)\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        for dino_name in [\"dino_v2_large\", \"dino_v2_giant\", \"dino_v2_base\"]:\n",
    "            dino_root = root / dino_name\n",
    "            if not dino_root.exists():\n",
    "                continue\n",
    "\n",
    "            for cfg in dino_root.iterdir():\n",
    "                if not (cfg.is_dir() and cfg.name.startswith(\"cfg_\")):\n",
    "                    continue\n",
    "                mf = cfg / \"manifest_train_all.csv\"\n",
    "                if not mf.exists():\n",
    "                    continue\n",
    "\n",
    "                n = _fast_count_rows_csv(mf)\n",
    "                mt = _safe_mtime(cfg)\n",
    "                key = (prio.get(dino_name, 9), -n, -mt, cfg.name)\n",
    "                # choose best: lowest prio, largest n, newest mt\n",
    "                if best is None or key < best_key:\n",
    "                    best = cfg\n",
    "                    best_key = key\n",
    "\n",
    "    return best  # can be None\n",
    "\n",
    "# ============================================================\n",
    "# 0) Locate roots\n",
    "# ============================================================\n",
    "COMP_ROOT = find_comp_root(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "OUT_DS_ROOT = find_output_dataset_root()\n",
    "OUT_ROOT = resolve_out_root(OUT_DS_ROOT)  # dataset input: .../recodai_luc\n",
    "\n",
    "WORK_OUT_ROOT = Path(\"/kaggle/working/recodai_luc\")\n",
    "\n",
    "# Cache roots: prefer working if exists, plus input\n",
    "CACHE_ROOTS = []\n",
    "if (WORK_OUT_ROOT / \"cache\").exists():\n",
    "    CACHE_ROOTS.append(WORK_OUT_ROOT / \"cache\")\n",
    "if (OUT_ROOT / \"cache\").exists():\n",
    "    CACHE_ROOTS.append(OUT_ROOT / \"cache\")\n",
    "\n",
    "# Artifact roots: prefer working if exists, plus input\n",
    "ART_ROOTS = []\n",
    "if (WORK_OUT_ROOT / \"artifacts\").exists():\n",
    "    ART_ROOTS.append(WORK_OUT_ROOT / \"artifacts\")\n",
    "if (OUT_ROOT / \"artifacts\").exists():\n",
    "    ART_ROOTS.append(OUT_ROOT / \"artifacts\")\n",
    "\n",
    "# choose first existing artifact root\n",
    "ART_DIR = None\n",
    "for a in ART_ROOTS:\n",
    "    if a.exists():\n",
    "        ART_DIR = a\n",
    "        break\n",
    "if ART_DIR is None:\n",
    "    raise FileNotFoundError(\"ART_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n",
    "\n",
    "# cache dirs that exist\n",
    "CACHE_DIRS = [p for p in CACHE_ROOTS if Path(p).exists()]\n",
    "if not CACHE_DIRS:\n",
    "    raise FileNotFoundError(\"CACHE_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) Competition paths (raw images/masks)\n",
    "# ============================================================\n",
    "PATHS = {}\n",
    "PATHS[\"COMP_ROOT\"] = str(COMP_ROOT)\n",
    "PATHS[\"SAMPLE_SUB\"] = str(COMP_ROOT / \"sample_submission.csv\")\n",
    "\n",
    "PATHS[\"TRAIN_IMAGES\"] = str(COMP_ROOT / \"train_images\")\n",
    "PATHS[\"TEST_IMAGES\"]  = str(COMP_ROOT / \"test_images\")\n",
    "PATHS[\"TRAIN_MASKS\"]  = str(COMP_ROOT / \"train_masks\")\n",
    "PATHS[\"SUPP_IMAGES\"]  = str(COMP_ROOT / \"supplemental_images\")\n",
    "PATHS[\"SUPP_MASKS\"]   = str(COMP_ROOT / \"supplemental_masks\")\n",
    "\n",
    "PATHS[\"TRAIN_AUTH_DIR\"] = str(COMP_ROOT / \"train_images\" / \"authentic\")\n",
    "PATHS[\"TRAIN_FORG_DIR\"] = str(COMP_ROOT / \"train_images\" / \"forged\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Output dataset paths (clean artifacts + cache)\n",
    "# ============================================================\n",
    "PATHS[\"OUT_DS_ROOT\"] = str(OUT_DS_ROOT)\n",
    "PATHS[\"OUT_ROOT\"]    = str(OUT_ROOT)\n",
    "\n",
    "PATHS[\"ART_DIR\"]     = str(ART_DIR)\n",
    "PATHS[\"CACHE_DIRS\"]  = [str(x) for x in CACHE_DIRS]  # list\n",
    "\n",
    "# artifacts utama\n",
    "PATHS[\"DF_TRAIN_ALL\"] = str(Path(ART_DIR) / \"df_train_all.parquet\")\n",
    "PATHS[\"DF_TRAIN_CLS\"] = str(Path(ART_DIR) / \"df_train_cls.parquet\")\n",
    "PATHS[\"DF_TRAIN_SEG\"] = str(Path(ART_DIR) / \"df_train_seg.parquet\")\n",
    "PATHS[\"DF_TEST\"]      = str(Path(ART_DIR) / \"df_test.parquet\")\n",
    "\n",
    "PATHS[\"CV_CASE_FOLDS\"]   = str(Path(ART_DIR) / \"cv_case_folds.csv\")\n",
    "PATHS[\"CV_SAMPLE_FOLDS\"] = str(Path(ART_DIR) / \"cv_sample_folds.csv\")\n",
    "\n",
    "PATHS[\"IMG_PROFILE_TRAIN\"] = str(Path(ART_DIR) / \"image_profile_train.parquet\")\n",
    "PATHS[\"IMG_PROFILE_TEST\"]  = str(Path(ART_DIR) / \"image_profile_test.parquet\")\n",
    "PATHS[\"MASK_PROFILE\"]      = str(Path(ART_DIR) / \"mask_profile.parquet\")\n",
    "PATHS[\"CASE_SUMMARY\"]      = str(Path(ART_DIR) / \"case_summary.parquet\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Select best MATCH/PRED CFG dirs automatically (TOP-K + scoring)\n",
    "# ============================================================\n",
    "# MATCH candidates: must have match_features_train_all.csv\n",
    "MATCH_CFG_INFO = pick_top_cfgs(\n",
    "    CACHE_DIRS,\n",
    "    prefixes=[\"match_base_cfg_\"],\n",
    "    required_train_file=\"match_features_train_all.csv\",\n",
    "    prefer_files=[\n",
    "        \"match_features_test.csv\",\n",
    "        \"manifest_match_test.csv\",\n",
    "        \"manifest_match_train_all.csv\",\n",
    "    ],\n",
    "    extra_prefer_dirs=[],  # biasanya match cfg tidak punya npz\n",
    "    max_return=max(1, TOPK_MATCH_CFGS),\n",
    ")\n",
    "\n",
    "if not MATCH_CFG_INFO:\n",
    "    raise FileNotFoundError(\"Tidak menemukan match cfg folder yang valid (match_features_train_all.csv).\")\n",
    "\n",
    "MATCH_CFG_DIRS = [x[\"dir\"] for x in MATCH_CFG_INFO]\n",
    "MATCH_CFG_DIR  = MATCH_CFG_DIRS[0]  # primary (kompatibilitas stage lanjut)\n",
    "\n",
    "# PRED candidates: must have pred_features_train_all.csv\n",
    "# Prefer: pred_features_test, manifests, summary, and availability of npz dirs\n",
    "PRED_CFG_INFO = pick_top_cfgs(\n",
    "    CACHE_DIRS,\n",
    "    prefixes=[\"pred_base\"],\n",
    "    required_train_file=\"pred_features_train_all.csv\",\n",
    "    prefer_files=[\n",
    "        \"pred_features_test.csv\",\n",
    "        \"manifest_pred_test.csv\",\n",
    "        \"manifest_pred_train_all.csv\",\n",
    "        \"pred_summary.json\",\n",
    "    ],\n",
    "    extra_prefer_dirs=[\"test\", \"train_all\"],  # prefer having npz predictions\n",
    "    max_return=max(1, TOPK_PRED_CFGS),\n",
    ")\n",
    "\n",
    "if not PRED_CFG_INFO:\n",
    "    raise FileNotFoundError(\"Tidak menemukan pred cfg folder yang valid (pred_features_train_all.csv).\")\n",
    "\n",
    "PRED_CFG_DIRS = [x[\"dir\"] for x in PRED_CFG_INFO]\n",
    "PRED_CFG_DIR  = PRED_CFG_DIRS[0]  # primary (kompatibilitas stage lanjut)\n",
    "\n",
    "# DINO cache cfg (opsional)\n",
    "DINO_CFG_DIR = detect_dino_cache_cfg(CACHE_DIRS)\n",
    "\n",
    "# simpan cfg dir ke PATHS\n",
    "PATHS[\"MATCH_CFG_DIR\"]    = str(MATCH_CFG_DIR)\n",
    "PATHS[\"PRED_CFG_DIR\"]     = str(PRED_CFG_DIR)\n",
    "PATHS[\"DINO_CFG_DIR\"]     = str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\"\n",
    "\n",
    "# multi-cfg (baru, opsional)\n",
    "PATHS[\"MATCH_CFG_DIRS\"]   = [str(x) for x in MATCH_CFG_DIRS]\n",
    "PATHS[\"PRED_CFG_DIRS\"]    = [str(x) for x in PRED_CFG_DIRS]\n",
    "\n",
    "# feature paths dari cfg primary\n",
    "PATHS[\"MATCH_FEAT_TRAIN\"] = str(MATCH_CFG_DIR / \"match_features_train_all.csv\")\n",
    "PATHS[\"MATCH_FEAT_TEST\"]  = str(MATCH_CFG_DIR / \"match_features_test.csv\")\n",
    "\n",
    "PATHS[\"PRED_FEAT_TRAIN\"]  = str(PRED_CFG_DIR / \"pred_features_train_all.csv\")\n",
    "PATHS[\"PRED_FEAT_TEST\"]   = str(PRED_CFG_DIR / \"pred_features_test.csv\")  # bisa missing (warning)\n",
    "\n",
    "# pred manifests (sering kepakai untuk infer/submission)\n",
    "PATHS[\"PRED_MAN_TRAIN\"] = str(PRED_CFG_DIR / \"manifest_pred_train_all.csv\")\n",
    "PATHS[\"PRED_MAN_TEST\"]  = str(PRED_CFG_DIR / \"manifest_pred_test.csv\")\n",
    "PATHS[\"PRED_SUMMARY\"]   = str(PRED_CFG_DIR / \"pred_summary.json\")\n",
    "\n",
    "# match manifests (kalau ada)\n",
    "PATHS[\"MATCH_MAN_TRAIN\"] = str(MATCH_CFG_DIR / \"manifest_match_train_all.csv\")\n",
    "PATHS[\"MATCH_MAN_TEST\"]  = str(MATCH_CFG_DIR / \"manifest_match_test.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) DINO model dir (offline)\n",
    "# ============================================================\n",
    "DINO_DIR = detect_dino_dir()\n",
    "PATHS[\"DINO_DIR\"] = str(DINO_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 5) Sanity checks (wajib ada untuk training)\n",
    "# ============================================================\n",
    "must_exist = [\n",
    "    (\"sample_submission.csv\", PATHS[\"SAMPLE_SUB\"]),\n",
    "    (\"df_train_all.parquet\",  PATHS[\"DF_TRAIN_ALL\"]),\n",
    "    (\"cv_case_folds.csv\",     PATHS[\"CV_CASE_FOLDS\"]),\n",
    "    (\"match_features_train_all.csv\", PATHS[\"MATCH_FEAT_TRAIN\"]),\n",
    "    (\"pred_features_train_all.csv\",  PATHS[\"PRED_FEAT_TRAIN\"]),\n",
    "]\n",
    "missing = [name for name, p in must_exist if not Path(p).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\"Missing required files: \" + \", \".join(missing))\n",
    "\n",
    "# opsional tapi penting untuk inference: test feature files\n",
    "opt_warn = []\n",
    "if not Path(PATHS[\"MATCH_FEAT_TEST\"]).exists():\n",
    "    opt_warn.append(\"match_features_test.csv (MATCH_FEAT_TEST)\")\n",
    "if not Path(PATHS[\"PRED_FEAT_TEST\"]).exists():\n",
    "    opt_warn.append(\"pred_features_test.csv (PRED_FEAT_TEST)\")\n",
    "if opt_warn:\n",
    "    print(\"WARNING: File opsional untuk inference belum ada:\")\n",
    "    for w in opt_warn:\n",
    "        print(\" -\", w)\n",
    "    print(\"Catatan: training masih aman (pakai *_train_all). Untuk inference gate ke test, file ini biasanya dibutuhkan.\")\n",
    "\n",
    "# DINO model dir opsional (warning saja)\n",
    "if not Path(PATHS[\"DINO_DIR\"]).exists():\n",
    "    print(f\"WARNING: DINO dir tidak ditemukan: {PATHS['DINO_DIR']}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) Print summary + export helpers\n",
    "# ============================================================\n",
    "SELECTED = {\n",
    "    \"ART_DIR\": str(ART_DIR),\n",
    "    \"CACHE_DIRS\": [str(x) for x in CACHE_DIRS],\n",
    "    \"MATCH_CFG_DIR\": str(MATCH_CFG_DIR),\n",
    "    \"PRED_CFG_DIR\": str(PRED_CFG_DIR),\n",
    "    \"DINO_CFG_DIR\": str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\",\n",
    "    \"DINO_DIR\": str(DINO_DIR),\n",
    "    \"TOPK_MATCH_CFGS\": TOPK_MATCH_CFGS,\n",
    "    \"TOPK_PRED_CFGS\": TOPK_PRED_CFGS,\n",
    "}\n",
    "\n",
    "# Training plan: disiapkan untuk upgrade lanjutan (stacking + calibration + threshold tune)\n",
    "TRAIN_PLAN = {\n",
    "    \"seed\": 2025,\n",
    "    \"group_col\": \"case_id\",\n",
    "    \"target_col\": \"y_forged\",\n",
    "    \"n_folds\": 5,\n",
    "    \"use_calibration\": True,\n",
    "    \"calibration\": \"isotonic\",  # robust untuk tabular probs (OOF)\n",
    "    \"tune_threshold_on_oof\": True,\n",
    "    \"multi_cfg\": {\n",
    "        \"enabled\": True,\n",
    "        \"topk_match_cfgs\": TOPK_MATCH_CFGS,\n",
    "        \"topk_pred_cfgs\": TOPK_PRED_CFGS,\n",
    "        \"primary_match\": MATCH_CFG_DIR.name,\n",
    "        \"primary_pred\": PRED_CFG_DIR.name,\n",
    "    },\n",
    "}\n",
    "\n",
    "def _print_top_cfg_table(title: str, info_list: list, max_rows: int = 5):\n",
    "    print(title)\n",
    "    if not info_list:\n",
    "        print(\"  (none)\")\n",
    "        return\n",
    "    show = info_list[:max_rows]\n",
    "    for i, x in enumerate(show, 1):\n",
    "        d = x[\"dir\"]\n",
    "        ph = x[\"prefer_hits\"]\n",
    "        dh = x[\"dir_hits\"]\n",
    "        tr = x[\"train_rows\"]\n",
    "        print(f\"  #{i:02d} {d.name} | score={x['score']:.1f} | train_rows={tr} | prefer_hits={ph} | dir_hits={dh}\")\n",
    "\n",
    "print(\"OK — Roots\")\n",
    "print(\"  COMP_ROOT   :\", COMP_ROOT)\n",
    "print(\"  OUT_DS_ROOT :\", OUT_DS_ROOT)\n",
    "print(\"  OUT_ROOT    :\", OUT_ROOT)\n",
    "print(\"  ART_DIR(use):\", ART_DIR)\n",
    "print(\"  CACHE_DIRS  :\", [str(x) for x in CACHE_DIRS])\n",
    "\n",
    "print(\"\\nOK — Selected CFG (PRIMARY)\")\n",
    "print(\"  MATCH_CFG_DIR:\", MATCH_CFG_DIR.name)\n",
    "print(\"  PRED_CFG_DIR :\", PRED_CFG_DIR.name)\n",
    "print(\"  DINO_CFG_DIR :\", (DINO_CFG_DIR.name if DINO_CFG_DIR else \"(not found / optional)\"))\n",
    "\n",
    "print(\"\\nOK — Top candidates (for MULTI-CFG training)\")\n",
    "_print_top_cfg_table(\"MATCH CFG TOP:\", MATCH_CFG_INFO, max_rows=min(5, len(MATCH_CFG_INFO)))\n",
    "_print_top_cfg_table(\"PRED  CFG TOP:\", PRED_CFG_INFO,  max_rows=min(5, len(PRED_CFG_INFO)))\n",
    "\n",
    "print(\"\\nOK — Key files (train)\")\n",
    "for k in [\"DF_TRAIN_ALL\", \"CV_CASE_FOLDS\", \"MATCH_FEAT_TRAIN\", \"PRED_FEAT_TRAIN\", \"IMG_PROFILE_TRAIN\"]:\n",
    "    p = Path(PATHS[k])\n",
    "    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing/optional)'}\")\n",
    "\n",
    "print(\"\\nOK — Key files (test/infer, optional)\")\n",
    "for k in [\"MATCH_FEAT_TEST\", \"PRED_FEAT_TEST\", \"PRED_MAN_TEST\", \"PRED_SUMMARY\"]:\n",
    "    p = Path(PATHS[k])\n",
    "    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing)'}\")\n",
    "\n",
    "print(\"\\nOK — DINO model dir\")\n",
    "print(\"  DINO_DIR:\", DINO_DIR, \"(exists)\" if DINO_DIR.exists() else \"(missing)\")\n",
    "\n",
    "# export globals (kompatibilitas)\n",
    "globals().update({\n",
    "    \"MATCH_CFG_DIR\": MATCH_CFG_DIR,\n",
    "    \"PRED_CFG_DIR\": PRED_CFG_DIR,\n",
    "    \"DINO_CFG_DIR\": DINO_CFG_DIR,\n",
    "    \"CACHE_ROOTS\": [Path(x) for x in CACHE_DIRS],\n",
    "    \"SELECTED\": SELECTED,\n",
    "    \"TRAIN_PLAN\": TRAIN_PLAN,\n",
    "\n",
    "    # extra (multi-cfg)\n",
    "    \"MATCH_CFG_DIRS\": MATCH_CFG_DIRS,\n",
    "    \"PRED_CFG_DIRS\": PRED_CFG_DIRS,\n",
    "    \"MATCH_CFG_INFO\": MATCH_CFG_INFO,\n",
    "    \"PRED_CFG_INFO\": PRED_CFG_INFO,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4b546",
   "metadata": {
    "papermill": {
     "duration": 0.005672,
     "end_time": "2026-01-08T11:34:40.398786",
     "exception": false,
     "start_time": "2026-01-08T11:34:40.393114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Training Table (X, y, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ab8b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T11:34:40.412355Z",
     "iopub.status.busy": "2026-01-08T11:34:40.411900Z",
     "iopub.status.idle": "2026-01-08T11:34:41.032675Z",
     "shell.execute_reply": "2026-01-08T11:34:41.031792Z"
    },
    "papermill": {
     "duration": 0.629972,
     "end_time": "2026-01-08T11:34:41.034383",
     "exception": false,
     "start_time": "2026-01-08T11:34:40.404411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using (PRIMARY):\n",
      "  DF_TRAIN_ALL     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet\n",
      "  CV_CASE_FOLDS    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv\n",
      "  PRED_FEAT_TRAIN  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv\n",
      "  MATCH_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv (optional)\n",
      "  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet (optional)\n",
      "\n",
      "OK — Training table built\n",
      "  df_train_tabular: (5176, 89)\n",
      "  X_train: (5176, 84) | y pos%: 54.07650695517774\n",
      "  folds: 5 unique folds\n",
      "  feature_cols: 84\n",
      "  dropped_constant_features: 6\n",
      "  variant_dummies: 3\n",
      "\n",
      "Feature head: ['has_peak', 'peak_ratio', 'best_weight', 'best_count', 'best_mean_sim', 'n_pairs_thr', 'n_pairs_mnn', 'best_inlier_ratio', 'best_weight_frac', 'inlier_ratio', 'pair_count', 'uniq_src', 'uniq_dst', 'mean_sim', 'thr_used', 'cnt_thr_used', 'relaxed_used', 'min_pairs_used', 'area_frac', 'n_comp', 'largest_comp', 'grid_area_frac', 'grid_thr_q_used', 'grid_cnt_q_used', 'n_peaks']\n",
      "Feature tail: ['sim_x_count', 'peak_x_sim', 'haspeak_x_sim', 'area_x_sim', 'area_x_count', 'mask_grid_ratio', 'mnn_ratio', 'pairs_per_cell', 'inlier_x_pairs', 'log1p_pairs_thr', 'log1p_best_count', 'log1p_area_frac', 'v_auth', 'v_forg', 'v_supp']\n",
      "\n",
      "Saved -> /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      "Saved -> /kaggle/working/recodai_luc_gate_artifacts/feature_schema.json\n",
      "Saved -> /kaggle/working/recodai_luc_gate_artifacts/df_train_tabular.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2 — Build Training Table (X, y, folds) — REVISI FULL v3.0 (MAX-UPGRADE, robust, anti-error)\n",
    "#\n",
    "# Upgrade utama v3.0 (sesuai strategi naik score):\n",
    "# - MULTI-CFG support (dari STAGE 0 v3.0):\n",
    "#     * Primary CFG: load FULL pred_features + (opsional) FULL match_features\n",
    "#     * Extra CFGs: load CORE columns saja + buat agregasi row-wise (min/max/mean) -> stabil & kuat\n",
    "# - Label hygiene:\n",
    "#     * Drop unlabeled (y not in {0,1}) -> menghindari noise supplemental y=-1\n",
    "# - Robust parquet read (safe columns)\n",
    "# - Feature engineering lebih kuat + aman:\n",
    "#     * missing indicators\n",
    "#     * clipping caps\n",
    "#     * logabs transforms\n",
    "#     * interactions (lebih lengkap)\n",
    "# - Output tetap kompatibel:\n",
    "#   globals: df_train_tabular, FEATURE_COLS, X_train, y_train, folds\n",
    "#   save: feature_cols.json, feature_schema.json, df_train_tabular.parquet\n",
    "# ============================================================\n",
    "\n",
    "import os, json, gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require PATHS\n",
    "# ----------------------------\n",
    "if \"PATHS\" not in globals() or not isinstance(PATHS, dict):\n",
    "    raise RuntimeError(\"Missing PATHS. Jalankan dulu STAGE 0 — Set Paths & Select Config (CFG).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Feature Engineering Config\n",
    "# ----------------------------\n",
    "FE_CFG = {\n",
    "    # sources\n",
    "    \"use_match_features\": True,\n",
    "    \"use_image_profile\": True,\n",
    "\n",
    "    # Multi-CFG (requires STAGE0 v3 which sets PATHS['PRED_CFG_DIRS'], PATHS['MATCH_CFG_DIRS'])\n",
    "    \"multi_cfg_enabled\": True,\n",
    "    \"multi_cfg_max_pred\": int(os.environ.get(\"MULTI_CFG_MAX_PRED\", \"6\")),   # pakai top-k pred cfg (primary + extra)\n",
    "    \"multi_cfg_max_match\": int(os.environ.get(\"MULTI_CFG_MAX_MATCH\", \"3\")), # pakai top-k match cfg (primary + extra)\n",
    "    \"multi_cfg_extra_mode\": \"core+agg\",  # \"core\" | \"core+agg\" (recommended)\n",
    "\n",
    "    # variant encoding\n",
    "    \"encode_variant_onehot\": True,\n",
    "    \"variant_min_count\": 1,\n",
    "\n",
    "    # transforms\n",
    "    \"add_log_features\": True,\n",
    "    \"add_sqrt_features\": False,\n",
    "    \"add_interactions\": True,\n",
    "    \"add_missing_indicators\": True,\n",
    "\n",
    "    # outlier control\n",
    "    \"clip_by_quantile\": True,\n",
    "    \"clip_q\": 0.999,\n",
    "    \"clip_max_fallback\": 1e9,\n",
    "\n",
    "    # fill\n",
    "    \"fillna_value\": 0.0,\n",
    "\n",
    "    # prune\n",
    "    \"drop_constant_features\": True,\n",
    "\n",
    "    # dtype\n",
    "    \"cast_float32\": True,\n",
    "\n",
    "    # label handling\n",
    "    \"drop_unlabeled\": True,          # drop y not in {0,1}\n",
    "    \"positive_value\": 1,             # keep standard\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Prefer WORKING features if exist (kalau regen di /kaggle/working)\n",
    "# ----------------------------\n",
    "def _prefer_existing(*paths):\n",
    "    for p in paths:\n",
    "        if p is None:\n",
    "            continue\n",
    "        p = Path(str(p))\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return Path(str(paths[0])) if paths and paths[0] is not None else Path(\"\")\n",
    "\n",
    "def _to_str_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).replace({\"nan\": \"\", \"None\": \"\"})\n",
    "\n",
    "def _ensure_uid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"uid\" not in df.columns:\n",
    "        for alt in [\"sample_id\", \"id\", \"key\"]:\n",
    "            if alt in df.columns:\n",
    "                df = df.rename(columns={alt: \"uid\"})\n",
    "                break\n",
    "    if \"uid\" not in df.columns:\n",
    "        raise ValueError(\"Cannot find uid column. Expected 'uid' or 'sample_id'.\")\n",
    "    df[\"uid\"] = _to_str_series(df[\"uid\"])\n",
    "    return df\n",
    "\n",
    "def _parse_case_variant_from_uid(uid_s: pd.Series) -> pd.DataFrame:\n",
    "    uid = _to_str_series(uid_s)\n",
    "    case1 = uid.str.extract(r\"^(\\d+)__\")[0]\n",
    "    var1  = uid.str.extract(r\"__(.+)$\")[0]\n",
    "    case2 = uid.str.extract(r\"^(\\d+)_\")[0]\n",
    "    var2  = uid.str.extract(r\"_(\\w+)$\")[0]\n",
    "    case = case1.fillna(case2)\n",
    "    var  = var1.fillna(var2).fillna(\"unk\")\n",
    "    return pd.DataFrame({\"case_id\": case, \"variant\": var})\n",
    "\n",
    "def _ensure_case_variant(df: pd.DataFrame, df_base_map: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    df = _ensure_uid(df)\n",
    "\n",
    "    if \"case_id\" in df.columns:\n",
    "        df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n",
    "    if \"variant\" in df.columns:\n",
    "        df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n",
    "\n",
    "    # merge from base map preferred\n",
    "    if df_base_map is not None and {\"uid\", \"case_id\", \"variant\"}.issubset(df_base_map.columns):\n",
    "        need_merge = (\"case_id\" not in df.columns) or (\"variant\" not in df.columns) or df[\"case_id\"].isna().any()\n",
    "        if need_merge:\n",
    "            df = df.merge(df_base_map[[\"uid\", \"case_id\", \"variant\"]], on=\"uid\", how=\"left\", suffixes=(\"\", \"_base\"))\n",
    "            if \"case_id_base\" in df.columns:\n",
    "                df[\"case_id\"] = df[\"case_id\"].fillna(df[\"case_id_base\"])\n",
    "                df = df.drop(columns=[\"case_id_base\"])\n",
    "            if \"variant_base\" in df.columns:\n",
    "                df[\"variant\"] = df[\"variant\"].where(df[\"variant\"].astype(str).str.len() > 0, df[\"variant_base\"])\n",
    "                df = df.drop(columns=[\"variant_base\"])\n",
    "\n",
    "    if (\"case_id\" not in df.columns) or (\"variant\" not in df.columns) or df[\"case_id\"].isna().any():\n",
    "        pv = _parse_case_variant_from_uid(df[\"uid\"])\n",
    "        if \"case_id\" not in df.columns:\n",
    "            df[\"case_id\"] = pd.to_numeric(pv[\"case_id\"], errors=\"coerce\")\n",
    "        else:\n",
    "            df[\"case_id\"] = df[\"case_id\"].fillna(pd.to_numeric(pv[\"case_id\"], errors=\"coerce\"))\n",
    "        if \"variant\" not in df.columns:\n",
    "            df[\"variant\"] = pv[\"variant\"]\n",
    "        else:\n",
    "            v = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n",
    "            df[\"variant\"] = v.where(v.str.len() > 0, pv[\"variant\"])\n",
    "\n",
    "    df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n",
    "    return df\n",
    "\n",
    "def _pick_label_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return \"\"\n",
    "\n",
    "# parquet safe columns\n",
    "def _read_parquet_cols_safe(path: Path, desired_cols: list) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    try:\n",
    "        import pyarrow.parquet as pq\n",
    "        cols = pq.ParquetFile(path).schema.names\n",
    "        use = [c for c in desired_cols if c in cols]\n",
    "        if not use:\n",
    "            return pd.read_parquet(path)\n",
    "        return pd.read_parquet(path, columns=use)\n",
    "    except Exception:\n",
    "        # fallback: read full then subset if possible\n",
    "        df = pd.read_parquet(path)\n",
    "        use = [c for c in desired_cols if c in df.columns]\n",
    "        return df[use].copy() if use else df\n",
    "\n",
    "# ----------------------------\n",
    "# Resolve primary paths\n",
    "# ----------------------------\n",
    "match_cfg_name = Path(PATHS.get(\"MATCH_CFG_DIR\", \"\")).name if PATHS.get(\"MATCH_CFG_DIR\") else \"\"\n",
    "pred_cfg_name  = Path(PATHS.get(\"PRED_CFG_DIR\", \"\")).name  if PATHS.get(\"PRED_CFG_DIR\") else \"\"\n",
    "\n",
    "WORK_CACHE_ROOT = Path(\"/kaggle/working/recodai_luc/cache\")\n",
    "match_feat_work = (WORK_CACHE_ROOT / match_cfg_name / \"match_features_train_all.csv\") if match_cfg_name else None\n",
    "pred_feat_work  = (WORK_CACHE_ROOT / pred_cfg_name  / \"pred_features_train_all.csv\")  if pred_cfg_name  else None\n",
    "\n",
    "PRED_FEAT_TRAIN  = _prefer_existing(pred_feat_work,  PATHS.get(\"PRED_FEAT_TRAIN\", \"\"))\n",
    "MATCH_FEAT_TRAIN = _prefer_existing(match_feat_work, PATHS.get(\"MATCH_FEAT_TRAIN\", \"\"))\n",
    "\n",
    "DF_TRAIN_ALL      = Path(PATHS[\"DF_TRAIN_ALL\"])\n",
    "CV_CASE_FOLDS     = Path(PATHS[\"CV_CASE_FOLDS\"])\n",
    "IMG_PROFILE_TRAIN = Path(PATHS.get(\"IMG_PROFILE_TRAIN\", \"\"))\n",
    "\n",
    "for need_name, need_path in [\n",
    "    (\"df_train_all.parquet\", DF_TRAIN_ALL),\n",
    "    (\"cv_case_folds.csv\", CV_CASE_FOLDS),\n",
    "    (\"pred_features_train_all.csv\", PRED_FEAT_TRAIN),\n",
    "]:\n",
    "    if not Path(need_path).exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {need_name} -> {need_path}\")\n",
    "\n",
    "print(\"Using (PRIMARY):\")\n",
    "print(\"  DF_TRAIN_ALL     :\", DF_TRAIN_ALL)\n",
    "print(\"  CV_CASE_FOLDS    :\", CV_CASE_FOLDS)\n",
    "print(\"  PRED_FEAT_TRAIN  :\", PRED_FEAT_TRAIN)\n",
    "print(\"  MATCH_FEAT_TRAIN :\", MATCH_FEAT_TRAIN, \"(optional)\" if Path(MATCH_FEAT_TRAIN).exists() else \"(missing/skip)\")\n",
    "print(\"  IMG_PROFILE_TRAIN:\", IMG_PROFILE_TRAIN, \"(optional)\" if Path(IMG_PROFILE_TRAIN).exists() else \"(missing/skip)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load minimal inputs\n",
    "# ----------------------------\n",
    "# df_train_all: ambil kolom minimal (safe)\n",
    "base_cols_want = [\"sample_id\", \"uid\", \"case_id\", \"variant\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]\n",
    "df_base = _read_parquet_cols_safe(DF_TRAIN_ALL, base_cols_want)\n",
    "\n",
    "df_cv   = pd.read_csv(CV_CASE_FOLDS)\n",
    "\n",
    "# primary pred features (full)\n",
    "df_pred_primary = pd.read_csv(PRED_FEAT_TRAIN, low_memory=False)\n",
    "\n",
    "# primary match features (optional, full)\n",
    "df_match_primary = None\n",
    "if FE_CFG[\"use_match_features\"] and Path(MATCH_FEAT_TRAIN).exists():\n",
    "    try:\n",
    "        df_match_primary = pd.read_csv(MATCH_FEAT_TRAIN, low_memory=False)\n",
    "    except Exception:\n",
    "        df_match_primary = None\n",
    "\n",
    "# image profile (optional)\n",
    "df_prof = None\n",
    "if FE_CFG[\"use_image_profile\"] and Path(IMG_PROFILE_TRAIN).exists():\n",
    "    try:\n",
    "        df_prof = pd.read_parquet(IMG_PROFILE_TRAIN)\n",
    "    except Exception:\n",
    "        df_prof = None\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Prepare base mapping from df_train_all\n",
    "# ----------------------------\n",
    "df_base = df_base.copy()\n",
    "if \"uid\" not in df_base.columns:\n",
    "    if \"sample_id\" in df_base.columns:\n",
    "        df_base = df_base.rename(columns={\"sample_id\": \"uid\"})\n",
    "    elif (\"case_id\" in df_base.columns and \"variant\" in df_base.columns):\n",
    "        df_base[\"uid\"] = _to_str_series(df_base[\"case_id\"]) + \"__\" + _to_str_series(df_base[\"variant\"])\n",
    "\n",
    "df_base = _ensure_uid(df_base)\n",
    "\n",
    "if \"case_id\" in df_base.columns:\n",
    "    df_base[\"case_id\"] = pd.to_numeric(df_base[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"variant\" in df_base.columns:\n",
    "    df_base[\"variant\"] = df_base[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n",
    "\n",
    "label_col = _pick_label_col(df_base)\n",
    "if not label_col:\n",
    "    raise ValueError(\"Cannot find label column in df_train_all (y_forged/has_mask/is_forged/forged).\")\n",
    "\n",
    "df_base_map = df_base.drop_duplicates(subset=[\"uid\"], keep=\"first\").copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Prepare folds\n",
    "# ----------------------------\n",
    "if \"case_id\" not in df_cv.columns or \"fold\" not in df_cv.columns:\n",
    "    raise ValueError(\"cv_case_folds.csv must contain columns: case_id, fold\")\n",
    "\n",
    "df_cv = df_cv[[\"case_id\", \"fold\"]].copy()\n",
    "df_cv[\"case_id\"] = pd.to_numeric(df_cv[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_cv[\"fold\"]    = pd.to_numeric(df_cv[\"fold\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_cv = df_cv.dropna().astype({\"case_id\": int, \"fold\": int}).drop_duplicates(\"case_id\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Start from PRIMARY pred features (1 row per uid)\n",
    "# ----------------------------\n",
    "df_pred_primary = _ensure_case_variant(df_pred_primary, df_base_map=df_base_map)\n",
    "if df_pred_primary[\"uid\"].duplicated().any():\n",
    "    df_pred_primary = df_pred_primary.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "df_train = df_pred_primary.copy()\n",
    "\n",
    "# attach label from base map (sumber paling aman)\n",
    "df_train = df_train.merge(\n",
    "    df_base_map[[\"uid\", label_col]].rename(columns={label_col: \"y\"}),\n",
    "    on=\"uid\", how=\"left\"\n",
    ")\n",
    "\n",
    "if df_train[\"y\"].isna().any():\n",
    "    miss = int(df_train[\"y\"].isna().sum())\n",
    "    raise ValueError(f\"Label merge produced NaN in y: {miss} rows. Check df_train_all vs pred_features alignment.\")\n",
    "\n",
    "df_train[\"y\"] = pd.to_numeric(df_train[\"y\"], errors=\"coerce\")\n",
    "\n",
    "# drop unlabeled y not in {0,1} (recommended)\n",
    "if FE_CFG[\"drop_unlabeled\"]:\n",
    "    before = len(df_train)\n",
    "    df_train = df_train[df_train[\"y\"].isin([0, 1])].copy()\n",
    "    after = len(df_train)\n",
    "    if before != after:\n",
    "        print(f\"NOTE: Dropped unlabeled rows (y not in {{0,1}}): {before-after} rows\")\n",
    "\n",
    "df_train[\"y\"] = df_train[\"y\"].astype(int)\n",
    "\n",
    "# attach folds by case_id\n",
    "df_train = df_train.drop(columns=[\"fold\"], errors=\"ignore\").merge(df_cv, on=\"case_id\", how=\"left\")\n",
    "if df_train[\"fold\"].isna().any():\n",
    "    miss = int(df_train[\"fold\"].isna().sum())\n",
    "    raise ValueError(f\"Missing fold after merging cv_case_folds.csv: {miss} rows.\")\n",
    "df_train[\"fold\"] = df_train[\"fold\"].astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Optional merge PRIMARY match features (new cols only)\n",
    "# ----------------------------\n",
    "if df_match_primary is not None:\n",
    "    dfm = _ensure_case_variant(df_match_primary, df_base_map=df_base_map)\n",
    "    if dfm[\"uid\"].duplicated().any():\n",
    "        dfm = dfm.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    base_cols = set(df_train.columns)\n",
    "    new_cols = [c for c in dfm.columns if c not in base_cols and c not in [\"case_id\", \"variant\"]]\n",
    "    if new_cols:\n",
    "        df_train = df_train.merge(dfm[[\"uid\"] + new_cols], on=\"uid\", how=\"left\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) MULTI-CFG: load EXTRA core features from additional CFG dirs (PRED + MATCH)\n",
    "# ----------------------------\n",
    "def _short_cfg_tag(cfg_dir: Path, idx: int) -> str:\n",
    "    # tag pendek agar kolom tidak kepanjangan\n",
    "    nm = cfg_dir.name\n",
    "    # ambil hash kalau ada\n",
    "    m = re.search(r\"(cfg_[0-9a-f]{6,})\", nm)\n",
    "    tag = m.group(1) if m else nm\n",
    "    tag = tag.replace(\"pred_base_\", \"p_\").replace(\"match_base_cfg_\", \"m_\")\n",
    "    tag = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", tag)\n",
    "    return f\"{idx:02d}_{tag[:24]}\"\n",
    "\n",
    "# core cols yang paling berpengaruh untuk gate (stabil across cfg)\n",
    "CORE_PRED_COLS = [\n",
    "    \"area_frac\", \"grid_area_frac\", \"log_pred_area\", \"pred_area_frac\",\n",
    "    \"best_count\", \"best_mean_sim\", \"peak_ratio\", \"best_weight\", \"best_weight_frac\",\n",
    "    \"inlier_ratio\", \"n_pairs_thr\", \"n_pairs_mnn\", \"overmask_tighten_steps\",\n",
    "    \"grid_h\", \"grid_w\", \"has_peak\",\n",
    "]\n",
    "CORE_MATCH_COLS = [\n",
    "    \"best_count\", \"best_mean_sim\", \"peak_ratio\", \"best_weight\", \"best_weight_frac\",\n",
    "    \"inlier_ratio\", \"n_pairs_thr\", \"n_pairs_mnn\", \"grid_h\", \"grid_w\", \"has_peak\",\n",
    "]\n",
    "\n",
    "def _load_csv_core(csv_path: Path, core_cols: list) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    df = _ensure_uid(df)\n",
    "    keep = [\"uid\"] + [c for c in core_cols if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "    return df\n",
    "\n",
    "# get cfg dirs list from PATHS (stage0 v3)\n",
    "pred_cfg_dirs = [Path(x) for x in PATHS.get(\"PRED_CFG_DIRS\", [])] if FE_CFG[\"multi_cfg_enabled\"] else []\n",
    "match_cfg_dirs = [Path(x) for x in PATHS.get(\"MATCH_CFG_DIRS\", [])] if FE_CFG[\"multi_cfg_enabled\"] else []\n",
    "\n",
    "# ensure primary is first (and exists)\n",
    "if pred_cfg_dirs:\n",
    "    # keep only existing dirs\n",
    "    pred_cfg_dirs = [d for d in pred_cfg_dirs if d.exists()]\n",
    "    # cap to max\n",
    "    pred_cfg_dirs = pred_cfg_dirs[:max(1, FE_CFG[\"multi_cfg_max_pred\"])]\n",
    "else:\n",
    "    pred_cfg_dirs = [Path(PATHS[\"PRED_CFG_DIR\"])]\n",
    "\n",
    "if match_cfg_dirs:\n",
    "    match_cfg_dirs = [d for d in match_cfg_dirs if d.exists()]\n",
    "    match_cfg_dirs = match_cfg_dirs[:max(1, FE_CFG[\"multi_cfg_max_match\"])]\n",
    "else:\n",
    "    match_cfg_dirs = [Path(PATHS[\"MATCH_CFG_DIR\"])] if FE_CFG[\"use_match_features\"] else []\n",
    "\n",
    "# EXTRA PRED cfgs (exclude primary already loaded)\n",
    "extra_pred_cfgs = [d for d in pred_cfg_dirs if str(d) != str(Path(PATHS[\"PRED_CFG_DIR\"]))]\n",
    "\n",
    "pred_core_cols_added = []\n",
    "pred_core_matrix_cols = {}  # base_name -> list of suffixed cols\n",
    "\n",
    "for i, cfg_dir in enumerate(extra_pred_cfgs, 1):\n",
    "    fp = cfg_dir / \"pred_features_train_all.csv\"\n",
    "    if not fp.exists():\n",
    "        continue\n",
    "    try:\n",
    "        tag = _short_cfg_tag(cfg_dir, i)\n",
    "        df_extra = _load_csv_core(fp, CORE_PRED_COLS)\n",
    "        # ensure joinable to df_train uids\n",
    "        df_extra = _ensure_case_variant(df_extra, df_base_map=df_base_map)\n",
    "        # rename core columns with suffix tag\n",
    "        ren = {}\n",
    "        for c in df_extra.columns:\n",
    "            if c == \"uid\":\n",
    "                continue\n",
    "            ren[c] = f\"{c}__{tag}\"\n",
    "            pred_core_matrix_cols.setdefault(c, []).append(ren[c])\n",
    "        df_extra = df_extra.rename(columns=ren)\n",
    "        # merge\n",
    "        df_train = df_train.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n",
    "        pred_core_cols_added.extend(list(ren.values()))\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: skip extra pred cfg {cfg_dir.name} due to error: {e}\")\n",
    "\n",
    "# Aggregate across CFGs for core pred metrics (recommended)\n",
    "# include primary columns if exist\n",
    "if FE_CFG[\"multi_cfg_extra_mode\"] == \"core+agg\":\n",
    "    for base_name, cols_suff in pred_core_matrix_cols.items():\n",
    "        # build list: [primary_col(if exists)] + extra cols\n",
    "        cols_all = []\n",
    "        if base_name in df_train.columns and pd.api.types.is_numeric_dtype(df_train[base_name]):\n",
    "            cols_all.append(base_name)\n",
    "        cols_all.extend([c for c in cols_suff if c in df_train.columns])\n",
    "        cols_all = [c for c in cols_all if c in df_train.columns]\n",
    "        if len(cols_all) < 2:\n",
    "            continue\n",
    "\n",
    "        # rowwise stats (NaN-safe)\n",
    "        mat = df_train[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        df_train[f\"cfg_mean_{base_name}\"] = mat.mean(axis=1, skipna=True)\n",
    "        df_train[f\"cfg_max_{base_name}\"]  = mat.max(axis=1, skipna=True)\n",
    "        df_train[f\"cfg_min_{base_name}\"]  = mat.min(axis=1, skipna=True)\n",
    "        df_train[f\"cfg_std_{base_name}\"]  = mat.std(axis=1, skipna=True)\n",
    "\n",
    "# EXTRA MATCH cfgs (optional)\n",
    "match_core_cols_added = []\n",
    "match_core_matrix_cols = {}\n",
    "\n",
    "if FE_CFG[\"use_match_features\"]:\n",
    "    extra_match_cfgs = [d for d in match_cfg_dirs if str(d) != str(Path(PATHS[\"MATCH_CFG_DIR\"]))]\n",
    "    for i, cfg_dir in enumerate(extra_match_cfgs, 1):\n",
    "        fp = cfg_dir / \"match_features_train_all.csv\"\n",
    "        if not fp.exists():\n",
    "            continue\n",
    "        try:\n",
    "            tag = _short_cfg_tag(cfg_dir, i)\n",
    "            df_extra = _load_csv_core(fp, CORE_MATCH_COLS)\n",
    "            df_extra = _ensure_case_variant(df_extra, df_base_map=df_base_map)\n",
    "            ren = {}\n",
    "            for c in df_extra.columns:\n",
    "                if c == \"uid\":\n",
    "                    continue\n",
    "                ren[c] = f\"{c}__{tag}\"\n",
    "                match_core_matrix_cols.setdefault(c, []).append(ren[c])\n",
    "            df_extra = df_extra.rename(columns=ren)\n",
    "            df_train = df_train.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n",
    "            match_core_cols_added.extend(list(ren.values()))\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: skip extra match cfg {cfg_dir.name} due to error: {e}\")\n",
    "\n",
    "    if FE_CFG[\"multi_cfg_extra_mode\"] == \"core+agg\":\n",
    "        for base_name, cols_suff in match_core_matrix_cols.items():\n",
    "            cols_all = []\n",
    "            if base_name in df_train.columns and pd.api.types.is_numeric_dtype(df_train[base_name]):\n",
    "                cols_all.append(base_name)\n",
    "            cols_all.extend([c for c in cols_suff if c in df_train.columns])\n",
    "            cols_all = [c for c in cols_all if c in df_train.columns]\n",
    "            if len(cols_all) < 2:\n",
    "                continue\n",
    "            mat = df_train[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n",
    "            df_train[f\"cfg_mean_match_{base_name}\"] = mat.mean(axis=1, skipna=True)\n",
    "            df_train[f\"cfg_max_match_{base_name}\"]  = mat.max(axis=1, skipna=True)\n",
    "            df_train[f\"cfg_min_match_{base_name}\"]  = mat.min(axis=1, skipna=True)\n",
    "            df_train[f\"cfg_std_match_{base_name}\"]  = mat.std(axis=1, skipna=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Optional merge image profile by case_id (numeric only, prefixed)\n",
    "# ----------------------------\n",
    "if df_prof is not None and \"case_id\" in df_prof.columns:\n",
    "    df_prof2 = df_prof.copy()\n",
    "    df_prof2[\"case_id\"] = pd.to_numeric(df_prof2[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df_prof2 = df_prof2.dropna(subset=[\"case_id\"]).astype({\"case_id\": int})\n",
    "    df_prof2 = df_prof2.drop_duplicates(\"case_id\")\n",
    "\n",
    "    prof_num = [\"case_id\"] + [\n",
    "        c for c in df_prof2.columns\n",
    "        if c != \"case_id\" and pd.api.types.is_numeric_dtype(df_prof2[c])\n",
    "    ]\n",
    "    df_prof2 = df_prof2[prof_num].copy()\n",
    "\n",
    "    ren = {c: f\"profile_{c}\" for c in df_prof2.columns if c != \"case_id\"}\n",
    "    df_prof2 = df_prof2.rename(columns=ren)\n",
    "    df_train = df_train.merge(df_prof2, on=\"case_id\", how=\"left\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Feature engineering helpers\n",
    "# ----------------------------\n",
    "def _num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def safe_log1p_nonneg(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x = np.where(np.isfinite(x), x, 0.0)\n",
    "    x = np.clip(x, 0.0, None)\n",
    "    return np.log1p(x)\n",
    "\n",
    "def safe_sqrt_nonneg(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x = np.where(np.isfinite(x), x, 0.0)\n",
    "    x = np.clip(x, 0.0, None)\n",
    "    return np.sqrt(x)\n",
    "\n",
    "def get_clip_cap(series: pd.Series, q: float, fallback: float):\n",
    "    s = _num(series).astype(float).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if len(s) == 0:\n",
    "        return float(fallback)\n",
    "    s = s[np.isfinite(s)]\n",
    "    cap = float(np.quantile(np.abs(s.values), q))\n",
    "    if (not np.isfinite(cap)) or (cap <= 0):\n",
    "        return float(fallback)\n",
    "    return float(cap)\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Candidate numeric feature list (pre-fill)\n",
    "# ----------------------------\n",
    "TARGET_COLS = {\"y\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"}\n",
    "SPLIT_COLS  = {\"fold\"}\n",
    "ID_DROP_NUM = {\"case_id\"}  # jangan jadi feature\n",
    "\n",
    "# Replace inf -> NaN for numeric cols\n",
    "for c in df_train.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_train[c]):\n",
    "        df_train[c] = df_train[c].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Missing indicators (sebelum fill)\n",
    "missing_ind_cols = []\n",
    "if FE_CFG[\"add_missing_indicators\"]:\n",
    "    for c in df_train.columns:\n",
    "        if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df_train[c]) and df_train[c].isna().any():\n",
    "            ind = f\"isna_{c}\"\n",
    "            df_train[ind] = df_train[c].isna().astype(np.uint8)\n",
    "            missing_ind_cols.append(ind)\n",
    "\n",
    "# Heavy-tail candidates\n",
    "heavy_candidates = set([\n",
    "    \"peak_ratio\", \"best_weight\", \"best_count\", \"best_weight_frac\",\n",
    "    \"pair_count\", \"n_pairs_thr\", \"n_pairs_mnn\", \"overmask_tighten_steps\",\n",
    "    \"largest_comp\", \"n_comp\", \"grid_h\", \"grid_w\",\n",
    "    \"grid_area_frac\", \"area_frac\", \"inlier_ratio\",\n",
    "])\n",
    "for c in df_train.columns:\n",
    "    cl = c.lower()\n",
    "    if any(k in cl for k in [\"count\", \"pairs\", \"weight\", \"ratio\", \"area\", \"comp\", \"std_\"]):\n",
    "        if pd.api.types.is_numeric_dtype(df_train[c]):\n",
    "            heavy_candidates.add(c)\n",
    "\n",
    "clip_caps = {}\n",
    "if FE_CFG[\"clip_by_quantile\"]:\n",
    "    for c in sorted(list(heavy_candidates)):\n",
    "        if c in df_train.columns and pd.api.types.is_numeric_dtype(df_train[c]):\n",
    "            clip_caps[c] = get_clip_cap(df_train[c], FE_CFG[\"clip_q\"], FE_CFG[\"clip_max_fallback\"])\n",
    "\n",
    "# Clipped + log/sqrt transforms\n",
    "if FE_CFG[\"add_log_features\"] or FE_CFG[\"add_sqrt_features\"]:\n",
    "    for c, cap in clip_caps.items():\n",
    "        x = _num(df_train[c]).fillna(0.0).astype(float).values\n",
    "        x = np.clip(x, -cap, cap)\n",
    "        df_train[f\"{c}_cap\"] = x.astype(np.float32)\n",
    "\n",
    "        if FE_CFG[\"add_log_features\"]:\n",
    "            df_train[f\"logabs_{c}\"] = safe_log1p_nonneg(np.abs(x)).astype(np.float32)\n",
    "        if FE_CFG[\"add_sqrt_features\"]:\n",
    "            df_train[f\"sqrtabs_{c}\"] = safe_sqrt_nonneg(np.abs(x)).astype(np.float32)\n",
    "\n",
    "# Interactions (lebih kaya + aman)\n",
    "if FE_CFG[\"add_interactions\"]:\n",
    "    def getf(col, default=0.0):\n",
    "        if col in df_train.columns:\n",
    "            return _num(df_train[col]).fillna(default).astype(float).values\n",
    "        return np.full(len(df_train), default, dtype=np.float64)\n",
    "\n",
    "    best_mean_sim = getf(\"best_mean_sim\", 0.0)\n",
    "    best_count    = getf(\"best_count\", 0.0)\n",
    "    peak_ratio    = getf(\"peak_ratio\", 0.0)\n",
    "    has_peak      = getf(\"has_peak\", 0.0)\n",
    "    grid_area     = getf(\"grid_area_frac\", 0.0)\n",
    "    area_frac     = getf(\"area_frac\", 0.0)\n",
    "    n_pairs_thr   = getf(\"n_pairs_thr\", 0.0)\n",
    "    n_pairs_mnn   = getf(\"n_pairs_mnn\", 0.0)\n",
    "    inlier_ratio  = getf(\"inlier_ratio\", 0.0)\n",
    "    gh = getf(\"grid_h\", 0.0)\n",
    "    gw = getf(\"grid_w\", 0.0)\n",
    "\n",
    "    gridN = np.clip(gh * gw, 0.0, None)\n",
    "\n",
    "    df_train[\"sim_x_count\"]      = (best_mean_sim * best_count).astype(np.float32)\n",
    "    df_train[\"peak_x_sim\"]       = (peak_ratio * best_mean_sim).astype(np.float32)\n",
    "    df_train[\"haspeak_x_sim\"]    = (has_peak * best_mean_sim).astype(np.float32)\n",
    "    df_train[\"area_x_sim\"]       = (grid_area * best_mean_sim).astype(np.float32)\n",
    "    df_train[\"area_x_count\"]     = (grid_area * best_count).astype(np.float32)\n",
    "    df_train[\"mask_grid_ratio\"]  = (area_frac / (1e-6 + grid_area)).astype(np.float32)\n",
    "    df_train[\"mnn_ratio\"]        = (n_pairs_mnn / (1.0 + n_pairs_thr)).astype(np.float32)\n",
    "    df_train[\"pairs_per_cell\"]   = (n_pairs_thr / (1.0 + gridN)).astype(np.float32)\n",
    "    df_train[\"inlier_x_pairs\"]   = (inlier_ratio * n_pairs_thr).astype(np.float32)\n",
    "\n",
    "    # tambahan yang sering bantu stabil:\n",
    "    df_train[\"log1p_pairs_thr\"]  = safe_log1p_nonneg(n_pairs_thr).astype(np.float32)\n",
    "    df_train[\"log1p_best_count\"] = safe_log1p_nonneg(best_count).astype(np.float32)\n",
    "    df_train[\"log1p_area_frac\"]  = safe_log1p_nonneg(np.clip(area_frac, 0, None)).astype(np.float32)\n",
    "\n",
    "# Fill NaN numeric -> 0\n",
    "num_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
    "df_train[num_cols] = df_train[num_cols].fillna(FE_CFG[\"fillna_value\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Variant encoding (optional one-hot)\n",
    "# ----------------------------\n",
    "variant_dummy_cols = []\n",
    "if FE_CFG[\"encode_variant_onehot\"]:\n",
    "    vc = df_train[\"variant\"].astype(str).fillna(\"unk\")\n",
    "    counts = vc.value_counts()\n",
    "    keep = set(counts[counts >= int(FE_CFG[\"variant_min_count\"])].index.tolist())\n",
    "    vc = vc.where(vc.isin(keep), other=\"rare\")\n",
    "\n",
    "    dummies = pd.get_dummies(vc, prefix=\"v\", dummy_na=False).astype(np.uint8)\n",
    "    variant_dummy_cols = dummies.columns.tolist()\n",
    "    df_train = pd.concat([df_train, dummies], axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Select final feature columns (numeric only)\n",
    "# ----------------------------\n",
    "feature_cols = []\n",
    "for c in df_train.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(df_train[c]):\n",
    "        continue\n",
    "    if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n",
    "        continue\n",
    "    feature_cols.append(c)\n",
    "\n",
    "# Drop constant features\n",
    "dropped_constant = []\n",
    "if FE_CFG[\"drop_constant_features\"]:\n",
    "    nun = df_train[feature_cols].nunique(dropna=False)\n",
    "    nonconst = nun[nun > 1].index.tolist()\n",
    "    dropped_constant = sorted(set(feature_cols) - set(nonconst))\n",
    "    feature_cols = nonconst\n",
    "\n",
    "# Cast float32 for numeric features\n",
    "if FE_CFG[\"cast_float32\"]:\n",
    "    df_train[feature_cols] = df_train[feature_cols].astype(np.float32)\n",
    "\n",
    "FEATURE_COLS = list(feature_cols)\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Final outputs\n",
    "# ----------------------------\n",
    "base_out_cols = [\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]\n",
    "df_train_tabular = df_train[base_out_cols + FEATURE_COLS].copy()\n",
    "\n",
    "X_train = df_train_tabular[FEATURE_COLS]\n",
    "y_train = df_train_tabular[\"y\"].astype(int)\n",
    "folds   = df_train_tabular[\"fold\"].astype(int)\n",
    "\n",
    "print(\"\\nOK — Training table built\")\n",
    "print(\"  df_train_tabular:\", df_train_tabular.shape)\n",
    "print(\"  X_train:\", X_train.shape, \"| y pos%:\", float(y_train.mean()) * 100.0)\n",
    "print(\"  folds:\", int(folds.nunique()), \"unique folds\")\n",
    "print(\"  feature_cols:\", int(len(FEATURE_COLS)))\n",
    "if dropped_constant:\n",
    "    print(\"  dropped_constant_features:\", len(dropped_constant))\n",
    "if variant_dummy_cols:\n",
    "    print(\"  variant_dummies:\", len(variant_dummy_cols))\n",
    "if missing_ind_cols:\n",
    "    print(\"  missing_indicators:\", len(missing_ind_cols))\n",
    "if pred_core_cols_added:\n",
    "    print(\"  extra_pred_core_cols:\", len(pred_core_cols_added))\n",
    "if match_core_cols_added:\n",
    "    print(\"  extra_match_core_cols:\", len(match_core_cols_added))\n",
    "\n",
    "# hard sanity\n",
    "if X_train.shape[0] != y_train.shape[0]:\n",
    "    raise RuntimeError(\"X_train and y_train row mismatch\")\n",
    "if y_train.isna().any():\n",
    "    raise RuntimeError(\"y_train contains NaN\")\n",
    "if folds.isna().any():\n",
    "    raise RuntimeError(\"folds contains NaN\")\n",
    "\n",
    "print(\"\\nFeature head:\", FEATURE_COLS[:25])\n",
    "print(\"Feature tail:\", FEATURE_COLS[-15:])\n",
    "\n",
    "# ----------------------------\n",
    "# 15) Save reproducible schema\n",
    "# ----------------------------\n",
    "OUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OUT_ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(OUT_ART / \"feature_cols.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_COLS, f, indent=2)\n",
    "\n",
    "schema = {\n",
    "    \"fe_cfg\": FE_CFG,\n",
    "    \"label_col_source\": label_col,\n",
    "    \"clip_caps\": clip_caps,\n",
    "    \"dropped_constant_features\": dropped_constant,\n",
    "    \"variant_dummy_cols\": variant_dummy_cols,\n",
    "    \"missing_indicator_cols\": missing_ind_cols,\n",
    "    \"multi_cfg\": {\n",
    "        \"pred_cfg_dirs_used\": [str(Path(PATHS[\"PRED_CFG_DIR\"]))] + [str(d) for d in extra_pred_cfgs],\n",
    "        \"match_cfg_dirs_used\": [str(Path(PATHS[\"MATCH_CFG_DIR\"]))] + ([str(d) for d in match_cfg_dirs if str(d) != str(Path(PATHS[\"MATCH_CFG_DIR\"]))] if FE_CFG[\"use_match_features\"] else []),\n",
    "        \"extra_pred_core_cols_added\": pred_core_cols_added[:200],\n",
    "        \"extra_match_core_cols_added\": match_core_cols_added[:200],\n",
    "    },\n",
    "    \"n_features\": int(len(FEATURE_COLS)),\n",
    "    \"example_feature_head\": FEATURE_COLS[:30],\n",
    "}\n",
    "with open(OUT_ART / \"feature_schema.json\", \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "df_train_tabular.to_parquet(OUT_ART / \"df_train_tabular.parquet\", index=False)\n",
    "\n",
    "print(f\"\\nSaved -> {OUT_ART/'feature_cols.json'}\")\n",
    "print(f\"Saved -> {OUT_ART/'feature_schema.json'}\")\n",
    "print(f\"Saved -> {OUT_ART/'df_train_tabular.parquet'}\")\n",
    "\n",
    "# export globals\n",
    "globals().update({\n",
    "    \"df_train_tabular\": df_train_tabular,\n",
    "    \"FEATURE_COLS\": FEATURE_COLS,\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"folds\": folds,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58610a58",
   "metadata": {
    "papermill": {
     "duration": 0.006227,
     "end_time": "2026-01-08T11:34:41.046868",
     "exception": false,
     "start_time": "2026-01-08T11:34:41.040641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build & Export Test Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306b91da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T11:34:41.060681Z",
     "iopub.status.busy": "2026-01-08T11:34:41.060351Z",
     "iopub.status.idle": "2026-01-08T11:34:56.739539Z",
     "shell.execute_reply": "2026-01-08T11:34:56.738778Z"
    },
    "papermill": {
     "duration": 15.688486,
     "end_time": "2026-01-08T11:34:56.741303",
     "exception": false,
     "start_time": "2026-01-08T11:34:41.052817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base TEST source: PATHS(/kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_test.parquet) | shape: (1, 3)\n",
      "ID columns used: ['uid', 'case_id']\n",
      "Features present from base: 0/84 | missing: 84\n",
      "External feature files found: 3 (after scoring)\n",
      "  [1] merged: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_test_cfg_5dbf0aa165.csv | join_keys=['uid'] | gained_cols=24 | missing_now=60\n",
      "  [2] merged: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_test.csv | join_keys=['uid'] | gained_cols=0 | missing_now=60\n",
      "  [3] merged: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_test.csv | join_keys=['uid'] | gained_cols=4 | missing_now=56\n",
      "\n",
      "Final TEST feature table: (1, 86)\n",
      "WARNING: 56 feature cols were not found anywhere and set to 0.0\n",
      "  examples: ['profile_img_H', 'profile_img_W', 'profile_aspect', 'profile_is_gray', 'profile_bg_white_frac', 'profile_roi_x0', 'profile_roi_y0', 'profile_roi_x1', 'profile_roi_y1', 'profile_roi_area_frac', 'profile_edge_density', 'area_frac_cap', 'logabs_area_frac', 'best_count_cap', 'logabs_best_count', 'best_inlier_ratio_cap', 'logabs_best_inlier_ratio', 'best_weight_cap', 'logabs_best_weight', 'best_weight_frac_cap', 'logabs_best_weight_frac', 'grid_area_frac_cap', 'logabs_grid_area_frac', 'inlier_ratio_cap', 'logabs_inlier_ratio']\n",
      "\n",
      "Saved:\n",
      "  -> /kaggle/working/recodai_luc_gate_artifacts/pred_features_test.csv\n",
      "  -> /kaggle/working/recodai_luc_gate_artifacts/pred_features_test_cfg_bf03779a3773.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 2.5 — Build & Export Test Feature Table (pred_features_test*)\n",
    "# ONE CELL (Kaggle-ready) — REVISI FULL v1.0\n",
    "#\n",
    "# Tujuan:\n",
    "# - Bangun tabel fitur TEST dengan skema yang sama seperti training (FEATURE_COLS)\n",
    "# - Output:\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/pred_features_test.csv\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/pred_features_test_cfg_<hash>.csv\n",
    "#\n",
    "# REQUIRE (minimal):\n",
    "# - FEATURE_COLS (list)\n",
    "# - Salah satu dari: df_test_tabular / df_test / PATHS[\"DF_TEST\"] / PATHS[\"DF_TEST_ALL\"]\n",
    "#\n",
    "# Catatan:\n",
    "# - Script ini akan mencoba \"mengisi\" FEATURE_COLS dari:\n",
    "#   (1) df_test_tabular jika sudah ada\n",
    "#   (2) df_test (atau CSV meta test) jika ada\n",
    "#   (3) scan file fitur test (*.csv/*.parquet) di /kaggle/working dan /kaggle/input\n",
    "# - Kalau ada kolom fitur yang tetap tidak ketemu, akan diisi 0.0 (dan diwarn)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, hashlib, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require FEATURE_COLS\n",
    "# ----------------------------\n",
    "if \"FEATURE_COLS\" not in globals():\n",
    "    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 (Build Training Table) dulu.\")\n",
    "FEATURE_COLS = list(FEATURE_COLS)\n",
    "if len(FEATURE_COLS) == 0:\n",
    "    raise RuntimeError(\"FEATURE_COLS kosong. Cek Step 2.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helper: load base test dataframe\n",
    "# ----------------------------\n",
    "def _read_table_any(p: Path):\n",
    "    p = Path(p)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    if p.suffix.lower() in [\".parquet\"]:\n",
    "        return pd.read_parquet(p)\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "def _pick_first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p is None:\n",
    "            continue\n",
    "        p = Path(p)\n",
    "        if p.exists() and p.is_file():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _get_base_test_df():\n",
    "    # Priority:\n",
    "    # 1) df_test_tabular (already has features)\n",
    "    # 2) df_test (meta)\n",
    "    # 3) PATHS DF_TEST / DF_TEST_ALL (csv/parquet)\n",
    "    if \"df_test_tabular\" in globals() and isinstance(globals()[\"df_test_tabular\"], pd.DataFrame):\n",
    "        return globals()[\"df_test_tabular\"].copy(), \"globals(df_test_tabular)\"\n",
    "    if \"df_test\" in globals() and isinstance(globals()[\"df_test\"], pd.DataFrame):\n",
    "        return globals()[\"df_test\"].copy(), \"globals(df_test)\"\n",
    "    if \"PATHS\" in globals() and isinstance(globals()[\"PATHS\"], dict):\n",
    "        cand = _pick_first_existing([\n",
    "            globals()[\"PATHS\"].get(\"DF_TEST\", None),\n",
    "            globals()[\"PATHS\"].get(\"DF_TEST_ALL\", None),\n",
    "            globals()[\"PATHS\"].get(\"TEST_META\", None),\n",
    "        ])\n",
    "        if cand is not None:\n",
    "            df = _read_table_any(cand)\n",
    "            if df is not None:\n",
    "                return df.copy(), f\"PATHS({cand})\"\n",
    "    # fallback scan common places\n",
    "    cand = _pick_first_existing([\n",
    "        Path(\"/kaggle/working/df_test.csv\"),\n",
    "        Path(\"/kaggle/working/test.csv\"),\n",
    "        Path(\"/kaggle/working/df_test.parquet\"),\n",
    "    ])\n",
    "    if cand is not None:\n",
    "        df = _read_table_any(cand)\n",
    "        if df is not None:\n",
    "            return df.copy(), f\"fallback({cand})\"\n",
    "    raise FileNotFoundError(\n",
    "        \"Tidak menemukan df_test_tabular / df_test / PATHS[DF_TEST]. \"\n",
    "        \"Pastikan Step 1/2 sudah membentuk df_test atau PATHS mengarah ke test meta.\"\n",
    "    )\n",
    "\n",
    "df_base, base_src = _get_base_test_df()\n",
    "print(\"Base TEST source:\", base_src, \"| shape:\", df_base.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Normalize id columns (uid/case_id/variant)\n",
    "# ----------------------------\n",
    "# Try to ensure: uid exists\n",
    "def _ensure_uid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    cols = set(df.columns)\n",
    "    # standardize common id column names\n",
    "    if \"case_id\" not in cols:\n",
    "        for alt in [\"case\", \"caseid\", \"image_id\", \"img_id\", \"id\", \"sample_id\"]:\n",
    "            if alt in cols:\n",
    "                df = df.rename(columns={alt: \"case_id\"})\n",
    "                cols = set(df.columns)\n",
    "                break\n",
    "    if \"variant\" not in cols:\n",
    "        for alt in [\"var\", \"aug\", \"view\", \"split_variant\"]:\n",
    "            if alt in cols:\n",
    "                df = df.rename(columns={alt: \"variant\"})\n",
    "                cols = set(df.columns)\n",
    "                break\n",
    "\n",
    "    if \"uid\" not in cols:\n",
    "        if \"case_id\" in cols and \"variant\" in cols:\n",
    "            df[\"uid\"] = df[\"case_id\"].astype(str) + \"_\" + df[\"variant\"].astype(str)\n",
    "        elif \"case_id\" in cols:\n",
    "            df[\"uid\"] = df[\"case_id\"].astype(str)\n",
    "        elif \"id\" in cols:\n",
    "            df[\"uid\"] = df[\"id\"].astype(str)\n",
    "        else:\n",
    "            # last resort: index-based uid\n",
    "            df[\"uid\"] = np.arange(len(df)).astype(str)\n",
    "\n",
    "    # ensure string uid\n",
    "    df[\"uid\"] = df[\"uid\"].astype(str)\n",
    "    return df\n",
    "\n",
    "df_base = _ensure_uid(df_base)\n",
    "\n",
    "id_cols = []\n",
    "for c in [\"uid\", \"case_id\", \"variant\"]:\n",
    "    if c in df_base.columns:\n",
    "        id_cols.append(c)\n",
    "\n",
    "print(\"ID columns used:\", id_cols)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build output frame skeleton\n",
    "# ----------------------------\n",
    "df_out = df_base[id_cols].copy()\n",
    "\n",
    "# Fill any features already present in base\n",
    "present = [c for c in FEATURE_COLS if c in df_base.columns]\n",
    "if len(present) > 0:\n",
    "    df_out = df_out.merge(df_base[[\"uid\"] + present].drop_duplicates(\"uid\"), on=\"uid\", how=\"left\")\n",
    "\n",
    "missing = [c for c in FEATURE_COLS if c not in df_out.columns]\n",
    "print(f\"Features present from base: {len(present)}/{len(FEATURE_COLS)} | missing: {len(missing)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Optional: scan and merge external feature tables (CSV/Parquet)\n",
    "# ----------------------------\n",
    "def _walk_with_depth(root: Path, max_depth=4):\n",
    "    root = Path(root)\n",
    "    root_depth = len(root.parts)\n",
    "    for cur, dirs, files in os.walk(root):\n",
    "        cur_p = Path(cur)\n",
    "        depth = len(cur_p.parts) - root_depth\n",
    "        if depth > max_depth:\n",
    "            dirs[:] = []\n",
    "            continue\n",
    "        yield cur_p, files\n",
    "\n",
    "def _find_feature_files(max_files=80):\n",
    "    patterns = [\n",
    "        \"pred_features_test\", \"test_features\", \"features_test\", \"pred_feat_test\", \"gate_features_test\",\n",
    "        \"match_features_test\", \"match_feat_test\",\n",
    "    ]\n",
    "    exts = {\".csv\", \".parquet\"}\n",
    "    found = []\n",
    "\n",
    "    # Search working first (usually fastest)\n",
    "    for root, depth in [(Path(\"/kaggle/working\"), 6), (Path(\"/kaggle/input\"), 4)]:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for cur_p, files in _walk_with_depth(root, max_depth=depth):\n",
    "            for fn in files:\n",
    "                p = cur_p / fn\n",
    "                if p.suffix.lower() not in exts:\n",
    "                    continue\n",
    "                low = p.name.lower()\n",
    "                if any(k in low for k in patterns):\n",
    "                    found.append(p)\n",
    "                    if len(found) >= max_files:\n",
    "                        return found\n",
    "    return found\n",
    "\n",
    "def _score_feat_file(p: Path, need_cols: set):\n",
    "    # quick header read for csv; for parquet we must read schema (still ok)\n",
    "    try:\n",
    "        if p.suffix.lower() == \".csv\":\n",
    "            dfh = pd.read_csv(p, nrows=5)\n",
    "        else:\n",
    "            dfh = pd.read_parquet(p)\n",
    "            dfh = dfh.head(5)\n",
    "    except Exception:\n",
    "        return -1, None\n",
    "\n",
    "    cols = set(dfh.columns)\n",
    "    # join keys compatibility score\n",
    "    key_score = 0\n",
    "    if \"uid\" in cols: key_score += 5\n",
    "    if (\"case_id\" in cols) and (\"variant\" in cols): key_score += 3\n",
    "    if \"case_id\" in cols: key_score += 1\n",
    "\n",
    "    overlap = len(cols & need_cols)\n",
    "    score = key_score * 1000 + overlap  # prioritize joinability heavily\n",
    "    return score, list(cols)\n",
    "\n",
    "def _infer_join_keys(dfA: pd.DataFrame, dfB: pd.DataFrame):\n",
    "    A = set(dfA.columns); B = set(dfB.columns)\n",
    "    for keys in ([\"uid\"], [\"case_id\", \"variant\"], [\"case_id\"], [\"id\"]):\n",
    "        if all(k in A for k in keys) and all(k in B for k in keys):\n",
    "            return keys\n",
    "    return None\n",
    "\n",
    "if len(missing) > 0:\n",
    "    need_cols = set([\"uid\", \"case_id\", \"variant\"] + FEATURE_COLS)\n",
    "    files = _find_feature_files(max_files=80)\n",
    "\n",
    "    scored = []\n",
    "    for p in files:\n",
    "        sc, cols = _score_feat_file(p, need_cols)\n",
    "        if sc > 0:\n",
    "            scored.append((sc, p, cols))\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    print(f\"External feature files found: {len(scored)} (after scoring)\")\n",
    "\n",
    "    # merge in descending quality until missing small or list exhausted\n",
    "    for rank, (sc, p, cols) in enumerate(scored[:12], 1):\n",
    "        if len(missing) == 0:\n",
    "            break\n",
    "        try:\n",
    "            df_feat = _read_table_any(p)\n",
    "            if df_feat is None or len(df_feat) == 0:\n",
    "                continue\n",
    "            df_feat = _ensure_uid(df_feat)\n",
    "\n",
    "            join_keys = _infer_join_keys(df_out, df_feat)\n",
    "            if join_keys is None:\n",
    "                continue\n",
    "\n",
    "            # keep only useful columns\n",
    "            use_cols = list(dict.fromkeys(join_keys + [c for c in FEATURE_COLS if c in df_feat.columns]))\n",
    "            if len(use_cols) <= len(join_keys):\n",
    "                continue\n",
    "\n",
    "            df_feat = df_feat[use_cols].copy()\n",
    "\n",
    "            # drop dup on join keys\n",
    "            if len(join_keys) == 1:\n",
    "                df_feat = df_feat.drop_duplicates(join_keys[0])\n",
    "            else:\n",
    "                df_feat = df_feat.drop_duplicates(join_keys)\n",
    "\n",
    "            before_missing = len(missing)\n",
    "            df_out = df_out.merge(df_feat, on=join_keys, how=\"left\", suffixes=(\"\", \"_dup\"))\n",
    "\n",
    "            # if any *_dup created, prefer existing non-null then drop dup\n",
    "            dup_cols = [c for c in df_out.columns if c.endswith(\"_dup\")]\n",
    "            for dc in dup_cols:\n",
    "                basec = dc[:-4]\n",
    "                if basec in df_out.columns:\n",
    "                    df_out[basec] = df_out[basec].fillna(df_out[dc])\n",
    "                df_out = df_out.drop(columns=[dc])\n",
    "\n",
    "            missing = [c for c in FEATURE_COLS if c not in df_out.columns]\n",
    "            gained = before_missing - len(missing)\n",
    "            print(f\"  [{rank}] merged: {p} | join_keys={join_keys} | gained_cols={gained} | missing_now={len(missing)}\")\n",
    "\n",
    "            del df_feat\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"  [{rank}] skip (read/merge error): {p} | err={repr(e)}\")\n",
    "            continue\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Finalize: ensure all FEATURE_COLS exist, numeric float32, fillna(0)\n",
    "# ----------------------------\n",
    "still_missing = [c for c in FEATURE_COLS if c not in df_out.columns]\n",
    "if len(still_missing) > 0:\n",
    "    # add missing columns as 0.0\n",
    "    for c in still_missing:\n",
    "        df_out[c] = 0.0\n",
    "\n",
    "# coerce feature cols to numeric float32\n",
    "for c in FEATURE_COLS:\n",
    "    if c in df_out.columns:\n",
    "        df_out[c] = pd.to_numeric(df_out[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "# order columns: ids then features\n",
    "df_out = df_out[id_cols + FEATURE_COLS].copy()\n",
    "\n",
    "# sanity\n",
    "assert df_out.shape[0] == df_base.shape[0], \"Row count changed unexpectedly after merges.\"\n",
    "assert df_out[\"uid\"].astype(str).nunique() <= df_out.shape[0], \"uid issue.\"\n",
    "print(\"\\nFinal TEST feature table:\", df_out.shape)\n",
    "\n",
    "# Report missing originally (if any)\n",
    "if len(still_missing) > 0:\n",
    "    print(f\"WARNING: {len(still_missing)} feature cols were not found anywhere and set to 0.0\")\n",
    "    print(\"  examples:\", still_missing[:25])\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save pred_features_test*\n",
    "# ----------------------------\n",
    "cfg_hash = hashlib.sha1(json.dumps(FEATURE_COLS, ensure_ascii=False).encode(\"utf-8\")).hexdigest()[:12]\n",
    "p_main = OUT_DIR / \"pred_features_test.csv\"\n",
    "p_cfg  = OUT_DIR / f\"pred_features_test_cfg_{cfg_hash}.csv\"\n",
    "\n",
    "df_out.to_csv(p_main, index=False)\n",
    "df_out.to_csv(p_cfg, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"  ->\", p_main)\n",
    "print(\"  ->\", p_cfg)\n",
    "\n",
    "# Export global for next steps\n",
    "df_test_tabular = df_out\n",
    "PRED_FEATURES_TEST_CSV = str(p_main)\n",
    "PRED_FEATURES_TEST_CFG_CSV = str(p_cfg)\n",
    "PRED_FEATURES_TEST_CFG_HASH = cfg_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6bbf1",
   "metadata": {
    "papermill": {
     "duration": 0.005962,
     "end_time": "2026-01-08T11:34:56.753793",
     "exception": false,
     "start_time": "2026-01-08T11:34:56.747831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Baseline Model (Leakage-Safe CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa51f2be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T11:34:56.768533Z",
     "iopub.status.busy": "2026-01-08T11:34:56.767950Z",
     "iopub.status.idle": "2026-01-08T11:50:58.563636Z",
     "shell.execute_reply": "2026-01-08T11:50:58.562655Z"
    },
    "papermill": {
     "duration": 961.805414,
     "end_time": "2026-01-08T11:50:58.565262",
     "exception": false,
     "start_time": "2026-01-08T11:34:56.759848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | AMP: True | CFG: SAFE\n",
      "Setup:\n",
      "  rows      : 5176\n",
      "  folds     : 5 | [0, 1, 2, 3, 4]\n",
      "  pos%      : 54.07650695517774\n",
      "  n_features: 84\n",
      "\n",
      "SEED plan: [2025]\n",
      "\n",
      "==============================\n",
      "== SEED 2025 (1/1)\n",
      "==============================\n",
      "\n",
      "[Seed 2025 | Fold 0]\n",
      "  epoch 001/60 | train_loss=0.21723 | val_logloss=0.70783 | val_auc=0.06474154975990962 | opt_steps=9 | dt=6.1s\n",
      "  epoch 002/60 | train_loss=0.09139 | val_logloss=0.70457 | val_auc=0.09654458148950193 | opt_steps=9 | dt=4.5s\n",
      "  epoch 003/60 | train_loss=0.03457 | val_logloss=0.69920 | val_auc=0.200711797382544 | opt_steps=9 | dt=4.5s\n",
      "  epoch 004/60 | train_loss=0.01954 | val_logloss=0.69166 | val_auc=0.44811411354863007 | opt_steps=9 | dt=4.5s\n",
      "  epoch 005/60 | train_loss=0.02331 | val_logloss=0.68211 | val_auc=0.7642142924395066 | opt_steps=9 | dt=4.5s\n",
      "  epoch 006/60 | train_loss=0.00354 | val_logloss=0.67043 | val_auc=0.9357499293851803 | opt_steps=9 | dt=4.5s\n",
      "  epoch 007/60 | train_loss=0.00282 | val_logloss=0.65666 | val_auc=0.974949628095283 | opt_steps=9 | dt=4.5s\n",
      "  epoch 008/60 | train_loss=0.00205 | val_logloss=0.64117 | val_auc=0.9926221636380754 | opt_steps=9 | dt=4.5s\n",
      "  epoch 009/60 | train_loss=0.00134 | val_logloss=0.62413 | val_auc=0.9974729309857829 | opt_steps=9 | dt=4.5s\n",
      "  epoch 010/60 | train_loss=0.00053 | val_logloss=0.60576 | val_auc=0.9988325016476791 | opt_steps=9 | dt=4.5s\n",
      "  epoch 011/60 | train_loss=0.00192 | val_logloss=0.58660 | val_auc=0.999450145937294 | opt_steps=9 | dt=4.5s\n",
      "  epoch 012/60 | train_loss=0.00140 | val_logloss=0.56707 | val_auc=0.999860653422465 | opt_steps=9 | dt=4.5s\n",
      "  epoch 013/60 | train_loss=0.00289 | val_logloss=0.54740 | val_auc=0.9999585726391111 | opt_steps=9 | dt=4.5s\n",
      "  epoch 014/60 | train_loss=0.00110 | val_logloss=0.52682 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 015/60 | train_loss=0.00633 | val_logloss=0.50643 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 016/60 | train_loss=0.00128 | val_logloss=0.48820 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 017/60 | train_loss=0.00142 | val_logloss=0.47063 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 018/60 | train_loss=0.00029 | val_logloss=0.45556 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 019/60 | train_loss=0.00163 | val_logloss=0.44293 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 020/60 | train_loss=0.00115 | val_logloss=0.43399 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 021/60 | train_loss=0.00065 | val_logloss=0.42533 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 022/60 | train_loss=0.00169 | val_logloss=0.41926 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 023/60 | train_loss=0.00099 | val_logloss=0.41697 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 024/60 | train_loss=0.00081 | val_logloss=0.41540 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 025/60 | train_loss=0.00067 | val_logloss=0.41449 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 026/60 | train_loss=0.00108 | val_logloss=0.41549 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 027/60 | train_loss=0.00071 | val_logloss=0.41794 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 028/60 | train_loss=0.00140 | val_logloss=0.42136 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 029/60 | train_loss=0.00138 | val_logloss=0.42475 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 030/60 | train_loss=0.00084 | val_logloss=0.42804 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 031/60 | train_loss=0.00049 | val_logloss=0.43150 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 032/60 | train_loss=0.00103 | val_logloss=0.43523 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 033/60 | train_loss=0.00035 | val_logloss=0.43854 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 034/60 | train_loss=0.00170 | val_logloss=0.44149 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 035/60 | train_loss=0.00037 | val_logloss=0.44456 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  early stop at epoch 35, best_epoch=25, best_val_logloss=0.41449\n",
      "\n",
      "[Seed 2025 | Fold 1]\n",
      "  epoch 001/60 | train_loss=0.21430 | val_logloss=0.71669 | val_auc=0.4045324569221628 | opt_steps=9 | dt=4.6s\n",
      "  epoch 002/60 | train_loss=0.07951 | val_logloss=0.71317 | val_auc=0.447277926322044 | opt_steps=9 | dt=4.5s\n",
      "  epoch 003/60 | train_loss=0.04075 | val_logloss=0.70732 | val_auc=0.5376578282828283 | opt_steps=9 | dt=4.5s\n",
      "  epoch 004/60 | train_loss=0.02035 | val_logloss=0.69923 | val_auc=0.6554961378490789 | opt_steps=9 | dt=4.5s\n",
      "  epoch 005/60 | train_loss=0.00776 | val_logloss=0.68882 | val_auc=0.7779764557338087 | opt_steps=9 | dt=4.5s\n",
      "  epoch 006/60 | train_loss=0.00501 | val_logloss=0.67604 | val_auc=0.9067067736185384 | opt_steps=9 | dt=4.5s\n",
      "  epoch 007/60 | train_loss=0.00149 | val_logloss=0.66095 | val_auc=0.9775902406417112 | opt_steps=9 | dt=4.5s\n",
      "  epoch 008/60 | train_loss=0.00120 | val_logloss=0.64344 | val_auc=0.9973577688651218 | opt_steps=9 | dt=4.5s\n",
      "  epoch 009/60 | train_loss=0.00248 | val_logloss=0.62346 | val_auc=0.99961564171123 | opt_steps=9 | dt=4.5s\n",
      "  epoch 010/60 | train_loss=0.00074 | val_logloss=0.60105 | val_auc=0.9999442959001782 | opt_steps=9 | dt=4.5s\n",
      "  epoch 011/60 | train_loss=0.00145 | val_logloss=0.57627 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 012/60 | train_loss=0.00240 | val_logloss=0.54968 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 013/60 | train_loss=0.00169 | val_logloss=0.52164 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 014/60 | train_loss=0.00172 | val_logloss=0.49263 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 015/60 | train_loss=0.00122 | val_logloss=0.46323 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 016/60 | train_loss=0.00099 | val_logloss=0.43417 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 017/60 | train_loss=0.00228 | val_logloss=0.40737 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 018/60 | train_loss=0.00105 | val_logloss=0.38387 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 019/60 | train_loss=0.00091 | val_logloss=0.36290 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 020/60 | train_loss=0.00087 | val_logloss=0.34394 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 021/60 | train_loss=0.00238 | val_logloss=0.33149 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 022/60 | train_loss=0.00097 | val_logloss=0.32451 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 023/60 | train_loss=0.00052 | val_logloss=0.32129 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 024/60 | train_loss=0.00140 | val_logloss=0.32064 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 025/60 | train_loss=0.00089 | val_logloss=0.32217 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 026/60 | train_loss=0.00077 | val_logloss=0.32857 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 027/60 | train_loss=0.00097 | val_logloss=0.33880 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 028/60 | train_loss=0.00054 | val_logloss=0.35164 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 029/60 | train_loss=0.00146 | val_logloss=0.36404 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 030/60 | train_loss=0.00061 | val_logloss=0.37236 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 031/60 | train_loss=0.00103 | val_logloss=0.37920 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 032/60 | train_loss=0.00052 | val_logloss=0.38582 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 033/60 | train_loss=0.00071 | val_logloss=0.39156 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 034/60 | train_loss=0.00042 | val_logloss=0.40000 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  early stop at epoch 34, best_epoch=24, best_val_logloss=0.32064\n",
      "\n",
      "[Seed 2025 | Fold 2]\n",
      "  epoch 001/60 | train_loss=0.20521 | val_logloss=0.68498 | val_auc=0.6073345259391772 | opt_steps=9 | dt=4.6s\n",
      "  epoch 002/60 | train_loss=0.05168 | val_logloss=0.67899 | val_auc=0.6853562878441191 | opt_steps=9 | dt=4.5s\n",
      "  epoch 003/60 | train_loss=0.01479 | val_logloss=0.66863 | val_auc=0.7835817508613614 | opt_steps=9 | dt=4.5s\n",
      "  epoch 004/60 | train_loss=0.08187 | val_logloss=0.65455 | val_auc=0.8869791646968499 | opt_steps=9 | dt=4.5s\n",
      "  epoch 005/60 | train_loss=0.05178 | val_logloss=0.63646 | val_auc=0.9759953405167032 | opt_steps=9 | dt=4.5s\n",
      "  epoch 006/60 | train_loss=0.01498 | val_logloss=0.61456 | val_auc=0.9899359699251532 | opt_steps=9 | dt=4.5s\n",
      "  epoch 007/60 | train_loss=0.00368 | val_logloss=0.58953 | val_auc=0.994521703283196 | opt_steps=9 | dt=4.5s\n",
      "  epoch 008/60 | train_loss=0.00214 | val_logloss=0.56201 | val_auc=0.9984455782184285 | opt_steps=9 | dt=4.5s\n",
      "  epoch 009/60 | train_loss=0.00262 | val_logloss=0.53292 | val_auc=0.9998487180747863 | opt_steps=9 | dt=4.5s\n",
      "  epoch 010/60 | train_loss=0.00130 | val_logloss=0.50378 | val_auc=0.9999962179518697 | opt_steps=9 | dt=4.5s\n",
      "  epoch 011/60 | train_loss=0.00082 | val_logloss=0.47536 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 012/60 | train_loss=0.00074 | val_logloss=0.44783 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 013/60 | train_loss=0.00059 | val_logloss=0.42235 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 014/60 | train_loss=0.00059 | val_logloss=0.39944 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 015/60 | train_loss=0.00082 | val_logloss=0.37938 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 016/60 | train_loss=0.00053 | val_logloss=0.36176 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 017/60 | train_loss=0.00055 | val_logloss=0.34635 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 018/60 | train_loss=0.00103 | val_logloss=0.33344 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 019/60 | train_loss=0.00053 | val_logloss=0.32375 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 020/60 | train_loss=0.00049 | val_logloss=0.31684 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 021/60 | train_loss=0.00026 | val_logloss=0.31229 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 022/60 | train_loss=0.00047 | val_logloss=0.30915 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 023/60 | train_loss=0.00056 | val_logloss=0.30810 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 024/60 | train_loss=0.00048 | val_logloss=0.30854 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 025/60 | train_loss=0.00082 | val_logloss=0.31131 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 026/60 | train_loss=0.00047 | val_logloss=0.31724 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 027/60 | train_loss=0.00047 | val_logloss=0.32467 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 028/60 | train_loss=0.00070 | val_logloss=0.33500 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 029/60 | train_loss=0.00056 | val_logloss=0.34755 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 030/60 | train_loss=0.00168 | val_logloss=0.36126 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 031/60 | train_loss=0.00100 | val_logloss=0.37250 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 032/60 | train_loss=0.00090 | val_logloss=0.37945 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 033/60 | train_loss=0.00083 | val_logloss=0.38329 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  early stop at epoch 33, best_epoch=23, best_val_logloss=0.30810\n",
      "\n",
      "[Seed 2025 | Fold 3]\n",
      "  epoch 001/60 | train_loss=0.19830 | val_logloss=0.68016 | val_auc=0.7179323308270676 | opt_steps=9 | dt=4.6s\n",
      "  epoch 002/60 | train_loss=0.08007 | val_logloss=0.67552 | val_auc=0.7935827067669173 | opt_steps=9 | dt=4.5s\n",
      "  epoch 003/60 | train_loss=0.03255 | val_logloss=0.66730 | val_auc=0.8890169172932331 | opt_steps=9 | dt=4.5s\n",
      "  epoch 004/60 | train_loss=0.01432 | val_logloss=0.65580 | val_auc=0.9585996240601504 | opt_steps=9 | dt=4.5s\n",
      "  epoch 005/60 | train_loss=0.01715 | val_logloss=0.64127 | val_auc=0.9883609022556391 | opt_steps=9 | dt=4.5s\n",
      "  epoch 006/60 | train_loss=0.00670 | val_logloss=0.62388 | val_auc=0.9964229323308271 | opt_steps=9 | dt=4.5s\n",
      "  epoch 007/60 | train_loss=0.00832 | val_logloss=0.60388 | val_auc=0.9990300751879699 | opt_steps=9 | dt=4.5s\n",
      "  epoch 008/60 | train_loss=0.00463 | val_logloss=0.58174 | val_auc=0.9996804511278194 | opt_steps=9 | dt=4.5s\n",
      "  epoch 009/60 | train_loss=0.00123 | val_logloss=0.55683 | val_auc=0.9999172932330828 | opt_steps=9 | dt=4.5s\n",
      "  epoch 010/60 | train_loss=0.00187 | val_logloss=0.52988 | val_auc=0.9999774436090224 | opt_steps=9 | dt=4.5s\n",
      "  epoch 011/60 | train_loss=0.00342 | val_logloss=0.50188 | val_auc=0.9999887218045113 | opt_steps=9 | dt=4.5s\n",
      "  epoch 012/60 | train_loss=0.00338 | val_logloss=0.47441 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 013/60 | train_loss=0.00182 | val_logloss=0.44744 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 014/60 | train_loss=0.00220 | val_logloss=0.42177 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 015/60 | train_loss=0.00163 | val_logloss=0.39788 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 016/60 | train_loss=0.00167 | val_logloss=0.37619 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 017/60 | train_loss=0.00116 | val_logloss=0.35707 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 018/60 | train_loss=0.00063 | val_logloss=0.34133 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 019/60 | train_loss=0.00065 | val_logloss=0.32902 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 020/60 | train_loss=0.00061 | val_logloss=0.32073 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 021/60 | train_loss=0.00084 | val_logloss=0.31601 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 022/60 | train_loss=0.00081 | val_logloss=0.31403 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 023/60 | train_loss=0.00081 | val_logloss=0.31427 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 024/60 | train_loss=0.00046 | val_logloss=0.31659 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 025/60 | train_loss=0.00080 | val_logloss=0.32146 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 026/60 | train_loss=0.00040 | val_logloss=0.32896 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 027/60 | train_loss=0.00131 | val_logloss=0.33754 | val_auc=1.0000000000000002 | opt_steps=9 | dt=4.5s\n",
      "  epoch 028/60 | train_loss=0.00079 | val_logloss=0.34635 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 029/60 | train_loss=0.00077 | val_logloss=0.35571 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 030/60 | train_loss=0.00088 | val_logloss=0.36422 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 031/60 | train_loss=0.00199 | val_logloss=0.37133 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 032/60 | train_loss=0.00116 | val_logloss=0.38414 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  early stop at epoch 32, best_epoch=22, best_val_logloss=0.31403\n",
      "\n",
      "[Seed 2025 | Fold 4]\n",
      "  epoch 001/60 | train_loss=0.21496 | val_logloss=0.71840 | val_auc=0.27335556057866184 | opt_steps=9 | dt=4.6s\n",
      "  epoch 002/60 | train_loss=0.07359 | val_logloss=0.71550 | val_auc=0.28407361362266426 | opt_steps=9 | dt=4.5s\n",
      "  epoch 003/60 | train_loss=0.01407 | val_logloss=0.71076 | val_auc=0.31407097649186255 | opt_steps=9 | dt=4.5s\n",
      "  epoch 004/60 | train_loss=0.01101 | val_logloss=0.70428 | val_auc=0.41830357142857144 | opt_steps=9 | dt=4.5s\n",
      "  epoch 005/60 | train_loss=0.00992 | val_logloss=0.69588 | val_auc=0.638462552742616 | opt_steps=9 | dt=4.5s\n",
      "  epoch 006/60 | train_loss=0.00348 | val_logloss=0.68532 | val_auc=0.7163445599758891 | opt_steps=9 | dt=4.5s\n",
      "  epoch 007/60 | train_loss=0.00148 | val_logloss=0.67264 | val_auc=0.9061991410488246 | opt_steps=9 | dt=4.5s\n",
      "  epoch 008/60 | train_loss=0.00226 | val_logloss=0.65779 | val_auc=0.9799220162748643 | opt_steps=9 | dt=4.5s\n",
      "  epoch 009/60 | train_loss=0.00311 | val_logloss=0.64081 | val_auc=0.9997645418927065 | opt_steps=9 | dt=4.5s\n",
      "  epoch 010/60 | train_loss=0.00216 | val_logloss=0.62158 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 011/60 | train_loss=0.00181 | val_logloss=0.59997 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 012/60 | train_loss=0.00113 | val_logloss=0.57632 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 013/60 | train_loss=0.00284 | val_logloss=0.55087 | val_auc=0.9999999999999999 | opt_steps=9 | dt=4.5s\n",
      "  epoch 014/60 | train_loss=0.00161 | val_logloss=0.52444 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 015/60 | train_loss=0.00121 | val_logloss=0.49689 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 016/60 | train_loss=0.00090 | val_logloss=0.46946 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 017/60 | train_loss=0.00064 | val_logloss=0.44237 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 018/60 | train_loss=0.00075 | val_logloss=0.41623 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 019/60 | train_loss=0.00046 | val_logloss=0.39186 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 020/60 | train_loss=0.00099 | val_logloss=0.37064 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 021/60 | train_loss=0.00054 | val_logloss=0.35463 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 022/60 | train_loss=0.00093 | val_logloss=0.34653 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 023/60 | train_loss=0.00074 | val_logloss=0.34636 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 024/60 | train_loss=0.00058 | val_logloss=0.35256 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 025/60 | train_loss=0.00059 | val_logloss=0.36388 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 026/60 | train_loss=0.00062 | val_logloss=0.37579 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 027/60 | train_loss=0.00063 | val_logloss=0.38773 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 028/60 | train_loss=0.00183 | val_logloss=0.40052 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 029/60 | train_loss=0.00044 | val_logloss=0.41557 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 030/60 | train_loss=0.00042 | val_logloss=0.43135 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 031/60 | train_loss=0.00246 | val_logloss=0.44707 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 032/60 | train_loss=0.00089 | val_logloss=0.46123 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  epoch 033/60 | train_loss=0.00083 | val_logloss=0.47320 | val_auc=1.0 | opt_steps=9 | dt=4.5s\n",
      "  early stop at epoch 33, best_epoch=23, best_val_logloss=0.34636\n",
      "\n",
      "Per-fold report:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>n_val</th>\n",
       "      <th>pos_val</th>\n",
       "      <th>auc_raw</th>\n",
       "      <th>logloss_raw</th>\n",
       "      <th>auc_cal</th>\n",
       "      <th>logloss_cal</th>\n",
       "      <th>thr_used</th>\n",
       "      <th>f1@thr</th>\n",
       "      <th>f0.5@thr</th>\n",
       "      <th>f2@thr</th>\n",
       "      <th>precision@thr</th>\n",
       "      <th>recall@thr</th>\n",
       "      <th>best_val_logloss</th>\n",
       "      <th>best_val_auc</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>thr_search</th>\n",
       "      <th>used_calibration</th>\n",
       "      <th>calibration_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>1034</td>\n",
       "      <td>559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.414488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.414488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>{'thr': 0.00125, 'score': 1.0, 'f1': 1.0, 'f05...</td>\n",
       "      <td>True</td>\n",
       "      <td>isotonic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1041</td>\n",
       "      <td>561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.320638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.20625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.320638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>{'thr': 0.20625000000000002, 'score': 1.0, 'f1...</td>\n",
       "      <td>True</td>\n",
       "      <td>isotonic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>1032</td>\n",
       "      <td>559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>{'thr': 0.00125, 'score': 1.0, 'f1': 1.0, 'f05...</td>\n",
       "      <td>True</td>\n",
       "      <td>isotonic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>1035</td>\n",
       "      <td>560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>{'thr': 0.00125, 'score': 1.0, 'f1': 1.0, 'f05...</td>\n",
       "      <td>True</td>\n",
       "      <td>isotonic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>1034</td>\n",
       "      <td>560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346357</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.00500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346357</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>{'thr': 0.005, 'score': 1.0, 'f1': 1.0, 'f05':...</td>\n",
       "      <td>True</td>\n",
       "      <td>isotonic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  fold  n_val  pos_val  auc_raw  logloss_raw  auc_cal  logloss_cal  \\\n",
       "0  2025     0   1034      559      1.0     0.414488      1.0     0.000001   \n",
       "1  2025     1   1041      561      1.0     0.320638      1.0     0.000222   \n",
       "2  2025     2   1032      559      1.0     0.308105      1.0     0.000001   \n",
       "3  2025     3   1035      560      1.0     0.314030      1.0     0.000001   \n",
       "4  2025     4   1034      560      1.0     0.346357      1.0     0.000097   \n",
       "\n",
       "   thr_used  f1@thr  f0.5@thr  f2@thr  precision@thr  recall@thr  \\\n",
       "0   0.00125     1.0       1.0     1.0            1.0         1.0   \n",
       "1   0.20625     1.0       1.0     1.0            1.0         1.0   \n",
       "2   0.00125     1.0       1.0     1.0            1.0         1.0   \n",
       "3   0.00125     1.0       1.0     1.0            1.0         1.0   \n",
       "4   0.00500     1.0       1.0     1.0            1.0         1.0   \n",
       "\n",
       "   best_val_logloss  best_val_auc  best_epoch  \\\n",
       "0          0.414488           1.0          25   \n",
       "1          0.320638           1.0          24   \n",
       "2          0.308105           1.0          23   \n",
       "3          0.314030           1.0          22   \n",
       "4          0.346357           1.0          23   \n",
       "\n",
       "                                          thr_search  used_calibration  \\\n",
       "0  {'thr': 0.00125, 'score': 1.0, 'f1': 1.0, 'f05...              True   \n",
       "1  {'thr': 0.20625000000000002, 'score': 1.0, 'f1...              True   \n",
       "2  {'thr': 0.00125, 'score': 1.0, 'f1': 1.0, 'f05...              True   \n",
       "3  {'thr': 0.00125, 'score': 1.0, 'f1': 1.0, 'f05...              True   \n",
       "4  {'thr': 0.005, 'score': 1.0, 'f1': 1.0, 'f05':...              True   \n",
       "\n",
       "  calibration_type  \n",
       "0         isotonic  \n",
       "1         isotonic  \n",
       "2         isotonic  \n",
       "3         isotonic  \n",
       "4         isotonic  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF overall (seed):\n",
      "{'seed': 2025, 'rows': 5176, 'folds': 5, 'pos_total': 2799, 'pos_rate': 0.5407650695517774, 'oof_auc_raw': 0.9999998496969063, 'oof_logloss_raw': 0.3407038727618474, 'oof_auc_cal': 1.0, 'oof_logloss_cal': 6.458565854921662e-05, 'oof_f1@0.5': 1.0, 'oof_f0.5@0.5': 1.0, 'oof_precision@0.5': 1.0, 'oof_recall@0.5': 1.0, 'oof_best_thr_cal': 0.20625000000000002, 'oof_best_thr_detail': {'thr': 0.20625000000000002, 'score': 1.0, 'f1': 1.0, 'f05': 1.0, 'f2': 1.0, 'precision': 1.0, 'recall': 1.0, 'objective': 'f0.5'}, 'best_epochs': [25, 24, 23, 22, 23]}\n",
      "\n",
      "==============================\n",
      "OOF ENSEMBLE overall:\n",
      "{'model': 'mHC-FTTransformer (tabular gate) v3.0', 'cfg_name': 'SAFE', 'seeds': [2025], 'feature_count': 84, 'oof_auc_raw': 0.9999998496969063, 'oof_logloss_raw': 0.3407038727618474, 'oof_auc_cal': 1.0, 'oof_logloss_cal': 6.458565854921662e-05, 'oof_f1@0.5': 1.0, 'oof_f0.5@0.5': 1.0, 'oof_precision@0.5': 1.0, 'oof_recall@0.5': 1.0, 'oof_best_thr_cal': 0.20625000000000002, 'oof_best_thr_detail': {'thr': 0.20625000000000002, 'score': 1.0, 'f1': 1.0, 'f05': 1.0, 'f2': 1.0, 'precision': 1.0, 'recall': 1.0, 'objective': 'f0.5'}}\n",
      "==============================\n",
      "\n",
      "\n",
      "Training FULL mHC transformer for 26 epochs | seed=2025 ...\n",
      "  full epoch 001/26 | loss=0.19112\n",
      "  full epoch 002/26 | loss=0.03481\n",
      "  full epoch 003/26 | loss=0.01716\n",
      "  full epoch 004/26 | loss=0.00707\n",
      "  full epoch 005/26 | loss=0.00342\n",
      "  full epoch 006/26 | loss=0.00161\n",
      "  full epoch 007/26 | loss=0.00154\n",
      "  full epoch 008/26 | loss=0.00145\n",
      "  full epoch 009/26 | loss=0.00148\n",
      "  full epoch 010/26 | loss=0.00145\n",
      "  full epoch 011/26 | loss=0.00163\n",
      "  full epoch 012/26 | loss=0.00147\n",
      "  full epoch 013/26 | loss=0.00135\n",
      "  full epoch 014/26 | loss=0.00348\n",
      "  full epoch 015/26 | loss=0.00098\n",
      "  full epoch 016/26 | loss=0.00159\n",
      "  full epoch 017/26 | loss=0.00146\n",
      "  full epoch 018/26 | loss=0.00106\n",
      "  full epoch 019/26 | loss=0.00121\n",
      "  full epoch 020/26 | loss=0.00076\n",
      "  full epoch 021/26 | loss=0.00178\n",
      "  full epoch 022/26 | loss=0.00073\n",
      "  full epoch 023/26 | loss=0.00050\n",
      "  full epoch 024/26 | loss=0.00091\n",
      "  full epoch 025/26 | loss=0.00048\n",
      "  full epoch 026/26 | loss=0.00029\n",
      "\n",
      "Saved artifacts:\n",
      "  fold models  -> /kaggle/working/recodai_luc_gate_artifacts (mhc_transformer_folds_seed_*/)\n",
      "  full model   -> /kaggle/working/recodai_luc_gate_artifacts/mhc_transformer_model_full.pt\n",
      "  oof preds    -> /kaggle/working/recodai_luc_gate_artifacts/oof_mhc_transformer.csv\n",
      "  oof npy      -> /kaggle/working/recodai_luc_gate_artifacts/oof_pred_mhc_raw.npy and /kaggle/working/recodai_luc_gate_artifacts/oof_pred_mhc_cal.npy\n",
      "  cv report    -> /kaggle/working/recodai_luc_gate_artifacts/mhc_transformer_cv_report.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 3 — Train Stronger Model (Leakage-Safe CV) — REVISI FULL v3.0 (MAX-UPGRADE, robust)\n",
    "#\n",
    "# Upgrade v3.0 (sesuai strategi naik score & anti error):\n",
    "# - Multi-SEED bagging (auto: 1 seed CPU/small GPU, 2 seed untuk GPU besar; bisa override ENV N_SEEDS)\n",
    "# - Leakage-safe PER-FOLD calibration (isotonic/sigmoid/none) -> oof_cal lebih stabil untuk thresholding\n",
    "# - Simpan OOF RAW + OOF CAL (csv + npy) + fold packs lengkap (mu/sig + calibrator + cfg + metrics)\n",
    "# - Threshold search lebih kaya: F1 + F0.5 + F2 (default pilih F0.5 utk menekan FP)\n",
    "# - Better early stopping: monitor val_logloss (EMA-eval) + simpan best_state (EMA weights)\n",
    "# - Guard kuat: NaN/Inf, fold sanity, amp safe, workers safe\n",
    "#\n",
    "# Output globals:\n",
    "# - OOF_PRED_MHC_RAW, OOF_PRED_MHC_CAL (np.ndarray)\n",
    "# - BASELINE_MHC_TF_OVERALL, BASELINE_MHC_TF_FOLD_REPORTS, BASELINE_MHC_TF_BEST_EPOCHS\n",
    "# - FULL_PACK_PATH (str)\n",
    "# ============================================================\n",
    "\n",
    "import json, gc, math, time, os, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require outputs from Step 2\n",
    "# ----------------------------\n",
    "need_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\n",
    "for v in need_vars:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n",
    "\n",
    "df_train_tabular = df_train_tabular.copy()\n",
    "FEATURE_COLS = list(FEATURE_COLS)\n",
    "\n",
    "required_cols = {\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"}\n",
    "missing_cols = [c for c in required_cols if c not in df_train_tabular.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"df_train_tabular missing columns: {missing_cols}.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CFG (AUTO: CPU/SAFE/STRONG)\n",
    "# ----------------------------\n",
    "CFG_CPU = {\n",
    "    \"seed\": 2025,\n",
    "\n",
    "    \"n_streams\": 2,\n",
    "    \"sinkhorn_tmax\": 10,\n",
    "    \"alpha_init\": 0.01,\n",
    "\n",
    "    \"d_model\": 192,\n",
    "    \"n_layers\": 4,\n",
    "    \"n_heads\": 6,\n",
    "    \"ffn_mult\": 4,\n",
    "    \"dropout\": 0.12,\n",
    "    \"attn_dropout\": 0.08,\n",
    "\n",
    "    \"feat_token_drop_p\": 0.05,\n",
    "    \"input_noise_std\": 0.008,\n",
    "    \"label_smoothing\": 0.00,\n",
    "    \"focal_gamma\": 1.2,\n",
    "\n",
    "    \"batch_size\": 256,\n",
    "    \"accum_steps\": 2,\n",
    "    \"epochs\": 45,\n",
    "\n",
    "    \"lr\": 3e-4,\n",
    "    \"betas\": (0.9, 0.95),\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": 5e-2,\n",
    "\n",
    "    \"warmup_frac\": 0.08,\n",
    "    \"lr_decay_milestones\": (0.80, 0.90),\n",
    "    \"lr_decay_values\": (0.316, 0.10),\n",
    "\n",
    "    \"grad_clip\": 1.0,\n",
    "\n",
    "    \"early_stop_patience\": 10,\n",
    "    \"early_stop_min_delta\": 1e-4,\n",
    "\n",
    "    \"use_ema\": True,\n",
    "    \"ema_decay\": 0.999,\n",
    "\n",
    "    # calibration (fold-wise; leakage-safe)\n",
    "    \"use_calibration\": True,\n",
    "    \"calibration\": None,  # auto from TRAIN_PLAN if available else \"isotonic\"\n",
    "    \"calib_min_samples\": 200,  # isotonic butuh cukup sample\n",
    "\n",
    "    # reporting/thresholding\n",
    "    \"search_best_thr\": True,\n",
    "    \"thr_grid\": 801,          # lebih rapat, tetap aman\n",
    "    \"thr_objective\": \"f0.5\",  # \"f1\" | \"f0.5\" | \"f2\"  (f0.5 menekan FP)\n",
    "    \"report_thr\": 0.5,\n",
    "}\n",
    "\n",
    "CFG_SAFE = {\n",
    "    **CFG_CPU,\n",
    "    \"n_streams\": 4,\n",
    "    \"sinkhorn_tmax\": 20,\n",
    "\n",
    "    \"d_model\": 256,\n",
    "    \"n_layers\": 6,\n",
    "    \"n_heads\": 8,\n",
    "\n",
    "    \"epochs\": 60,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 5e-2,\n",
    "\n",
    "    \"feat_token_drop_p\": 0.05,\n",
    "    \"input_noise_std\": 0.01,\n",
    "    \"focal_gamma\": 1.5,\n",
    "\n",
    "    \"batch_size\": 256,\n",
    "    \"accum_steps\": 2,\n",
    "}\n",
    "\n",
    "CFG_STRONG = {\n",
    "    **CFG_SAFE,\n",
    "    \"d_model\": 384,\n",
    "    \"n_layers\": 8,\n",
    "    \"dropout\": 0.16,\n",
    "    \"epochs\": 80,\n",
    "    \"lr\": 2e-4,\n",
    "    \"weight_decay\": 7e-2,\n",
    "    \"feat_token_drop_p\": 0.06,\n",
    "    \"input_noise_std\": 0.012,\n",
    "    \"focal_gamma\": 1.5,\n",
    "}\n",
    "\n",
    "# auto select by device/memory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "\n",
    "CFG = dict(CFG_SAFE)\n",
    "CFG_NAME = \"SAFE\"\n",
    "if device.type != \"cuda\":\n",
    "    CFG = dict(CFG_CPU)\n",
    "    CFG_NAME = \"CPU\"\n",
    "else:\n",
    "    try:\n",
    "        mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        if mem_gb >= 30:\n",
    "            CFG = dict(CFG_STRONG)\n",
    "            CFG_NAME = \"STRONG\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# calibration preference from TRAIN_PLAN if available\n",
    "if \"TRAIN_PLAN\" in globals() and isinstance(TRAIN_PLAN, dict):\n",
    "    if CFG.get(\"use_calibration\", True):\n",
    "        CFG[\"use_calibration\"] = bool(TRAIN_PLAN.get(\"use_calibration\", True))\n",
    "        CFG[\"calibration\"] = TRAIN_PLAN.get(\"calibration\", CFG.get(\"calibration\", None))\n",
    "# default calibration\n",
    "if CFG.get(\"use_calibration\", True) and (CFG.get(\"calibration\") is None):\n",
    "    CFG[\"calibration\"] = \"isotonic\"  # default terbaik untuk tabular probs\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Seed + device optim\n",
    "# ----------------------------\n",
    "def seed_everything(seed: int = 2025):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(int(CFG[\"seed\"]))\n",
    "\n",
    "print(\"Device:\", device, \"| AMP:\", use_amp, \"| CFG:\", CFG_NAME)\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build arrays + guard\n",
    "# ----------------------------\n",
    "X = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "y = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n",
    "folds = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\n",
    "\n",
    "if not np.isfinite(X).all():\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "n = len(df_train_tabular)\n",
    "unique_folds = sorted(pd.Series(folds).unique().tolist())\n",
    "n_folds = len(unique_folds)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "print(\"Setup:\")\n",
    "print(\"  rows      :\", n)\n",
    "print(\"  folds     :\", n_folds, \"|\", unique_folds)\n",
    "print(\"  pos%      :\", float(y.mean()) * 100.0)\n",
    "print(\"  n_features:\", n_features)\n",
    "if n_folds < 2:\n",
    "    raise ValueError(\"Need >=2 folds. Check cv_case_folds.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset\n",
    "# ----------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Normalization (leakage-safe)\n",
    "# ----------------------------\n",
    "def fit_standardizer(X_tr: np.ndarray):\n",
    "    mu = X_tr.mean(axis=0, dtype=np.float64)\n",
    "    sig = X_tr.std(axis=0, dtype=np.float64)\n",
    "    sig = np.where(sig < 1e-8, 1.0, sig)\n",
    "    return mu.astype(np.float32), sig.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n",
    "    return ((X_in - mu) / sig).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Metrics helpers\n",
    "# ----------------------------\n",
    "def safe_auc(y_true, p):\n",
    "    y_true = np.asarray(y_true)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return None\n",
    "    return float(roc_auc_score(y_true, p))\n",
    "\n",
    "def safe_logloss(y_true, p):\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-6, 1-1e-6)\n",
    "    return float(log_loss(y_true, p, labels=[0, 1]))\n",
    "\n",
    "def fbeta_np(y_true, yhat, beta=1.0):\n",
    "    # y_true,yhat in {0,1}\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    yhat = np.asarray(yhat).astype(int)\n",
    "    tp = int(((y_true == 1) & (yhat == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (yhat == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (yhat == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    b2 = beta * beta\n",
    "    return float((1 + b2) * tp / max(1e-12, (1 + b2) * tp + b2 * fn + fp))\n",
    "\n",
    "def find_best_threshold(y_true, p, n_grid=801, objective=\"f0.5\"):\n",
    "    \"\"\"\n",
    "    objective: f1 / f0.5 / f2\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    p = np.asarray(p).astype(np.float64)\n",
    "    if objective == \"f2\":\n",
    "        beta = 2.0\n",
    "    elif objective == \"f1\":\n",
    "        beta = 1.0\n",
    "    else:\n",
    "        beta = 0.5\n",
    "\n",
    "    best = {\"thr\": 0.5, \"score\": -1.0, \"f1\": 0.0, \"f05\": 0.0, \"f2\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    for thr in np.linspace(0.0, 1.0, int(n_grid)):\n",
    "        yh = (p >= thr).astype(int)\n",
    "        sc = fbeta_np(y_true, yh, beta=beta)\n",
    "        if sc > best[\"score\"]:\n",
    "            best[\"thr\"] = float(thr)\n",
    "            best[\"score\"] = float(sc)\n",
    "            best[\"f1\"] = float(f1_score(y_true, yh, zero_division=0))\n",
    "            best[\"f05\"] = float(fbeta_np(y_true, yh, beta=0.5))\n",
    "            best[\"f2\"] = float(fbeta_np(y_true, yh, beta=2.0))\n",
    "            best[\"precision\"] = float(precision_score(y_true, yh, zero_division=0))\n",
    "            best[\"recall\"] = float(recall_score(y_true, yh, zero_division=0))\n",
    "    best[\"objective\"] = str(objective)\n",
    "    return best\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Sinkhorn projection\n",
    "# ----------------------------\n",
    "def sinkhorn_doubly_stochastic(logits: torch.Tensor, tmax: int = 20, eps: float = 1e-6):\n",
    "    z = logits - logits.max()\n",
    "    M = torch.exp(z)\n",
    "    for _ in range(int(tmax)):\n",
    "        M = M / (M.sum(dim=-1, keepdim=True) + eps)\n",
    "        M = M / (M.sum(dim=-2, keepdim=True) + eps)\n",
    "    return M\n",
    "\n",
    "# ----------------------------\n",
    "# 8) RMSNorm\n",
    "# ----------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()\n",
    "        return x * rms * self.weight\n",
    "\n",
    "# ----------------------------\n",
    "# 9) EMA helper\n",
    "# ----------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = float(decay)\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        d = self.decay\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            self.backup[name] = p.detach().clone()\n",
    "            p.copy_(self.shadow[name])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, model: nn.Module):\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            p.copy_(self.backup[name])\n",
    "        self.backup = {}\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Model blocks (feature token dropout)\n",
    "# ----------------------------\n",
    "class MHCAttnBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ffn_mult, dropout, attn_dropout,\n",
    "                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01):\n",
    "        super().__init__()\n",
    "        self.n_streams = int(n_streams)\n",
    "        self.sinkhorn_tmax = int(sinkhorn_tmax)\n",
    "\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=attn_dropout, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ffn_mult * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_mult * d_model, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.h_logits = nn.Parameter(torch.zeros(self.n_streams, self.n_streams))\n",
    "        nn.init.zeros_(self.h_logits)\n",
    "\n",
    "        a0 = float(alpha_init)\n",
    "        a0 = min(max(a0, 1e-4), 1 - 1e-4)\n",
    "        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, streams):\n",
    "        # streams: (B, n_streams, S, D)\n",
    "        B, nS, S, D = streams.shape\n",
    "\n",
    "        x = streams[:, 0]  # (B,S,D)\n",
    "\n",
    "        x0 = x\n",
    "        q = self.norm1(x)\n",
    "        attn_out, _ = self.attn(q, q, q, need_weights=False)\n",
    "        x = x0 + self.drop1(attn_out)\n",
    "\n",
    "        x1 = x\n",
    "        h = self.norm2(x)\n",
    "        h = self.ffn(h)\n",
    "        x = x1 + self.drop2(h)\n",
    "\n",
    "        # minimal clone (avoid in-place on view)\n",
    "        streams = streams.clone()\n",
    "        streams[:, 0] = x\n",
    "\n",
    "        H = sinkhorn_doubly_stochastic(self.h_logits, tmax=self.sinkhorn_tmax)\n",
    "        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype)\n",
    "        I = torch.eye(self.n_streams, device=streams.device, dtype=streams.dtype)\n",
    "        Hres = (1.0 - alpha) * I + alpha * H.to(dtype=streams.dtype)\n",
    "\n",
    "        mixed = torch.einsum(\"ij,bjtd->bitd\", Hres, streams)\n",
    "        return mixed\n",
    "\n",
    "class MHCFTTransformer(nn.Module):\n",
    "    def __init__(self, n_features,\n",
    "                 d_model=256, n_heads=8, n_layers=6, ffn_mult=4,\n",
    "                 dropout=0.12, attn_dropout=0.08,\n",
    "                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01,\n",
    "                 feat_token_drop_p=0.0):\n",
    "        super().__init__()\n",
    "        self.n_features = int(n_features)\n",
    "        self.d_model = int(d_model)\n",
    "        self.feat_token_drop_p = float(feat_token_drop_p)\n",
    "\n",
    "        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n",
    "        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n",
    "        self.in_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MHCAttnBlock(\n",
    "                d_model=self.d_model,\n",
    "                n_heads=n_heads,\n",
    "                ffn_mult=ffn_mult,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout,\n",
    "                n_streams=n_streams,\n",
    "                sinkhorn_tmax=sinkhorn_tmax,\n",
    "                alpha_init=alpha_init\n",
    "            )\n",
    "            for _ in range(int(n_layers))\n",
    "        ])\n",
    "\n",
    "        self.out_norm = RMSNorm(self.d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.d_model, 1),\n",
    "        )\n",
    "\n",
    "        self.n_streams = int(n_streams)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,F)\n",
    "        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)   # (B,F,D)\n",
    "        tok = tok + self.feat_emb.unsqueeze(0)\n",
    "\n",
    "        # feature-token dropout (do not drop CLS)\n",
    "        if self.training and self.feat_token_drop_p > 0:\n",
    "            B, F, D = tok.shape\n",
    "            keep = (torch.rand(B, F, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n",
    "            tok = tok * keep.unsqueeze(-1)\n",
    "\n",
    "        B = tok.size(0)\n",
    "        cls = self.cls.expand(B, -1, -1)                                    # (B,1,D)\n",
    "        seq = torch.cat([cls, tok], dim=1)                                  # (B,1+F,D)\n",
    "        seq = self.in_drop(seq)\n",
    "\n",
    "        streams = seq.unsqueeze(1).repeat(1, self.n_streams, 1, 1)          # (B,nS,S,D)\n",
    "        for blk in self.blocks:\n",
    "            streams = blk(streams)\n",
    "\n",
    "        z = streams[:, 0, 0]\n",
    "        z = self.out_norm(z)\n",
    "        logit = self.head(z).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "# ----------------------------\n",
    "# 11) LR Scheduler\n",
    "# ----------------------------\n",
    "def make_warmup_step_scheduler(optimizer, total_steps: int, warmup_steps: int,\n",
    "                              milestones_frac=(0.8, 0.9), decay_values=(0.316, 0.1)):\n",
    "    m1 = int(float(milestones_frac[0]) * total_steps)\n",
    "    m2 = int(float(milestones_frac[1]) * total_steps)\n",
    "    d1 = float(decay_values[0])\n",
    "    d2 = float(decay_values[1])\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "        if step < m1:\n",
    "            return 1.0\n",
    "        elif step < m2:\n",
    "            return d1\n",
    "        else:\n",
    "            return d2\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Loss: BCE / Focal-BCE\n",
    "# ----------------------------\n",
    "def focal_bce_with_logits(logits, targets, pos_weight=None, gamma=0.0):\n",
    "    bce = F.binary_cross_entropy_with_logits(\n",
    "        logits, targets, reduction=\"none\", pos_weight=pos_weight\n",
    "    )\n",
    "    if gamma and gamma > 0:\n",
    "        p = torch.sigmoid(logits)\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)\n",
    "        mod = (1.0 - p_t).clamp_min(0.0).pow(gamma)\n",
    "        bce = bce * mod\n",
    "    return bce.mean()\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Predict helper (optionally with EMA)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, ema: EMA = None):\n",
    "    model.eval()\n",
    "    if ema is not None:\n",
    "        ema.apply_shadow(model)\n",
    "    probs = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(xb)\n",
    "            p = torch.sigmoid(logits)\n",
    "        probs.append(p.detach().cpu().numpy())\n",
    "    out = np.concatenate(probs, axis=0).astype(np.float32)\n",
    "    if ema is not None:\n",
    "        ema.restore(model)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Calibration (fold-wise; leakage-safe)\n",
    "#   - isotonic: store (x,y) knots -> apply via np.interp\n",
    "#   - sigmoid: store (a,b) for calibrated_logit = a*logit(p)+b\n",
    "# ----------------------------\n",
    "def _fit_isotonic(p, y, min_samples=200):\n",
    "    from sklearn.isotonic import IsotonicRegression\n",
    "    p = np.asarray(p, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "    ok = np.isfinite(p)\n",
    "    p = p[ok]; y = y[ok]\n",
    "    if len(p) < int(min_samples) or len(np.unique(y)) < 2:\n",
    "        return None\n",
    "    iso = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True, out_of_bounds=\"clip\")\n",
    "    iso.fit(p, y)\n",
    "    # store knots for portable inference\n",
    "    return {\"type\": \"isotonic\", \"x\": iso.X_thresholds_.astype(np.float32).tolist(),\n",
    "            \"y\": iso.y_thresholds_.astype(np.float32).tolist()}\n",
    "\n",
    "def _apply_isotonic(cal, p):\n",
    "    x = np.asarray(cal[\"x\"], dtype=np.float32)\n",
    "    yk = np.asarray(cal[\"y\"], dtype=np.float32)\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    return np.interp(p, x, yk).astype(np.float32)\n",
    "\n",
    "def _fit_sigmoid(p, y, min_samples=200):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    p = np.asarray(p, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "    ok = np.isfinite(p)\n",
    "    p = p[ok]; y = y[ok]\n",
    "    if len(p) < int(min_samples) or len(np.unique(y)) < 2:\n",
    "        return None\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    logit = np.log(p / (1 - p)).reshape(-1, 1)\n",
    "    lr = LogisticRegression(C=1000.0, solver=\"lbfgs\", max_iter=200)\n",
    "    lr.fit(logit, y)\n",
    "    a = float(lr.coef_.ravel()[0])\n",
    "    b = float(lr.intercept_.ravel()[0])\n",
    "    return {\"type\": \"sigmoid\", \"a\": a, \"b\": b}\n",
    "\n",
    "def _apply_sigmoid(cal, p):\n",
    "    p = np.asarray(p, dtype=np.float64)\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    logit = np.log(p / (1 - p))\n",
    "    z = cal[\"a\"] * logit + cal[\"b\"]\n",
    "    out = 1.0 / (1.0 + np.exp(-z))\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def fit_calibrator(calib_type, p_tr, y_tr, min_samples=200):\n",
    "    if calib_type is None or str(calib_type).lower() in [\"none\", \"off\", \"false\"]:\n",
    "        return None\n",
    "    calib_type = str(calib_type).lower()\n",
    "    if calib_type == \"isotonic\":\n",
    "        return _fit_isotonic(p_tr, y_tr, min_samples=min_samples)\n",
    "    elif calib_type in [\"sigmoid\", \"platt\"]:\n",
    "        return _fit_sigmoid(p_tr, y_tr, min_samples=min_samples)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def apply_calibrator(cal, p):\n",
    "    if cal is None:\n",
    "        return np.asarray(p, dtype=np.float32)\n",
    "    if cal[\"type\"] == \"isotonic\":\n",
    "        return _apply_isotonic(cal, p)\n",
    "    if cal[\"type\"] == \"sigmoid\":\n",
    "        return _apply_sigmoid(cal, p)\n",
    "    return np.asarray(p, dtype=np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 15) Train one fold (EMA + focal + noise + best_state + fold calibration)\n",
    "# ----------------------------\n",
    "def train_one_fold(X_tr, y_tr, X_va, y_va, cfg):\n",
    "    mu, sig = fit_standardizer(X_tr)\n",
    "    X_trn = apply_standardizer(X_tr, mu, sig)\n",
    "    X_van = apply_standardizer(X_va, mu, sig)\n",
    "\n",
    "    ds_tr = TabDataset(X_trn, y_tr)\n",
    "    ds_va = TabDataset(X_van, y_va)\n",
    "\n",
    "    # deterministic-ish loader config\n",
    "    cpu_cnt = os.cpu_count() or 2\n",
    "    nw = 2 if cpu_cnt >= 4 else 0\n",
    "    pin = (device.type == \"cuda\")\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n",
    "                       num_workers=nw, pin_memory=pin, drop_last=False,\n",
    "                       persistent_workers=(nw > 0))\n",
    "    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n",
    "                       num_workers=nw, pin_memory=pin, drop_last=False,\n",
    "                       persistent_workers=(nw > 0))\n",
    "    # for calibration: need train preds with shuffle False\n",
    "    dl_tr_eval = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n",
    "                            num_workers=nw, pin_memory=pin, drop_last=False,\n",
    "                            persistent_workers=(nw > 0))\n",
    "\n",
    "    model = MHCFTTransformer(\n",
    "        n_features=n_features,\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        ffn_mult=int(cfg[\"ffn_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        attn_dropout=float(cfg[\"attn_dropout\"]),\n",
    "        n_streams=int(cfg[\"n_streams\"]),\n",
    "        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n",
    "        alpha_init=float(cfg[\"alpha_init\"]),\n",
    "        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n",
    "    ).to(device)\n",
    "\n",
    "    # imbalance pos_weight\n",
    "    pos = int(y_tr.sum())\n",
    "    neg = int(len(y_tr) - pos)\n",
    "    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        betas=tuple(cfg[\"betas\"]),\n",
    "        eps=float(cfg[\"eps\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "    )\n",
    "\n",
    "    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    optim_steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n",
    "    total_optim_steps = int(cfg[\"epochs\"]) * max(1, optim_steps_per_epoch)\n",
    "    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n",
    "\n",
    "    sch = make_warmup_step_scheduler(\n",
    "        opt,\n",
    "        total_steps=total_optim_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        milestones_frac=cfg[\"lr_decay_milestones\"],\n",
    "        decay_values=cfg[\"lr_decay_values\"],\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n",
    "\n",
    "    best = {\"val_logloss\": 1e9, \"val_auc\": None, \"epoch\": -1}\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n",
    "    label_smooth = float(cfg.get(\"label_smoothing\", 0.0))\n",
    "    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n",
    "\n",
    "    for epoch in range(int(cfg[\"epochs\"])):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        micro_step = 0\n",
    "        optim_step_in_epoch = 0\n",
    "\n",
    "        for xb, yb in dl_tr:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            if input_noise_std and input_noise_std > 0:\n",
    "                xb = xb + torch.randn_like(xb) * input_noise_std\n",
    "\n",
    "            if label_smooth and label_smooth > 0:\n",
    "                yb = yb * (1.0 - label_smooth) + 0.5 * label_smooth\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss = focal_bce_with_logits(logits, yb, pos_weight=pos_weight, gamma=focal_gamma)\n",
    "                loss = loss / accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            micro_step += 1\n",
    "\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "            if (micro_step % accum_steps) == 0:\n",
    "                if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "                optim_step_in_epoch += 1\n",
    "\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        # flush last partial\n",
    "        if (micro_step % accum_steps) != 0:\n",
    "            if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            sch.step()\n",
    "            optim_step_in_epoch += 1\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        # validate (EMA-eval)\n",
    "        p_va_raw = predict_proba(model, dl_va, ema=ema)\n",
    "        vll = safe_logloss(y_va, p_va_raw)\n",
    "        vauc = safe_auc(y_va, p_va_raw)\n",
    "\n",
    "        tr_loss = loss_sum / max(1, n_sum)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"  epoch {epoch+1:03d}/{cfg['epochs']} | train_loss={tr_loss:.5f} | val_logloss={vll:.5f} | val_auc={vauc} | opt_steps={optim_step_in_epoch} | dt={dt:.1f}s\")\n",
    "\n",
    "        improved = (best[\"val_logloss\"] - vll) > float(cfg[\"early_stop_min_delta\"])\n",
    "        if improved:\n",
    "            best[\"val_logloss\"] = float(vll)\n",
    "            best[\"val_auc\"] = vauc\n",
    "            best[\"epoch\"] = int(epoch)\n",
    "\n",
    "            # save best EMA-weight (since eval used EMA)\n",
    "            if ema is not None:\n",
    "                ema.apply_shadow(model)\n",
    "                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "                ema.restore(model)\n",
    "            else:\n",
    "                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= int(cfg[\"early_stop_patience\"]):\n",
    "                print(f\"  early stop at epoch {epoch+1}, best_epoch={best['epoch']+1}, best_val_logloss={best['val_logloss']:.5f}\")\n",
    "                break\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # load best_state (already EMA weights if used)\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "\n",
    "    # predict RAW on train & val (best weights)\n",
    "    p_tr_raw = predict_proba(model, dl_tr_eval, ema=None)\n",
    "    p_va_raw = predict_proba(model, dl_va, ema=None)\n",
    "\n",
    "    # fit fold calibrator on TRAIN preds only (leakage-safe)\n",
    "    cal = None\n",
    "    p_va_cal = p_va_raw\n",
    "    if bool(cfg.get(\"use_calibration\", True)):\n",
    "        cal_type = cfg.get(\"calibration\", None)\n",
    "        cal = fit_calibrator(cal_type, p_tr_raw, y_tr, min_samples=int(cfg.get(\"calib_min_samples\", 200)))\n",
    "        if cal is not None:\n",
    "            p_va_cal = apply_calibrator(cal, p_va_raw)\n",
    "\n",
    "    pack = {\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": cfg,\n",
    "        \"best\": best,\n",
    "        \"calibrator\": cal,  # portable dict or None\n",
    "    }\n",
    "    return pack, p_va_raw, p_va_cal, best\n",
    "\n",
    "# ----------------------------\n",
    "# 16) Multi-seed CV loop\n",
    "# ----------------------------\n",
    "# auto N_SEEDS\n",
    "try:\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3) if device.type == \"cuda\" else 0\n",
    "except Exception:\n",
    "    mem_gb = 0\n",
    "\n",
    "N_SEEDS_ENV = os.environ.get(\"N_SEEDS\", \"\").strip()\n",
    "if N_SEEDS_ENV:\n",
    "    N_SEEDS = int(N_SEEDS_ENV)\n",
    "else:\n",
    "    N_SEEDS = 2 if (device.type == \"cuda\" and mem_gb >= 30) else 1\n",
    "\n",
    "SEEDS = [int(CFG[\"seed\"]) + i * 17 for i in range(max(1, N_SEEDS))]\n",
    "print(\"\\nSEED plan:\", SEEDS)\n",
    "\n",
    "out_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_seed_reports = []\n",
    "all_seed_oof_raw = []\n",
    "all_seed_oof_cal = []\n",
    "best_epochs_all = []\n",
    "\n",
    "for seed_i, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"== SEED {seed} ({seed_i+1}/{len(SEEDS)})\")\n",
    "    print(f\"==============================\")\n",
    "    seed_everything(seed)\n",
    "\n",
    "    oof_raw = np.zeros(n, dtype=np.float32)\n",
    "    oof_cal = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    fold_reports = []\n",
    "    best_epochs = []\n",
    "\n",
    "    models_dir = out_dir / f\"mhc_transformer_folds_seed_{seed}\"\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for f in unique_folds:\n",
    "        print(f\"\\n[Seed {seed} | Fold {f}]\")\n",
    "        tr_idx = np.where(folds != f)[0]\n",
    "        va_idx = np.where(folds == f)[0]\n",
    "\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "        pack, p_va_raw, p_va_cal, best = train_one_fold(X_tr, y_tr, X_va, y_va, CFG)\n",
    "\n",
    "        oof_raw[va_idx] = p_va_raw\n",
    "        oof_cal[va_idx] = p_va_cal\n",
    "        best_epochs.append(int(best[\"epoch\"] + 1))\n",
    "\n",
    "        # metrics raw\n",
    "        auc_raw = safe_auc(y_va, p_va_raw)\n",
    "        ll_raw  = safe_logloss(y_va, p_va_raw)\n",
    "\n",
    "        # metrics calibrated\n",
    "        auc_cal = safe_auc(y_va, p_va_cal)\n",
    "        ll_cal  = safe_logloss(y_va, p_va_cal)\n",
    "\n",
    "        # best threshold search for calibrated (recommended)\n",
    "        if bool(CFG.get(\"search_best_thr\", True)):\n",
    "            bt = find_best_threshold(y_va, p_va_cal, n_grid=int(CFG.get(\"thr_grid\", 801)),\n",
    "                                     objective=str(CFG.get(\"thr_objective\", \"f0.5\")).lower())\n",
    "            thr_use = float(bt[\"thr\"])\n",
    "        else:\n",
    "            thr_use = float(CFG.get(\"report_thr\", 0.5))\n",
    "            bt = None\n",
    "\n",
    "        yhat = (p_va_cal >= thr_use).astype(np.int32)\n",
    "\n",
    "        rep = {\n",
    "            \"seed\": int(seed),\n",
    "            \"fold\": int(f),\n",
    "            \"n_val\": int(len(va_idx)),\n",
    "            \"pos_val\": int(y_va.sum()),\n",
    "            \"auc_raw\": auc_raw,\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"auc_cal\": auc_cal,\n",
    "            \"logloss_cal\": ll_cal,\n",
    "            \"thr_used\": thr_use,\n",
    "            \"f1@thr\": float(f1_score(y_va, yhat, zero_division=0)),\n",
    "            \"f0.5@thr\": float(fbeta_np(y_va, yhat, beta=0.5)),\n",
    "            \"f2@thr\": float(fbeta_np(y_va, yhat, beta=2.0)),\n",
    "            \"precision@thr\": float(precision_score(y_va, yhat, zero_division=0)),\n",
    "            \"recall@thr\": float(recall_score(y_va, yhat, zero_division=0)),\n",
    "            \"best_val_logloss\": float(best[\"val_logloss\"]),\n",
    "            \"best_val_auc\": best[\"val_auc\"],\n",
    "            \"best_epoch\": int(best[\"epoch\"] + 1),\n",
    "            \"thr_search\": bt,\n",
    "            \"used_calibration\": bool(pack.get(\"calibrator\") is not None),\n",
    "            \"calibration_type\": (pack[\"calibrator\"][\"type\"] if pack.get(\"calibrator\") else None),\n",
    "        }\n",
    "        fold_reports.append(rep)\n",
    "\n",
    "        torch.save(\n",
    "            {\"pack\": pack, \"feature_cols\": FEATURE_COLS, \"seed\": int(seed), \"fold\": int(f)},\n",
    "            models_dir / f\"mhc_transformer_fold_{f}.pt\"\n",
    "        )\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # overall metrics for seed\n",
    "    oof_auc_raw = safe_auc(y, oof_raw)\n",
    "    oof_ll_raw  = safe_logloss(y, oof_raw)\n",
    "    oof_auc_cal = safe_auc(y, oof_cal)\n",
    "    oof_ll_cal  = safe_logloss(y, oof_cal)\n",
    "\n",
    "    bt_oof = None\n",
    "    best_thr = None\n",
    "    if bool(CFG.get(\"search_best_thr\", True)):\n",
    "        bt_oof = find_best_threshold(y, oof_cal, n_grid=int(CFG.get(\"thr_grid\", 801)),\n",
    "                                     objective=str(CFG.get(\"thr_objective\", \"f0.5\")).lower())\n",
    "        best_thr = float(bt_oof[\"thr\"])\n",
    "\n",
    "    # fixed thr baseline\n",
    "    thr_fixed = float(CFG.get(\"report_thr\", 0.5))\n",
    "    yhat_fixed = (oof_cal >= thr_fixed).astype(np.int32)\n",
    "\n",
    "    overall = {\n",
    "        \"seed\": int(seed),\n",
    "        \"rows\": int(n),\n",
    "        \"folds\": int(n_folds),\n",
    "        \"pos_total\": int(y.sum()),\n",
    "        \"pos_rate\": float(y.mean()),\n",
    "        \"oof_auc_raw\": oof_auc_raw,\n",
    "        \"oof_logloss_raw\": oof_ll_raw,\n",
    "        \"oof_auc_cal\": oof_auc_cal,\n",
    "        \"oof_logloss_cal\": oof_ll_cal,\n",
    "        f\"oof_f1@{thr_fixed}\": float(f1_score(y, yhat_fixed, zero_division=0)),\n",
    "        f\"oof_f0.5@{thr_fixed}\": float(fbeta_np(y, yhat_fixed, beta=0.5)),\n",
    "        f\"oof_precision@{thr_fixed}\": float(precision_score(y, yhat_fixed, zero_division=0)),\n",
    "        f\"oof_recall@{thr_fixed}\": float(recall_score(y, yhat_fixed, zero_division=0)),\n",
    "        \"oof_best_thr_cal\": best_thr,\n",
    "        \"oof_best_thr_detail\": bt_oof,\n",
    "        \"best_epochs\": best_epochs,\n",
    "    }\n",
    "\n",
    "    df_rep = pd.DataFrame(fold_reports).sort_values([\"seed\", \"fold\"]).reset_index(drop=True)\n",
    "    print(\"\\nPer-fold report:\")\n",
    "    display(df_rep)\n",
    "    print(\"\\nOOF overall (seed):\")\n",
    "    print(overall)\n",
    "\n",
    "    # save per-seed report\n",
    "    with open(out_dir / f\"mhc_transformer_cv_report_seed_{seed}.json\", \"w\") as f:\n",
    "        json.dump({\"cfg_name\": CFG_NAME, \"cfg\": CFG, \"fold_reports\": fold_reports, \"overall\": overall}, f, indent=2)\n",
    "\n",
    "    all_seed_reports.append({\"seed\": seed, \"fold_reports\": fold_reports, \"overall\": overall})\n",
    "    all_seed_oof_raw.append(oof_raw)\n",
    "    all_seed_oof_cal.append(oof_cal)\n",
    "    best_epochs_all.append(best_epochs)\n",
    "\n",
    "# ----------------------------\n",
    "# 17) Seed-ensemble OOF (avg)\n",
    "# ----------------------------\n",
    "OOF_PRED_MHC_RAW = np.mean(np.stack(all_seed_oof_raw, axis=0), axis=0).astype(np.float32)\n",
    "OOF_PRED_MHC_CAL = np.mean(np.stack(all_seed_oof_cal, axis=0), axis=0).astype(np.float32)\n",
    "\n",
    "ens_auc_raw = safe_auc(y, OOF_PRED_MHC_RAW)\n",
    "ens_ll_raw  = safe_logloss(y, OOF_PRED_MHC_RAW)\n",
    "ens_auc_cal = safe_auc(y, OOF_PRED_MHC_CAL)\n",
    "ens_ll_cal  = safe_logloss(y, OOF_PRED_MHC_CAL)\n",
    "\n",
    "thr_fixed = float(CFG.get(\"report_thr\", 0.5))\n",
    "yhat_fixed = (OOF_PRED_MHC_CAL >= thr_fixed).astype(np.int32)\n",
    "\n",
    "bt_ens = None\n",
    "ens_best_thr = None\n",
    "if bool(CFG.get(\"search_best_thr\", True)):\n",
    "    bt_ens = find_best_threshold(y, OOF_PRED_MHC_CAL, n_grid=int(CFG.get(\"thr_grid\", 801)),\n",
    "                                 objective=str(CFG.get(\"thr_objective\", \"f0.5\")).lower())\n",
    "    ens_best_thr = float(bt_ens[\"thr\"])\n",
    "\n",
    "BASELINE_MHC_TF_OVERALL = {\n",
    "    \"model\": \"mHC-FTTransformer (tabular gate) v3.0\",\n",
    "    \"cfg_name\": CFG_NAME,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"feature_count\": int(len(FEATURE_COLS)),\n",
    "    \"oof_auc_raw\": ens_auc_raw,\n",
    "    \"oof_logloss_raw\": ens_ll_raw,\n",
    "    \"oof_auc_cal\": ens_auc_cal,\n",
    "    \"oof_logloss_cal\": ens_ll_cal,\n",
    "    f\"oof_f1@{thr_fixed}\": float(f1_score(y, yhat_fixed, zero_division=0)),\n",
    "    f\"oof_f0.5@{thr_fixed}\": float(fbeta_np(y, yhat_fixed, beta=0.5)),\n",
    "    f\"oof_precision@{thr_fixed}\": float(precision_score(y, yhat_fixed, zero_division=0)),\n",
    "    f\"oof_recall@{thr_fixed}\": float(recall_score(y, yhat_fixed, zero_division=0)),\n",
    "    \"oof_best_thr_cal\": ens_best_thr,\n",
    "    \"oof_best_thr_detail\": bt_ens,\n",
    "}\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"OOF ENSEMBLE overall:\")\n",
    "print(BASELINE_MHC_TF_OVERALL)\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "# flatten fold reports for export\n",
    "BASELINE_MHC_TF_FOLD_REPORTS = []\n",
    "for sr in all_seed_reports:\n",
    "    BASELINE_MHC_TF_FOLD_REPORTS.extend(sr[\"fold_reports\"])\n",
    "\n",
    "BASELINE_MHC_TF_BEST_EPOCHS = [int(x) for xs in best_epochs_all for x in xs]\n",
    "\n",
    "# ----------------------------\n",
    "# 18) Train FULL model (epochs = median(best_epoch) * 1.15)\n",
    "#    (pakai seed pertama; fold model sudah disimpan per seed)\n",
    "# ----------------------------\n",
    "def train_full_fixed(X_full_raw, y_full, cfg, epochs_full: int, seed: int):\n",
    "    seed_everything(seed)\n",
    "    mu, sig = fit_standardizer(X_full_raw)\n",
    "    X_full = apply_standardizer(X_full_raw, mu, sig)\n",
    "\n",
    "    ds_full = TabDataset(X_full, y_full)\n",
    "    cpu_cnt = os.cpu_count() or 2\n",
    "    nw = 2 if cpu_cnt >= 4 else 0\n",
    "    pin = (device.type == \"cuda\")\n",
    "\n",
    "    dl_full = DataLoader(ds_full, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n",
    "                         num_workers=nw, pin_memory=pin, drop_last=False,\n",
    "                         persistent_workers=(nw > 0))\n",
    "\n",
    "    model = MHCFTTransformer(\n",
    "        n_features=n_features,\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        ffn_mult=int(cfg[\"ffn_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        attn_dropout=float(cfg[\"attn_dropout\"]),\n",
    "        n_streams=int(cfg[\"n_streams\"]),\n",
    "        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n",
    "        alpha_init=float(cfg[\"alpha_init\"]),\n",
    "        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n",
    "    ).to(device)\n",
    "\n",
    "    pos = int(y_full.sum())\n",
    "    neg = int(len(y_full) - pos)\n",
    "    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        betas=tuple(cfg[\"betas\"]),\n",
    "        eps=float(cfg[\"eps\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "    )\n",
    "\n",
    "    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    optim_steps_per_epoch = int(math.ceil(len(dl_full) / accum_steps))\n",
    "    total_optim_steps = int(epochs_full) * max(1, optim_steps_per_epoch)\n",
    "    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n",
    "\n",
    "    sch = make_warmup_step_scheduler(\n",
    "        opt,\n",
    "        total_steps=total_optim_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        milestones_frac=cfg[\"lr_decay_milestones\"],\n",
    "        decay_values=cfg[\"lr_decay_values\"],\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n",
    "\n",
    "    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n",
    "    label_smooth = float(cfg.get(\"label_smoothing\", 0.0))\n",
    "    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n",
    "\n",
    "    print(f\"\\nTraining FULL mHC transformer for {epochs_full} epochs | seed={seed} ...\")\n",
    "\n",
    "    for epoch in range(int(epochs_full)):\n",
    "        model.train()\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        micro_step = 0\n",
    "\n",
    "        for xb, yb in dl_full:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            if input_noise_std and input_noise_std > 0:\n",
    "                xb = xb + torch.randn_like(xb) * input_noise_std\n",
    "\n",
    "            if label_smooth and label_smooth > 0:\n",
    "                yb = yb * (1.0 - label_smooth) + 0.5 * label_smooth\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss = focal_bce_with_logits(logits, yb, pos_weight=pos_weight, gamma=focal_gamma)\n",
    "                loss = loss / accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            micro_step += 1\n",
    "\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "            if (micro_step % accum_steps) == 0:\n",
    "                if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        if (micro_step % accum_steps) != 0:\n",
    "            if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            sch.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        print(f\"  full epoch {epoch+1:03d}/{epochs_full} | loss={loss_sum/max(1,n_sum):.5f}\")\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # save EMA weights if used\n",
    "    used_ema = bool(ema is not None)\n",
    "    if ema is not None:\n",
    "        ema.apply_shadow(model)\n",
    "\n",
    "    full_pack = {\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": cfg,\n",
    "        \"epochs_full\": int(epochs_full),\n",
    "        \"used_ema\": used_ema,\n",
    "        \"seed\": int(seed),\n",
    "        \"recommended_thr\": BASELINE_MHC_TF_OVERALL.get(\"oof_best_thr_cal\", None),\n",
    "        \"recommended_thr_detail\": BASELINE_MHC_TF_OVERALL.get(\"oof_best_thr_detail\", None),\n",
    "    }\n",
    "\n",
    "    if ema is not None:\n",
    "        ema.restore(model)\n",
    "\n",
    "    return full_pack\n",
    "\n",
    "# decide epochs_full from all folds, all seeds (median)\n",
    "flat_best_epochs = np.array(BASELINE_MHC_TF_BEST_EPOCHS, dtype=np.int32)\n",
    "med_best = int(np.median(flat_best_epochs)) if len(flat_best_epochs) else int(max(12, CFG[\"epochs\"] * 0.7))\n",
    "epochs_full = int(max(12, round(med_best * 1.15)))\n",
    "epochs_full = int(min(epochs_full, int(CFG[\"epochs\"])))  # safety\n",
    "\n",
    "full_pack = train_full_fixed(X, y, CFG, epochs_full=epochs_full, seed=int(SEEDS[0]))\n",
    "\n",
    "FULL_PACK_PATH = str(out_dir / \"mhc_transformer_model_full.pt\")\n",
    "torch.save({\"pack\": full_pack, \"feature_cols\": FEATURE_COLS}, FULL_PACK_PATH)\n",
    "\n",
    "# ----------------------------\n",
    "# 19) Save OOF + report\n",
    "# ----------------------------\n",
    "df_oof = df_train_tabular[[\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]].copy()\n",
    "df_oof[\"oof_pred_mhc_raw\"] = OOF_PRED_MHC_RAW\n",
    "df_oof[\"oof_pred_mhc_cal\"] = OOF_PRED_MHC_CAL\n",
    "df_oof.to_csv(out_dir / \"oof_mhc_transformer.csv\", index=False)\n",
    "\n",
    "np.save(out_dir / \"oof_pred_mhc_raw.npy\", OOF_PRED_MHC_RAW)\n",
    "np.save(out_dir / \"oof_pred_mhc_cal.npy\", OOF_PRED_MHC_CAL)\n",
    "\n",
    "report = {\n",
    "    \"model\": \"mHC-FTTransformer (numeric tabular gate) v3.0\",\n",
    "    \"cfg_name\": CFG_NAME,\n",
    "    \"cfg\": CFG,\n",
    "    \"feature_count\": int(len(FEATURE_COLS)),\n",
    "    \"seeds\": SEEDS,\n",
    "    \"seed_reports\": all_seed_reports,\n",
    "    \"ensemble_overall\": BASELINE_MHC_TF_OVERALL,\n",
    "    \"epochs_full\": int(epochs_full),\n",
    "    \"full_pack_path\": FULL_PACK_PATH,\n",
    "}\n",
    "with open(out_dir / \"mhc_transformer_cv_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\"  fold models  ->\", out_dir, \"(mhc_transformer_folds_seed_*/)\")\n",
    "print(\"  full model   ->\", FULL_PACK_PATH)\n",
    "print(\"  oof preds    ->\", out_dir / \"oof_mhc_transformer.csv\")\n",
    "print(\"  oof npy      ->\", out_dir / \"oof_pred_mhc_raw.npy\", \"and\", out_dir / \"oof_pred_mhc_cal.npy\")\n",
    "print(\"  cv report    ->\", out_dir / \"mhc_transformer_cv_report.json\")\n",
    "\n",
    "# Export globals\n",
    "globals().update({\n",
    "    \"OOF_PRED_MHC_RAW\": OOF_PRED_MHC_RAW,\n",
    "    \"OOF_PRED_MHC_CAL\": OOF_PRED_MHC_CAL,\n",
    "    \"BASELINE_MHC_TF_OVERALL\": BASELINE_MHC_TF_OVERALL,\n",
    "    \"BASELINE_MHC_TF_FOLD_REPORTS\": BASELINE_MHC_TF_FOLD_REPORTS,\n",
    "    \"BASELINE_MHC_TF_BEST_EPOCHS\": BASELINE_MHC_TF_BEST_EPOCHS,\n",
    "    \"FULL_PACK_PATH\": FULL_PACK_PATH,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb0630",
   "metadata": {
    "papermill": {
     "duration": 0.012916,
     "end_time": "2026-01-08T11:50:58.591786",
     "exception": false,
     "start_time": "2026-01-08T11:50:58.578870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimize Model & Hyperparameters (Iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c0ca1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T11:50:58.620456Z",
     "iopub.status.busy": "2026-01-08T11:50:58.620182Z",
     "iopub.status.idle": "2026-01-08T14:46:49.928308Z",
     "shell.execute_reply": "2026-01-08T14:46:49.927305Z"
    },
    "papermill": {
     "duration": 10551.325174,
     "end_time": "2026-01-08T14:46:49.929905",
     "exception": false,
     "start_time": "2026-01-08T11:50:58.604731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize setup (Transformer-only, mHC-lite):\n",
      "  rows=5176 | folds=5 | pos%=54.08 | n_features=84\n",
      "Device: cuda | AMP: True\n",
      "GPU mem GB: 15.887939453125\n",
      "\n",
      "Total Transformer candidates: 4\n",
      "Primary score: OOF best F-beta (beta=0.5)\n",
      "\n",
      "Stage-1 folds subset: [0, 2, 4]\n",
      "\n",
      "[Stage-1 01/4] CV(subset) -> mhc_384x8_main\n",
      "  stage1 best_fbeta: 1.000000 | thr: 0.412 | logloss: 0.151695\n",
      "\n",
      "[Stage-1 02/4] CV(subset) -> mhc_384x10_reg\n",
      "  stage1 best_fbeta: 1.000000 | thr: 0.382 | logloss: 0.126817\n",
      "\n",
      "[Stage-1 03/4] CV(subset) -> mhc_384x8_ffn2\n",
      "  stage1 best_fbeta: 1.000000 | thr: 0.343 | logloss: 0.175593\n",
      "\n",
      "[Stage-1 04/4] CV(subset) -> mhc_256x6_fast\n",
      "  stage1 best_fbeta: 1.000000 | thr: 0.235 | logloss: 0.149304\n",
      "\n",
      "Stage-1 ranking (top):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cfg</th>\n",
       "      <th>stage</th>\n",
       "      <th>oof_auc</th>\n",
       "      <th>oof_logloss</th>\n",
       "      <th>oof_best_fbeta</th>\n",
       "      <th>oof_best_thr</th>\n",
       "      <th>oof_best_prec</th>\n",
       "      <th>oof_best_rec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>...</th>\n",
       "      <th>adam_eps</th>\n",
       "      <th>lr_decay_ratio1</th>\n",
       "      <th>lr_decay_ratio2</th>\n",
       "      <th>lr_decay_rate1</th>\n",
       "      <th>lr_decay_rate2</th>\n",
       "      <th>patience</th>\n",
       "      <th>min_delta</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>use_ema</th>\n",
       "      <th>ema_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mhc_384x10_reg</td>\n",
       "      <td>subset3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.126817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mhc_256x6_fast</td>\n",
       "      <td>subset3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.149304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mhc_384x8_main</td>\n",
       "      <td>subset3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.151695</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mhc_384x8_ffn2</td>\n",
       "      <td>subset3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cfg    stage  oof_auc  oof_logloss  oof_best_fbeta  \\\n",
       "0  mhc_384x10_reg  subset3      1.0     0.126817             1.0   \n",
       "1  mhc_256x6_fast  subset3      1.0     0.149304             1.0   \n",
       "2  mhc_384x8_main  subset3      1.0     0.151695             1.0   \n",
       "3  mhc_384x8_ffn2  subset3      1.0     0.175593             1.0   \n",
       "\n",
       "   oof_best_thr  oof_best_prec  oof_best_rec  d_model  n_layers  ...  \\\n",
       "0        0.3824            1.0           1.0      384        10  ...   \n",
       "1        0.2354            1.0           1.0      256         6  ...   \n",
       "2        0.4118            1.0           1.0      384         8  ...   \n",
       "3        0.3432            1.0           1.0      384         8  ...   \n",
       "\n",
       "       adam_eps  lr_decay_ratio1  lr_decay_ratio2  lr_decay_rate1  \\\n",
       "0  1.000000e-08              0.8              0.9           0.316   \n",
       "1  1.000000e-08              0.8              0.9           0.316   \n",
       "2  1.000000e-08              0.8              0.9           0.316   \n",
       "3  1.000000e-08              0.8              0.9           0.316   \n",
       "\n",
       "   lr_decay_rate2  patience  min_delta  grad_clip  use_ema  ema_decay  \n",
       "0             0.1         6     0.0001        1.0     True      0.999  \n",
       "1             0.1         6     0.0001        1.0     True      0.999  \n",
       "2             0.1         6     0.0001        1.0     True      0.999  \n",
       "3             0.1         6     0.0001        1.0     True      0.999  \n",
       "\n",
       "[4 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage-2 will run full CV for: ['mhc_384x10_reg', 'mhc_256x6_fast', 'mhc_384x8_main']\n",
      "\n",
      "[Stage-2 01/3] CV(full) -> mhc_384x10_reg\n",
      "  OOF best_fbeta: 1.000000 | thr: 0.074 | auc: 1.000000 | logloss: 0.030143\n",
      "\n",
      "[Stage-2 02/3] CV(full) -> mhc_256x6_fast\n",
      "  OOF best_fbeta: 1.000000 | thr: 0.201 | auc: 1.000000 | logloss: 0.096908\n",
      "\n",
      "[Stage-2 03/3] CV(full) -> mhc_384x8_main\n",
      "  OOF best_fbeta: 1.000000 | thr: 0.294 | auc: 1.000000 | logloss: 0.064275\n",
      "\n",
      "Stage-2 top candidates (full CV):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cfg</th>\n",
       "      <th>stage</th>\n",
       "      <th>oof_auc</th>\n",
       "      <th>oof_logloss</th>\n",
       "      <th>oof_best_fbeta</th>\n",
       "      <th>oof_best_thr</th>\n",
       "      <th>oof_best_prec</th>\n",
       "      <th>oof_best_rec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>...</th>\n",
       "      <th>adam_eps</th>\n",
       "      <th>lr_decay_ratio1</th>\n",
       "      <th>lr_decay_ratio2</th>\n",
       "      <th>lr_decay_rate1</th>\n",
       "      <th>lr_decay_rate2</th>\n",
       "      <th>patience</th>\n",
       "      <th>min_delta</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>use_ema</th>\n",
       "      <th>ema_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mhc_384x10_reg</td>\n",
       "      <td>full</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mhc_384x8_main</td>\n",
       "      <td>full</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.064275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mhc_256x6_fast</td>\n",
       "      <td>full</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cfg stage  oof_auc  oof_logloss  oof_best_fbeta  oof_best_thr  \\\n",
       "0  mhc_384x10_reg  full      1.0     0.030143             1.0        0.0737   \n",
       "1  mhc_384x8_main  full      1.0     0.064275             1.0        0.2942   \n",
       "2  mhc_256x6_fast  full      1.0     0.096908             1.0        0.2011   \n",
       "\n",
       "   oof_best_prec  oof_best_rec  d_model  n_layers  ...      adam_eps  \\\n",
       "0            1.0           1.0      384        10  ...  1.000000e-08   \n",
       "1            1.0           1.0      384         8  ...  1.000000e-08   \n",
       "2            1.0           1.0      256         6  ...  1.000000e-08   \n",
       "\n",
       "   lr_decay_ratio1  lr_decay_ratio2  lr_decay_rate1  lr_decay_rate2  patience  \\\n",
       "0              0.8              0.9           0.316             0.1        12   \n",
       "1              0.8              0.9           0.316             0.1        10   \n",
       "2              0.8              0.9           0.316             0.1         9   \n",
       "\n",
       "   min_delta  grad_clip  use_ema  ema_decay  \n",
       "0     0.0001        1.0     True      0.999  \n",
       "1     0.0001        1.0     True      0.999  \n",
       "2     0.0001        1.0     True      0.999  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved best artifacts:\n",
      "  best model (fold packs) -> /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt\n",
      "  best config             -> /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n",
      "  opt results             -> /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n",
      "  fold detail             -> /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n",
      "  stage1 cache            -> /kaggle/working/recodai_luc_gate_artifacts/opt_search/stage1_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 4 — Optimize Model & Hyperparameters (Iterative) — TRANSFORMER ONLY\n",
    "# REVISI FULL v3.2 (mHC-lite PDF-inspired, differentiable + EMA + accum + reg)\n",
    "#\n",
    "# Perbaikan v3.2 (dibanding draft kamu):\n",
    "# - Stage-1 folds subset dipilih benar-benar “evenly spaced” (bukan slicing raw)\n",
    "# - AMP/GradScaler aman: otomatis OFF di CPU (hindari warning/bug)\n",
    "# - Scheduler benar-benar on optimizer-steps (sudah), + total_steps guard\n",
    "# - Early-stop logging lebih jelas + cleanup lebih agresif\n",
    "# - Save best_gate_model.pt sekarang juga menyimpan recommended_thr + best_oof_score\n",
    "# - Save stage1_results.csv + bisa skip kandidat yang sudah ada (resume-safe)\n",
    "#\n",
    "# Primary score: OOF best F-beta (beta=0.5)\n",
    "#\n",
    "# Output:\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/stage1_results.csv\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.json\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/oof_preds_<cfg_name>.csv (top configs)\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt\n",
    "# ============================================================\n",
    "\n",
    "import os, json, gc, math, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require data from Step 2\n",
    "# ----------------------------\n",
    "need_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\n",
    "for v in need_vars:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n",
    "\n",
    "df_train_tabular = df_train_tabular.copy()\n",
    "FEATURE_COLS = list(FEATURE_COLS)\n",
    "\n",
    "X_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "y_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n",
    "folds_all = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\n",
    "uids_all = df_train_tabular[\"uid\"].astype(str).to_numpy()\n",
    "\n",
    "if not np.isfinite(X_all).all():\n",
    "    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "unique_folds = sorted(pd.Series(folds_all).unique().tolist())\n",
    "n = len(y_all)\n",
    "pos_rate = float(y_all.mean())\n",
    "n_features = X_all.shape[1]\n",
    "\n",
    "print(\"Optimize setup (Transformer-only, mHC-lite):\")\n",
    "print(f\"  rows={n} | folds={len(unique_folds)} | pos%={pos_rate*100:.2f} | n_features={n_features}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Global settings\n",
    "# ----------------------------\n",
    "SEED = 2025\n",
    "BETA = 0.5\n",
    "THR_GRID = 201\n",
    "\n",
    "# 2-stage runtime controls\n",
    "STAGE1_FOLDS = min(3, len(unique_folds))\n",
    "STAGE1_EPOCH_CAP = 35\n",
    "STAGE1_PAT_CAP = 6\n",
    "\n",
    "STAGE2_TOPM = 3\n",
    "REPORT_TOPK_OOF = 3\n",
    "\n",
    "# Optional time budget (0 = off)\n",
    "TIME_BUDGET_SEC = 0  # contoh: 2.5*60*60\n",
    "\n",
    "def seed_everything(seed=2025):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "print(\"Device:\", device, \"| AMP:\", use_amp)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def get_mem_gb():\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "MEM_GB = get_mem_gb()\n",
    "print(\"GPU mem GB:\", MEM_GB)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Metrics helpers (F-beta primary)\n",
    "# ----------------------------\n",
    "def best_fbeta_fast(y_true, p, beta=0.5, grid=201):\n",
    "    y = (np.asarray(y_true).astype(np.int32) == 1)\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1.0 - 1e-8)\n",
    "\n",
    "    thrs = np.linspace(0.01, 0.99, int(grid), dtype=np.float64)\n",
    "    pred = (p[:, None] >= thrs[None, :])\n",
    "\n",
    "    y1 = y[:, None]\n",
    "    tp = (pred & y1).sum(axis=0).astype(np.float64)\n",
    "    fp = (pred & (~y1)).sum(axis=0).astype(np.float64)\n",
    "    fn = (y.sum().astype(np.float64) - tp)\n",
    "\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n",
    "    recall    = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n",
    "\n",
    "    b2 = beta * beta\n",
    "    denom = (b2 * precision + recall)\n",
    "    fbeta = np.divide((1.0 + b2) * precision * recall, denom, out=np.zeros_like(precision), where=denom > 0)\n",
    "\n",
    "    j = int(np.argmax(fbeta))\n",
    "    return {\n",
    "        \"fbeta\": float(fbeta[j]),\n",
    "        \"thr\": float(thrs[j]),\n",
    "        \"precision\": float(precision[j]),\n",
    "        \"recall\": float(recall[j]),\n",
    "    }\n",
    "\n",
    "def safe_auc(y_true, p):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return None\n",
    "    return float(roc_auc_score(y_true, p))\n",
    "\n",
    "def safe_logloss(y_true, p):\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n",
    "    return float(log_loss(y_true, p, labels=[0, 1]))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset + Standardizer (no leakage)\n",
    "# ----------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def fit_standardizer(X_tr: np.ndarray):\n",
    "    mu = X_tr.mean(axis=0, dtype=np.float64)\n",
    "    sig = X_tr.std(axis=0, dtype=np.float64)\n",
    "    sig = np.where(sig < 1e-8, 1.0, sig)\n",
    "    return mu.astype(np.float32), sig.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n",
    "    return ((X_in - mu) / sig).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) RMSNorm + Differentiable Sinkhorn + EMA\n",
    "# ----------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = float(eps)\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.mean(x * x, dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(rms + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "def sinkhorn_knopp(P, tmax=20, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Differentiable Sinkhorn-Knopp\n",
    "    P: (B,n,n) non-negative\n",
    "    \"\"\"\n",
    "    M = P.clamp_min(eps)\n",
    "    for _ in range(int(tmax)):\n",
    "        M = M / (M.sum(dim=-1, keepdim=True).clamp_min(eps))  # row\n",
    "        M = M / (M.sum(dim=-2, keepdim=True).clamp_min(eps))  # col\n",
    "    return M\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = float(decay)\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        d = self.decay\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            self.backup[name] = p.detach().clone()\n",
    "            p.copy_(self.shadow[name])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, model: nn.Module):\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            p.copy_(self.backup[name])\n",
    "        self.backup = {}\n",
    "\n",
    "# ----------------------------\n",
    "# 5) mHC-lite on CLS streams\n",
    "# ----------------------------\n",
    "class MHCLite(nn.Module):\n",
    "    \"\"\"\n",
    "    Maintain n_streams for CLS only.\n",
    "    Build per-sample mixing matrix -> Sinkhorn -> residual blend with Identity.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_streams=4, alpha_init=0.01, tmax=20, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n = int(n_streams)\n",
    "        self.tmax = int(tmax)\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "\n",
    "        self.norm = RMSNorm(d_model, eps=1e-6)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, self.n * self.n),\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        a0 = float(alpha_init)\n",
    "        a0 = min(max(a0, 1e-4), 1.0 - 1e-4)\n",
    "        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, streams, cls_vec):\n",
    "        # streams: (B,n,D), cls_vec: (B,D)\n",
    "        B, n, D = streams.shape\n",
    "        h = self.norm(cls_vec)\n",
    "        logits = self.mlp(h).view(B, n, n)  # (B,n,n)\n",
    "\n",
    "        P = self.softplus(logits)           # non-negative\n",
    "        M = sinkhorn_knopp(P, tmax=self.tmax, eps=1e-6)\n",
    "\n",
    "        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype, device=streams.device)\n",
    "        I = torch.eye(n, device=streams.device, dtype=streams.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "        H = (1.0 - alpha) * I + alpha * M\n",
    "\n",
    "        mixed = torch.einsum(\"bij,bjd->bid\", H, streams)\n",
    "        injected = mixed + cls_vec.unsqueeze(1)\n",
    "        return self.drop(injected)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model, eps=1e-6)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, num_heads=int(n_heads),\n",
    "            dropout=float(attn_dropout), batch_first=True\n",
    "        )\n",
    "        self.drop1 = nn.Dropout(float(dropout))\n",
    "\n",
    "        self.norm2 = RMSNorm(d_model, eps=1e-6)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, int(ffn_mult) * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "            nn.Linear(int(ffn_mult) * d_model, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(float(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.norm1(x)\n",
    "        attn_out, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = x + self.drop1(attn_out)\n",
    "\n",
    "        h = self.norm2(x)\n",
    "        x = x + self.drop2(self.ffn(h))\n",
    "        return x\n",
    "\n",
    "class FTTransformer_MHCLite(nn.Module):\n",
    "    \"\"\"\n",
    "    Numeric FT-Transformer + CLS-stream mHC-lite between blocks.\n",
    "    Regularizer: feature-token drop (zero some feature tokens, not CLS).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n",
    "                 dropout=0.2, attn_dropout=0.1,\n",
    "                 n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n",
    "                 feat_token_drop_p=0.0):\n",
    "        super().__init__()\n",
    "        self.n_features = int(n_features)\n",
    "        self.d_model = int(d_model)\n",
    "        self.n_layers = int(n_layers)\n",
    "        self.feat_token_drop_p = float(feat_token_drop_p)\n",
    "\n",
    "        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n",
    "        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n",
    "        self.in_drop = nn.Dropout(float(dropout))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=self.d_model,\n",
    "                n_heads=n_heads,\n",
    "                ffn_mult=ffn_mult,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout\n",
    "            ) for _ in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.mhc = nn.ModuleList([\n",
    "            MHCLite(\n",
    "                d_model=self.d_model,\n",
    "                n_streams=n_streams,\n",
    "                alpha_init=alpha_init,\n",
    "                tmax=sinkhorn_tmax,\n",
    "                dropout=mhc_dropout\n",
    "            ) for _ in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.out_norm = RMSNorm(self.d_model, eps=1e-6)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "            nn.Linear(self.d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n",
    "        tok = tok + self.feat_emb.unsqueeze(0)\n",
    "\n",
    "        if self.training and self.feat_token_drop_p > 0:\n",
    "            B, F_, D = tok.shape\n",
    "            keep = (torch.rand(B, F_, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n",
    "            tok = tok * keep.unsqueeze(-1)\n",
    "\n",
    "        B = tok.size(0)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        seq = torch.cat([cls, tok], dim=1)\n",
    "        seq = self.in_drop(seq)\n",
    "\n",
    "        nS = self.mhc[0].n\n",
    "        streams = seq[:, 0, :].unsqueeze(1).expand(B, nS, self.d_model).contiguous()\n",
    "\n",
    "        for l, blk in enumerate(self.blocks):\n",
    "            cls_in = streams.mean(dim=1).unsqueeze(1)\n",
    "            seq = torch.cat([cls_in, seq[:, 1:, :]], dim=1)\n",
    "\n",
    "            seq = blk(seq)\n",
    "            cls_vec = seq[:, 0, :]\n",
    "\n",
    "            streams = self.mhc[l](streams, cls_vec)\n",
    "\n",
    "        out = self.out_norm(streams.mean(dim=1))\n",
    "        logit = self.head(out).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Scheduler (optimizer-steps)\n",
    "# ----------------------------\n",
    "def make_warmup_step_scheduler(optimizer, total_steps, warmup_steps,\n",
    "                              r1=0.8, r2=0.9, d1=0.316, d2=0.1):\n",
    "    total_steps = int(max(1, total_steps))\n",
    "    warmup_steps = int(max(0, min(warmup_steps, total_steps)))\n",
    "\n",
    "    m1 = int(float(r1) * total_steps)\n",
    "    m2 = int(float(r2) * total_steps)\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if warmup_steps > 0 and step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "        mult = 1.0\n",
    "        if step >= m1:\n",
    "            mult *= float(d1)\n",
    "        if step >= m2:\n",
    "            mult *= float(d2)\n",
    "        return mult\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Predict helper\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, ema: EMA = None):\n",
    "    model.eval()\n",
    "    if ema is not None:\n",
    "        ema.apply_shadow(model)\n",
    "\n",
    "    probs = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                logits = model(xb)\n",
    "                p = torch.sigmoid(logits)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            p = torch.sigmoid(logits)\n",
    "        probs.append(p.detach().cpu().numpy())\n",
    "\n",
    "    out = np.concatenate(probs, axis=0).astype(np.float32)\n",
    "    if ema is not None:\n",
    "        ema.restore(model)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Train one fold\n",
    "# ----------------------------\n",
    "def train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg):\n",
    "    mu, sig = fit_standardizer(X_tr)\n",
    "    X_trn = apply_standardizer(X_tr, mu, sig)\n",
    "    X_van = apply_standardizer(X_va, mu, sig)\n",
    "\n",
    "    ds_tr = TabDataset(X_trn, y_tr)\n",
    "    ds_va = TabDataset(X_van, y_va)\n",
    "\n",
    "    cpu_cnt = os.cpu_count() or 2\n",
    "    nw = 2 if cpu_cnt >= 4 else 0\n",
    "    pin = (device.type == \"cuda\")\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n",
    "                       num_workers=nw, pin_memory=pin, drop_last=False,\n",
    "                       persistent_workers=(nw > 0))\n",
    "    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n",
    "                       num_workers=nw, pin_memory=pin, drop_last=False,\n",
    "                       persistent_workers=(nw > 0))\n",
    "\n",
    "    model = FTTransformer_MHCLite(\n",
    "        n_features=n_features,\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        ffn_mult=int(cfg[\"ffn_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        attn_dropout=float(cfg[\"attn_dropout\"]),\n",
    "        n_streams=int(cfg[\"n_streams\"]),\n",
    "        alpha_init=float(cfg[\"alpha_init\"]),\n",
    "        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n",
    "        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n",
    "        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n",
    "    ).to(device)\n",
    "\n",
    "    pos = int(y_tr.sum())\n",
    "    neg = int(len(y_tr) - pos)\n",
    "    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n",
    "\n",
    "    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n",
    "    label_smoothing = float(cfg.get(\"label_smoothing\", 0.0))\n",
    "    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n",
    "\n",
    "    def loss_fn(logits, targets):\n",
    "        if label_smoothing and label_smoothing > 0:\n",
    "            targets = targets * (1.0 - label_smoothing) + 0.5 * label_smoothing\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\", pos_weight=pos_weight)\n",
    "        if focal_gamma and focal_gamma > 0:\n",
    "            p = torch.sigmoid(logits)\n",
    "            p_t = p * targets + (1 - p) * (1 - targets)\n",
    "            mod = (1.0 - p_t).clamp_min(0.0).pow(focal_gamma)\n",
    "            bce = bce * mod\n",
    "        return bce.mean()\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n",
    "        eps=float(cfg[\"adam_eps\"]),\n",
    "    )\n",
    "\n",
    "    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n",
    "    total_steps = int(cfg[\"epochs\"]) * max(1, steps_per_epoch)\n",
    "    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n",
    "\n",
    "    sch = make_warmup_step_scheduler(\n",
    "        opt,\n",
    "        total_steps=total_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        r1=float(cfg[\"lr_decay_ratio1\"]),\n",
    "        r2=float(cfg[\"lr_decay_ratio2\"]),\n",
    "        d1=float(cfg[\"lr_decay_rate1\"]),\n",
    "        d2=float(cfg[\"lr_decay_rate2\"]),\n",
    "    )\n",
    "\n",
    "    if use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n",
    "\n",
    "    best_val = 1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    bad = 0\n",
    "    opt_step = 0\n",
    "\n",
    "    for epoch in range(int(cfg[\"epochs\"])):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "        micro = 0\n",
    "\n",
    "        for xb, yb in dl_tr:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            if input_noise_std and input_noise_std > 0:\n",
    "                xb = xb + torch.randn_like(xb) * input_noise_std\n",
    "\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    logits = model(xb)\n",
    "                    loss = loss_fn(logits, yb) / accum_steps\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb) / accum_steps\n",
    "                loss.backward()\n",
    "\n",
    "            micro += 1\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "            if (micro % accum_steps) == 0:\n",
    "                if float(cfg[\"grad_clip\"]) and float(cfg[\"grad_clip\"]) > 0:\n",
    "                    if use_amp:\n",
    "                        scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "\n",
    "                if use_amp:\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    opt.step()\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "                opt_step += 1\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        # flush last partial\n",
    "        if (micro % accum_steps) != 0:\n",
    "            if float(cfg[\"grad_clip\"]) and float(cfg[\"grad_clip\"]) > 0:\n",
    "                if use_amp:\n",
    "                    scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                opt.step()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            sch.step()\n",
    "            opt_step += 1\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        # validate with EMA (if enabled)\n",
    "        p_va = predict_proba(model, dl_va, ema=ema)\n",
    "        vll = safe_logloss(y_va, p_va)\n",
    "\n",
    "        improved = (best_val - vll) > float(cfg[\"min_delta\"])\n",
    "        if improved:\n",
    "            best_val = float(vll)\n",
    "            best_epoch = int(epoch)\n",
    "\n",
    "            if ema is not None:\n",
    "                ema.apply_shadow(model)\n",
    "                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "                ema.restore(model)\n",
    "            else:\n",
    "                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= int(cfg[\"patience\"]):\n",
    "                break\n",
    "\n",
    "        gc.collect()\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "\n",
    "    p_va = predict_proba(model, dl_va, ema=None)\n",
    "\n",
    "    pack = {\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": dict(cfg),\n",
    "        \"best_epoch\": int(best_epoch + 1),\n",
    "        \"best_val_logloss\": float(best_val),\n",
    "    }\n",
    "    return pack, p_va\n",
    "\n",
    "# ----------------------------\n",
    "# 9) CV evaluator for a config\n",
    "# ----------------------------\n",
    "def run_cv_config(cfg, cfg_name, folds_subset=None, beta=0.5, thr_grid=201):\n",
    "    oof = np.zeros(n, dtype=np.float32)\n",
    "    fold_rows = []\n",
    "    fold_packs = []\n",
    "\n",
    "    use_folds = unique_folds if folds_subset is None else list(folds_subset)\n",
    "\n",
    "    for f in use_folds:\n",
    "        tr = np.where(folds_all != f)[0]\n",
    "        va = np.where(folds_all == f)[0]\n",
    "\n",
    "        X_tr, y_tr = X_all[tr], y_all[tr]\n",
    "        X_va, y_va = X_all[va], y_all[va]\n",
    "\n",
    "        pack, p_va = train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg)\n",
    "        oof[va] = p_va\n",
    "\n",
    "        fold_auc = safe_auc(y_va, p_va)\n",
    "        fold_ll  = safe_logloss(y_va, p_va)\n",
    "        best_fold = best_fbeta_fast(y_va, p_va, beta=beta, grid=max(81, thr_grid//2))\n",
    "\n",
    "        fold_rows.append({\n",
    "            \"cfg\": cfg_name,\n",
    "            \"fold\": int(f),\n",
    "            \"n_val\": int(len(va)),\n",
    "            \"pos_val\": int(y_va.sum()),\n",
    "            \"auc\": fold_auc,\n",
    "            \"logloss\": fold_ll,\n",
    "            \"best_fbeta\": best_fold[\"fbeta\"],\n",
    "            \"best_thr\": best_fold[\"thr\"],\n",
    "            \"best_prec\": best_fold[\"precision\"],\n",
    "            \"best_rec\": best_fold[\"recall\"],\n",
    "            \"best_val_logloss\": float(pack[\"best_val_logloss\"]),\n",
    "            \"best_epoch\": int(pack[\"best_epoch\"]),\n",
    "        })\n",
    "\n",
    "        pack2 = dict(pack)\n",
    "        pack2[\"fold\"] = int(f)\n",
    "        fold_packs.append(pack2)\n",
    "\n",
    "        del pack\n",
    "        gc.collect()\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if folds_subset is None:\n",
    "        idx_eval = np.arange(n)\n",
    "    else:\n",
    "        idx_eval = np.where(np.isin(folds_all, np.array(use_folds)))[0]\n",
    "\n",
    "    oof_eval = oof[idx_eval]\n",
    "    y_eval = y_all[idx_eval]\n",
    "\n",
    "    oof_auc = safe_auc(y_eval, oof_eval)\n",
    "    oof_ll  = safe_logloss(y_eval, oof_eval)\n",
    "    best_oof = best_fbeta_fast(y_eval, oof_eval, beta=beta, grid=thr_grid)\n",
    "\n",
    "    summary = {\n",
    "        \"cfg\": cfg_name,\n",
    "        \"stage\": \"full\" if folds_subset is None else f\"subset{len(use_folds)}\",\n",
    "        \"oof_auc\": oof_auc,\n",
    "        \"oof_logloss\": oof_ll,\n",
    "        \"oof_best_fbeta\": best_oof[\"fbeta\"],\n",
    "        \"oof_best_thr\": best_oof[\"thr\"],\n",
    "        \"oof_best_prec\": best_oof[\"precision\"],\n",
    "        \"oof_best_rec\": best_oof[\"recall\"],\n",
    "\n",
    "        \"d_model\": cfg[\"d_model\"],\n",
    "        \"n_layers\": cfg[\"n_layers\"],\n",
    "        \"n_heads\": cfg[\"n_heads\"],\n",
    "        \"ffn_mult\": cfg[\"ffn_mult\"],\n",
    "        \"dropout\": cfg[\"dropout\"],\n",
    "        \"attn_dropout\": cfg[\"attn_dropout\"],\n",
    "\n",
    "        \"n_streams\": cfg[\"n_streams\"],\n",
    "        \"alpha_init\": cfg[\"alpha_init\"],\n",
    "        \"sinkhorn_tmax\": cfg[\"sinkhorn_tmax\"],\n",
    "        \"mhc_dropout\": cfg[\"mhc_dropout\"],\n",
    "\n",
    "        \"feat_token_drop_p\": cfg.get(\"feat_token_drop_p\", 0.0),\n",
    "        \"input_noise_std\": cfg.get(\"input_noise_std\", 0.0),\n",
    "        \"focal_gamma\": cfg.get(\"focal_gamma\", 0.0),\n",
    "        \"label_smoothing\": cfg.get(\"label_smoothing\", 0.0),\n",
    "\n",
    "        \"batch_size\": cfg[\"batch_size\"],\n",
    "        \"accum_steps\": cfg.get(\"accum_steps\", 1),\n",
    "        \"epochs\": cfg[\"epochs\"],\n",
    "        \"lr\": cfg[\"lr\"],\n",
    "        \"weight_decay\": cfg[\"weight_decay\"],\n",
    "        \"warmup_frac\": cfg[\"warmup_frac\"],\n",
    "        \"beta1\": cfg[\"beta1\"],\n",
    "        \"beta2\": cfg[\"beta2\"],\n",
    "        \"adam_eps\": cfg[\"adam_eps\"],\n",
    "        \"lr_decay_ratio1\": cfg[\"lr_decay_ratio1\"],\n",
    "        \"lr_decay_ratio2\": cfg[\"lr_decay_ratio2\"],\n",
    "        \"lr_decay_rate1\": cfg[\"lr_decay_rate1\"],\n",
    "        \"lr_decay_rate2\": cfg[\"lr_decay_rate2\"],\n",
    "        \"patience\": cfg[\"patience\"],\n",
    "        \"min_delta\": cfg[\"min_delta\"],\n",
    "        \"grad_clip\": cfg[\"grad_clip\"],\n",
    "        \"use_ema\": cfg.get(\"use_ema\", True),\n",
    "        \"ema_decay\": cfg.get(\"ema_decay\", 0.999),\n",
    "    }\n",
    "    return summary, fold_rows, oof, fold_packs\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Candidate configs\n",
    "# ----------------------------\n",
    "def make_base():\n",
    "    if device.type == \"cuda\":\n",
    "        if MEM_GB >= 30:\n",
    "            bs, acc = 512, 2\n",
    "        elif MEM_GB >= 16:\n",
    "            bs, acc = 384, 2\n",
    "        else:\n",
    "            bs, acc = 256, 2\n",
    "    else:\n",
    "        bs, acc = 256, 1\n",
    "\n",
    "    return dict(\n",
    "        batch_size=bs,\n",
    "        accum_steps=acc,\n",
    "        epochs=75 if device.type == \"cuda\" else 40,\n",
    "        lr=2e-4,\n",
    "        weight_decay=1.0e-2,\n",
    "        warmup_frac=0.10,\n",
    "        grad_clip=1.0,\n",
    "        patience=10,\n",
    "        min_delta=1e-4,\n",
    "\n",
    "        beta1=0.9,\n",
    "        beta2=0.95,\n",
    "        adam_eps=1e-8,\n",
    "\n",
    "        lr_decay_ratio1=0.8,\n",
    "        lr_decay_ratio2=0.9,\n",
    "        lr_decay_rate1=0.316,\n",
    "        lr_decay_rate2=0.1,\n",
    "\n",
    "        n_streams=4,\n",
    "        alpha_init=0.01,\n",
    "        sinkhorn_tmax=20,\n",
    "        mhc_dropout=0.00,\n",
    "\n",
    "        feat_token_drop_p=0.05,\n",
    "        input_noise_std=0.01,\n",
    "        focal_gamma=1.5,\n",
    "        label_smoothing=0.00,\n",
    "\n",
    "        use_ema=True,\n",
    "        ema_decay=0.999,\n",
    "    )\n",
    "\n",
    "BASE = make_base()\n",
    "\n",
    "candidates = []\n",
    "candidates.append((\"mhc_384x8_main\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=4, dropout=0.18, attn_dropout=0.10)))\n",
    "candidates.append((\"mhc_384x10_reg\", dict(BASE, d_model=384, n_layers=10, n_heads=8,  ffn_mult=4, dropout=0.24, attn_dropout=0.12,\n",
    "                                         lr=1.6e-4, weight_decay=1.5e-2, patience=12,\n",
    "                                         feat_token_drop_p=0.06, input_noise_std=0.012)))\n",
    "candidates.append((\"mhc_384x8_ffn2\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=2, dropout=0.16, attn_dropout=0.10,\n",
    "                                         lr=2.2e-4, weight_decay=8e-3)))\n",
    "candidates.append((\"mhc_256x6_fast\", dict(BASE, d_model=256, n_layers=6,  n_heads=8,  ffn_mult=4, dropout=0.16, attn_dropout=0.08,\n",
    "                                         lr=3e-4, weight_decay=6e-3, epochs=min(int(BASE[\"epochs\"]), 60), patience=9,\n",
    "                                         feat_token_drop_p=0.04, input_noise_std=0.010)))\n",
    "\n",
    "if device.type == \"cuda\" and MEM_GB >= 20:\n",
    "    candidates.append((\"mhc_512x10_big\", dict(BASE, d_model=512, n_layers=10, n_heads=16, ffn_mult=4, dropout=0.26, attn_dropout=0.14,\n",
    "                                             lr=1.2e-4, weight_decay=2.0e-2, epochs=max(int(BASE[\"epochs\"]), 85), patience=12,\n",
    "                                             mhc_dropout=0.05, feat_token_drop_p=0.06)))\n",
    "\n",
    "print(f\"\\nTotal Transformer candidates: {len(candidates)}\")\n",
    "print(\"Primary score: OOF best F-beta (beta=0.5)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Run 2-stage search\n",
    "# ----------------------------\n",
    "OUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OPT_DIR = OUT_DIR / \"opt_search\"\n",
    "OPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STAGE1_PATH = OPT_DIR / \"stage1_results.csv\"\n",
    "\n",
    "# stage-1 folds subset: evenly spaced indices\n",
    "if STAGE1_FOLDS >= len(unique_folds):\n",
    "    folds_subset = unique_folds\n",
    "else:\n",
    "    idxs = np.linspace(0, len(unique_folds) - 1, STAGE1_FOLDS)\n",
    "    idxs = np.round(idxs).astype(int)\n",
    "    idxs = np.unique(idxs).tolist()\n",
    "    folds_subset = [unique_folds[i] for i in idxs]\n",
    "    if len(folds_subset) < STAGE1_FOLDS:\n",
    "        # pad if collision happened\n",
    "        for f in unique_folds:\n",
    "            if f not in folds_subset:\n",
    "                folds_subset.append(f)\n",
    "            if len(folds_subset) >= STAGE1_FOLDS:\n",
    "                break\n",
    "\n",
    "print(\"\\nStage-1 folds subset:\", folds_subset)\n",
    "\n",
    "# resume-safe: load already evaluated stage1 cfg names\n",
    "done_stage1 = set()\n",
    "if STAGE1_PATH.exists():\n",
    "    try:\n",
    "        df_prev = pd.read_csv(STAGE1_PATH)\n",
    "        if \"cfg\" in df_prev.columns:\n",
    "            done_stage1 = set(df_prev[\"cfg\"].astype(str).tolist())\n",
    "            print(f\"Resume: found {len(done_stage1)} configs already in stage1_results.csv\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "t0 = time.time()\n",
    "stage1_rows = []\n",
    "stage1_fold_rows = []\n",
    "\n",
    "for i, (name, cfg) in enumerate(candidates, 1):\n",
    "    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n",
    "        print(\"Time budget reached. Stop search.\")\n",
    "        break\n",
    "\n",
    "    if name in done_stage1:\n",
    "        print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] SKIP (already done) -> {name}\")\n",
    "        continue\n",
    "\n",
    "    cfg1 = dict(cfg)\n",
    "    cfg1[\"epochs\"] = int(min(int(cfg1[\"epochs\"]), int(STAGE1_EPOCH_CAP)))\n",
    "    cfg1[\"patience\"] = int(min(int(cfg1[\"patience\"]), int(STAGE1_PAT_CAP)))\n",
    "\n",
    "    print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] CV(subset) -> {name}\")\n",
    "    summ, fold_rows, _, _ = run_cv_config(cfg1, name, folds_subset=folds_subset, beta=BETA, thr_grid=101)\n",
    "\n",
    "    stage1_rows.append(summ)\n",
    "    stage1_fold_rows.extend(fold_rows)\n",
    "\n",
    "    print(f\"  stage1 best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f} | logloss: {summ['oof_logloss']:.6f}\")\n",
    "\n",
    "    # append-progress to disk (resume-safe)\n",
    "    try:\n",
    "        df_append = pd.DataFrame([summ])\n",
    "        if STAGE1_PATH.exists():\n",
    "            df_old = pd.read_csv(STAGE1_PATH)\n",
    "            df_new = pd.concat([df_old, df_append], axis=0, ignore_index=True)\n",
    "        else:\n",
    "            df_new = df_append\n",
    "        df_new.to_csv(STAGE1_PATH, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# build stage1 ranking from disk (preferred) + current in-memory\n",
    "if STAGE1_PATH.exists():\n",
    "    df_s1 = pd.read_csv(STAGE1_PATH)\n",
    "else:\n",
    "    df_s1 = pd.DataFrame(stage1_rows)\n",
    "\n",
    "if len(df_s1) == 0:\n",
    "    raise RuntimeError(\"Stage-1 menghasilkan 0 hasil. Cek runtime/VRAM atau kecilkan kandidat/epochs.\")\n",
    "\n",
    "df_s1 = df_s1.sort_values([\"oof_best_fbeta\",\"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nStage-1 ranking (top):\")\n",
    "display(df_s1.head(10))\n",
    "\n",
    "topM = min(int(STAGE2_TOPM), len(df_s1))\n",
    "stage2_names = df_s1[\"cfg\"].head(topM).astype(str).tolist()\n",
    "print(\"\\nStage-2 will run full CV for:\", stage2_names)\n",
    "\n",
    "all_summaries = []\n",
    "all_fold_rows = []\n",
    "oof_store = {}\n",
    "pack_store = {}\n",
    "\n",
    "for j, nm in enumerate(stage2_names, 1):\n",
    "    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n",
    "        print(\"Time budget reached. Stop stage-2.\")\n",
    "        break\n",
    "\n",
    "    cfg = None\n",
    "    for (nname, ccfg) in candidates:\n",
    "        if nname == nm:\n",
    "            cfg = ccfg\n",
    "            break\n",
    "    if cfg is None:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[Stage-2 {j:02d}/{len(stage2_names)}] CV(full) -> {nm}\")\n",
    "    summ, fold_rows, oof, fold_packs = run_cv_config(cfg, nm, folds_subset=None, beta=BETA, thr_grid=THR_GRID)\n",
    "\n",
    "    all_summaries.append(summ)\n",
    "    all_fold_rows.extend(fold_rows)\n",
    "    oof_store[nm] = oof\n",
    "    pack_store[nm] = fold_packs\n",
    "\n",
    "    print(f\"  OOF best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f}\"\n",
    "          f\" | auc: {(summ['oof_auc'] if summ['oof_auc'] is not None else float('nan')):.6f}\"\n",
    "          f\" | logloss: {summ['oof_logloss']:.6f}\")\n",
    "\n",
    "df_sum = pd.DataFrame(all_summaries)\n",
    "df_fold = pd.DataFrame(all_fold_rows)\n",
    "\n",
    "if len(df_sum) == 0:\n",
    "    raise RuntimeError(\"Stage-2 produced no results. Turunkan kandidat/epochs atau cek device/VRAM.\")\n",
    "\n",
    "df_sum = df_sum.sort_values([\"oof_best_fbeta\", \"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nStage-2 top candidates (full CV):\")\n",
    "display(df_sum)\n",
    "\n",
    "# save search results\n",
    "df_sum.to_csv(OPT_DIR / \"opt_results.csv\", index=False)\n",
    "with open(OPT_DIR / \"opt_results.json\", \"w\") as f:\n",
    "    json.dump(df_sum.to_dict(orient=\"records\"), f, indent=2)\n",
    "df_fold.to_csv(OPT_DIR / \"opt_fold_details.csv\", index=False)\n",
    "\n",
    "# save OOF preds for top configs (debug)\n",
    "top_names = df_sum[\"cfg\"].head(min(REPORT_TOPK_OOF, len(df_sum))).astype(str).tolist()\n",
    "for nm in top_names:\n",
    "    df_o = pd.DataFrame({\n",
    "        \"uid\": uids_all,\n",
    "        \"y\": y_all,\n",
    "        \"fold\": folds_all,\n",
    "        f\"oof_pred_{nm}\": oof_store[nm]\n",
    "    })\n",
    "    df_o.to_csv(OPT_DIR / f\"oof_preds_{nm}.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Choose best config + save BEST fold packs (tanpa retrain ulang)\n",
    "# ----------------------------\n",
    "best_single = df_sum.iloc[0].to_dict()\n",
    "best_cfg_name = str(best_single[\"cfg\"])\n",
    "\n",
    "best_cfg = None\n",
    "for nm, cfg in candidates:\n",
    "    if nm == best_cfg_name:\n",
    "        best_cfg = cfg\n",
    "        break\n",
    "if best_cfg is None:\n",
    "    raise RuntimeError(\"Best cfg not found in candidates list (unexpected).\")\n",
    "\n",
    "best_fold_packs = pack_store[best_cfg_name]\n",
    "best_oof = oof_store[best_cfg_name]\n",
    "best_oof_best = best_fbeta_fast(y_all, best_oof, beta=BETA, grid=THR_GRID)\n",
    "\n",
    "best_model_path = OUT_DIR / \"best_gate_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"type\": \"mhc_lite_ft_transformer_v3\",\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"fold_packs\": best_fold_packs,\n",
    "        \"cfg_name\": best_cfg_name,\n",
    "        \"cfg\": best_cfg,\n",
    "        \"seed\": SEED,\n",
    "        \"beta_for_tuning\": BETA,\n",
    "        \"recommended_thr\": best_oof_best[\"thr\"],\n",
    "        \"recommended_score\": best_oof_best[\"fbeta\"],\n",
    "    },\n",
    "    best_model_path\n",
    ")\n",
    "\n",
    "best_bundle = {\n",
    "    \"type\": \"mhc_lite_ft_transformer_v3\",\n",
    "    \"model_name\": best_cfg_name,\n",
    "    \"members\": [best_cfg_name],\n",
    "    \"random_seed\": SEED,\n",
    "    \"beta_for_tuning\": BETA,\n",
    "\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"cfg\": best_cfg,\n",
    "\n",
    "    \"oof_best_thr\": best_oof_best[\"thr\"],\n",
    "    \"oof_best_fbeta\": best_oof_best[\"fbeta\"],\n",
    "    \"oof_best_prec\": best_oof_best[\"precision\"],\n",
    "    \"oof_best_rec\": best_oof_best[\"recall\"],\n",
    "    \"oof_auc\": safe_auc(y_all, best_oof),\n",
    "    \"oof_logloss\": safe_logloss(y_all, best_oof),\n",
    "\n",
    "    \"notes\": \"Best config from Step 4 (Transformer-only, mHC-lite differentiable + EMA + accum + reg). Best model saved as fold_packs (no retrain).\",\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / \"best_gate_config.json\", \"w\") as f:\n",
    "    json.dump(best_bundle, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved best artifacts:\")\n",
    "print(\"  best model (fold packs) ->\", best_model_path)\n",
    "print(\"  best config             ->\", OUT_DIR / \"best_gate_config.json\")\n",
    "print(\"  opt results             ->\", OPT_DIR / \"opt_results.csv\")\n",
    "print(\"  fold detail             ->\", OPT_DIR / \"opt_fold_details.csv\")\n",
    "print(\"  stage1 cache            ->\", STAGE1_PATH)\n",
    "\n",
    "# Export globals for Step 5\n",
    "BEST_GATE_BUNDLE = best_bundle\n",
    "BEST_TF_CFG_NAME = best_cfg_name\n",
    "BEST_TF_CFG = best_cfg\n",
    "OPT_RESULTS_DF = df_sum\n",
    "BEST_TF_OOF = best_oof\n",
    "BEST_TF_OOF_METRIC = best_oof_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7ae41",
   "metadata": {
    "papermill": {
     "duration": 0.013997,
     "end_time": "2026-01-08T14:46:49.958251",
     "exception": false,
     "start_time": "2026-01-08T14:46:49.944254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Training (Train on Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d104d1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T14:46:49.987920Z",
     "iopub.status.busy": "2026-01-08T14:46:49.987690Z",
     "iopub.status.idle": "2026-01-08T15:13:35.335426Z",
     "shell.execute_reply": "2026-01-08T15:13:35.334468Z"
    },
    "papermill": {
     "duration": 1605.36527,
     "end_time": "2026-01-08T15:13:35.337118",
     "exception": false,
     "start_time": "2026-01-08T14:46:49.971848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data:\n",
      "  rows=5176 | pos%=54.08 | n_features=84\n",
      "\n",
      "Loaded cfg from: memory(BEST_GATE_BUNDLE)\n",
      "Warning: failed to load best_gate_model.* fold_packs: UnpicklingError('Weights only load failed. This file can still be loaded, to do so you have two options, \\x1b[1mdo those steps only if you trust the source of the checkpoint\\x1b[0m. \\n\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\n\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\n\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\\n\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.')\n",
      "\n",
      "Device: cuda | AMP: True | VRAM_GB: 15.9\n",
      "\n",
      "CFG (final):\n",
      "  d_model: 384\n",
      "  n_layers: 10\n",
      "  n_heads: 8\n",
      "  ffn_mult: 4\n",
      "  dropout: 0.24\n",
      "  attn_dropout: 0.12\n",
      "  n_streams: 4\n",
      "  alpha_init: 0.01\n",
      "  sinkhorn_tmax: 20\n",
      "  mhc_dropout: 0.0\n",
      "  batch_size: 256\n",
      "  accum_steps: 4\n",
      "  epochs: 75\n",
      "  lr: 0.00016\n",
      "  weight_decay: 0.015\n",
      "  warmup_frac: 0.1\n",
      "  patience: 12\n",
      "\n",
      "[Final Train v4.1] seed=2025\n",
      "\n",
      "Internal val split: train=4762 | val=414 | val_pos%=51.93\n",
      "  epoch 001/75 | tr_loss=0.22686 | val_ll=0.69988 | val_auc=0.39817 | bad=0\n",
      "  epoch 002/75 | tr_loss=0.19510 | val_ll=0.69912 | val_auc=0.40799 | bad=0\n",
      "  epoch 003/75 | tr_loss=0.11404 | val_ll=0.69741 | val_auc=0.43477 | bad=0\n",
      "  epoch 004/75 | tr_loss=0.03136 | val_ll=0.69472 | val_auc=0.48263 | bad=0\n",
      "  epoch 005/75 | tr_loss=0.01792 | val_ll=0.69098 | val_auc=0.55512 | bad=0\n",
      "  epoch 006/75 | tr_loss=0.00890 | val_ll=0.68635 | val_auc=0.62365 | bad=0\n",
      "  epoch 007/75 | tr_loss=0.00775 | val_ll=0.68081 | val_auc=0.69312 | bad=0\n",
      "  epoch 008/75 | tr_loss=0.00366 | val_ll=0.67439 | val_auc=0.74773 | bad=0\n",
      "  epoch 009/75 | tr_loss=0.00299 | val_ll=0.66707 | val_auc=0.80485 | bad=0\n",
      "  epoch 010/75 | tr_loss=0.00253 | val_ll=0.65901 | val_auc=0.87421 | bad=0\n",
      "  epoch 011/75 | tr_loss=0.00262 | val_ll=0.65017 | val_auc=0.93990 | bad=0\n",
      "  epoch 012/75 | tr_loss=0.00345 | val_ll=0.64049 | val_auc=0.97712 | bad=0\n",
      "  epoch 013/75 | tr_loss=0.00164 | val_ll=0.63008 | val_auc=0.99258 | bad=0\n",
      "  epoch 014/75 | tr_loss=0.00339 | val_ll=0.61891 | val_auc=0.99683 | bad=0\n",
      "  epoch 015/75 | tr_loss=0.00197 | val_ll=0.60702 | val_auc=0.99819 | bad=0\n",
      "  epoch 016/75 | tr_loss=0.00174 | val_ll=0.59440 | val_auc=0.99897 | bad=0\n",
      "  epoch 017/75 | tr_loss=0.00283 | val_ll=0.58110 | val_auc=0.99967 | bad=0\n",
      "  epoch 018/75 | tr_loss=0.00111 | val_ll=0.56713 | val_auc=0.99993 | bad=0\n",
      "  epoch 019/75 | tr_loss=0.00125 | val_ll=0.55260 | val_auc=1.00000 | bad=0\n",
      "  epoch 020/75 | tr_loss=0.00152 | val_ll=0.53736 | val_auc=1.00000 | bad=0\n",
      "  epoch 021/75 | tr_loss=0.00222 | val_ll=0.52166 | val_auc=1.00000 | bad=0\n",
      "  epoch 022/75 | tr_loss=0.00179 | val_ll=0.50546 | val_auc=1.00000 | bad=0\n",
      "  epoch 023/75 | tr_loss=0.00136 | val_ll=0.48904 | val_auc=1.00000 | bad=0\n",
      "  epoch 024/75 | tr_loss=0.00135 | val_ll=0.47224 | val_auc=1.00000 | bad=0\n",
      "  epoch 025/75 | tr_loss=0.00122 | val_ll=0.45508 | val_auc=1.00000 | bad=0\n",
      "  epoch 026/75 | tr_loss=0.00119 | val_ll=0.43776 | val_auc=1.00000 | bad=0\n",
      "  epoch 027/75 | tr_loss=0.00100 | val_ll=0.42037 | val_auc=1.00000 | bad=0\n",
      "  epoch 028/75 | tr_loss=0.00142 | val_ll=0.40256 | val_auc=1.00000 | bad=0\n",
      "  epoch 029/75 | tr_loss=0.00198 | val_ll=0.38498 | val_auc=1.00000 | bad=0\n",
      "  epoch 030/75 | tr_loss=0.00120 | val_ll=0.36711 | val_auc=1.00000 | bad=0\n",
      "  epoch 031/75 | tr_loss=0.00225 | val_ll=0.34946 | val_auc=1.00000 | bad=0\n",
      "  epoch 032/75 | tr_loss=0.00114 | val_ll=0.33216 | val_auc=1.00000 | bad=0\n",
      "  epoch 033/75 | tr_loss=0.00080 | val_ll=0.31506 | val_auc=1.00000 | bad=0\n",
      "  epoch 034/75 | tr_loss=0.00091 | val_ll=0.29832 | val_auc=1.00000 | bad=0\n",
      "  epoch 035/75 | tr_loss=0.00085 | val_ll=0.28204 | val_auc=1.00000 | bad=0\n",
      "  epoch 036/75 | tr_loss=0.00172 | val_ll=0.26661 | val_auc=1.00000 | bad=0\n",
      "  epoch 037/75 | tr_loss=0.00106 | val_ll=0.25184 | val_auc=1.00000 | bad=0\n",
      "  epoch 038/75 | tr_loss=0.00104 | val_ll=0.23770 | val_auc=1.00000 | bad=0\n",
      "  epoch 039/75 | tr_loss=0.00139 | val_ll=0.22404 | val_auc=1.00000 | bad=0\n",
      "  epoch 040/75 | tr_loss=0.00075 | val_ll=0.21144 | val_auc=1.00000 | bad=0\n",
      "  epoch 041/75 | tr_loss=0.00085 | val_ll=0.19988 | val_auc=1.00000 | bad=0\n",
      "  epoch 042/75 | tr_loss=0.00106 | val_ll=0.18916 | val_auc=1.00000 | bad=0\n",
      "  epoch 043/75 | tr_loss=0.00108 | val_ll=0.17915 | val_auc=1.00000 | bad=0\n",
      "  epoch 044/75 | tr_loss=0.00057 | val_ll=0.16980 | val_auc=1.00000 | bad=0\n",
      "  epoch 045/75 | tr_loss=0.00118 | val_ll=0.16113 | val_auc=1.00000 | bad=0\n",
      "  epoch 046/75 | tr_loss=0.00111 | val_ll=0.15323 | val_auc=1.00000 | bad=0\n",
      "  epoch 047/75 | tr_loss=0.00116 | val_ll=0.14623 | val_auc=1.00000 | bad=0\n",
      "  epoch 048/75 | tr_loss=0.00090 | val_ll=0.13983 | val_auc=1.00000 | bad=0\n",
      "  epoch 049/75 | tr_loss=0.00113 | val_ll=0.13404 | val_auc=1.00000 | bad=0\n",
      "  epoch 050/75 | tr_loss=0.00112 | val_ll=0.12883 | val_auc=1.00000 | bad=0\n",
      "  epoch 051/75 | tr_loss=0.00071 | val_ll=0.12429 | val_auc=1.00000 | bad=0\n",
      "  epoch 052/75 | tr_loss=0.00160 | val_ll=0.12034 | val_auc=1.00000 | bad=0\n",
      "  epoch 053/75 | tr_loss=0.00075 | val_ll=0.11694 | val_auc=1.00000 | bad=0\n",
      "  epoch 054/75 | tr_loss=0.00062 | val_ll=0.11403 | val_auc=1.00000 | bad=0\n",
      "  epoch 055/75 | tr_loss=0.00080 | val_ll=0.11148 | val_auc=1.00000 | bad=0\n",
      "  epoch 056/75 | tr_loss=0.00069 | val_ll=0.10920 | val_auc=1.00000 | bad=0\n",
      "  epoch 057/75 | tr_loss=0.00109 | val_ll=0.10710 | val_auc=1.00000 | bad=0\n",
      "  epoch 058/75 | tr_loss=0.00065 | val_ll=0.10512 | val_auc=1.00000 | bad=0\n",
      "  epoch 059/75 | tr_loss=0.00104 | val_ll=0.10332 | val_auc=1.00000 | bad=0\n",
      "  epoch 060/75 | tr_loss=0.00155 | val_ll=0.10155 | val_auc=1.00000 | bad=0\n",
      "  epoch 061/75 | tr_loss=0.00149 | val_ll=0.10004 | val_auc=1.00000 | bad=0\n",
      "  epoch 062/75 | tr_loss=0.00089 | val_ll=0.09886 | val_auc=1.00000 | bad=0\n",
      "  epoch 063/75 | tr_loss=0.00087 | val_ll=0.09799 | val_auc=1.00000 | bad=0\n",
      "  epoch 064/75 | tr_loss=0.00146 | val_ll=0.09740 | val_auc=1.00000 | bad=0\n",
      "  epoch 065/75 | tr_loss=0.00074 | val_ll=0.09691 | val_auc=1.00000 | bad=0\n",
      "  epoch 066/75 | tr_loss=0.00093 | val_ll=0.09652 | val_auc=1.00000 | bad=0\n",
      "  epoch 067/75 | tr_loss=0.00201 | val_ll=0.09629 | val_auc=1.00000 | bad=0\n",
      "  epoch 068/75 | tr_loss=0.00102 | val_ll=0.09617 | val_auc=1.00000 | bad=0\n",
      "  epoch 069/75 | tr_loss=0.00108 | val_ll=0.09618 | val_auc=1.00000 | bad=1\n",
      "  epoch 070/75 | tr_loss=0.00092 | val_ll=0.09618 | val_auc=1.00000 | bad=2\n",
      "  epoch 071/75 | tr_loss=0.00082 | val_ll=0.09618 | val_auc=1.00000 | bad=3\n",
      "  epoch 072/75 | tr_loss=0.00086 | val_ll=0.09624 | val_auc=1.00000 | bad=4\n",
      "  epoch 073/75 | tr_loss=0.00144 | val_ll=0.09622 | val_auc=1.00000 | bad=5\n",
      "  epoch 074/75 | tr_loss=0.00095 | val_ll=0.09617 | val_auc=1.00000 | bad=6\n",
      "  epoch 075/75 | tr_loss=0.00091 | val_ll=0.09611 | val_auc=1.00000 | bad=7\n",
      "\n",
      "Best_epoch(from internal val)=68 -> Retrain FULL for E_FULL=71\n",
      "  full epoch 001/71 | loss=0.22769\n",
      "  full epoch 002/71 | loss=0.19068\n",
      "  full epoch 003/71 | loss=0.09518\n",
      "  full epoch 004/71 | loss=0.02472\n",
      "  full epoch 005/71 | loss=0.02557\n",
      "  full epoch 006/71 | loss=0.00916\n",
      "  full epoch 007/71 | loss=0.00538\n",
      "  full epoch 008/71 | loss=0.00472\n",
      "  full epoch 009/71 | loss=0.00254\n",
      "  full epoch 010/71 | loss=0.00325\n",
      "  full epoch 011/71 | loss=0.00226\n",
      "  full epoch 012/71 | loss=0.00420\n",
      "  full epoch 013/71 | loss=0.00227\n",
      "  full epoch 014/71 | loss=0.00190\n",
      "  full epoch 015/71 | loss=0.00218\n",
      "  full epoch 016/71 | loss=0.00178\n",
      "  full epoch 017/71 | loss=0.00185\n",
      "  full epoch 018/71 | loss=0.00114\n",
      "  full epoch 019/71 | loss=0.00210\n",
      "  full epoch 020/71 | loss=0.00199\n",
      "  full epoch 021/71 | loss=0.00209\n",
      "  full epoch 022/71 | loss=0.00160\n",
      "  full epoch 023/71 | loss=0.00175\n",
      "  full epoch 024/71 | loss=0.00186\n",
      "  full epoch 025/71 | loss=0.00118\n",
      "  full epoch 026/71 | loss=0.00142\n",
      "  full epoch 027/71 | loss=0.00099\n",
      "  full epoch 028/71 | loss=0.00092\n",
      "  full epoch 029/71 | loss=0.00104\n",
      "  full epoch 030/71 | loss=0.00080\n",
      "  full epoch 031/71 | loss=0.00164\n",
      "  full epoch 032/71 | loss=0.00092\n",
      "  full epoch 033/71 | loss=0.00089\n",
      "  full epoch 034/71 | loss=0.00061\n",
      "  full epoch 035/71 | loss=0.00142\n",
      "  full epoch 036/71 | loss=0.00155\n",
      "  full epoch 037/71 | loss=0.00077\n",
      "  full epoch 038/71 | loss=0.00076\n",
      "  full epoch 039/71 | loss=0.00080\n",
      "  full epoch 040/71 | loss=0.00104\n",
      "  full epoch 041/71 | loss=0.00096\n",
      "  full epoch 042/71 | loss=0.00161\n",
      "  full epoch 043/71 | loss=0.00077\n",
      "  full epoch 044/71 | loss=0.00076\n",
      "  full epoch 045/71 | loss=0.00104\n",
      "  full epoch 046/71 | loss=0.00079\n",
      "  full epoch 047/71 | loss=0.00103\n",
      "  full epoch 048/71 | loss=0.00096\n",
      "  full epoch 049/71 | loss=0.00068\n",
      "  full epoch 050/71 | loss=0.00101\n",
      "  full epoch 051/71 | loss=0.00069\n",
      "  full epoch 052/71 | loss=0.00074\n",
      "  full epoch 053/71 | loss=0.00103\n",
      "  full epoch 054/71 | loss=0.00095\n",
      "  full epoch 055/71 | loss=0.00053\n",
      "  full epoch 056/71 | loss=0.00132\n",
      "  full epoch 057/71 | loss=0.00130\n",
      "  full epoch 058/71 | loss=0.00081\n",
      "  full epoch 059/71 | loss=0.00102\n",
      "  full epoch 060/71 | loss=0.00087\n",
      "  full epoch 061/71 | loss=0.00083\n",
      "  full epoch 062/71 | loss=0.00116\n",
      "  full epoch 063/71 | loss=0.00062\n",
      "  full epoch 064/71 | loss=0.00119\n",
      "  full epoch 065/71 | loss=0.00078\n",
      "  full epoch 066/71 | loss=0.00073\n",
      "  full epoch 067/71 | loss=0.00103\n",
      "  full epoch 068/71 | loss=0.00115\n",
      "  full epoch 069/71 | loss=0.00057\n",
      "  full epoch 070/71 | loss=0.00088\n",
      "  full epoch 071/71 | loss=0.00050\n",
      "\n",
      "Saved final training artifacts:\n",
      "  model  -> /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n",
      "  bundle -> /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 5 — Final Training (Train on Full Data) — TRANSFORMER ONLY\n",
    "# REVISI FULL v4.1 (match Step 4 v3.2: mHC-lite differentiable + EMA + accum)\n",
    "#\n",
    "# Fix v4.1:\n",
    "# - STRICT match best cfg dari Step 4 (tanpa auto “bigger model” kecuali kamu ON-kan)\n",
    "# - AMP/GradScaler aman (CPU = no-amp, no-scaler)\n",
    "# - CFG guard: d_model harus divisible oleh n_heads (auto-fix saat fallback/OOM)\n",
    "# - Internal val case-level: robust fallback kalau val kosong / 1-class\n",
    "# - OOM fallback lebih aman: turunkan batch -> d_model -> n_layers -> n_heads\n",
    "# - Scheduler total_steps guard (>=1) + warmup clamp\n",
    "#\n",
    "# Output:\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n",
    "# ============================================================\n",
    "\n",
    "import os, json, gc, math, time, warnings\n",
    "from pathlib import Path\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "# ----------------------------\n",
    "# 0) REQUIRE\n",
    "# ----------------------------\n",
    "if \"df_train_tabular\" not in globals():\n",
    "    raise RuntimeError(\"Missing `df_train_tabular`. Jalankan Step 2 dulu.\")\n",
    "if \"FEATURE_COLS\" not in globals():\n",
    "    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 dulu.\")\n",
    "\n",
    "df_train_tabular = df_train_tabular.copy()\n",
    "FEATURE_COLS = list(FEATURE_COLS)\n",
    "\n",
    "need_cols = {\"uid\", \"case_id\", \"y\"}\n",
    "miss = [c for c in need_cols if c not in df_train_tabular.columns]\n",
    "if miss:\n",
    "    raise ValueError(f\"df_train_tabular missing columns: {miss}\")\n",
    "\n",
    "X_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "y_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n",
    "\n",
    "if not np.isfinite(X_all).all():\n",
    "    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "OUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Final training data:\")\n",
    "print(f\"  rows={len(y_all)} | pos%={float(y_all.mean())*100:.2f} | n_features={X_all.shape[1]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load best cfg (Step 4 output)\n",
    "# ----------------------------\n",
    "best_bundle = None\n",
    "source = None\n",
    "\n",
    "cfg_path = OUT_DIR / \"best_gate_config.json\"\n",
    "best_model_candidates = [\n",
    "    OUT_DIR / \"best_gate_model.pt\",\n",
    "    OUT_DIR / \"best_gate_model.pth\",\n",
    "]\n",
    "\n",
    "if \"BEST_GATE_BUNDLE\" in globals() and isinstance(BEST_GATE_BUNDLE, dict):\n",
    "    best_bundle = BEST_GATE_BUNDLE\n",
    "    source = \"memory(BEST_GATE_BUNDLE)\"\n",
    "elif cfg_path.exists():\n",
    "    best_bundle = json.loads(cfg_path.read_text())\n",
    "    source = str(cfg_path)\n",
    "\n",
    "if best_bundle is not None and isinstance(best_bundle, dict) and isinstance(best_bundle.get(\"cfg\", None), dict):\n",
    "    base_cfg = dict(best_bundle[\"cfg\"])\n",
    "    print(\"\\nLoaded cfg from:\", source)\n",
    "else:\n",
    "    base_cfg = {}\n",
    "    print(\"\\nNo best_gate_config found. Using strong default cfg.\")\n",
    "\n",
    "# optional: load fold_packs dari best_gate_model.pt (biar final file bisa bawa ensemble fold juga)\n",
    "fold_packs_from_step4 = None\n",
    "best_gate_model_path = None\n",
    "for p in best_model_candidates:\n",
    "    if p.exists():\n",
    "        best_gate_model_path = p\n",
    "        break\n",
    "\n",
    "if best_gate_model_path is not None:\n",
    "    try:\n",
    "        obj = torch.load(best_gate_model_path, map_location=\"cpu\")\n",
    "        if isinstance(obj, dict) and isinstance(obj.get(\"fold_packs\", None), list):\n",
    "            fold_packs_from_step4 = obj[\"fold_packs\"]\n",
    "            print(\"Loaded fold_packs from:\", str(best_gate_model_path))\n",
    "    except Exception as e:\n",
    "        print(\"Warning: failed to load best_gate_model.* fold_packs:\", repr(e))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Device + seed\n",
    "# ----------------------------\n",
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "FINAL_SEED = 2025\n",
    "if isinstance(best_bundle, dict):\n",
    "    FINAL_SEED = int(best_bundle.get(\"seed\", best_bundle.get(\"random_seed\", 2025)))\n",
    "\n",
    "seed_everything(FINAL_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "vram_gb = None\n",
    "if device.type == \"cuda\":\n",
    "    vram_gb = float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n",
    "\n",
    "print(\"\\nDevice:\", device, \"| AMP:\", use_amp, \"| VRAM_GB:\", (f\"{vram_gb:.1f}\" if vram_gb else \"CPU\"))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Training policy\n",
    "# ----------------------------\n",
    "# IMPORTANT: defaultnya STRICT match Step 4 best cfg (tidak dibesarkan otomatis)\n",
    "ALLOW_UPSCALE = False           # set True kalau kamu mau auto-besar sesuai VRAM\n",
    "USE_INTERNAL_VAL = True         # cari best_epoch dari val (case-level)\n",
    "VAL_FRAC_CASE = 0.08            # 8% case untuk val\n",
    "EARLY_STOP = True\n",
    "\n",
    "# runtime: 1 seed default\n",
    "N_SEEDS = 1\n",
    "\n",
    "# target effective batch\n",
    "TARGET_EFF_BATCH = 1024 if device.type == \"cuda\" else 256\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset + Standardizer\n",
    "# ----------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def fit_standardizer(X_tr: np.ndarray):\n",
    "    mu = X_tr.mean(axis=0, dtype=np.float64)\n",
    "    sig = X_tr.std(axis=0, dtype=np.float64)\n",
    "    sig = np.where(sig < 1e-8, 1.0, sig)\n",
    "    return mu.astype(np.float32), sig.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n",
    "    return ((X_in - mu) / sig).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Metrics helpers\n",
    "# ----------------------------\n",
    "def safe_logloss(y_true, p):\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n",
    "    return float(log_loss(y_true, p, labels=[0, 1]))\n",
    "\n",
    "def safe_auc(y_true, p):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return None\n",
    "    return float(roc_auc_score(y_true, p))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Model: FTTransformer_MHCLite (MATCH Step 4)\n",
    "# ----------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = float(eps)\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "    def forward(self, x):\n",
    "        rms = torch.mean(x * x, dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(rms + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "def sinkhorn_knopp(P, tmax=20, eps=1e-6):\n",
    "    M = P.clamp_min(eps)\n",
    "    for _ in range(int(tmax)):\n",
    "        M = M / (M.sum(dim=-1, keepdim=True).clamp_min(eps))  # row\n",
    "        M = M / (M.sum(dim=-2, keepdim=True).clamp_min(eps))  # col\n",
    "    return M\n",
    "\n",
    "class MHCLite(nn.Module):\n",
    "    def __init__(self, d_model, n_streams=4, alpha_init=0.01, tmax=20, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n = int(n_streams)\n",
    "        self.tmax = int(tmax)\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "\n",
    "        self.norm = RMSNorm(d_model, eps=1e-6)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, self.n * self.n),\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        a0 = float(alpha_init)\n",
    "        a0 = min(max(a0, 1e-4), 1.0 - 1e-4)\n",
    "        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, streams, cls_vec):\n",
    "        B, n, D = streams.shape\n",
    "        h = self.norm(cls_vec)\n",
    "        logits = self.mlp(h).view(B, n, n)\n",
    "        P = self.softplus(logits)\n",
    "        M = sinkhorn_knopp(P, tmax=self.tmax, eps=1e-6)\n",
    "\n",
    "        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype, device=streams.device)\n",
    "        I = torch.eye(n, device=streams.device, dtype=streams.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "        H = (1.0 - alpha) * I + alpha * M\n",
    "\n",
    "        mixed = torch.einsum(\"bij,bjd->bid\", H, streams)\n",
    "        injected = mixed + cls_vec.unsqueeze(1)\n",
    "        return self.drop(injected)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model, eps=1e-6)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, num_heads=int(n_heads),\n",
    "            dropout=float(attn_dropout), batch_first=True\n",
    "        )\n",
    "        self.drop1 = nn.Dropout(float(dropout))\n",
    "\n",
    "        self.norm2 = RMSNorm(d_model, eps=1e-6)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, int(ffn_mult) * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "            nn.Linear(int(ffn_mult) * d_model, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(float(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.norm1(x)\n",
    "        attn_out, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = x + self.drop1(attn_out)\n",
    "        h = self.norm2(x)\n",
    "        x = x + self.drop2(self.ffn(h))\n",
    "        return x\n",
    "\n",
    "class FTTransformer_MHCLite(nn.Module):\n",
    "    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n",
    "                 dropout=0.2, attn_dropout=0.1,\n",
    "                 n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n",
    "                 feat_token_drop_p=0.0):\n",
    "        super().__init__()\n",
    "        self.n_features = int(n_features)\n",
    "        self.d_model = int(d_model)\n",
    "        self.n_layers = int(n_layers)\n",
    "        self.feat_token_drop_p = float(feat_token_drop_p)\n",
    "\n",
    "        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n",
    "        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n",
    "        self.in_drop = nn.Dropout(float(dropout))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=self.d_model,\n",
    "                n_heads=n_heads,\n",
    "                ffn_mult=ffn_mult,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout\n",
    "            ) for _ in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.mhc = nn.ModuleList([\n",
    "            MHCLite(\n",
    "                d_model=self.d_model,\n",
    "                n_streams=n_streams,\n",
    "                alpha_init=alpha_init,\n",
    "                tmax=sinkhorn_tmax,\n",
    "                dropout=mhc_dropout\n",
    "            ) for _ in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.out_norm = RMSNorm(self.d_model, eps=1e-6)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "            nn.Linear(self.d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n",
    "        tok = tok + self.feat_emb.unsqueeze(0)\n",
    "\n",
    "        if self.training and self.feat_token_drop_p > 0:\n",
    "            B, F_, D = tok.shape\n",
    "            keep = (torch.rand(B, F_, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n",
    "            tok = tok * keep.unsqueeze(-1)\n",
    "\n",
    "        B = tok.size(0)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        seq = torch.cat([cls, tok], dim=1)\n",
    "        seq = self.in_drop(seq)\n",
    "\n",
    "        nS = self.mhc[0].n\n",
    "        streams = seq[:, 0, :].unsqueeze(1).expand(B, nS, self.d_model).contiguous()\n",
    "\n",
    "        for l, blk in enumerate(self.blocks):\n",
    "            cls_in = streams.mean(dim=1).unsqueeze(1)\n",
    "            seq = torch.cat([cls_in, seq[:, 1:, :]], dim=1)\n",
    "            seq = blk(seq)\n",
    "            cls_vec = seq[:, 0, :]\n",
    "            streams = self.mhc[l](streams, cls_vec)\n",
    "\n",
    "        out = self.out_norm(streams.mean(dim=1))\n",
    "        logit = self.head(out).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "# ----------------------------\n",
    "# 7) EMA + scheduler (optimizer-steps)\n",
    "# ----------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = float(decay)\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        d = self.decay\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            self.backup[name] = p.detach().clone()\n",
    "            p.copy_(self.shadow[name])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, model: nn.Module):\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            p.copy_(self.backup[name])\n",
    "        self.backup = {}\n",
    "\n",
    "def make_warmup_step_scheduler(optimizer, total_steps, warmup_steps,\n",
    "                              r1=0.8, r2=0.9, d1=0.316, d2=0.1):\n",
    "    total_steps = int(max(1, total_steps))\n",
    "    warmup_steps = int(max(0, min(int(warmup_steps), total_steps)))\n",
    "\n",
    "    m1 = int(float(r1) * total_steps)\n",
    "    m2 = int(float(r2) * total_steps)\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if warmup_steps > 0 and step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "        mult = 1.0\n",
    "        if step >= m1:\n",
    "            mult *= float(d1)\n",
    "        if step >= m2:\n",
    "            mult *= float(d2)\n",
    "        return mult\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, ema: EMA = None):\n",
    "    model.eval()\n",
    "    if ema is not None:\n",
    "        ema.apply_shadow(model)\n",
    "\n",
    "    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n",
    "    ps = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with ctx:\n",
    "            logits = model(xb)\n",
    "            p = torch.sigmoid(logits)\n",
    "        ps.append(p.detach().cpu().numpy())\n",
    "\n",
    "    out = np.concatenate(ps, axis=0).astype(np.float32)\n",
    "    if ema is not None:\n",
    "        ema.restore(model)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 8) CFG merge + guards\n",
    "# ----------------------------\n",
    "CFG = dict(\n",
    "    # arch\n",
    "    d_model=384, n_layers=8, n_heads=8, ffn_mult=4,\n",
    "    dropout=0.20, attn_dropout=0.10,\n",
    "\n",
    "    # mHC-lite\n",
    "    n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n",
    "\n",
    "    # regularization\n",
    "    feat_token_drop_p=0.05,\n",
    "    input_noise_std=0.01,\n",
    "    focal_gamma=1.5,\n",
    "    label_smoothing=0.00,\n",
    "\n",
    "    # optim\n",
    "    lr=2e-4,\n",
    "    weight_decay=1.0e-2,\n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    adam_eps=1e-8,\n",
    "\n",
    "    # sched\n",
    "    warmup_frac=0.10,\n",
    "    lr_decay_ratio1=0.8,\n",
    "    lr_decay_ratio2=0.9,\n",
    "    lr_decay_rate1=0.316,\n",
    "    lr_decay_rate2=0.1,\n",
    "\n",
    "    # train\n",
    "    batch_size=512 if device.type == \"cuda\" else 256,\n",
    "    epochs=75 if device.type == \"cuda\" else 35,\n",
    "    accum_steps=2 if device.type == \"cuda\" else 1,\n",
    "    grad_clip=1.0,\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "\n",
    "    # EMA\n",
    "    use_ema=True,\n",
    "    ema_decay=0.999,\n",
    ")\n",
    "\n",
    "# merge best cfg -> CFG (strict match Step 4)\n",
    "for k, v in base_cfg.items():\n",
    "    if k in CFG:\n",
    "        CFG[k] = v\n",
    "\n",
    "def cpu_safe_cfg(cfg: dict):\n",
    "    cfg = dict(cfg)\n",
    "    if device.type != \"cuda\":\n",
    "        # keep stable + fast on CPU\n",
    "        cfg[\"d_model\"] = min(int(cfg[\"d_model\"]), 256)\n",
    "        cfg[\"n_layers\"] = min(int(cfg[\"n_layers\"]), 6)\n",
    "        cfg[\"n_heads\"]  = min(int(cfg[\"n_heads\"]), 8)\n",
    "        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 256)\n",
    "        cfg[\"epochs\"] = min(int(cfg[\"epochs\"]), 35)\n",
    "        cfg[\"accum_steps\"] = 1\n",
    "    return cfg\n",
    "\n",
    "def maybe_upscale_cfg(cfg: dict):\n",
    "    if not (device.type == \"cuda\" and ALLOW_UPSCALE and vram_gb is not None):\n",
    "        return dict(cfg)\n",
    "    cfg = dict(cfg)\n",
    "    # mild upscale only (optional)\n",
    "    if vram_gb >= 24:\n",
    "        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 512)\n",
    "        cfg[\"accum_steps\"] = max(int(cfg.get(\"accum_steps\", 2)), 2)\n",
    "    elif vram_gb >= 16:\n",
    "        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 384)\n",
    "        cfg[\"accum_steps\"] = max(int(cfg.get(\"accum_steps\", 2)), 2)\n",
    "    return cfg\n",
    "\n",
    "def fix_heads_divisibility(cfg: dict):\n",
    "    cfg = dict(cfg)\n",
    "    d = int(cfg[\"d_model\"])\n",
    "    h = int(cfg[\"n_heads\"])\n",
    "    if h < 1:\n",
    "        h = 1\n",
    "    if d % h == 0:\n",
    "        cfg[\"n_heads\"] = h\n",
    "        return cfg\n",
    "    # find largest divisor <= h\n",
    "    cand = []\n",
    "    for k in range(h, 0, -1):\n",
    "        if d % k == 0:\n",
    "            cand.append(k)\n",
    "            break\n",
    "    cfg[\"n_heads\"] = cand[0] if cand else 1\n",
    "    return cfg\n",
    "\n",
    "CFG = cpu_safe_cfg(CFG)\n",
    "CFG = maybe_upscale_cfg(CFG)\n",
    "\n",
    "# effective batch close to target\n",
    "CFG[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(CFG[\"batch_size\"]))))\n",
    "\n",
    "# final divisibility guard\n",
    "CFG = fix_heads_divisibility(CFG)\n",
    "\n",
    "print(\"\\nCFG (final):\")\n",
    "for k in [\n",
    "    \"d_model\",\"n_layers\",\"n_heads\",\"ffn_mult\",\"dropout\",\"attn_dropout\",\n",
    "    \"n_streams\",\"alpha_init\",\"sinkhorn_tmax\",\"mhc_dropout\",\n",
    "    \"batch_size\",\"accum_steps\",\"epochs\",\"lr\",\"weight_decay\",\"warmup_frac\",\"patience\"\n",
    "]:\n",
    "    print(f\"  {k}: {CFG[k]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Internal val split (case_id-safe) + robust fallback\n",
    "# ----------------------------\n",
    "def make_case_split(df: pd.DataFrame, val_frac=0.08, seed=2025):\n",
    "    g = df.groupby(\"case_id\")[\"y\"].max().reset_index().rename(columns={\"y\": \"case_y\"})\n",
    "    pos_cases = g.loc[g[\"case_y\"] == 1, \"case_id\"].astype(str).to_numpy()\n",
    "    neg_cases = g.loc[g[\"case_y\"] == 0, \"case_id\"].astype(str).to_numpy()\n",
    "\n",
    "    rng = np.random.RandomState(int(seed))\n",
    "    rng.shuffle(pos_cases)\n",
    "    rng.shuffle(neg_cases)\n",
    "\n",
    "    if len(pos_cases) == 0 or len(neg_cases) == 0:\n",
    "        # extreme case: cannot stratify by case; fallback to row stratified\n",
    "        idx = np.arange(len(df))\n",
    "        rng.shuffle(idx)\n",
    "        n_val = max(1, int(len(df) * float(val_frac)))\n",
    "        val_idx = idx[:n_val]\n",
    "        is_val = np.zeros(len(df), dtype=bool)\n",
    "        is_val[val_idx] = True\n",
    "        return is_val\n",
    "\n",
    "    n_val_pos = max(1, int(len(pos_cases) * float(val_frac)))\n",
    "    n_val_neg = max(1, int(len(neg_cases) * float(val_frac)))\n",
    "\n",
    "    val_cases = np.concatenate([pos_cases[:n_val_pos], neg_cases[:n_val_neg]])\n",
    "    val_set = set(val_cases.tolist())\n",
    "    is_val = df[\"case_id\"].astype(str).isin(val_set).to_numpy(dtype=bool)\n",
    "\n",
    "    # if too small / degenerate, fallback to row stratified\n",
    "    va_idx = np.where(is_val)[0]\n",
    "    if len(va_idx) < 32 or len(np.unique(df.loc[is_val, \"y\"].values)) < 2:\n",
    "        idx_pos = np.where(df[\"y\"].values == 1)[0]\n",
    "        idx_neg = np.where(df[\"y\"].values == 0)[0]\n",
    "        rng.shuffle(idx_pos); rng.shuffle(idx_neg)\n",
    "        n_val = max(32, int(len(df) * float(val_frac)))\n",
    "        n_val_pos = max(1, int(n_val * float(df[\"y\"].mean())))\n",
    "        n_val_pos = min(n_val_pos, len(idx_pos))\n",
    "        n_val_neg = min(n_val - n_val_pos, len(idx_neg))\n",
    "        val_idx = np.concatenate([idx_pos[:n_val_pos], idx_neg[:n_val_neg]])\n",
    "        is_val = np.zeros(len(df), dtype=bool)\n",
    "        is_val[val_idx] = True\n",
    "\n",
    "    return is_val\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Training helpers (workers/amp/scaler)\n",
    "# ----------------------------\n",
    "def make_loader(ds, batch_size, shuffle):\n",
    "    cpu_cnt = os.cpu_count() or 2\n",
    "    nw = 2 if cpu_cnt >= 4 else 0\n",
    "    pin = (device.type == \"cuda\")\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=bool(shuffle),\n",
    "        num_workers=nw,\n",
    "        pin_memory=pin,\n",
    "        drop_last=False,\n",
    "        persistent_workers=(nw > 0),\n",
    "    )\n",
    "\n",
    "def build_loss_fn(y_for_pos_weight, cfg):\n",
    "    pos = float(np.sum(y_for_pos_weight))\n",
    "    neg = float(len(y_for_pos_weight) - pos)\n",
    "    pos_weight = torch.tensor([neg / max(1.0, pos)], device=device, dtype=torch.float32)\n",
    "\n",
    "    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n",
    "    label_smoothing = float(cfg.get(\"label_smoothing\", 0.0))\n",
    "\n",
    "    def loss_fn(logits, targets):\n",
    "        if label_smoothing and label_smoothing > 0:\n",
    "            targets = targets * (1.0 - label_smoothing) + 0.5 * label_smoothing\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\", pos_weight=pos_weight)\n",
    "        if focal_gamma and focal_gamma > 0:\n",
    "            p = torch.sigmoid(logits)\n",
    "            p_t = p * targets + (1 - p) * (1 - targets)\n",
    "            mod = (1.0 - p_t).clamp_min(0.0).pow(focal_gamma)\n",
    "            bce = bce * mod\n",
    "        return bce.mean()\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "def build_model(cfg, n_features):\n",
    "    cfg = fix_heads_divisibility(cfg)\n",
    "    return FTTransformer_MHCLite(\n",
    "        n_features=int(n_features),\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        ffn_mult=int(cfg[\"ffn_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        attn_dropout=float(cfg[\"attn_dropout\"]),\n",
    "        n_streams=int(cfg[\"n_streams\"]),\n",
    "        alpha_init=float(cfg[\"alpha_init\"]),\n",
    "        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n",
    "        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n",
    "        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n",
    "    ).to(device)\n",
    "\n",
    "def build_opt_and_sch(model, cfg, steps_per_epoch, epochs):\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n",
    "        eps=float(cfg[\"adam_eps\"]),\n",
    "    )\n",
    "    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    total_steps = int(max(1, int(epochs) * int(max(1, steps_per_epoch))))\n",
    "    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n",
    "    sch = make_warmup_step_scheduler(\n",
    "        opt,\n",
    "        total_steps=total_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        r1=float(cfg[\"lr_decay_ratio1\"]),\n",
    "        r2=float(cfg[\"lr_decay_ratio2\"]),\n",
    "        d1=float(cfg[\"lr_decay_rate1\"]),\n",
    "        d2=float(cfg[\"lr_decay_rate2\"]),\n",
    "    )\n",
    "    return opt, sch\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Train with internal val to get best_epoch\n",
    "# ----------------------------\n",
    "def train_with_internal_val_get_best_epoch(X_raw, y_raw, cfg, seed=2025):\n",
    "    seed_everything(int(seed))\n",
    "\n",
    "    is_val = make_case_split(df_train_tabular, val_frac=float(VAL_FRAC_CASE), seed=int(seed))\n",
    "    tr_idx = np.where(~is_val)[0]\n",
    "    va_idx = np.where(is_val)[0]\n",
    "\n",
    "    X_tr, y_tr = X_raw[tr_idx], y_raw[tr_idx]\n",
    "    X_va, y_va = X_raw[va_idx], y_raw[va_idx]\n",
    "\n",
    "    mu, sig = fit_standardizer(X_tr)\n",
    "    X_trn = apply_standardizer(X_tr, mu, sig)\n",
    "    X_van = apply_standardizer(X_va, mu, sig)\n",
    "\n",
    "    ds_tr = TabDataset(X_trn, y_tr)\n",
    "    ds_va = TabDataset(X_van, y_va)\n",
    "\n",
    "    dl_tr = make_loader(ds_tr, cfg[\"batch_size\"], shuffle=True)\n",
    "    dl_va = make_loader(ds_va, cfg[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model = build_model(cfg, n_features=X_raw.shape[1])\n",
    "\n",
    "    loss_fn = build_loss_fn(y_tr, cfg)\n",
    "\n",
    "    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    steps_per_epoch = int(math.ceil(len(dl_tr) / accum))\n",
    "    opt, sch = build_opt_and_sch(model, cfg, steps_per_epoch=steps_per_epoch, epochs=int(cfg[\"epochs\"]))\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True) if use_amp else None\n",
    "    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n",
    "\n",
    "    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n",
    "\n",
    "    best_val = 1e18\n",
    "    best_epoch = -1\n",
    "    bad = 0\n",
    "\n",
    "    print(f\"\\nInternal val split: train={len(tr_idx)} | val={len(va_idx)} | val_pos%={float(y_va.mean())*100:.2f}\")\n",
    "\n",
    "    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n",
    "\n",
    "    for epoch in range(int(cfg[\"epochs\"])):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "        micro = 0\n",
    "\n",
    "        for xb, yb in dl_tr:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            if input_noise_std and input_noise_std > 0:\n",
    "                xb = xb + torch.randn_like(xb) * input_noise_std\n",
    "\n",
    "            with ctx:\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb) / float(accum)\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            micro += 1\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "            if (micro % accum) == 0:\n",
    "                if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n",
    "                    if use_amp:\n",
    "                        scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n",
    "\n",
    "                if use_amp:\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    opt.step()\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        # flush last partial\n",
    "        if (micro % accum) != 0:\n",
    "            if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n",
    "                if use_amp:\n",
    "                    scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                opt.step()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            sch.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        p_va = predict_proba(model, dl_va, ema=ema)\n",
    "        vll = safe_logloss(y_va, p_va)\n",
    "        vauc = safe_auc(y_va, p_va)\n",
    "\n",
    "        improved = (best_val - vll) > float(cfg[\"min_delta\"])\n",
    "        if improved:\n",
    "            best_val = float(vll)\n",
    "            best_epoch = int(epoch) + 1\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        print(f\"  epoch {epoch+1:03d}/{int(cfg['epochs'])} | tr_loss={loss_sum/max(1,n_sum):.5f} | val_ll={vll:.5f} | val_auc={(vauc if vauc is not None else float('nan')):.5f} | bad={bad}\")\n",
    "\n",
    "        if EARLY_STOP and bad >= int(cfg[\"patience\"]):\n",
    "            break\n",
    "\n",
    "        gc.collect()\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if best_epoch < 1:\n",
    "        best_epoch = max(12, int(cfg[\"epochs\"] * 0.6))\n",
    "\n",
    "    return {\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_logloss\": float(best_val) if best_val < 1e18 else None,\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Train FULL data for fixed epochs (best_epoch)\n",
    "# ----------------------------\n",
    "def train_full_fixed_epochs(X_raw, y_raw, cfg, epochs_fixed, seed=2025):\n",
    "    seed_everything(int(seed))\n",
    "\n",
    "    mu, sig = fit_standardizer(X_raw)\n",
    "    Xn = apply_standardizer(X_raw, mu, sig)\n",
    "\n",
    "    ds = TabDataset(Xn, y_raw)\n",
    "    dl = make_loader(ds, cfg[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = build_model(cfg, n_features=X_raw.shape[1])\n",
    "    loss_fn = build_loss_fn(y_raw, cfg)\n",
    "\n",
    "    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    steps_per_epoch = int(math.ceil(len(dl) / accum))\n",
    "    opt, sch = build_opt_and_sch(model, cfg, steps_per_epoch=steps_per_epoch, epochs=int(epochs_fixed))\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True) if use_amp else None\n",
    "    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n",
    "    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n",
    "\n",
    "    t0 = time.time()\n",
    "    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n",
    "\n",
    "    for epoch in range(int(epochs_fixed)):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "        micro = 0\n",
    "\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            if input_noise_std and input_noise_std > 0:\n",
    "                xb = xb + torch.randn_like(xb) * input_noise_std\n",
    "\n",
    "            with ctx:\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb) / float(accum)\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            micro += 1\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "            if (micro % accum) == 0:\n",
    "                if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n",
    "                    if use_amp:\n",
    "                        scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n",
    "\n",
    "                if use_amp:\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    opt.step()\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        # flush last partial\n",
    "        if (micro % accum) != 0:\n",
    "            if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n",
    "                if use_amp:\n",
    "                    scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                opt.step()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            sch.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        print(f\"  full epoch {epoch+1:03d}/{int(epochs_fixed)} | loss={loss_sum/max(1,n_sum):.5f}\")\n",
    "\n",
    "        gc.collect()\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # save EMA weights if enabled\n",
    "    if ema is not None:\n",
    "        ema.apply_shadow(model)\n",
    "\n",
    "    pack = {\n",
    "        \"type\": \"mhc_lite_ft_transformer_full_v4\",\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": dict(cfg),\n",
    "        \"seed\": int(seed),\n",
    "        \"train_rows\": int(len(y_raw)),\n",
    "        \"pos_rate\": float(np.mean(y_raw)),\n",
    "        \"epochs_fixed\": int(epochs_fixed),\n",
    "        \"accum_steps\": int(accum),\n",
    "        \"train_time_s\": float(time.time() - t0),\n",
    "        \"used_ema_weights\": bool(ema is not None),\n",
    "    }\n",
    "    return pack\n",
    "\n",
    "# ----------------------------\n",
    "# 13) OOM fallback policy\n",
    "# ----------------------------\n",
    "def apply_oom_fallback(cfg: dict):\n",
    "    cfg = dict(cfg)\n",
    "\n",
    "    # 1) reduce batch first\n",
    "    cfg[\"batch_size\"] = max(64, int(cfg[\"batch_size\"]) // 2)\n",
    "\n",
    "    # recompute accum to keep eff batch\n",
    "    cfg[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(cfg[\"batch_size\"]))))\n",
    "\n",
    "    # 2) reduce width/depth gradually\n",
    "    cfg[\"d_model\"] = max(256, int(cfg[\"d_model\"]) - 64)\n",
    "    cfg[\"n_layers\"] = max(6, int(cfg[\"n_layers\"]) - 2)\n",
    "\n",
    "    # 3) fix heads divisibility (also may reduce heads)\n",
    "    cfg = fix_heads_divisibility(cfg)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Train final (OOM-safe)\n",
    "# ----------------------------\n",
    "final_full_packs = []\n",
    "internal_val_infos = []\n",
    "\n",
    "for s in range(int(N_SEEDS)):\n",
    "    seed_i = FINAL_SEED + s\n",
    "    print(f\"\\n[Final Train v4.1] seed={seed_i}\")\n",
    "\n",
    "    cfg_run = dict(CFG)\n",
    "\n",
    "    # retry loop for OOM\n",
    "    for attempt in range(6):\n",
    "        try:\n",
    "            # Phase A: get best_epoch from internal val\n",
    "            if USE_INTERNAL_VAL:\n",
    "                info = train_with_internal_val_get_best_epoch(X_all, y_all, cfg_run, seed=seed_i)\n",
    "                best_epoch = int(info[\"best_epoch\"])\n",
    "                internal_val_infos.append({\"seed\": seed_i, **info, \"cfg_used\": dict(cfg_run)})\n",
    "\n",
    "                # retrain full data (slight +5% for stability, capped by cfg_run[\"epochs\"])\n",
    "                E_FULL = int(min(int(cfg_run[\"epochs\"]), max(12, round(best_epoch * 1.05))))\n",
    "                print(f\"\\nBest_epoch(from internal val)={best_epoch} -> Retrain FULL for E_FULL={E_FULL}\")\n",
    "            else:\n",
    "                E_FULL = int(cfg_run[\"epochs\"])\n",
    "\n",
    "            full_pack = train_full_fixed_epochs(X_all, y_all, cfg_run, epochs_fixed=E_FULL, seed=seed_i)\n",
    "            final_full_packs.append(full_pack)\n",
    "            break\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e).lower()\n",
    "            if (\"out of memory\" in msg) and device.type == \"cuda\":\n",
    "                print(f\"  OOM detected (attempt {attempt+1}). Applying fallback.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                cfg_run = apply_oom_fallback(cfg_run)\n",
    "                print(\"  New CFG after fallback:\")\n",
    "                for k in [\"d_model\",\"n_layers\",\"n_heads\",\"batch_size\",\"accum_steps\",\"epochs\"]:\n",
    "                    print(f\"    {k}: {cfg_run[k]}\")\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if len(final_full_packs) == 0:\n",
    "    raise RuntimeError(\"Final training failed: no full_packs produced.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 15) Save artifacts\n",
    "# ----------------------------\n",
    "final_model_path = OUT_DIR / \"final_gate_model.pt\"\n",
    "\n",
    "# threshold recommended from Step 4 (OOF)\n",
    "best_thr = None\n",
    "if isinstance(best_bundle, dict):\n",
    "    best_thr = best_bundle.get(\"oof_best_thr\", None)\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"type\": \"final_gate_v4_1\",\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "\n",
    "        # keep fold ensemble from Step 4 if available\n",
    "        \"fold_packs\": fold_packs_from_step4,\n",
    "\n",
    "        # full-data trained packs (list; usually 1 seed)\n",
    "        \"full_packs\": final_full_packs,\n",
    "\n",
    "        # training meta\n",
    "        \"internal_val_infos\": internal_val_infos,\n",
    "        \"recommended_thr\": best_thr,\n",
    "        \"bundle_source\": source,\n",
    "        \"seed_base\": int(FINAL_SEED),\n",
    "    },\n",
    "    final_model_path\n",
    ")\n",
    "\n",
    "final_bundle = {\n",
    "    \"type\": \"final_gate_v4_1\",\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"n_seeds\": int(len(final_full_packs)),\n",
    "    \"seeds\": [int(p[\"seed\"]) for p in final_full_packs],\n",
    "    \"cfg_final\": dict(CFG),\n",
    "\n",
    "    \"use_internal_val\": bool(USE_INTERNAL_VAL),\n",
    "    \"val_frac_case\": float(VAL_FRAC_CASE) if USE_INTERNAL_VAL else 0.0,\n",
    "    \"early_stop\": bool(EARLY_STOP) if USE_INTERNAL_VAL else False,\n",
    "\n",
    "    \"train_rows\": int(len(y_all)),\n",
    "    \"pos_rate\": float(np.mean(y_all)),\n",
    "\n",
    "    \"recommended_thr\": best_thr,\n",
    "    \"has_fold_packs_from_step4\": bool(fold_packs_from_step4 is not None),\n",
    "    \"best_cfg_source\": source,\n",
    "    \"notes\": \"Final model uses Step4-matched mHC-lite FTTransformer with EMA+accum; best_epoch estimated via internal case-level val then retrain full.\",\n",
    "}\n",
    "\n",
    "final_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\n",
    "final_bundle_path.write_text(json.dumps(final_bundle, indent=2))\n",
    "\n",
    "print(\"\\nSaved final training artifacts:\")\n",
    "print(\"  model  ->\", final_model_path)\n",
    "print(\"  bundle ->\", final_bundle_path)\n",
    "\n",
    "# Export globals\n",
    "FINAL_GATE_MODEL_PT = str(final_model_path)\n",
    "FINAL_GATE_BUNDLE = final_bundle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab1ef8",
   "metadata": {
    "papermill": {
     "duration": 0.019477,
     "end_time": "2026-01-08T15:13:35.378316",
     "exception": false,
     "start_time": "2026-01-08T15:13:35.358839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Finalize & Save Model Bundle (Reproducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c428e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:13:35.418467Z",
     "iopub.status.busy": "2026-01-08T15:13:35.417943Z",
     "iopub.status.idle": "2026-01-08T15:14:04.400514Z",
     "shell.execute_reply": "2026-01-08T15:14:04.399642Z"
    },
    "papermill": {
     "duration": 29.004913,
     "end_time": "2026-01-08T15:14:04.402175",
     "exception": false,
     "start_time": "2026-01-08T15:13:35.397262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT (write): /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3\n",
      "\n",
      "SOURCE candidate picked: /kaggle/working/recodai_luc_gate_artifacts\n",
      "Score: 157\n",
      "\n",
      "Found artifacts (read):\n",
      "  final_model        : /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n",
      "  final_bundle       : /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n",
      "  feature_cols       : /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      "  best_gate_config   : /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n",
      "  best_gate_model    : /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt\n",
      "  baseline_report    : (missing/skip)\n",
      "  opt_results_csv    : /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n",
      "  opt_fold_csv       : /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n",
      "  oof_baseline_csv   : (missing/skip)\n",
      "\n",
      "Copying core files into OUT_DIR (portable):\n",
      "  [copied] -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/final_gate_model.pt\n",
      "  [copied] -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/final_gate_bundle.json\n",
      "  [copied] -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/feature_cols.json\n",
      "Warning: failed to read final_gate_model.pt for recommended_thr: UnpicklingError('Weights only load failed. This file can still be loaded, to do so you have two options, \\x1b[1mdo those steps only if you trust the source of the checkpoint\\x1b[0m. \\n\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\n\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\n\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\\n\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.')\n",
      "\n",
      "Threshold resolved:\n",
      "  T_gate: 0.07369999999999999\n",
      "  thresholds.json -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/thresholds.json\n",
      "\n",
      "OK — Model bundle finalized (Notebook-3 compatible)\n",
      "  SRC_DIR selected -> /kaggle/working/recodai_luc_gate_artifacts\n",
      "  MODEL_DIR        -> /kaggle/working/recodai_luc_gate_artifacts\n",
      "  OUT_DIR          -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3\n",
      "  manifest         -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/model_bundle_manifest.json\n",
      "  pack (json)      -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/model_bundle_pack.json\n",
      "  pack (joblib)    -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/model_bundle_pack.joblib\n",
      "  thresholds       -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/thresholds.json\n",
      "  zip              -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/model_bundle_v5_notebook3.zip\n",
      "\n",
      "Bundle summary:\n",
      "  bundle_version: v5_notebook3\n",
      "  model_format  : torch_pt\n",
      "  feature_cnt   : 84\n",
      "  T_gate        : 0.07369999999999999\n",
      "  task          : Recod.ai/LUC — Gate Model — DINOv2 features + Transformer gate (.pt)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 6 — Finalize & Save Model Bundle (Notebook-3 / Inference Notebook)\n",
    "# REVISI FULL v5.1 (portable + robust source discovery + copies core files)\n",
    "#\n",
    "# Goals:\n",
    "# - READ artifacts from /kaggle/input/... (read-only) OR /kaggle/working/... (if exists)\n",
    "# - AUTO-find the *actual* bundle folder (supports nested model_bundle_v*/ etc.)\n",
    "# - COPY core files into writable bundle folder:\n",
    "#     /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/\n",
    "# - WRITE thresholds/manifest/pack + ZIP to that bundle folder\n",
    "# - Compatible with Step 5 v4+:\n",
    "#     final_gate_model.pt may contain {fold_packs, full_packs, recommended_thr}\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time, platform, warnings, zipfile, hashlib, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0) IO roots\n",
    "# ----------------------------\n",
    "OUT_ROOT = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BUNDLE_VERSION = \"v5_notebook3\"\n",
    "OUT_DIR = OUT_ROOT / f\"model_bundle_{BUNDLE_VERSION}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"OUTPUT (write):\", OUT_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def read_json_safe(p: Path, default=None):\n",
    "    try:\n",
    "        return json.loads(Path(p).read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def sha256_file(p: Path, chunk=1024 * 1024):\n",
    "    p = Path(p)\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def file_meta(p: Path):\n",
    "    p = Path(p)\n",
    "    if not p.exists() or not p.is_file():\n",
    "        return None\n",
    "    st = p.stat()\n",
    "    return {\n",
    "        \"path\": str(p),\n",
    "        \"name\": p.name,\n",
    "        \"bytes\": int(st.st_size),\n",
    "        \"sha256\": sha256_file(p),\n",
    "        \"mtime_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(st.st_mtime)),\n",
    "    }\n",
    "\n",
    "def safe_add(zf: zipfile.ZipFile, p: Path, arcname: str):\n",
    "    if p is None:\n",
    "        return\n",
    "    p = Path(p)\n",
    "    if p.exists() and p.is_file():\n",
    "        zf.write(p, arcname=arcname)\n",
    "\n",
    "def pick_first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p is None:\n",
    "            continue\n",
    "        p = Path(p)\n",
    "        if p.exists() and p.is_file():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def find_file_near(root: Path, filename: str, max_depth: int = 3):\n",
    "    \"\"\"\n",
    "    Search root, then 1..max_depth levels deep for a file named filename.\n",
    "    Cheap bounded search (no full rglob).\n",
    "    \"\"\"\n",
    "    root = Path(root)\n",
    "    if (root / filename).exists():\n",
    "        return root / filename\n",
    "\n",
    "    # depth 1\n",
    "    for p1 in root.glob(\"*\"):\n",
    "        if p1.is_dir() and (p1 / filename).exists():\n",
    "            return p1 / filename\n",
    "\n",
    "    if max_depth >= 2:\n",
    "        for p2 in root.glob(\"*/*\"):\n",
    "            if p2.is_dir() and (p2 / filename).exists():\n",
    "                return p2 / filename\n",
    "\n",
    "    if max_depth >= 3:\n",
    "        for p3 in root.glob(\"*/*/*\"):\n",
    "            if p3.is_dir() and (p3 / filename).exists():\n",
    "                return p3 / filename\n",
    "\n",
    "    return None\n",
    "\n",
    "def copy_if_needed(src: Path, dst: Path, verbose=True):\n",
    "    src, dst = Path(src), Path(dst)\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"Missing src: {src}\")\n",
    "\n",
    "    if dst.exists() and dst.is_file():\n",
    "        try:\n",
    "            if src.stat().st_size == dst.stat().st_size:\n",
    "                # optional fast check: same size; verify by sha if you want strict\n",
    "                # strict sha check (safe):\n",
    "                if sha256_file(src) == sha256_file(dst):\n",
    "                    if verbose:\n",
    "                        print(\"  [skip copy] already identical:\", dst.name)\n",
    "                    return dst\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    shutil.copy2(src, dst)\n",
    "    if verbose:\n",
    "        print(\"  [copied] ->\", dst)\n",
    "    return dst\n",
    "\n",
    "def _score_dir(p: Path):\n",
    "    \"\"\"\n",
    "    Score candidate directory based on presence of key artifacts directly inside.\n",
    "    \"\"\"\n",
    "    p = Path(p)\n",
    "    score = 0\n",
    "    if (p / \"final_gate_model.pt\").exists(): score += 100\n",
    "    if (p / \"final_gate_bundle.json\").exists(): score += 30\n",
    "    if (p / \"feature_cols.json\").exists(): score += 20\n",
    "    if (p / \"thresholds.json\").exists(): score += 5\n",
    "    if (p / \"best_gate_config.json\").exists(): score += 3\n",
    "    if (p / \"best_gate_model.pt\").exists(): score += 3\n",
    "    if (p / \"opt_search\" / \"opt_results.csv\").exists(): score += 1\n",
    "    return score\n",
    "\n",
    "def _gather_candidate_dirs():\n",
    "    \"\"\"\n",
    "    Collect candidate directories that *might* contain final_gate_model.pt.\n",
    "    Supports nested structures like:\n",
    "      /kaggle/input/<ds>/recodai_luc_gate_artifacts/model_bundle_v3/final_gate_model.pt\n",
    "    \"\"\"\n",
    "    cands = []\n",
    "\n",
    "    # Working roots (if you ran training in same notebook)\n",
    "    for base in [\n",
    "        Path(\"/kaggle/working/recodai_luc_gate_artifacts\"),\n",
    "        OUT_ROOT,  # current bundle root too\n",
    "    ]:\n",
    "        if base.exists():\n",
    "            cands.append(base)\n",
    "            # common nested\n",
    "            for sub in base.glob(\"model_bundle_*\"):\n",
    "                if sub.is_dir():\n",
    "                    cands.append(sub)\n",
    "\n",
    "    # Inputs (read-only datasets)\n",
    "    inp = Path(\"/kaggle/input\")\n",
    "    if inp.exists():\n",
    "        for ds in inp.iterdir():\n",
    "            if not ds.is_dir():\n",
    "                continue\n",
    "\n",
    "            # bounded-depth directory walk: ds, ds/*, ds/*/*, ds/*/*/*\n",
    "            level0 = [ds]\n",
    "            level1 = [p for p in ds.glob(\"*\") if p.is_dir()]\n",
    "            level2 = [p for p in ds.glob(\"*/*\") if p.is_dir()]\n",
    "            level3 = [p for p in ds.glob(\"*/*/*\") if p.is_dir()]\n",
    "            for d in (level0 + level1 + level2 + level3):\n",
    "                # if directory name hints artifacts, include it\n",
    "                if d.name == \"recodai_luc_gate_artifacts\" or \"recodai\" in d.name.lower() or \"bundle\" in d.name.lower():\n",
    "                    cands.append(d)\n",
    "                # if it directly contains final model, include it\n",
    "                if (d / \"final_gate_model.pt\").exists():\n",
    "                    cands.append(d)\n",
    "                # if it's the artifacts root, also include common nested bundles\n",
    "                if d.name == \"recodai_luc_gate_artifacts\":\n",
    "                    for sub in d.glob(\"model_bundle_*\"):\n",
    "                        if sub.is_dir():\n",
    "                            cands.append(sub)\n",
    "\n",
    "    # de-dup preserve order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in cands:\n",
    "        p = Path(p)\n",
    "        sp = str(p)\n",
    "        if sp not in seen and p.exists() and p.is_dir():\n",
    "            seen.add(sp)\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Pick best SRC_DIR\n",
    "# ----------------------------\n",
    "SRC_CANDS = _gather_candidate_dirs()\n",
    "if len(SRC_CANDS) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        \"Tidak menemukan kandidat folder artifacts di /kaggle/working maupun /kaggle/input.\\n\"\n",
    "        \"Pastikan kamu sudah add dataset output training ke notebook ini.\"\n",
    "    )\n",
    "\n",
    "# If none has high score, we still pick best and do a deeper file search below.\n",
    "SRC_DIR = max(SRC_CANDS, key=_score_dir)\n",
    "\n",
    "print(\"\\nSOURCE candidate picked:\", SRC_DIR)\n",
    "print(\"Score:\", _score_dir(SRC_DIR))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Locate required artifacts (robust)\n",
    "# ----------------------------\n",
    "final_model_pt = find_file_near(SRC_DIR, \"final_gate_model.pt\", max_depth=3)\n",
    "if final_model_pt is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing final_gate_model.pt in selected SRC_DIR (even after nested search).\\n\"\n",
    "        f\"SRC_DIR={SRC_DIR}\\n\"\n",
    "        \"Cek struktur dataset input kamu (mungkin bundle ada lebih dalam).\"\n",
    "    )\n",
    "\n",
    "# For bundle json and feature cols: search near final_model folder first (more reliable)\n",
    "MODEL_DIR = final_model_pt.parent\n",
    "\n",
    "final_bundle_json = find_file_near(MODEL_DIR, \"final_gate_bundle.json\", max_depth=2)\n",
    "feature_cols_path = find_file_near(MODEL_DIR, \"feature_cols.json\", max_depth=2)\n",
    "\n",
    "# Fallback: search near SRC_DIR if still missing\n",
    "if final_bundle_json is None:\n",
    "    final_bundle_json = find_file_near(SRC_DIR, \"final_gate_bundle.json\", max_depth=3)\n",
    "if feature_cols_path is None:\n",
    "    feature_cols_path = find_file_near(SRC_DIR, \"feature_cols.json\", max_depth=3)\n",
    "\n",
    "if feature_cols_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing feature_cols.json (jalankan Step 2 dulu di training notebook / pastikan ikut dibundle).\"\n",
    "    )\n",
    "\n",
    "# Optional / extras\n",
    "baseline_report_path = pick_first_existing([\n",
    "    find_file_near(MODEL_DIR, \"baseline_mhc_transformer_cv_report.json\", max_depth=2),\n",
    "    find_file_near(MODEL_DIR, \"baseline_transformer_cv_report.json\", max_depth=2),\n",
    "    find_file_near(MODEL_DIR, \"baseline_cv_report.json\", max_depth=2),\n",
    "    find_file_near(SRC_DIR,   \"baseline_mhc_transformer_cv_report.json\", max_depth=3),\n",
    "    find_file_near(SRC_DIR,   \"baseline_transformer_cv_report.json\", max_depth=3),\n",
    "    find_file_near(SRC_DIR,   \"baseline_cv_report.json\", max_depth=3),\n",
    "])\n",
    "\n",
    "best_gate_config_path = pick_first_existing([\n",
    "    find_file_near(MODEL_DIR, \"best_gate_config.json\", max_depth=2),\n",
    "    find_file_near(SRC_DIR,   \"best_gate_config.json\", max_depth=3),\n",
    "])\n",
    "\n",
    "best_gate_model_path = pick_first_existing([\n",
    "    find_file_near(MODEL_DIR, \"best_gate_model.pt\", max_depth=2),\n",
    "    find_file_near(SRC_DIR,   \"best_gate_model.pt\", max_depth=3),\n",
    "])\n",
    "\n",
    "opt_results_csv = pick_first_existing([\n",
    "    find_file_near(MODEL_DIR, \"opt_results.csv\", max_depth=3),\n",
    "    find_file_near(SRC_DIR,   \"opt_results.csv\", max_depth=3),\n",
    "    (MODEL_DIR / \"opt_search\" / \"opt_results.csv\") if (MODEL_DIR / \"opt_search\" / \"opt_results.csv\").exists() else None,\n",
    "    (SRC_DIR   / \"opt_search\" / \"opt_results.csv\") if (SRC_DIR   / \"opt_search\" / \"opt_results.csv\").exists() else None,\n",
    "])\n",
    "\n",
    "opt_fold_csv = pick_first_existing([\n",
    "    find_file_near(MODEL_DIR, \"opt_fold_details.csv\", max_depth=3),\n",
    "    find_file_near(SRC_DIR,   \"opt_fold_details.csv\", max_depth=3),\n",
    "    (MODEL_DIR / \"opt_search\" / \"opt_fold_details.csv\") if (MODEL_DIR / \"opt_search\" / \"opt_fold_details.csv\").exists() else None,\n",
    "    (SRC_DIR   / \"opt_search\" / \"opt_fold_details.csv\") if (SRC_DIR   / \"opt_search\" / \"opt_fold_details.csv\").exists() else None,\n",
    "])\n",
    "\n",
    "oof_baseline_csv = pick_first_existing([\n",
    "    find_file_near(MODEL_DIR, \"oof_baseline_mhc_transformer.csv\", max_depth=2),\n",
    "    find_file_near(MODEL_DIR, \"oof_baseline_transformer.csv\", max_depth=2),\n",
    "    find_file_near(MODEL_DIR, \"oof_baseline.csv\", max_depth=2),\n",
    "    find_file_near(SRC_DIR,   \"oof_baseline_mhc_transformer.csv\", max_depth=3),\n",
    "    find_file_near(SRC_DIR,   \"oof_baseline_transformer.csv\", max_depth=3),\n",
    "    find_file_near(SRC_DIR,   \"oof_baseline.csv\", max_depth=3),\n",
    "])\n",
    "\n",
    "print(\"\\nFound artifacts (read):\")\n",
    "print(\"  final_model        :\", final_model_pt)\n",
    "print(\"  final_bundle       :\", final_bundle_json if (final_bundle_json and final_bundle_json.exists()) else \"(missing/skip)\")\n",
    "print(\"  feature_cols       :\", feature_cols_path)\n",
    "print(\"  best_gate_config   :\", best_gate_config_path if best_gate_config_path else \"(missing/skip)\")\n",
    "print(\"  best_gate_model    :\", best_gate_model_path if best_gate_model_path else \"(missing/skip)\")\n",
    "print(\"  baseline_report    :\", baseline_report_path if baseline_report_path else \"(missing/skip)\")\n",
    "print(\"  opt_results_csv    :\", opt_results_csv if opt_results_csv else \"(missing/skip)\")\n",
    "print(\"  opt_fold_csv       :\", opt_fold_csv if opt_fold_csv else \"(missing/skip)\")\n",
    "print(\"  oof_baseline_csv   :\", oof_baseline_csv if oof_baseline_csv else \"(missing/skip)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) COPY core artifacts into OUT_DIR (portable bundle)\n",
    "# ----------------------------\n",
    "print(\"\\nCopying core files into OUT_DIR (portable):\")\n",
    "final_model_dst = copy_if_needed(final_model_pt, OUT_DIR / \"final_gate_model.pt\")\n",
    "final_bundle_dst = None\n",
    "if final_bundle_json is not None and final_bundle_json.exists():\n",
    "    final_bundle_dst = copy_if_needed(final_bundle_json, OUT_DIR / \"final_gate_bundle.json\")\n",
    "feature_cols_dst = copy_if_needed(feature_cols_path, OUT_DIR / \"feature_cols.json\")\n",
    "\n",
    "# Copy extras (optional, but useful)\n",
    "extras_dir = OUT_DIR / \"extras\"\n",
    "extras_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "baseline_report_dst = None\n",
    "if baseline_report_path:\n",
    "    baseline_report_dst = copy_if_needed(baseline_report_path, extras_dir / Path(baseline_report_path).name, verbose=False)\n",
    "\n",
    "best_gate_config_dst = None\n",
    "if best_gate_config_path:\n",
    "    best_gate_config_dst = copy_if_needed(best_gate_config_path, extras_dir / Path(best_gate_config_path).name, verbose=False)\n",
    "\n",
    "best_gate_model_dst = None\n",
    "if best_gate_model_path:\n",
    "    best_gate_model_dst = copy_if_needed(best_gate_model_path, extras_dir / Path(best_gate_model_path).name, verbose=False)\n",
    "\n",
    "opt_dir = OUT_DIR / \"opt_search\"\n",
    "opt_dir.mkdir(parents=True, exist_ok=True)\n",
    "opt_results_dst = None\n",
    "if opt_results_csv:\n",
    "    opt_results_dst = copy_if_needed(opt_results_csv, opt_dir / Path(opt_results_csv).name, verbose=False)\n",
    "\n",
    "opt_fold_dst = None\n",
    "if opt_fold_csv:\n",
    "    opt_fold_dst = copy_if_needed(opt_fold_csv, opt_dir / Path(opt_fold_csv).name, verbose=False)\n",
    "\n",
    "oof_dir = OUT_DIR / \"oof\"\n",
    "oof_dir.mkdir(parents=True, exist_ok=True)\n",
    "oof_baseline_dst = None\n",
    "if oof_baseline_csv:\n",
    "    oof_baseline_dst = copy_if_needed(oof_baseline_csv, oof_dir / Path(oof_baseline_csv).name, verbose=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Load metadata (from copied files)\n",
    "# ----------------------------\n",
    "feature_cols = read_json_safe(feature_cols_dst, default=[])\n",
    "if not isinstance(feature_cols, list) or len(feature_cols) == 0:\n",
    "    raise ValueError(f\"feature_cols invalid/empty: {feature_cols_dst}\")\n",
    "\n",
    "final_bundle = read_json_safe(final_bundle_dst, default={}) if final_bundle_dst else {}\n",
    "baseline_report = read_json_safe(baseline_report_dst, default=None) if baseline_report_dst else None\n",
    "best_gate_config = read_json_safe(best_gate_config_dst, default=None) if best_gate_config_dst else None\n",
    "\n",
    "# recommended_thr from final_gate_model.pt\n",
    "recommended_thr_from_pt = None\n",
    "try:\n",
    "    import torch\n",
    "    obj = torch.load(final_model_dst, map_location=\"cpu\")\n",
    "    if isinstance(obj, dict):\n",
    "        recommended_thr_from_pt = obj.get(\"recommended_thr\", None)\n",
    "except Exception as e:\n",
    "    print(\"Warning: failed to read final_gate_model.pt for recommended_thr:\", repr(e))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Thresholds resolve (robust priority)\n",
    "# Priority:\n",
    "#   (a) SRC thresholds.json near original model dir\n",
    "#   (b) existing OUT thresholds.json (if rerun)\n",
    "#   (c) final_gate_bundle.json -> recommended_thr\n",
    "#   (d) final_gate_model.pt -> recommended_thr\n",
    "#   (e) best_gate_config.json -> oof_best_thr OR selection.oof_best_thr (legacy)\n",
    "#   (f) fallback 0.5\n",
    "# ----------------------------\n",
    "def extract_thr_from_best_gate_config(cfg: dict):\n",
    "    if not isinstance(cfg, dict):\n",
    "        return None\n",
    "    if \"oof_best_thr\" in cfg:\n",
    "        try:\n",
    "            return float(cfg[\"oof_best_thr\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    sel = cfg.get(\"selection\", None)\n",
    "    if isinstance(sel, dict) and (\"oof_best_thr\" in sel):\n",
    "        try:\n",
    "            return float(sel[\"oof_best_thr\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "T_gate = None\n",
    "src_thresh = find_file_near(MODEL_DIR, \"thresholds.json\", max_depth=2) or find_file_near(SRC_DIR, \"thresholds.json\", max_depth=3)\n",
    "out_thresh = OUT_DIR / \"thresholds.json\"\n",
    "\n",
    "# (a) SRC thresholds.json\n",
    "if src_thresh and src_thresh.exists():\n",
    "    tj = read_json_safe(src_thresh, default={})\n",
    "    if isinstance(tj, dict) and tj.get(\"T_gate\", None) is not None:\n",
    "        try:\n",
    "            T_gate = float(tj[\"T_gate\"])\n",
    "        except Exception:\n",
    "            T_gate = None\n",
    "\n",
    "# (b) OUT thresholds.json (existing)\n",
    "if T_gate is None and out_thresh.exists():\n",
    "    tj = read_json_safe(out_thresh, default={})\n",
    "    if isinstance(tj, dict) and tj.get(\"T_gate\", None) is not None:\n",
    "        try:\n",
    "            T_gate = float(tj[\"T_gate\"])\n",
    "        except Exception:\n",
    "            T_gate = None\n",
    "\n",
    "# (c) final_bundle recommended_thr\n",
    "if T_gate is None and isinstance(final_bundle, dict):\n",
    "    try:\n",
    "        if final_bundle.get(\"recommended_thr\", None) is not None:\n",
    "            T_gate = float(final_bundle[\"recommended_thr\"])\n",
    "    except Exception:\n",
    "        T_gate = None\n",
    "\n",
    "# (d) final_model.pt recommended_thr\n",
    "if T_gate is None and recommended_thr_from_pt is not None:\n",
    "    try:\n",
    "        T_gate = float(recommended_thr_from_pt)\n",
    "    except Exception:\n",
    "        T_gate = None\n",
    "\n",
    "# (e) best_gate_config oof_best_thr\n",
    "if T_gate is None and isinstance(best_gate_config, dict):\n",
    "    T_gate = extract_thr_from_best_gate_config(best_gate_config)\n",
    "\n",
    "# (f) fallback\n",
    "if T_gate is None:\n",
    "    T_gate = 0.5\n",
    "\n",
    "thresholds = {\n",
    "    \"T_gate\": float(T_gate),\n",
    "    \"beta_for_tuning\": float(best_gate_config.get(\"beta_for_tuning\", 0.5)) if isinstance(best_gate_config, dict) else 0.5,\n",
    "    \"guards\": {\"min_area_frac\": None, \"max_area_frac\": None, \"max_components\": None},\n",
    "    \"source_priority\": [\n",
    "        \"SRC thresholds.json (near model)\",\n",
    "        \"OUT_DIR/thresholds.json (existing)\",\n",
    "        \"final_gate_bundle.json.recommended_thr\",\n",
    "        \"final_gate_model.pt.recommended_thr\",\n",
    "        \"best_gate_config.json.oof_best_thr (or selection.oof_best_thr legacy)\",\n",
    "        \"fallback 0.5\",\n",
    "    ],\n",
    "    \"notes\": \"Gate threshold used for binary decision. Update after calibration/OOF tuning if needed.\",\n",
    "}\n",
    "\n",
    "thresholds_path = OUT_DIR / \"thresholds.json\"\n",
    "thresholds_path.write_text(json.dumps(thresholds, indent=2))\n",
    "\n",
    "print(\"\\nThreshold resolved:\")\n",
    "print(\"  T_gate:\", thresholds[\"T_gate\"])\n",
    "print(\"  thresholds.json ->\", thresholds_path)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) cfg_meta (optional)\n",
    "# ----------------------------\n",
    "cfg_meta = {}\n",
    "if \"PATHS\" in globals() and isinstance(PATHS, dict):\n",
    "    cfg_meta = {k: PATHS.get(k, None) for k in [\n",
    "        \"COMP_ROOT\",\"OUT_DS_ROOT\",\"OUT_ROOT\",\"MATCH_CFG_DIR\",\"PRED_CFG_DIR\",\"DINO_CFG_DIR\",\"DINO_LARGE_DIR\",\n",
    "        \"PRED_FEAT_TRAIN\",\"MATCH_FEAT_TRAIN\",\"DF_TRAIN_ALL\",\"CV_CASE_FOLDS\",\"IMG_PROFILE_TRAIN\"\n",
    "    ]}\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Manifest (reproducible) + sha256 index (for OUT_DIR files)\n",
    "# ----------------------------\n",
    "task_str = \"Recod.ai/LUC — Gate Model — DINOv2 features + Transformer gate (.pt)\"\n",
    "model_format = \"torch_pt\"\n",
    "\n",
    "artifact_paths = [\n",
    "    final_model_dst,\n",
    "    final_bundle_dst if final_bundle_dst else None,\n",
    "    feature_cols_dst,\n",
    "    thresholds_path,\n",
    "    baseline_report_dst,\n",
    "    best_gate_config_dst,\n",
    "    best_gate_model_dst,\n",
    "    opt_results_dst,\n",
    "    opt_fold_dst,\n",
    "    oof_baseline_dst,\n",
    "]\n",
    "artifact_paths = [p for p in artifact_paths if p is not None]\n",
    "\n",
    "artifacts_meta = {}\n",
    "for p in artifact_paths:\n",
    "    m = file_meta(p)\n",
    "    if m is not None:\n",
    "        artifacts_meta[m[\"name\"]] = m\n",
    "\n",
    "opt_summary = None\n",
    "if isinstance(best_gate_config, dict):\n",
    "    opt_summary = best_gate_config.get(\"selection\", None)\n",
    "    if opt_summary is None:\n",
    "        opt_summary = {\n",
    "            \"model_name\": best_gate_config.get(\"model_name\", None),\n",
    "            \"oof_best_thr\": best_gate_config.get(\"oof_best_thr\", None),\n",
    "            \"oof_best_fbeta\": best_gate_config.get(\"oof_best_fbeta\", None),\n",
    "            \"oof_auc\": best_gate_config.get(\"oof_auc\", None),\n",
    "            \"oof_logloss\": best_gate_config.get(\"oof_logloss\", None),\n",
    "        }\n",
    "\n",
    "manifest = {\n",
    "    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"bundle_version\": BUNDLE_VERSION,\n",
    "    \"task\": task_str,\n",
    "    \"model_format\": model_format,\n",
    "    \"source_dir_selected\": str(SRC_DIR),\n",
    "    \"source_model_dir\": str(MODEL_DIR),\n",
    "    \"output_dir\": str(OUT_DIR),\n",
    "    \"artifacts_index\": artifacts_meta,\n",
    "    \"cfg_meta\": cfg_meta,\n",
    "    \"model_summary\": {\n",
    "        \"type\": (final_bundle.get(\"type\") if isinstance(final_bundle, dict) else None),\n",
    "        \"n_seeds\": (final_bundle.get(\"n_seeds\") if isinstance(final_bundle, dict) else None),\n",
    "        \"seeds\": (final_bundle.get(\"seeds\") if isinstance(final_bundle, dict) else None),\n",
    "        \"train_rows\": (final_bundle.get(\"train_rows\") if isinstance(final_bundle, dict) else None),\n",
    "        \"pos_rate\": (final_bundle.get(\"pos_rate\") if isinstance(final_bundle, dict) else None),\n",
    "        \"feature_count\": int(len(feature_cols)),\n",
    "        \"T_gate\": float(thresholds.get(\"T_gate\", 0.5)),\n",
    "        \"recommended_thr_from_pt\": recommended_thr_from_pt,\n",
    "    },\n",
    "    \"baseline_summary\": (baseline_report.get(\"overall\") if isinstance(baseline_report, dict) else None),\n",
    "    \"opt_summary\": opt_summary,\n",
    "}\n",
    "\n",
    "manifest_path = OUT_DIR / \"model_bundle_manifest.json\"\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Bundle pack (portable JSON) + optional joblib\n",
    "# ----------------------------\n",
    "bundle_pack = {\n",
    "    \"bundle_version\": BUNDLE_VERSION,\n",
    "    \"model_format\": model_format,\n",
    "    \"bundle_files\": {\n",
    "        \"final_gate_model.pt\": \"final_gate_model.pt\",\n",
    "        \"final_gate_bundle.json\": \"final_gate_bundle.json\" if final_bundle_dst else None,\n",
    "        \"feature_cols.json\": \"feature_cols.json\",\n",
    "        \"thresholds.json\": \"thresholds.json\",\n",
    "        \"model_bundle_manifest.json\": \"model_bundle_manifest.json\",\n",
    "        \"model_bundle_pack.json\": \"model_bundle_pack.json\",\n",
    "        \"model_bundle_pack.joblib\": \"model_bundle_pack.joblib\",\n",
    "        \"model_bundle_v5_notebook3.zip\": \"model_bundle_v5_notebook3.zip\",\n",
    "    },\n",
    "    \"thresholds\": thresholds,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"cfg_meta\": cfg_meta,\n",
    "    \"manifest\": manifest,\n",
    "}\n",
    "\n",
    "bundle_pack_json = OUT_DIR / \"model_bundle_pack.json\"\n",
    "bundle_pack_json.write_text(json.dumps(bundle_pack, indent=2))\n",
    "\n",
    "bundle_pack_joblib = OUT_DIR / \"model_bundle_pack.joblib\"\n",
    "joblib_ok = False\n",
    "try:\n",
    "    import joblib\n",
    "    joblib.dump(bundle_pack, bundle_pack_joblib)\n",
    "    joblib_ok = True\n",
    "except Exception:\n",
    "    joblib_ok = False\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Create portable ZIP (writes to OUT_DIR)\n",
    "# ----------------------------\n",
    "zip_path = OUT_DIR / \"model_bundle_v5_notebook3.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    # core files (from OUT_DIR)\n",
    "    safe_add(zf, final_model_dst, \"final_gate_model.pt\")\n",
    "    safe_add(zf, final_bundle_dst, \"final_gate_bundle.json\")\n",
    "    safe_add(zf, feature_cols_dst, \"feature_cols.json\")\n",
    "    safe_add(zf, thresholds_path, \"thresholds.json\")\n",
    "    safe_add(zf, manifest_path, \"model_bundle_manifest.json\")\n",
    "    safe_add(zf, bundle_pack_json, \"model_bundle_pack.json\")\n",
    "    if joblib_ok:\n",
    "        safe_add(zf, bundle_pack_joblib, \"model_bundle_pack.joblib\")\n",
    "\n",
    "    # extras (already copied into OUT_DIR)\n",
    "    for p in (extras_dir.glob(\"*\") if extras_dir.exists() else []):\n",
    "        safe_add(zf, p, f\"extras/{p.name}\")\n",
    "\n",
    "    # opt_search\n",
    "    if opt_dir.exists():\n",
    "        for p in opt_dir.glob(\"*\"):\n",
    "            safe_add(zf, p, f\"opt_search/{p.name}\")\n",
    "\n",
    "    # oof\n",
    "    if oof_dir.exists():\n",
    "        for p in oof_dir.glob(\"*\"):\n",
    "            safe_add(zf, p, f\"oof/{p.name}\")\n",
    "\n",
    "print(\"\\nOK — Model bundle finalized (Notebook-3 compatible)\")\n",
    "print(\"  SRC_DIR selected ->\", SRC_DIR)\n",
    "print(\"  MODEL_DIR        ->\", MODEL_DIR)\n",
    "print(\"  OUT_DIR          ->\", OUT_DIR)\n",
    "print(\"  manifest         ->\", manifest_path)\n",
    "print(\"  pack (json)      ->\", bundle_pack_json)\n",
    "print(\"  pack (joblib)    ->\", (bundle_pack_joblib if joblib_ok else \"(skip; joblib not available)\"))\n",
    "print(\"  thresholds       ->\", thresholds_path)\n",
    "print(\"  zip              ->\", zip_path)\n",
    "\n",
    "print(\"\\nBundle summary:\")\n",
    "print(\"  bundle_version:\", BUNDLE_VERSION)\n",
    "print(\"  model_format  :\", model_format)\n",
    "print(\"  feature_cnt   :\", len(feature_cols))\n",
    "print(\"  T_gate        :\", thresholds.get(\"T_gate\"))\n",
    "print(\"  task          :\", task_str)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14878066,
     "sourceId": 113558,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15201056,
     "datasetId": 9171323,
     "sourceId": 14387840,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 6346563,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13172.649346,
   "end_time": "2026-01-08T15:14:07.982809",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T11:34:35.333463",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
