{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":113558,"databundleVersionId":14878066},{"sourceType":"datasetVersion","sourceId":14433337,"datasetId":9171323,"databundleVersionId":15251601},{"sourceType":"modelInstanceVersion","sourceId":4535,"databundleVersionId":6346563,"modelInstanceId":3327}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Paths & Select Config (CFG)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE — Set Paths & Select Config (CFG) (ONE CELL)\n# - Auto-load competition + prep artifacts\n# - Auto-pick latest TOKEN cache / MATCH cache / PRED_ENS dir (unless already in globals)\n# - Build and freeze feature_cols for Gate (LightGBM) from pred_features_train.csv\n# - Save:\n#     /kaggle/working/recodai_luc_gate_artifacts/train_cfg.json\n#     /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n# ============================================================\n\nimport os, json, hashlib, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Locations (fixed)\n# ----------------------------\nPROF_DIR = Path(\"/kaggle/working/recodai_luc_prof\")\nCACHE_DIR = Path(\"/kaggle/working/recodai_luc/cache\")\nART_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nfor req in [\"paths.json\", \"train_manifest.parquet\", \"test_manifest.parquet\", \"folds.parquet\"]:\n    p = PROF_DIR / req\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {p}. Run PREP stages first.\")\n\nPATHS = json.loads((PROF_DIR / \"paths.json\").read_text())\ndf_train = pd.read_parquet(PROF_DIR / \"train_manifest.parquet\")\ndf_test  = pd.read_parquet(PROF_DIR / \"test_manifest.parquet\")\ndf_folds = pd.read_parquet(PROF_DIR / \"folds.parquet\")\n\n# ----------------------------\n# Helpers: auto-pick latest cache roots\n# ----------------------------\ndef _pick_latest_dir(glob_pat: str, must_have: str):\n    cands = sorted(CACHE_DIR.glob(glob_pat))\n    cands = [c for c in cands if c.is_dir() and (c / must_have).exists()]\n    if not cands:\n        return None\n    cands = sorted(cands, key=lambda p: (p / must_have).stat().st_mtime, reverse=True)\n    return cands[0]\n\ndef pick_token_root():\n    if \"TOKEN_CACHE_ROOT\" in globals():\n        r = Path(str(globals()[\"TOKEN_CACHE_ROOT\"]))\n        if r.exists() and (r / \"cfg.json\").exists():\n            return r\n    return _pick_latest_dir(\"dinov2_*cfg_*\", \"cfg.json\")\n\ndef pick_match_root():\n    if \"MATCH_CACHE_ROOT\" in globals():\n        r = Path(str(globals()[\"MATCH_CACHE_ROOT\"]))\n        if r.exists() and (r / \"cfg.json\").exists():\n            return r\n    return _pick_latest_dir(\"match_cfg_*\", \"cfg.json\")\n\ndef pick_pred_ens_dir():\n    # preferred: from globals\n    if \"PRED_ENS_DIR\" in globals():\n        r = Path(str(globals()[\"PRED_ENS_DIR\"]))\n        if r.exists():\n            return r\n    # preferred canonical\n    r = CACHE_DIR / \"pred_ens\"\n    if r.exists():\n        return r\n    # fallback search\n    cands = [d for d in CACHE_DIR.glob(\"*\") if d.is_dir() and (d / \"pred_features_train.csv\").exists()]\n    if cands:\n        cands = sorted(cands, key=lambda p: (p / \"pred_features_train.csv\").stat().st_mtime, reverse=True)\n        return cands[0]\n    return None\n\nTOKEN_ROOT = pick_token_root()\nMATCH_ROOT = pick_match_root()\nPRED_ENS_DIR = pick_pred_ens_dir()\n\nif TOKEN_ROOT is None:\n    raise FileNotFoundError(\"TOKEN cache not found. Run 'DINOv2 Feature Cache' stage first.\")\nif MATCH_ROOT is None:\n    raise FileNotFoundError(\"MATCH cache not found. Run 'Robust Matching' stage first.\")\nif PRED_ENS_DIR is None:\n    raise FileNotFoundError(\"PRED_ENS_DIR not found. Run 'Verification, Mask Reconstruction & Postprocess' stage first.\")\n\nPRED_FEATURES_TRAIN = PRED_ENS_DIR / \"pred_features_train.csv\"\nPRED_FEATURES_TEST  = PRED_ENS_DIR / \"pred_features_test.csv\"\nif not PRED_FEATURES_TRAIN.exists():\n    raise FileNotFoundError(f\"Missing {PRED_FEATURES_TRAIN}. Run Verification stage first.\")\nif not PRED_FEATURES_TEST.exists():\n    raise FileNotFoundError(f\"Missing {PRED_FEATURES_TEST}. Run Verification stage first.\")\n\n# ----------------------------\n# 1) Feature columns (freeze)\n# ----------------------------\ndf_feat_train = pd.read_csv(PRED_FEATURES_TRAIN)\n# must have case_id\nif \"case_id\" not in df_feat_train.columns:\n    raise ValueError(\"pred_features_train.csv must contain 'case_id'\")\n\n# prioritize these (core features)\ncore = [\n    \"best_peak_score\", \"has_match\", \"n_inst\",\n    \"area_frac\", \"area_frac_tok\", \"mean_prob_tok\",\n    \"has_prob\"\n]\n# keep only numeric columns (exclude ids/paths)\nexclude = set([\"case_id\", \"split\", \"img_path\", \"npz_path\"])\nnumeric_cols = []\nfor c in df_feat_train.columns:\n    if c in exclude:\n        continue\n    # accept core even if dtype object (will coerce later), else require numeric-ish\n    if c in core:\n        numeric_cols.append(c)\n        continue\n    if pd.api.types.is_numeric_dtype(df_feat_train[c]):\n        numeric_cols.append(c)\n\n# ensure core first if present\nfeature_cols = [c for c in core if c in numeric_cols] + [c for c in numeric_cols if c not in core]\n\nif len(feature_cols) == 0:\n    raise RuntimeError(\"No usable feature columns found in pred_features_train.csv\")\n\n(ART_DIR / \"feature_cols.json\").write_text(json.dumps(feature_cols, indent=2))\n\n# ----------------------------\n# 2) CFG (Gate + calibration + postprocess knobs reference)\n# ----------------------------\nCFG = {\n    \"version\": \"gate_v1\",\n    \"created_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n    \"paths\": {\n        \"COMP_ROOT\": PATHS.get(\"COMP_ROOT\"),\n        \"SAMPLE_SUB\": PATHS.get(\"SAMPLE_SUB\"),\n        \"PROF_DIR\": str(PROF_DIR),\n        \"CACHE_DIR\": str(CACHE_DIR),\n        \"ART_DIR\": str(ART_DIR),\n        \"TOKEN_ROOT\": str(TOKEN_ROOT),\n        \"MATCH_ROOT\": str(MATCH_ROOT),\n        \"PRED_ENS_DIR\": str(PRED_ENS_DIR),\n        \"PRED_FEATURES_TRAIN\": str(PRED_FEATURES_TRAIN),\n        \"PRED_FEATURES_TEST\": str(PRED_FEATURES_TEST),\n    },\n    \"cv\": {\n        \"n_folds\": int(df_folds[\"fold\"].nunique()),\n        \"seed\": 42,\n        \"split_key\": \"case_id\",\n        \"fold_col\": \"fold\",\n        \"label_col\": \"y\",\n    },\n    \"features\": {\n        \"feature_cols_path\": str(ART_DIR / \"feature_cols.json\"),\n        \"feature_cols\": feature_cols,\n        \"missing_numeric_fill\": 0.0,\n    },\n    \"gate_model\": {\n        \"name\": \"LightGBM\",\n        \"params\": {\n            \"objective\": \"binary\",\n            \"metric\": \"binary_logloss\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 63,\n            \"min_data_in_leaf\": 50,\n            \"feature_fraction\": 0.9,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"lambda_l2\": 1.0,\n            \"max_depth\": -1,\n            \"n_estimators\": 2000,\n        },\n        \"early_stopping_rounds\": 200,\n    },\n    \"calibration\": {\n        \"enabled\": True,\n        \"method\": \"isotonic\",   # \"sigmoid\" or \"isotonic\"\n    },\n    # Reference knobs used in Verification/Postprocess (kept here for reproducibility)\n    \"postprocess_ref\": {\n        \"min_peak_score_keep\": 6,\n        \"min_area_frac_keep\": 0.0005,\n        \"max_inst_keep\": 8,\n    },\n}\n\ncfg_id = hashlib.sha1(json.dumps(CFG, sort_keys=True).encode()).hexdigest()[:12]\nCFG[\"cfg_id\"] = cfg_id\n(ART_DIR / \"train_cfg.json\").write_text(json.dumps(CFG, indent=2))\n\n# ----------------------------\n# 3) Print summary (tight)\n# ----------------------------\nprint(\"ART_DIR:\", ART_DIR)\nprint(\"CFG_ID :\", cfg_id)\nprint(\"TOKEN_ROOT:\", TOKEN_ROOT.name)\nprint(\"MATCH_ROOT:\", MATCH_ROOT.name)\nprint(\"PRED_ENS_DIR:\", str(PRED_ENS_DIR))\nprint(\"Features:\", len(feature_cols), \"| First 10:\", feature_cols[:10])\nprint(\"Saved:\")\nprint(\" -\", ART_DIR / \"train_cfg.json\")\nprint(\" -\", ART_DIR / \"feature_cols.json\")\n\n# Export globals for next stages\nGATE_ART_DIR = ART_DIR\nGATE_CFG = CFG\nFEATURE_COLS = feature_cols\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build Training Table (X, y, folds)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE — Build Training Table (X, y, folds) (ONE CELL)\n# Uses:\n# - /kaggle/working/recodai_luc_gate_artifacts/train_cfg.json\n# - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n# - /kaggle/working/recodai_luc_prof/train_manifest.parquet\n# - /kaggle/working/recodai_luc_prof/folds.parquet\n# - pred_features_train.csv (from Verification stage)\n#\n# Produces:\n# - /kaggle/working/recodai_luc_gate_artifacts/train_table.parquet\n# - /kaggle/working/recodai_luc_gate_artifacts/train_X.npy\n# - /kaggle/working/recodai_luc_gate_artifacts/train_y.npy\n# - /kaggle/working/recodai_luc_gate_artifacts/train_folds.npy\n# - /kaggle/working/recodai_luc_gate_artifacts/feature_stats.json\n#\n# Exports globals:\n# - X_train, y_train, folds_train, df_train_tabular\n# ============================================================\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nART_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nPROF_DIR = Path(\"/kaggle/working/recodai_luc_prof\")\n\ncfg_path = ART_DIR / \"train_cfg.json\"\ncols_path = ART_DIR / \"feature_cols.json\"\ntrain_manifest_pq = PROF_DIR / \"train_manifest.parquet\"\nfolds_pq = PROF_DIR / \"folds.parquet\"\n\nfor p in [cfg_path, cols_path, train_manifest_pq, folds_pq]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {p}. Run previous stages first.\")\n\nCFG = json.loads(cfg_path.read_text())\nFEATURE_COLS = json.loads(cols_path.read_text())\n\npred_feat_train = Path(CFG[\"paths\"][\"PRED_FEATURES_TRAIN\"])\nif not pred_feat_train.exists():\n    raise FileNotFoundError(f\"Missing pred_features_train.csv: {pred_feat_train}\")\n\nfill_value = float(CFG[\"features\"].get(\"missing_numeric_fill\", 0.0))\nlabel_col = CFG[\"cv\"][\"label_col\"]\nfold_col  = CFG[\"cv\"][\"fold_col\"]\n\n# ----------------------------\n# 1) Load base labels + folds\n# ----------------------------\ndf_label = pd.read_parquet(train_manifest_pq)[[\"case_id\", label_col]].copy()\ndf_folds = pd.read_parquet(folds_pq)[[\"case_id\", fold_col]].copy()\n\ndf_base = df_label.merge(df_folds, on=\"case_id\", how=\"left\")\nif df_base[fold_col].isna().any():\n    miss = df_base[df_base[fold_col].isna()][\"case_id\"].head(10).tolist()\n    raise RuntimeError(f\"Missing fold for some case_id (first 10): {miss}\")\n\n# ----------------------------\n# 2) Load features (from verification) and dedup per case_id\n# ----------------------------\ndf_feat = pd.read_csv(pred_feat_train)\nif \"case_id\" not in df_feat.columns:\n    raise ValueError(\"pred_features_train.csv must contain 'case_id'\")\n\n# keep only needed columns + case_id\nkeep = [\"case_id\"] + [c for c in FEATURE_COLS if c in df_feat.columns]\ndf_feat = df_feat[keep].copy()\n\n# Deduplicate: if multiple rows per case_id, keep the one with max best_peak_score, then max area_frac\nsort_keys = []\nif \"best_peak_score\" in df_feat.columns:\n    sort_keys.append(\"best_peak_score\")\nif \"area_frac\" in df_feat.columns:\n    sort_keys.append(\"area_frac\")\nif sort_keys:\n    df_feat = df_feat.sort_values(sort_keys, ascending=False).drop_duplicates(\"case_id\", keep=\"first\")\nelse:\n    df_feat = df_feat.drop_duplicates(\"case_id\", keep=\"first\")\n\n# ----------------------------\n# 3) Join to build train table\n# ----------------------------\ndf_train_tabular = df_base.merge(df_feat, on=\"case_id\", how=\"left\")\n\n# ensure all feature cols exist\nfor c in FEATURE_COLS:\n    if c not in df_train_tabular.columns:\n        df_train_tabular[c] = np.nan\n\n# coerce numeric + fill\nfor c in FEATURE_COLS:\n    df_train_tabular[c] = pd.to_numeric(df_train_tabular[c], errors=\"coerce\")\n\nn_missing_before = int(df_train_tabular[FEATURE_COLS].isna().sum().sum())\ndf_train_tabular[FEATURE_COLS] = df_train_tabular[FEATURE_COLS].fillna(fill_value)\nn_missing_after = int(df_train_tabular[FEATURE_COLS].isna().sum().sum())\n\n# ----------------------------\n# 4) Add a few safe derived features (does NOT touch prep)\n# ----------------------------\ndef safe_log1p(x): return np.log1p(np.maximum(x, 0))\ndef safe_sqrt(x):  return np.sqrt(np.maximum(x, 0))\n\nif \"best_peak_score\" in df_train_tabular.columns:\n    df_train_tabular[\"log1p_best_peak_score\"] = safe_log1p(df_train_tabular[\"best_peak_score\"].values.astype(np.float32))\nif \"area_frac_tok\" in df_train_tabular.columns:\n    df_train_tabular[\"sqrt_area_frac_tok\"] = safe_sqrt(df_train_tabular[\"area_frac_tok\"].values.astype(np.float32))\nif \"n_inst\" in df_train_tabular.columns:\n    df_train_tabular[\"has_inst\"] = (df_train_tabular[\"n_inst\"].values.astype(np.float32) > 0).astype(np.float32)\n\n# extend FEATURE_COLS with derived (and save back for consistency)\nderived = [c for c in [\"log1p_best_peak_score\",\"sqrt_area_frac_tok\",\"has_inst\"] if c in df_train_tabular.columns]\nFEATURE_COLS_FINAL = FEATURE_COLS + [c for c in derived if c not in FEATURE_COLS]\n\n(ART_DIR / \"feature_cols.json\").write_text(json.dumps(FEATURE_COLS_FINAL, indent=2))\n\n# ----------------------------\n# 5) Build arrays\n# ----------------------------\nX_train = df_train_tabular[FEATURE_COLS_FINAL].to_numpy(dtype=np.float32, copy=True)\ny_train = df_train_tabular[label_col].to_numpy(dtype=np.int64, copy=True)\nfolds_train = df_train_tabular[fold_col].to_numpy(dtype=np.int64, copy=True)\n\n# ----------------------------\n# 6) Save\n# ----------------------------\ndf_train_tabular.to_parquet(ART_DIR / \"train_table.parquet\", index=False)\nnp.save(ART_DIR / \"train_X.npy\", X_train)\nnp.save(ART_DIR / \"train_y.npy\", y_train)\nnp.save(ART_DIR / \"train_folds.npy\", folds_train)\n\n# feature stats (for sanity / later debugging)\nstats = {\n    \"n_rows\": int(len(df_train_tabular)),\n    \"pos_rate\": float(y_train.mean()),\n    \"n_folds\": int(np.unique(folds_train).size),\n    \"missing_before_fill\": n_missing_before,\n    \"missing_after_fill\": n_missing_after,\n    \"feature_cols\": FEATURE_COLS_FINAL,\n    \"feature_min\": np.nanmin(X_train, axis=0).tolist(),\n    \"feature_max\": np.nanmax(X_train, axis=0).tolist(),\n    \"feature_mean\": np.nanmean(X_train, axis=0).tolist(),\n}\n(ART_DIR / \"feature_stats.json\").write_text(json.dumps(stats, indent=2))\n\nprint(\"Saved:\")\nprint(\" -\", ART_DIR / \"train_table.parquet\")\nprint(\" -\", ART_DIR / \"train_X.npy\")\nprint(\" -\", ART_DIR / \"train_y.npy\")\nprint(\" -\", ART_DIR / \"train_folds.npy\")\nprint(\" -\", ART_DIR / \"feature_stats.json\")\nprint(\"Updated:\")\nprint(\" -\", ART_DIR / \"feature_cols.json (added derived if any)\")\nprint(\"-\"*60)\nprint(\"Train rows:\", len(df_train_tabular), \"| pos_rate:\", float(y_train.mean()))\nprint(\"Features:\", len(FEATURE_COLS_FINAL), \"| missing_before:\", n_missing_before, \"| missing_after:\", n_missing_after)\n\n# Export globals for next stages\nFEATURE_COLS = FEATURE_COLS_FINAL\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build & Export Test Feature Table","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE — Build & Export Test Feature Table (ONE CELL)\n# Uses:\n# - /kaggle/working/recodai_luc_gate_artifacts/train_cfg.json\n# - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n# - /kaggle/working/recodai_luc_prof/test_manifest.parquet\n# - pred_features_test.csv (from Verification stage)\n# - sample_submission.csv (to enforce ordering)\n#\n# Produces:\n# - /kaggle/working/recodai_luc_gate_artifacts/test_table.parquet\n# - /kaggle/working/recodai_luc_gate_artifacts/test_X.npy\n# - /kaggle/working/recodai_luc_gate_artifacts/test_case_id.npy\n# ============================================================\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nART_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nPROF_DIR = Path(\"/kaggle/working/recodai_luc_prof\")\n\ncfg_path = ART_DIR / \"train_cfg.json\"\ncols_path = ART_DIR / \"feature_cols.json\"\ntest_manifest_pq = PROF_DIR / \"test_manifest.parquet\"\npaths_json = PROF_DIR / \"paths.json\"\n\nfor p in [cfg_path, cols_path, test_manifest_pq, paths_json]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {p}. Run previous stages first.\")\n\nCFG = json.loads(cfg_path.read_text())\nFEATURE_COLS = json.loads(cols_path.read_text())\nPATHS = json.loads(paths_json.read_text())\n\npred_feat_test = Path(CFG[\"paths\"][\"PRED_FEATURES_TEST\"])\nif not pred_feat_test.exists():\n    raise FileNotFoundError(f\"Missing pred_features_test.csv: {pred_feat_test}\")\n\nsample_sub = Path(PATHS[\"SAMPLE_SUB\"])\nif not sample_sub.exists():\n    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_sub}\")\n\nfill_value = float(CFG[\"features\"].get(\"missing_numeric_fill\", 0.0))\n\n# ----------------------------\n# 1) Load base test ids (order must follow sample_submission)\n# ----------------------------\ndf_sub = pd.read_csv(sample_sub)\nif \"case_id\" not in df_sub.columns:\n    raise ValueError(\"sample_submission.csv must contain 'case_id'\")\n\nsub_ids = df_sub[\"case_id\"].astype(int).tolist()\n\ndf_test_meta = pd.read_parquet(test_manifest_pq)[[\"case_id\",\"img_path\",\"H\",\"W\"]].copy()\ndf_test_meta[\"case_id\"] = df_test_meta[\"case_id\"].astype(int)\n\n# Align to sample_submission (keep all)\ndf_test_meta = df_test_meta.set_index(\"case_id\").reindex(sub_ids).reset_index()\n\n# ----------------------------\n# 2) Load features and dedup per case_id\n# ----------------------------\ndf_feat = pd.read_csv(pred_feat_test)\nif \"case_id\" not in df_feat.columns:\n    raise ValueError(\"pred_features_test.csv must contain 'case_id'\")\n\n# keep only needed columns we might use\nkeep_cols = [\"case_id\"] + [c for c in FEATURE_COLS if c in df_feat.columns]\ndf_feat = df_feat[keep_cols].copy()\ndf_feat[\"case_id\"] = df_feat[\"case_id\"].astype(int)\n\n# Dedup if necessary (max best_peak_score then max area_frac)\nsort_keys = []\nif \"best_peak_score\" in df_feat.columns:\n    sort_keys.append(\"best_peak_score\")\nif \"area_frac\" in df_feat.columns:\n    sort_keys.append(\"area_frac\")\nif sort_keys:\n    df_feat = df_feat.sort_values(sort_keys, ascending=False).drop_duplicates(\"case_id\", keep=\"first\")\nelse:\n    df_feat = df_feat.drop_duplicates(\"case_id\", keep=\"first\")\n\n# ----------------------------\n# 3) Join to build test table\n# ----------------------------\ndf_test_tabular = df_test_meta.merge(df_feat, on=\"case_id\", how=\"left\")\n\n# ensure all features exist\nfor c in FEATURE_COLS:\n    if c not in df_test_tabular.columns:\n        df_test_tabular[c] = np.nan\n\n# coerce numeric + fill\nfor c in FEATURE_COLS:\n    df_test_tabular[c] = pd.to_numeric(df_test_tabular[c], errors=\"coerce\")\n\nn_missing_before = int(df_test_tabular[FEATURE_COLS].isna().sum().sum())\ndf_test_tabular[FEATURE_COLS] = df_test_tabular[FEATURE_COLS].fillna(fill_value)\nn_missing_after = int(df_test_tabular[FEATURE_COLS].isna().sum().sum())\n\n# ----------------------------\n# 4) Derived features must match TRAIN stage logic\n# (These were appended to feature_cols.json by the training-table stage.)\n# If they exist in FEATURE_COLS but not in df, compute here.\n# ----------------------------\ndef safe_log1p(x): return np.log1p(np.maximum(x, 0))\ndef safe_sqrt(x):  return np.sqrt(np.maximum(x, 0))\n\nif \"log1p_best_peak_score\" in FEATURE_COLS and \"log1p_best_peak_score\" not in df_test_tabular.columns:\n    if \"best_peak_score\" in df_test_tabular.columns:\n        df_test_tabular[\"log1p_best_peak_score\"] = safe_log1p(df_test_tabular[\"best_peak_score\"].values.astype(np.float32))\n    else:\n        df_test_tabular[\"log1p_best_peak_score\"] = 0.0\n\nif \"sqrt_area_frac_tok\" in FEATURE_COLS and \"sqrt_area_frac_tok\" not in df_test_tabular.columns:\n    if \"area_frac_tok\" in df_test_tabular.columns:\n        df_test_tabular[\"sqrt_area_frac_tok\"] = safe_sqrt(df_test_tabular[\"area_frac_tok\"].values.astype(np.float32))\n    else:\n        df_test_tabular[\"sqrt_area_frac_tok\"] = 0.0\n\nif \"has_inst\" in FEATURE_COLS and \"has_inst\" not in df_test_tabular.columns:\n    if \"n_inst\" in df_test_tabular.columns:\n        df_test_tabular[\"has_inst\"] = (df_test_tabular[\"n_inst\"].values.astype(np.float32) > 0).astype(np.float32)\n    else:\n        df_test_tabular[\"has_inst\"] = 0.0\n\n# Final safety: ensure all FEATURE_COLS present after derived\nfor c in FEATURE_COLS:\n    if c not in df_test_tabular.columns:\n        df_test_tabular[c] = fill_value\n\n# ----------------------------\n# 5) Build arrays\n# ----------------------------\nX_test = df_test_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ncase_id_test = df_test_tabular[\"case_id\"].to_numpy(dtype=np.int64, copy=True)\n\n# ----------------------------\n# 6) Save\n# ----------------------------\ndf_test_tabular.to_parquet(ART_DIR / \"test_table.parquet\", index=False)\nnp.save(ART_DIR / \"test_X.npy\", X_test)\nnp.save(ART_DIR / \"test_case_id.npy\", case_id_test)\n\nprint(\"Saved:\")\nprint(\" -\", ART_DIR / \"test_table.parquet\")\nprint(\" -\", ART_DIR / \"test_X.npy\")\nprint(\" -\", ART_DIR / \"test_case_id.npy\")\nprint(\"-\"*60)\nprint(\"Test rows:\", len(df_test_tabular), \"| missing_before:\", n_missing_before, \"| missing_after:\", n_missing_after)\n\n# Export globals for next stages\nX_test = X_test\ncase_id_test = case_id_test\ndf_test_tabular = df_test_tabular\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Baseline Model (Leakage-Safe CV)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE — Train Baseline Model (Leakage-Safe CV) (ONE CELL) — REVISI FULL v8.1 (STACK-CONSISTENT)\n# Gate model: LightGBM (fallback: HistGradientBoosting if LGBM missing)\n# + calibration (isotonic/sigmoid)\n# + threshold sweep using Dice-proxy computed from:\n#     - if gate predicts \"authentic\" => empty mask\n#     - else => use FULL-RES mask from pred_ens/train/{case_id}.npz (key: \"mask\")\n#     - GT mask union loaded from: train_masks / supplemental_masks (npy union OR png union)\n# Dice-proxy uses SAME postprocess spirit as submission (instance split & filtering),\n# but runs on FULL-RES masks (robust for any tok/grid mismatch).\n#\n# Requires:\n# - /kaggle/working/recodai_luc_gate_artifacts/train_cfg.json\n# - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n# - /kaggle/working/recodai_luc_gate_artifacts/train_X.npy, train_y.npy, train_folds.npy\n# - /kaggle/working/recodai_luc_prof/paths.json\n# - /kaggle/working/recodai_luc_gate_artifacts/train_table.parquet (case_id order)\n# - pred_ens train cache: /kaggle/working/recodai_luc/cache/pred_ens/train/{case_id}.npz\n#\n# Produces:\n# - models/: fold_*.txt (or .joblib fallback), calibration*.joblib, calibration.json\n# - oof/: oof_prob_raw.npy, oof_prob.npy, oof_prob.csv\n# - eval/: threshold_table.csv + best_threshold.json + fold_metrics.json\n# - final_gate_model.pt  (portable torch-save dict)\n# ============================================================\n\nimport os, json, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nwarnings.filterwarnings(\"ignore\")\n\nART_DIR  = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nPROF_DIR = Path(\"/kaggle/working/recodai_luc_prof\")\nCACHE_DIR = Path(\"/kaggle/working/recodai_luc/cache\")\n\ncfg_path  = ART_DIR / \"train_cfg.json\"\ncols_path = ART_DIR / \"feature_cols.json\"\nX_path    = ART_DIR / \"train_X.npy\"\ny_path    = ART_DIR / \"train_y.npy\"\nf_path    = ART_DIR / \"train_folds.npy\"\npaths_json = PROF_DIR / \"paths.json\"\ntrain_table_pq = ART_DIR / \"train_table.parquet\"\n\nfor p in [cfg_path, cols_path, X_path, y_path, f_path, paths_json, train_table_pq]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {p}. Run previous stages first.\")\n\nCFG = json.loads(cfg_path.read_text())\nFEATURE_COLS = json.loads(cols_path.read_text())\nPATHS = json.loads(paths_json.read_text())\n\nX = np.load(X_path).astype(np.float32)\ny = np.load(y_path).astype(np.int64)\nfolds = np.load(f_path).astype(np.int64)\nN = len(y)\n\nif X.shape[0] != N or folds.shape[0] != N:\n    raise RuntimeError(\"Shape mismatch among train_X/train_y/train_folds\")\n\n# dirs\nMODEL_DIR = ART_DIR / \"models\"\nOOF_DIR   = ART_DIR / \"oof\"\nEVAL_DIR  = ART_DIR / \"eval\"\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\nEVAL_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Model import (LGBM preferred)\n# ----------------------------\nuse_lgbm = True\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    use_lgbm = False\n\nfrom sklearn.metrics import roc_auc_score, log_loss, f1_score, precision_score, recall_score\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model import LogisticRegression\nimport joblib\n\n# SciPy optional (CC)\ntry:\n    import scipy.ndimage as ndi\n    _HAS_SCIPY = True\nexcept Exception:\n    _HAS_SCIPY = False\n\nseed = int(CFG.get(\"cv\", {}).get(\"seed\", 42))\nfold_ids = sorted(np.unique(folds).tolist())\nn_folds = len(fold_ids)\n\n# ----------------------------\n# Instance-split / filtering (FULL-RES) for Dice-proxy\n# Keep in sync with Verification intent\n# ----------------------------\nPP = CFG.get(\"postprocess_ref\", {})\nMIN_INST_PIX = int(PP.get(\"min_inst_pix\", 32))            # drop tiny CC at full-res\nMAX_AREA_FRAC = float(PP.get(\"max_area_frac\", 0.90))      # drop huge CC\nMAX_INST_KEEP = int(PP.get(\"max_inst_keep\", 8))           # keep top-K by area\n\n# ----------------------------\n# Helpers: GT union loader (npy union OR PNG union)\n# ----------------------------\nTRAIN_MASK_DIR = Path(PATHS.get(\"TRAIN_MASK_DIR\",\"\")) if PATHS.get(\"TRAIN_MASK_DIR\") else None\nSUP_MASK_DIR   = Path(PATHS.get(\"SUP_MASK_DIR\",\"\")) if PATHS.get(\"SUP_MASK_DIR\") else None\n\ndef _find_mask_files(mask_dir: Path, case_id: int):\n    if mask_dir is None or (not mask_dir.exists()):\n        return []\n    cid = str(int(case_id))\n    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n    pats = [\n        f\"{cid}*.png\", f\"{cid}*.jpg\", f\"{cid}*.jpeg\", f\"{cid}*.tif\", f\"{cid}*.tiff\", f\"{cid}*.bmp\",\n        f\"{cid}__*.png\", f\"{cid}_*.png\"\n    ]\n    out, seen = [], set()\n    for pat in pats:\n        for p in mask_dir.glob(pat):\n            if p.suffix.lower() in exts:\n                s = str(p)\n                if s not in seen:\n                    out.append(p); seen.add(s)\n    return sorted(out)\n\ndef load_gt_union(case_id: int):\n    # 1) union cache npy if exists\n    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n        if d is None or (not d.exists()):\n            continue\n        npy = d / f\"{int(case_id)}.npy\"\n        if npy.exists():\n            a = np.load(npy, mmap_mode=\"r\")\n            if a.ndim == 2:\n                return (np.asarray(a) > 0)\n            if a.ndim == 3:\n                return (np.asarray(a) > 0).any(axis=0)\n\n    # 2) union PNGs (multi instances)\n    files = []\n    if TRAIN_MASK_DIR is not None: files += _find_mask_files(TRAIN_MASK_DIR, case_id)\n    if SUP_MASK_DIR is not None:   files += _find_mask_files(SUP_MASK_DIR, case_id)\n    if not files:\n        return None\n\n    m = None\n    for p in files:\n        try:\n            im = Image.open(p).convert(\"L\")\n            a = (np.asarray(im) > 0)\n            m = a if m is None else (m | a)\n        except Exception:\n            continue\n    return m\n\n# ----------------------------\n# Pred mask loader (FULL-RES) from pred_ens\n# ----------------------------\nPRED_ENS_DIR = Path(CFG[\"paths\"][\"PRED_ENS_DIR\"]) if (\"paths\" in CFG and \"PRED_ENS_DIR\" in CFG[\"paths\"]) else (CACHE_DIR / \"pred_ens\")\nPRED_TRAIN_DIR = PRED_ENS_DIR / \"train\"\n\ndef load_pred_union(case_id: int):\n    p = PRED_TRAIN_DIR / f\"{int(case_id)}.npz\"\n    if not p.exists():\n        return None\n    z = np.load(p)\n    if \"mask\" not in z.files:\n        return None\n    m = z[\"mask\"]\n    return (m > 0)\n\n# ----------------------------\n# CC instance filtering (FULL-RES)\n# ----------------------------\ndef cc_union_filtered(mask_bool: np.ndarray):\n    if mask_bool is None:\n        return None, {\"n_inst\": 0, \"area\": 0}\n    m = mask_bool.astype(bool)\n    H, W = m.shape\n    area = int(m.sum())\n    if area == 0:\n        return m, {\"n_inst\": 0, \"area\": 0}\n\n    if (area / float(H*W)) > MAX_AREA_FRAC:\n        return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n\n    if not _HAS_SCIPY:\n        if area < MIN_INST_PIX:\n            return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n        return m, {\"n_inst\": 1, \"area\": area}\n\n    lab, n = ndi.label(m, structure=np.ones((3,3), dtype=np.uint8))\n    if n <= 0:\n        return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n\n    areas = ndi.sum(m.astype(np.uint8), lab, index=np.arange(1, n+1)).astype(np.int64)\n    keep = np.where(areas >= MIN_INST_PIX)[0]\n    if keep.size == 0:\n        return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n\n    keep = keep[np.argsort(areas[keep])[::-1]]\n    keep = keep[:MAX_INST_KEEP]\n\n    out = np.zeros_like(m, dtype=bool)\n    for k in keep:\n        out |= (lab == (k + 1))\n    return out, {\"n_inst\": int(len(keep)), \"area\": int(out.sum())}\n\ndef dice_score(pr: np.ndarray, gt: np.ndarray):\n    pr = pr.astype(bool)\n    gt = gt.astype(bool)\n    a = int(pr.sum()); b = int(gt.sum())\n    if a == 0 and b == 0:\n        return 1.0\n    if a == 0 or b == 0:\n        return 0.0\n    inter = int((pr & gt).sum())\n    return float((2.0 * inter) / (a + b))\n\n# ----------------------------\n# 1) Train fold models -> OOF prob (raw)\n# ----------------------------\noof_prob_raw = np.zeros(N, dtype=np.float32)\nfold_models = []\nfold_metrics = []\n\nparams = CFG.get(\"gate_model\", {}).get(\"params\", {})\nearly_rounds = int(CFG.get(\"gate_model\", {}).get(\"early_stopping_rounds\", 200))\n\nt0 = time.time()\nprint(\"Model:\", \"LightGBM\" if use_lgbm else \"HistGradientBoosting (fallback)\")\nprint(\"Folds:\", n_folds, \"| N:\", N, \"| pos_rate:\", float(y.mean()))\nprint(\"PRED_TRAIN_DIR:\", PRED_TRAIN_DIR)\nprint(\"InstanceSplit:\", {\"MIN_INST_PIX\": MIN_INST_PIX, \"MAX_AREA_FRAC\": MAX_AREA_FRAC, \"MAX_INST_KEEP\": MAX_INST_KEEP})\nprint(\"-\"*60)\n\nif use_lgbm:\n    for f in fold_ids:\n        tr_idx = np.where(folds != f)[0]\n        va_idx = np.where(folds == f)[0]\n\n        dtr = lgb.Dataset(X[tr_idx], label=y[tr_idx], feature_name=FEATURE_COLS, free_raw_data=True)\n        dva = lgb.Dataset(X[va_idx], label=y[va_idx], feature_name=FEATURE_COLS, free_raw_data=True)\n\n        booster = lgb.train(\n            params=params,\n            train_set=dtr,\n            valid_sets=[dva],\n            valid_names=[\"val\"],\n            num_boost_round=int(params.get(\"n_estimators\", 2000)),\n            callbacks=[lgb.early_stopping(stopping_rounds=early_rounds, verbose=False)]\n        )\n\n        p = booster.predict(X[va_idx], num_iteration=booster.best_iteration)\n        p = np.clip(p, 1e-6, 1-1e-6).astype(np.float32)\n        oof_prob_raw[va_idx] = p\n\n        model_path = MODEL_DIR / f\"fold_{f}.txt\"\n        booster.save_model(str(model_path))\n\n        auc = float(roc_auc_score(y[va_idx], p)) if len(np.unique(y[va_idx])) > 1 else float(\"nan\")\n        ll  = float(log_loss(y[va_idx], p, labels=[0,1]))\n        pred05 = (p >= 0.5).astype(int)\n        f1v = float(f1_score(y[va_idx], pred05))\n        fold_metrics.append({\"fold\": int(f), \"auc\": auc, \"logloss\": ll, \"f1@0.5\": f1v, \"best_iter\": int(booster.best_iteration)})\n        fold_models.append({\"fold\": int(f), \"path\": str(model_path), \"best_iter\": int(booster.best_iteration)})\n\n        print(f\"[fold {f}] auc={auc:.4f} logloss={ll:.4f} f1@0.5={f1v:.4f} iter={int(booster.best_iteration)}\")\n\nelse:\n    from sklearn.ensemble import HistGradientBoostingClassifier\n    for f in fold_ids:\n        tr_idx = np.where(folds != f)[0]\n        va_idx = np.where(folds == f)[0]\n\n        clf = HistGradientBoostingClassifier(\n            learning_rate=0.05,\n            max_depth=None,\n            max_leaf_nodes=63,\n            min_samples_leaf=50,\n            l2_regularization=1.0,\n            max_iter=500,\n            random_state=seed + int(f),\n        )\n        clf.fit(X[tr_idx], y[tr_idx])\n        p = clf.predict_proba(X[va_idx])[:, 1]\n        p = np.clip(p, 1e-6, 1-1e-6).astype(np.float32)\n        oof_prob_raw[va_idx] = p\n\n        model_path = MODEL_DIR / f\"fold_{f}.joblib\"\n        joblib.dump(clf, model_path)\n\n        auc = float(roc_auc_score(y[va_idx], p)) if len(np.unique(y[va_idx])) > 1 else float(\"nan\")\n        ll  = float(log_loss(y[va_idx], p, labels=[0,1]))\n        pred05 = (p >= 0.5).astype(int)\n        f1v = float(f1_score(y[va_idx], pred05))\n        fold_metrics.append({\"fold\": int(f), \"auc\": auc, \"logloss\": ll, \"f1@0.5\": f1v})\n        fold_models.append({\"fold\": int(f), \"path\": str(model_path)})\n\n        print(f\"[fold {f}] auc={auc:.4f} logloss={ll:.4f} f1@0.5={f1v:.4f}\")\n\nauc_all = float(roc_auc_score(y, oof_prob_raw)) if len(np.unique(y)) > 1 else float(\"nan\")\nll_all  = float(log_loss(y, oof_prob_raw, labels=[0,1]))\npred05_all = (oof_prob_raw >= 0.5).astype(int)\nf1_all = float(f1_score(y, pred05_all))\nprec_all = float(precision_score(y, pred05_all, zero_division=0))\nrec_all  = float(recall_score(y, pred05_all, zero_division=0))\n\nprint(\"-\"*60)\nprint(f\"OOF raw: auc={auc_all:.4f} logloss={ll_all:.4f} f1@0.5={f1_all:.4f} prec={prec_all:.4f} rec={rec_all:.4f}\")\nprint(\"Train CV finished in\", f\"{time.time()-t0:.1f}s\")\n\n# ----------------------------\n# 2) Calibration (fit on OOF)\n# ----------------------------\ncal_cfg = CFG.get(\"calibration\", {\"enabled\": True, \"method\": \"isotonic\"})\ncal_enabled = bool(cal_cfg.get(\"enabled\", True))\ncal_method = str(cal_cfg.get(\"method\", \"isotonic\")).lower()\n\noof_prob = oof_prob_raw.copy()\ncal_pack = {\"enabled\": False}\n\nif cal_enabled:\n    if cal_method == \"isotonic\":\n        iso = IsotonicRegression(out_of_bounds=\"clip\")\n        iso.fit(oof_prob_raw, y)\n        oof_prob = np.clip(iso.predict(oof_prob_raw).astype(np.float32), 1e-6, 1-1e-6)\n        cal_pack = {\"enabled\": True, \"method\": \"isotonic\"}\n        joblib.dump(iso, MODEL_DIR / \"calibration_isotonic.joblib\")\n    elif cal_method in [\"sigmoid\", \"platt\"]:\n        p = np.clip(oof_prob_raw, 1e-6, 1-1e-6)\n        logit = np.log(p/(1-p)).reshape(-1,1)\n        lr = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n        lr.fit(logit, y)\n        oof_prob = np.clip(lr.predict_proba(logit)[:,1].astype(np.float32), 1e-6, 1-1e-6)\n        cal_pack = {\"enabled\": True, \"method\": \"sigmoid\"}\n        joblib.dump(lr, MODEL_DIR / \"calibration_sigmoid.joblib\")\n    else:\n        print(\"WARNING: unknown calibration method:\", cal_method, \"-> skip calibration\")\n        cal_pack = {\"enabled\": False}\n\nauc_cal = float(roc_auc_score(y, oof_prob)) if len(np.unique(y)) > 1 else float(\"nan\")\nll_cal  = float(log_loss(y, oof_prob, labels=[0,1]))\npred05_cal = (oof_prob >= 0.5).astype(int)\nf1_cal = float(f1_score(y, pred05_cal))\n\n(MODEL_DIR / \"calibration.json\").write_text(json.dumps(cal_pack, indent=2))\nprint(\"-\"*60)\nprint(f\"OOF calibrated: auc={auc_cal:.4f} logloss={ll_cal:.4f} f1@0.5={f1_cal:.4f} | calibration={cal_pack}\")\n\n# save OOF arrays + csv\nnp.save(OOF_DIR / \"oof_prob_raw.npy\", oof_prob_raw)\nnp.save(OOF_DIR / \"oof_prob.npy\", oof_prob)\n\ndf_train_tab = pd.read_parquet(train_table_pq)[[\"case_id\"]].copy()\ndf_train_tab[\"case_id\"] = df_train_tab[\"case_id\"].astype(int)\nif len(df_train_tab) != N:\n    raise RuntimeError(\"train_table.parquet rows != train_y length (order mismatch). Rebuild train table.\")\ndf_train_tab[\"y\"] = y\ndf_train_tab[\"oof_prob_raw\"] = oof_prob_raw\ndf_train_tab[\"oof_prob\"] = oof_prob\ndf_train_tab.to_csv(OOF_DIR / \"oof_prob.csv\", index=False)\n\n# ----------------------------\n# 3) Dice-proxy arrays (FULL-RES, instance-split consistent)\n# ----------------------------\ncase_ids = df_train_tab[\"case_id\"].tolist()\n\ndice_use  = np.zeros(N, dtype=np.float32)  # if we \"use\" predicted mask\ndice_empty= np.zeros(N, dtype=np.float32)  # if gate outputs \"authentic\" (empty)\n\nt1 = time.time()\nmiss_gt = miss_pr = bad_shape = 0\n\nfor i, cid in enumerate(case_ids):\n    gt = load_gt_union(cid)\n    pr = load_pred_union(cid)\n\n    # GT missing => empty (safe)\n    if gt is None:\n        miss_gt += 1\n        gt_mask = None\n        gt_empty = True\n    else:\n        gt_mask = gt.astype(bool)\n        gt_empty = (gt_mask.sum() == 0)\n\n    dice_empty[i] = 1.0 if gt_empty else 0.0\n\n    if pr is None:\n        miss_pr += 1\n        dice_use[i] = dice_empty[i]\n        continue\n\n    pr_mask = pr.astype(bool)\n\n    # shape align if needed\n    if gt_mask is not None and pr_mask.shape != gt_mask.shape:\n        bad_shape += 1\n        # resize pred to gt shape\n        im = Image.fromarray((pr_mask.astype(np.uint8)*255))\n        im = im.resize((gt_mask.shape[1], gt_mask.shape[0]), resample=Image.NEAREST)\n        pr_mask = (np.asarray(im) > 127)\n\n    # instance filtering\n    pr_f, _ = cc_union_filtered(pr_mask)\n\n    if gt_mask is None:\n        dice_use[i] = 1.0 if (pr_f.sum() == 0) else 0.0\n    else:\n        gt_f, _ = cc_union_filtered(gt_mask)\n        dice_use[i] = dice_score(pr_f, gt_f)\n\n    if (i + 1) % 500 == 0:\n        print(f\"[dice-proxy] {i+1}/{N} | miss_gt={miss_gt} miss_pr={miss_pr} bad_shape={bad_shape} | {time.time()-t1:.1f}s\")\n\nprint(\"-\"*60)\nprint(f\"Dice-proxy ready | miss_gt={miss_gt} miss_pr={miss_pr} bad_shape={bad_shape} | {time.time()-t1:.1f}s\")\n\n# ----------------------------\n# 4) Threshold sweep (optimize proxy score)\n# ----------------------------\nthr_grid = np.linspace(0.0, 1.0, 201, dtype=np.float32)\nrows = []\nfor thr in thr_grid:\n    use = (oof_prob >= thr)\n    score = np.where(use, dice_use, dice_empty).mean()\n\n    pred = use.astype(int)\n    f1v = f1_score(y, pred)\n    prec = precision_score(y, pred, zero_division=0)\n    rec  = recall_score(y, pred, zero_division=0)\n    fp_rate = float(((pred==1) & (y==0)).mean())\n    fn_rate = float(((pred==0) & (y==1)).mean())\n\n    rows.append({\n        \"thr\": float(thr),\n        \"score_dice_proxy\": float(score),\n        \"f1_gate\": float(f1v),\n        \"precision_gate\": float(prec),\n        \"recall_gate\": float(rec),\n        \"fp_rate\": fp_rate,\n        \"fn_rate\": fn_rate,\n    })\n\ndf_thr = pd.DataFrame(rows)\nbest_i = int(df_thr[\"score_dice_proxy\"].values.argmax())\nbest_thr = float(df_thr.loc[best_i, \"thr\"])\nbest_score = float(df_thr.loc[best_i, \"score_dice_proxy\"])\n\ndf_thr.to_csv(EVAL_DIR / \"threshold_table.csv\", index=False)\n(EVAL_DIR / \"best_threshold.json\").write_text(json.dumps({\n    \"recommended_thr\": best_thr,\n    \"best_score_dice_proxy\": best_score,\n    \"best_row\": df_thr.loc[best_i].to_dict(),\n    \"instance_split_fullres\": {\"MIN_INST_PIX\": MIN_INST_PIX, \"MAX_AREA_FRAC\": MAX_AREA_FRAC, \"MAX_INST_KEEP\": MAX_INST_KEEP},\n}, indent=2))\n\nprint(\"-\"*60)\nprint(\"BEST (dice-proxy): thr =\", best_thr, \"| score =\", best_score)\nprint(\"Saved:\", EVAL_DIR / \"threshold_table.csv\")\nprint(\"Saved:\", EVAL_DIR / \"best_threshold.json\")\n\n# ----------------------------\n# 5) Save fold metrics + portable bundle\n# ----------------------------\nsummary = {\n    \"oof_raw\": {\"auc\": auc_all, \"logloss\": ll_all, \"f1@0.5\": f1_all},\n    \"oof_cal\": {\"auc\": auc_cal, \"logloss\": ll_cal, \"f1@0.5\": f1_cal},\n    \"recommended_thr\": best_thr,\n    \"best_score_dice_proxy\": best_score,\n    \"fold_metrics\": fold_metrics,\n    \"model_paths\": fold_models,\n    \"calibration\": cal_pack,\n    \"feature_cols\": FEATURE_COLS,\n    \"dice_proxy\": {\n        \"uses_fullres_pred_ens_mask\": True,\n        \"pred_train_dir\": str(PRED_TRAIN_DIR),\n        \"gt_dirs\": {\"train\": str(TRAIN_MASK_DIR) if TRAIN_MASK_DIR else None,\n                    \"supp\":  str(SUP_MASK_DIR) if SUP_MASK_DIR else None},\n        \"instance_split_fullres\": {\"MIN_INST_PIX\": MIN_INST_PIX, \"MAX_AREA_FRAC\": MAX_AREA_FRAC, \"MAX_INST_KEEP\": MAX_INST_KEEP},\n    }\n}\n(EVAL_DIR / \"fold_metrics.json\").write_text(json.dumps(summary, indent=2))\n\nimport torch\nbundle = {\n    \"cfg\": CFG,\n    \"feature_cols\": FEATURE_COLS,\n    \"fold_models\": fold_models,       # file paths\n    \"calibration\": cal_pack,\n    \"recommended_thr\": best_thr,\n    \"instance_split_fullres\": {\"MIN_INST_PIX\": MIN_INST_PIX, \"MAX_AREA_FRAC\": MAX_AREA_FRAC, \"MAX_INST_KEEP\": MAX_INST_KEEP},\n    \"notes\": {\n        \"oof_prob_raw\": str(OOF_DIR / \"oof_prob_raw.npy\"),\n        \"oof_prob\": str(OOF_DIR / \"oof_prob.npy\"),\n        \"threshold_table\": str(EVAL_DIR / \"threshold_table.csv\"),\n        \"fold_metrics\": str(EVAL_DIR / \"fold_metrics.json\"),\n        \"dice_proxy_note\": \"mean Dice proxy using pred_ens full-res mask when gate predicts forged; empty when authentic; CC-filter applied to mimic submission instance filtering\",\n    }\n}\ntorch.save(bundle, ART_DIR / \"final_gate_model.pt\")\n\nprint(\"-\"*60)\nprint(\"Saved:\", EVAL_DIR / \"fold_metrics.json\")\nprint(\"Saved:\", ART_DIR / \"final_gate_model.pt\")\nprint(\"Saved:\", OOF_DIR / \"oof_prob.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optimize Model & Hyperparameters (Iterative)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE — Optimize Model & Hyperparameters (Iterative) (ONE CELL)\n# Hybrid Model (OPSI-1): UNet(+ASPP) token-decoder + Gate Head (classification) in ONE NETWORK\n# Input  : DINOv2 token-grid (Ht,Wt,D) + seed (Ht,Wt,1) from Robust Matching\n# Output : prob_tok (seg) + p_forged (gate)\n# Score  : Dice-proxy on VAL (if p_forged<thr => empty else fuse(seg,seed)->inst-split->dice vs GT tok)\n#\n# REQUIRE (expected from earlier stages):\n# - train_table.parquet with columns (at minimum): case_id, fold, y, and a token path column:\n#     tok_path / token_path / dino_path / feat_path / emb_path  (auto-detect)\n# - Robust Matching cache: match_cfg_* with match_manifest_train.parquet\n# - paths.json for TRAIN_MASK_DIR / SUP_MASK_DIR (GT)\n#\n# OUTPUT:\n# - /kaggle/working/recodai_luc_hybrid_opt/trials.csv\n# - /kaggle/working/recodai_luc_hybrid_opt/best_config.json\n# - /kaggle/working/recodai_luc_hybrid_opt/best_model.pt\n# ============================================================\n\nimport os, json, time, math, random, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# Config (safe defaults; tweak via env vars if needed)\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_hybrid_opt\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nPROF_DIR  = Path(\"/kaggle/working/recodai_luc_prof\")\nCACHE_DIR = Path(\"/kaggle/working/recodai_luc/cache\")\n\nSEED = int(os.environ.get(\"SEED\", \"42\"))\nMAX_TRIALS = int(os.environ.get(\"MAX_TRIALS\", \"12\"))          # keep small; increase if time\nTRIAL_EPOCHS = int(os.environ.get(\"TRIAL_EPOCHS\", \"6\"))        # quick trials\nBATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"32\"))           # token-grid is small\nNUM_WORKERS = int(os.environ.get(\"NUM_WORKERS\", \"2\"))\nACCUM_STEPS = int(os.environ.get(\"ACCUM_STEPS\", \"1\"))\nUSE_AMP = bool(int(os.environ.get(\"USE_AMP\", \"1\")))\nEARLYSTOP_PATIENCE = int(os.environ.get(\"EARLYSTOP_PATIENCE\", \"2\"))\n\n# tune-space (reasonable)\nLR_RANGE = (2e-4, 2e-3)\nWD_RANGE = (0.0, 0.05)\nDROPOUT_RANGE = (0.0, 0.25)\nBASE_CH_CHOICES = [64, 96, 128]\nLAMBDA_SEG_RANGE = (0.6, 1.2)\nLAMBDA_CLS_RANGE = (0.3, 0.9)\nFOCAL_GAMMA_CHOICES = [0.0, 1.0, 2.0]  # 0 -> BCE only\n\n# postprocess / fuse params\nT1_RANGE = (0.50, 0.65)\nT0_RANGE = (0.25, 0.45)\nSEED_DILATE_CHOICES = [0, 1, 2]\nTHR_GATE_RANGE = (0.20, 0.80)\n\n# instance split token-space\nMIN_TOK_AREA_CHOICES = [1, 2, 3]\nMAX_TOK_AREA_FRAC_CHOICES = [0.70, 0.80, 0.90]\nMAX_INST_KEEP_CHOICES = [4, 8, 12]\n\n# guard\nMIN_PEAK_SCORE_KEEP_CHOICES = [5, 6, 7]\nMIN_AREA_FRAC_KEEP_CHOICES = [0.0003, 0.0005, 0.0010]\n\n# folds usage for fast opt (use 1 fold val each trial; rotate)\nVAL_FOLD_ROTATE = True\n\n# ----------------------------\n# Repro\n# ----------------------------\ndef seed_everything(s=42):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\nseed_everything(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"DEVICE:\", device, \"| AMP:\", USE_AMP and device.type == \"cuda\")\n\n# ----------------------------\n# Load PATHS + train_table\n# ----------------------------\npaths_json = PROF_DIR / \"paths.json\"\nif not paths_json.exists():\n    raise FileNotFoundError(f\"Missing {paths_json}\")\n\nPATHS = json.loads(paths_json.read_text())\nTRAIN_MASK_DIR = Path(PATHS.get(\"TRAIN_MASK_DIR\",\"\")) if PATHS.get(\"TRAIN_MASK_DIR\") else None\nSUP_MASK_DIR   = Path(PATHS.get(\"SUP_MASK_DIR\",\"\")) if PATHS.get(\"SUP_MASK_DIR\") else None\n\ntable_candidates = [\n    Path(\"/kaggle/working/recodai_luc_gate_artifacts/train_table.parquet\"),\n    Path(\"/kaggle/working/recodai_luc_hybrid_artifacts/train_table.parquet\"),\n    PROF_DIR / \"train_table.parquet\",\n]\nTRAIN_TABLE = None\nfor p in table_candidates:\n    if p.exists():\n        TRAIN_TABLE = p\n        break\nif TRAIN_TABLE is None:\n    raise FileNotFoundError(\"Cannot find train_table.parquet in known locations. Run Build Training Table stage first.\")\n\ndf = pd.read_parquet(TRAIN_TABLE).copy()\nfor need in [\"case_id\",\"fold\",\"y\"]:\n    if need not in df.columns:\n        raise ValueError(f\"train_table missing required col: {need}\")\ndf[\"case_id\"] = df[\"case_id\"].astype(int)\ndf[\"fold\"] = df[\"fold\"].astype(int)\ndf[\"y\"] = df[\"y\"].astype(int)\n\n# token path column auto-detect\ntok_col = None\nfor c in [\"tok_path\",\"token_path\",\"dino_path\",\"feat_path\",\"emb_path\",\"token_npz\",\"npz_path\"]:\n    if c in df.columns:\n        tok_col = c\n        break\nif tok_col is None:\n    raise ValueError(\"train_table.parquet must contain a token path column (tok_path/token_path/dino_path/feat_path/emb_path).\")\n\ndf[tok_col] = df[tok_col].astype(str)\nok_tok = df[tok_col].map(lambda p: Path(p).exists())\nif ok_tok.mean() < 0.90:\n    print(\"WARNING: token path existence rate is low:\", float(ok_tok.mean()))\ndf = df[ok_tok].reset_index(drop=True)\n\nfold_ids = sorted(df[\"fold\"].unique().tolist())\nprint(\"TRAIN_TABLE:\", TRAIN_TABLE, \"| rows:\", len(df), \"| folds:\", fold_ids, \"| pos_rate:\", float(df[\"y\"].mean()))\nprint(\"TOK_COL:\", tok_col)\n\n# ----------------------------\n# Latest MATCH_ROOT + map case_id -> (match_npz, best_peak_score)\n# ----------------------------\ndef pick_latest_match_root():\n    cands = sorted(CACHE_DIR.glob(\"match_cfg_*\"))\n    cands = [c for c in cands if (c/\"cfg.json\").exists() and (c/\"match_manifest_train.parquet\").exists()]\n    if not cands:\n        raise FileNotFoundError(\"Cannot find match_cfg_* under /kaggle/working/recodai_luc/cache. Run Robust Matching stage first.\")\n    cands = sorted(cands, key=lambda p: (p/\"cfg.json\").stat().st_mtime, reverse=True)\n    return cands[0]\n\nMATCH_ROOT = pick_latest_match_root()\nMATCH_CFG = json.loads((MATCH_ROOT/\"cfg.json\").read_text())\nPATCH = int(MATCH_CFG.get(\"patch\", MATCH_CFG.get(\"patch_size\", 14)))\nHTOK  = int(MATCH_CFG.get(\"Ht\", MATCH_CFG.get(\"htok\", 37)))\nWTOK  = int(MATCH_CFG.get(\"Wt\", MATCH_CFG.get(\"wtok\", 37)))\n\nmtrain_pq = MATCH_ROOT / \"match_manifest_train.parquet\"\ndf_m = pd.read_parquet(mtrain_pq).copy()\ndf_m[\"case_id\"] = df_m[\"case_id\"].astype(int)\nif \"match_npz\" not in df_m.columns:\n    raise ValueError(\"match_manifest_train.parquet missing match_npz column\")\n\n# pick best (prefer a score col if exists else latest mtime)\nscore_cols = [c for c in [\"best_peak_score\",\"peak_score_max\",\"max_peak_score\",\"score_max\",\"best_score\"] if c in df_m.columns]\nif score_cols:\n    sc = score_cols[0]\n    df_m[sc] = pd.to_numeric(df_m[sc], errors=\"coerce\").fillna(-1)\n    df_m = df_m.sort_values([\"case_id\", sc], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\nelse:\n    def _mtime(p):\n        try: return Path(p).stat().st_mtime\n        except Exception: return -1\n    df_m[\"_mtime\"] = df_m[\"match_npz\"].map(_mtime)\n    df_m = df_m.sort_values([\"case_id\",\"_mtime\"], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\n\nmatch_map = df_m.set_index(\"case_id\")[\"match_npz\"].to_dict()\nprint(\"MATCH_ROOT:\", MATCH_ROOT, \"| PATCH/HTOK/WTOK:\", PATCH, HTOK, WTOK)\n\n# ----------------------------\n# GT union loader + downsample to token grid\n# ----------------------------\ndef _find_mask_files(mask_dir: Path, case_id: int):\n    if mask_dir is None or (not mask_dir.exists()):\n        return []\n    cid = str(int(case_id))\n    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n    pats = [f\"{cid}*.png\", f\"{cid}*.jpg\", f\"{cid}*.jpeg\", f\"{cid}*.tif\", f\"{cid}*.tiff\", f\"{cid}*.bmp\",\n            f\"{cid}__*.png\", f\"{cid}_*.png\"]\n    out, seen = [], set()\n    for pat in pats:\n        for p in mask_dir.glob(pat):\n            if p.suffix.lower() in exts:\n                s = str(p)\n                if s not in seen:\n                    out.append(p); seen.add(s)\n    return sorted(out)\n\ndef load_gt_union_full(case_id: int):\n    # fast npy union if exists\n    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n        if d is None or (not d.exists()):\n            continue\n        npy = d / f\"{int(case_id)}.npy\"\n        if npy.exists():\n            a = np.load(npy, mmap_mode=\"r\")\n            if a.ndim == 2:\n                return (np.asarray(a) > 0)\n            if a.ndim == 3:\n                return (np.asarray(a) > 0).any(axis=0)\n    # png union\n    files = []\n    if TRAIN_MASK_DIR is not None: files += _find_mask_files(TRAIN_MASK_DIR, case_id)\n    if SUP_MASK_DIR is not None:   files += _find_mask_files(SUP_MASK_DIR, case_id)\n    if not files:\n        return None\n    m = None\n    for p in files:\n        try:\n            im = Image.open(p).convert(\"L\")\n            a = (np.asarray(im) > 0)\n            m = a if m is None else (m | a)\n        except Exception:\n            continue\n    return m\n\ndef downsample_bool_to_tok(mask_bool: np.ndarray):\n    if mask_bool is None:\n        return np.zeros((HTOK,WTOK), dtype=np.float32)\n    im = Image.fromarray((mask_bool.astype(np.uint8)*255))\n    im = im.resize((WTOK, HTOK), resample=Image.NEAREST)\n    return (np.asarray(im) > 127).astype(np.float32)\n\n# ----------------------------\n# Seed builder from match_npz (token union)\n# ----------------------------\ndef load_seed_tok(case_id: int, topk=None):\n    p = match_map.get(int(case_id), None)\n    if p is None or (not Path(p).exists()):\n        return np.zeros((HTOK,WTOK), dtype=np.float32), 0\n    z = np.load(p)\n    scores = z[\"peak_score\"] if \"peak_score\" in z.files else np.zeros((0,), np.int32)\n    src = z[\"src_masks\"] if \"src_masks\" in z.files else np.zeros((0,HTOK,WTOK), np.uint8)\n    tgt = z[\"tgt_masks\"] if \"tgt_masks\" in z.files else np.zeros((0,HTOK,WTOK), np.uint8)\n    if src.ndim != 3 or tgt.ndim != 3 or src.shape[0] == 0:\n        return np.zeros((HTOK,WTOK), dtype=np.float32), int(scores.max()) if len(scores) else 0\n    best_score = int(scores.max()) if len(scores) else 0\n    if topk is not None and src.shape[0] > topk:\n        # keep topk by score (desc)\n        idx = np.argsort(scores)[::-1][:topk]\n        src = src[idx]; tgt = tgt[idx]\n    seed = ((src>0) | (tgt>0)).any(axis=0).astype(np.float32)\n    return seed, best_score\n\n# ----------------------------\n# Morphology / CC (token space)\n# ----------------------------\ntry:\n    import scipy.ndimage as ndi\n    _HAS_SCIPY = True\nexcept Exception:\n    _HAS_SCIPY = False\n\ndef dilate_tok(x_bool, it=1):\n    if it <= 0: return x_bool\n    x = x_bool.astype(bool)\n    if _HAS_SCIPY:\n        return ndi.binary_dilation(x, iterations=it)\n    for _ in range(it):\n        xp = np.pad(x, 1, mode=\"constant\", constant_values=False)\n        y = np.zeros_like(x, dtype=bool)\n        for dy in (-1,0,1):\n            for dx in (-1,0,1):\n                y |= xp[1+dy:1+dy+x.shape[0], 1+dx:1+dx+x.shape[1]]\n        x = y\n    return x\n\ndef label_cc(x_bool):\n    x = x_bool.astype(bool)\n    if _HAS_SCIPY:\n        lab, n = ndi.label(x, structure=np.ones((3,3), dtype=np.uint8))\n        return lab, int(n)\n    H,W = x.shape\n    lab = np.zeros((H,W), dtype=np.int32); cur=0\n    for y in range(H):\n        for x0 in range(W):\n            if (not x[y,x0]) or lab[y,x0]!=0: continue\n            cur += 1\n            st=[(y,x0)]; lab[y,x0]=cur\n            while st:\n                yy,xx=st.pop()\n                for dy in (-1,0,1):\n                    for dx in (-1,0,1):\n                        if dy==0 and dx==0: continue\n                        ny,nx=yy+dy,xx+dx\n                        if 0<=ny<H and 0<=nx<W and x[ny,nx] and lab[ny,nx]==0:\n                            lab[ny,nx]=cur; st.append((ny,nx))\n    return lab, int(cur)\n\ndef inst_split_union_tok(mask_bool, min_area=2, max_area_frac=0.8, max_keep=8):\n    H,W = mask_bool.shape\n    lab,n = label_cc(mask_bool)\n    if n<=0:\n        return np.zeros((H,W), dtype=bool), 0\n    insts=[]\n    areas=[]\n    for k in range(1,n+1):\n        m = (lab==k)\n        a = int(m.sum())\n        if a < min_area: continue\n        if a / float(H*W) > max_area_frac: continue\n        insts.append(m); areas.append(a)\n    if not insts:\n        return np.zeros((H,W), dtype=bool), 0\n    order = np.argsort(np.asarray(areas))[::-1][:max_keep]\n    uni = np.zeros((H,W), dtype=bool)\n    for i in order:\n        uni |= insts[i]\n    return uni, int(len(order))\n\ndef dice(pr_bool, gt_bool):\n    a=int(pr_bool.sum()); b=int(gt_bool.sum())\n    if a==0 and b==0: return 1.0\n    if a==0 or b==0: return 0.0\n    inter=int((pr_bool & gt_bool).sum())\n    return float((2.0*inter)/(a+b))\n\n# ----------------------------\n# Dataset\n# ----------------------------\ndef load_tok_npz(path_str: str):\n    z = np.load(path_str)\n    # try common keys\n    for k in [\"tok\",\"tokens\",\"grid\",\"token_grid\",\"x\",\"feat\",\"emb\",\"f\"]:\n        if k in z.files:\n            a = z[k]\n            break\n    else:\n        # fallback: first key\n        keys = list(z.files)\n        if not keys: raise ValueError(\"empty npz\")\n        a = z[keys[0]]\n    a = np.asarray(a)\n    # ensure (Ht,Wt,D)\n    if a.ndim == 3 and a.shape[0] == HTOK and a.shape[1] == WTOK:\n        return a.astype(np.float32)\n    if a.ndim == 3 and a.shape[-2] == HTOK and a.shape[-1] == WTOK:\n        # (D,Ht,Wt) -> (Ht,Wt,D)\n        return np.transpose(a, (1,2,0)).astype(np.float32)\n    # last resort: resize spatial to tok\n    if a.ndim == 3:\n        # assume (H,W,D)\n        H,W,D = a.shape\n        # resize each channel (slow but rare)\n        out = np.zeros((HTOK,WTOK,D), np.float32)\n        for d in range(D):\n            im = Image.fromarray(a[:,:,d].astype(np.float32))\n            im = im.resize((WTOK,HTOK), resample=Image.BILINEAR)\n            out[:,:,d] = np.asarray(im).astype(np.float32)\n        return out\n    raise ValueError(f\"Unknown token array shape: {a.shape}\")\n\nclass HybridTokDS(Dataset):\n    def __init__(self, df_in: pd.DataFrame, tok_col: str):\n        self.df = df_in.reset_index(drop=True)\n        self.tok_col = tok_col\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        r = self.df.iloc[i]\n        cid = int(r[\"case_id\"])\n        y = float(r[\"y\"])\n        tok = load_tok_npz(r[self.tok_col])  # (Ht,Wt,D)\n        seed, best_score = load_seed_tok(cid, topk=8)\n        gt_full = load_gt_union_full(cid)\n        gt_tok = downsample_bool_to_tok(gt_full)  # (Ht,Wt) float 0/1\n        # build input: (C,Ht,Wt)\n        tok_ch = np.transpose(tok, (2,0,1))  # (D,Ht,Wt)\n        x = np.concatenate([tok_ch, seed[None,:,:]], axis=0).astype(np.float32)\n        return {\n            \"x\": torch.from_numpy(x),\n            \"gt\": torch.from_numpy(gt_tok[None,:,:].astype(np.float32)),  # (1,Ht,Wt)\n            \"y\": torch.tensor([y], dtype=torch.float32),\n            \"best_score\": torch.tensor([float(best_score)], dtype=torch.float32),\n            \"case_id\": torch.tensor([cid], dtype=torch.int64),\n        }\n\n# infer input dim\ntmp = load_tok_npz(df.iloc[0][tok_col])\nD_IN = int(tmp.shape[-1]) + 1\nprint(\"Token dim:\", int(tmp.shape[-1]), \"| Input channels:\", D_IN)\n\n# ----------------------------\n# Model: UNet-ish + ASPP + dual heads\n# ----------------------------\nclass ConvBNAct(nn.Module):\n    def __init__(self, c_in, c_out, k=3, s=1, p=1, drop=0.0):\n        super().__init__()\n        self.conv = nn.Conv2d(c_in, c_out, k, s, p, bias=False)\n        self.bn = nn.BatchNorm2d(c_out)\n        self.act = nn.SiLU(inplace=True)\n        self.drop = nn.Dropout2d(drop) if drop > 0 else nn.Identity()\n    def forward(self, x):\n        return self.drop(self.act(self.bn(self.conv(x))))\n\nclass ASPP(nn.Module):\n    def __init__(self, c_in, c_out, rates=(1,2,4)):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(c_in, c_out, 3, padding=r, dilation=r, bias=False),\n                nn.BatchNorm2d(c_out),\n                nn.SiLU(inplace=True),\n            ) for r in rates\n        ])\n        self.proj = nn.Sequential(\n            nn.Conv2d(len(rates)*c_out, c_out, 1, bias=False),\n            nn.BatchNorm2d(c_out),\n            nn.SiLU(inplace=True),\n        )\n    def forward(self, x):\n        ys = [b(x) for b in self.blocks]\n        y = torch.cat(ys, dim=1)\n        return self.proj(y)\n\nclass HybridUNet(nn.Module):\n    def __init__(self, c_in, base_ch=96, drop=0.1):\n        super().__init__()\n        c1, c2, c3 = base_ch, base_ch*2, base_ch*3\n\n        self.in_norm = nn.GroupNorm(8, c_in)\n\n        self.e1 = nn.Sequential(ConvBNAct(c_in, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n        self.p1 = nn.MaxPool2d(2)\n\n        self.e2 = nn.Sequential(ConvBNAct(c1, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n        self.p2 = nn.MaxPool2d(2)\n\n        self.e3 = nn.Sequential(ConvBNAct(c2, c3, drop=drop), ConvBNAct(c3, c3, drop=drop))\n\n        self.aspp = ASPP(c3, c3, rates=(1,2,4))\n\n        # decoder\n        self.u2 = nn.ConvTranspose2d(c3, c2, 2, stride=2)\n        self.d2 = nn.Sequential(ConvBNAct(c2+c2, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n\n        self.u1 = nn.ConvTranspose2d(c2, c1, 2, stride=2)\n        self.d1 = nn.Sequential(ConvBNAct(c1+c1, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n\n        # heads\n        self.seg_head = nn.Conv2d(c1, 1, 1)\n        self.cls_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(c3, c3//2),\n            nn.SiLU(inplace=True),\n            nn.Dropout(drop if drop>0 else 0.0),\n            nn.Linear(c3//2, 1),\n        )\n\n    def forward(self, x):\n        # x: (B,C,Ht,Wt)\n        x = self.in_norm(x)\n        e1 = self.e1(x)\n        e2 = self.e2(self.p1(e1))\n        e3 = self.e3(self.p2(e2))\n        b  = self.aspp(e3)\n\n        # cls from bottleneck\n        cls_logit = self.cls_head(b)\n\n        d2 = self.u2(b)\n        d2 = self.d2(torch.cat([d2, e2], dim=1))\n\n        d1 = self.u1(d2)\n        d1 = self.d1(torch.cat([d1, e1], dim=1))\n\n        seg_logit = self.seg_head(d1)\n        return seg_logit, cls_logit\n\n# ----------------------------\n# Loss: (BCE/Focal + Dice) + BCE cls\n# ----------------------------\ndef dice_loss_from_logits(logits, target, eps=1e-6):\n    # logits/target: (B,1,H,W)\n    p = torch.sigmoid(logits)\n    inter = (p * target).sum(dim=(2,3))\n    den = (p + target).sum(dim=(2,3)) + eps\n    d = (2.0 * inter) / den\n    return 1.0 - d.mean()\n\ndef bce_focal_from_logits(logits, target, gamma=2.0):\n    bce = F.binary_cross_entropy_with_logits(logits, target, reduction=\"none\")\n    if gamma <= 0:\n        return bce.mean()\n    p = torch.sigmoid(logits)\n    pt = target * p + (1-target) * (1-p)\n    w = (1-pt).pow(gamma)\n    return (w * bce).mean()\n\n# ----------------------------\n# Fuse + score (token space)\n# ----------------------------\n@torch.no_grad()\ndef eval_model(model, dl, cfg_pp):\n    model.eval()\n    scores = []\n    # unpack pp\n    T1 = cfg_pp[\"T1\"]; T0 = cfg_pp[\"T0\"]; dil_it = cfg_pp[\"seed_dilate_it\"]\n    thr_gate = cfg_pp[\"thr_gate\"]\n    min_tok_area = cfg_pp[\"min_tok_area\"]\n    max_tok_area_frac = cfg_pp[\"max_tok_area_frac\"]\n    max_inst_keep = cfg_pp[\"max_inst_keep\"]\n    min_peak_keep = cfg_pp[\"min_peak_score_keep\"]\n    min_area_frac_keep = cfg_pp[\"min_area_frac_keep\"]\n\n    for batch in dl:\n        x = batch[\"x\"].to(device, non_blocking=True)\n        gt = batch[\"gt\"].to(device, non_blocking=True)\n        best_score = batch[\"best_score\"].cpu().numpy().reshape(-1)\n        seg_logit, cls_logit = model(x)\n        p_gate = torch.sigmoid(cls_logit).detach().cpu().numpy().reshape(-1)\n        p_tok = torch.sigmoid(seg_logit).detach().cpu().numpy()[:,0]  # (B,Ht,Wt)\n        seed = x.detach().cpu().numpy()[:,-1]  # last channel (B,Ht,Wt)\n        gt_np = gt.detach().cpu().numpy()[:,0] > 0.5\n\n        for i in range(x.shape[0]):\n            if p_gate[i] < thr_gate:\n                pr = np.zeros((HTOK,WTOK), dtype=bool)\n            else:\n                prob = p_tok[i]\n                hard = prob >= T1\n                soft = prob >= T0\n                sd = dilate_tok(seed[i] > 0.5, dil_it)\n                fused = hard | (sd & soft)\n\n                # instance split + filter\n                uni, n_inst = inst_split_union_tok(\n                    fused, min_area=min_tok_area, max_area_frac=max_tok_area_frac, max_keep=max_inst_keep\n                )\n                area_frac = float(uni.mean())\n\n                # guard: weak match + tiny area -> drop\n                if (best_score[i] < min_peak_keep) and (area_frac < min_area_frac_keep):\n                    uni = np.zeros((HTOK,WTOK), dtype=bool)\n\n                pr = uni\n\n            scores.append(dice(pr, gt_np[i]))\n    return float(np.mean(scores)) if scores else 0.0\n\n# ----------------------------\n# Train one trial (single val fold)\n# ----------------------------\ndef sample_trial_cfg(trial_id: int, val_fold: int):\n    def log_uniform(a,b):\n        return float(np.exp(np.random.uniform(np.log(a), np.log(b))))\n    lr = log_uniform(*LR_RANGE)\n    wd = float(np.random.uniform(*WD_RANGE))\n    drop = float(np.random.uniform(*DROPOUT_RANGE))\n    base_ch = int(np.random.choice(BASE_CH_CHOICES))\n    lam_seg = float(np.random.uniform(*LAMBDA_SEG_RANGE))\n    lam_cls = float(np.random.uniform(*LAMBDA_CLS_RANGE))\n    gamma = float(np.random.choice(FOCAL_GAMMA_CHOICES))\n\n    T1 = float(np.random.uniform(*T1_RANGE))\n    T0 = float(np.random.uniform(*T0_RANGE))\n    if T0 > T1:\n        T0, T1 = T1-0.05, T1  # enforce T0 <= T1 (soft <= hard)\n        T0 = max(0.05, T0)\n    dil_it = int(np.random.choice(SEED_DILATE_CHOICES))\n    thr_gate = float(np.random.uniform(*THR_GATE_RANGE))\n\n    min_tok_area = int(np.random.choice(MIN_TOK_AREA_CHOICES))\n    max_tok_area_frac = float(np.random.choice(MAX_TOK_AREA_FRAC_CHOICES))\n    max_inst_keep = int(np.random.choice(MAX_INST_KEEP_CHOICES))\n\n    min_peak_keep = int(np.random.choice(MIN_PEAK_SCORE_KEEP_CHOICES))\n    min_area_frac_keep = float(np.random.choice(MIN_AREA_FRAC_KEEP_CHOICES))\n\n    return {\n        \"trial_id\": int(trial_id),\n        \"val_fold\": int(val_fold),\n        \"lr\": lr, \"weight_decay\": wd, \"dropout\": drop, \"base_ch\": base_ch,\n        \"lambda_seg\": lam_seg, \"lambda_cls\": lam_cls, \"focal_gamma\": gamma,\n        \"T1\": T1, \"T0\": T0, \"seed_dilate_it\": dil_it, \"thr_gate\": thr_gate,\n        \"min_tok_area\": min_tok_area, \"max_tok_area_frac\": max_tok_area_frac, \"max_inst_keep\": max_inst_keep,\n        \"min_peak_score_keep\": min_peak_keep, \"min_area_frac_keep\": min_area_frac_keep,\n    }\n\ndef train_trial(cfg_trial):\n    val_fold = cfg_trial[\"val_fold\"]\n    df_tr = df[df[\"fold\"] != val_fold].reset_index(drop=True)\n    df_va = df[df[\"fold\"] == val_fold].reset_index(drop=True)\n\n    # small balancing: oversample positives in train\n    pos = df_tr[df_tr[\"y\"]==1]\n    neg = df_tr[df_tr[\"y\"]==0]\n    if len(pos) > 0 and len(neg) > 0:\n        # target roughly 1:1 for stability\n        take = min(len(neg), len(pos)*3)  # allow more neg but not too skewed\n        neg_s = neg.sample(n=take, replace=False, random_state=SEED)\n        pos_s = pos.sample(n=take, replace=True, random_state=SEED)\n        df_tr_use = pd.concat([neg_s, pos_s], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n    else:\n        df_tr_use = df_tr\n\n    ds_tr = HybridTokDS(df_tr_use, tok_col=tok_col)\n    ds_va = HybridTokDS(df_va, tok_col=tok_col)\n\n    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n                       pin_memory=(device.type==\"cuda\"), drop_last=True)\n    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n                       pin_memory=(device.type==\"cuda\"), drop_last=False)\n\n    model = HybridUNet(c_in=D_IN, base_ch=cfg_trial[\"base_ch\"], drop=cfg_trial[\"dropout\"]).to(device)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg_trial[\"lr\"], weight_decay=cfg_trial[\"weight_decay\"])\n    scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type==\"cuda\"))\n\n    best_score = -1.0\n    best_state = None\n    bad = 0\n\n    for ep in range(1, TRIAL_EPOCHS+1):\n        model.train()\n        t0 = time.time()\n        loss_meter = 0.0\n        nsteps = 0\n\n        for step, batch in enumerate(dl_tr, start=1):\n            x = batch[\"x\"].to(device, non_blocking=True)\n            gt = batch[\"gt\"].to(device, non_blocking=True)\n            yb = batch[\"y\"].to(device, non_blocking=True)\n\n            with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type==\"cuda\")):\n                seg_logit, cls_logit = model(x)\n                l_seg = bce_focal_from_logits(seg_logit, gt, gamma=cfg_trial[\"focal_gamma\"]) + dice_loss_from_logits(seg_logit, gt)\n                l_cls = F.binary_cross_entropy_with_logits(cls_logit, yb)\n                loss = cfg_trial[\"lambda_seg\"] * l_seg + cfg_trial[\"lambda_cls\"] * l_cls\n                loss = loss / ACCUM_STEPS\n\n            scaler.scale(loss).backward()\n            if step % ACCUM_STEPS == 0:\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n\n            loss_meter += float(loss.item()) * ACCUM_STEPS\n            nsteps += 1\n\n        # eval\n        score = eval_model(model, dl_va, cfg_trial)\n        dt = time.time() - t0\n        print(f\"[trial {cfg_trial['trial_id']:02d} | fold {val_fold}] ep {ep}/{TRIAL_EPOCHS} \"\n              f\"loss={loss_meter/max(1,nsteps):.4f} val_dice_proxy={score:.5f} time={dt:.1f}s\")\n\n        if score > best_score + 1e-5:\n            best_score = score\n            best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= EARLYSTOP_PATIENCE:\n                break\n\n    return best_score, best_state\n\n# ----------------------------\n# Main loop\n# ----------------------------\ntrials = []\nglobal_best = {\"score\": -1.0, \"cfg\": None, \"state\": None}\n\nt_all = time.time()\n\nfor t in range(1, MAX_TRIALS+1):\n    val_fold = fold_ids[(t-1) % len(fold_ids)] if VAL_FOLD_ROTATE else fold_ids[0]\n    cfg_trial = sample_trial_cfg(t, val_fold)\n\n    # run\n    try:\n        score, state = train_trial(cfg_trial)\n    except Exception as e:\n        print(f\"[trial {t}] FAILED:\", repr(e))\n        cfg_trial[\"score\"] = float(\"nan\")\n        cfg_trial[\"status\"] = \"fail\"\n        trials.append(cfg_trial)\n        continue\n\n    cfg_trial[\"score\"] = float(score)\n    cfg_trial[\"status\"] = \"ok\"\n    trials.append(cfg_trial)\n\n    if score > global_best[\"score\"]:\n        global_best[\"score\"] = float(score)\n        global_best[\"cfg\"] = cfg_trial\n        global_best[\"state\"] = state\n\n    # persist trials each iteration\n    pd.DataFrame(trials).to_csv(OUT_DIR / \"trials.csv\", index=False)\n    (OUT_DIR / \"best_config.json\").write_text(json.dumps(global_best[\"cfg\"], indent=2) if global_best[\"cfg\"] else \"{}\")\n\n    print(\"-\"*60)\n    print(\"CURRENT BEST:\", global_best[\"score\"], \"| trial:\", global_best[\"cfg\"][\"trial_id\"], \"| val_fold:\", global_best[\"cfg\"][\"val_fold\"])\n    print(\"-\"*60)\n\n# save best model\nif global_best[\"state\"] is not None:\n    pack = {\n        \"model_type\": \"HybridUNet\",\n        \"input_channels\": D_IN,\n        \"HTOK\": HTOK, \"WTOK\": WTOK, \"PATCH\": PATCH,\n        \"tok_col\": tok_col,\n        \"match_root\": str(MATCH_ROOT),\n        \"train_table\": str(TRAIN_TABLE),\n        \"paths\": {\"TRAIN_MASK_DIR\": str(TRAIN_MASK_DIR) if TRAIN_MASK_DIR else None,\n                  \"SUP_MASK_DIR\": str(SUP_MASK_DIR) if SUP_MASK_DIR else None},\n        \"best_cfg\": global_best[\"cfg\"],\n        \"state_dict\": global_best[\"state\"],\n    }\n    torch.save(pack, OUT_DIR / \"best_model.pt\")\n\nprint(\"DONE in\", f\"{time.time()-t_all:.1f}s\")\nprint(\"Saved:\", OUT_DIR / \"trials.csv\")\nprint(\"Saved:\", OUT_DIR / \"best_config.json\")\nprint(\"Saved:\", OUT_DIR / \"best_model.pt\" if (OUT_DIR / \"best_model.pt\").exists() else \"(no best_model.pt)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Training (Train on Full Data)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE — Final Training (Train on Full Data) (ONE CELL) — HYBRID (OPSI-1)\n# Train ONE NETWORK: UNet(+ASPP) token-decoder + Gate head\n# Then:\n# - Export mask-prob cache  : /kaggle/working/recodai_luc/cache/mask_prob_hybrid_<hash>/{case_id}.npz  (key: prob_tok, p_gate)\n# - Sweep best gate threshold on TRAIN using Dice-proxy (token-space)\n# - Save final model bundle : /kaggle/working/recodai_luc_hybrid_artifacts/final_hybrid_model.pt\n#\n# Needs:\n# - /kaggle/working/recodai_luc_hybrid_opt/best_config.json (from Optimize stage)\n# - train_table.parquet with: case_id, y, tok_path/token_path/dino_path/... (auto-detect)\n# - (optional) test_table.parquet with: case_id, tok_path/... (auto-detect)\n# - Robust Matching outputs: /kaggle/working/recodai_luc/cache/match_cfg_*/match_manifest_{train,test}.parquet\n# - GT masks via /kaggle/working/recodai_luc_prof/paths.json (TRAIN_MASK_DIR / SUP_MASK_DIR)\n# ============================================================\n\nimport os, json, time, math, random, hashlib, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# Paths\n# ----------------------------\nOPT_DIR  = Path(\"/kaggle/working/recodai_luc_hybrid_opt\")\nOUT_DIR  = Path(\"/kaggle/working/recodai_luc_hybrid_artifacts\")\nPROF_DIR = Path(\"/kaggle/working/recodai_luc_prof\")\nCACHE_DIR= Path(\"/kaggle/working/recodai_luc/cache\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nbest_cfg_path = OPT_DIR / \"best_config.json\"\nif not best_cfg_path.exists():\n    raise FileNotFoundError(f\"Missing {best_cfg_path}. Run Optimize stage first.\")\nBEST = json.loads(best_cfg_path.read_text())\n\npaths_json = PROF_DIR / \"paths.json\"\nif not paths_json.exists():\n    raise FileNotFoundError(f\"Missing {paths_json}\")\nPATHS = json.loads(paths_json.read_text())\nTRAIN_MASK_DIR = Path(PATHS.get(\"TRAIN_MASK_DIR\",\"\")) if PATHS.get(\"TRAIN_MASK_DIR\") else None\nSUP_MASK_DIR   = Path(PATHS.get(\"SUP_MASK_DIR\",\"\")) if PATHS.get(\"SUP_MASK_DIR\") else None\n\n# ----------------------------\n# Repro / device\n# ----------------------------\nSEED = int(os.environ.get(\"SEED\", \"42\"))\ndef seed_everything(s=42):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\nseed_everything(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nUSE_AMP = bool(int(os.environ.get(\"USE_AMP\", \"1\"))) and (device.type == \"cuda\")\nprint(\"DEVICE:\", device, \"| AMP:\", USE_AMP)\n\n# ----------------------------\n# Find train/test tables\n# ----------------------------\ntrain_table_cands = [\n    Path(\"/kaggle/working/recodai_luc_gate_artifacts/train_table.parquet\"),\n    Path(\"/kaggle/working/recodai_luc_hybrid_artifacts/train_table.parquet\"),\n    PROF_DIR / \"train_table.parquet\",\n]\ntest_table_cands = [\n    Path(\"/kaggle/working/recodai_luc_gate_artifacts/test_table.parquet\"),\n    Path(\"/kaggle/working/recodai_luc_hybrid_artifacts/test_table.parquet\"),\n    PROF_DIR / \"test_table.parquet\",\n]\nTRAIN_TABLE = next((p for p in train_table_cands if p.exists()), None)\nTEST_TABLE  = next((p for p in test_table_cands if p.exists()), None)\nif TRAIN_TABLE is None:\n    raise FileNotFoundError(\"Cannot find train_table.parquet. Run Build Training Table stage first.\")\n\ndf_tr = pd.read_parquet(TRAIN_TABLE).copy()\nfor need in [\"case_id\",\"y\"]:\n    if need not in df_tr.columns:\n        raise ValueError(f\"train_table missing required col: {need}\")\ndf_tr[\"case_id\"] = df_tr[\"case_id\"].astype(int)\ndf_tr[\"y\"] = df_tr[\"y\"].astype(int)\n\ntok_col = None\nfor c in [\"tok_path\",\"token_path\",\"dino_path\",\"feat_path\",\"emb_path\",\"token_npz\",\"npz_path\"]:\n    if c in df_tr.columns:\n        tok_col = c\n        break\nif tok_col is None:\n    raise ValueError(\"train_table must contain a token path column (tok_path/token_path/dino_path/feat_path/emb_path).\")\n\ndf_tr[tok_col] = df_tr[tok_col].astype(str)\ndf_tr = df_tr[df_tr[tok_col].map(lambda p: Path(p).exists())].reset_index(drop=True)\n\ndf_te = None\nif TEST_TABLE is not None:\n    df_te = pd.read_parquet(TEST_TABLE).copy()\n    if \"case_id\" not in df_te.columns:\n        raise ValueError(\"test_table missing case_id\")\n    df_te[\"case_id\"] = df_te[\"case_id\"].astype(int)\n    if tok_col not in df_te.columns:\n        # try detect token col on test too\n        for c in [\"tok_path\",\"token_path\",\"dino_path\",\"feat_path\",\"emb_path\",\"token_npz\",\"npz_path\"]:\n            if c in df_te.columns:\n                tok_col_te = c\n                break\n        else:\n            raise ValueError(\"test_table missing token path column.\")\n    else:\n        tok_col_te = tok_col\n    df_te[tok_col_te] = df_te[tok_col_te].astype(str)\n    df_te = df_te[df_te[tok_col_te].map(lambda p: Path(p).exists())].reset_index(drop=True)\nelse:\n    tok_col_te = tok_col\n\nprint(\"TRAIN_TABLE:\", TRAIN_TABLE, \"| rows:\", len(df_tr), \"| pos_rate:\", float(df_tr[\"y\"].mean()))\nprint(\"TEST_TABLE :\", TEST_TABLE if TEST_TABLE else \"(none)\")\nprint(\"TOK_COL    :\", tok_col, \"| TOK_COL_TEST:\", tok_col_te)\n\n# ----------------------------\n# Latest MATCH_ROOT + Htok/Wtok/PATCH + match maps (train/test)\n# ----------------------------\ndef pick_latest_match_root():\n    cands = sorted(CACHE_DIR.glob(\"match_cfg_*\"))\n    cands = [c for c in cands if (c/\"cfg.json\").exists() and (c/\"match_manifest_train.parquet\").exists()]\n    if not cands:\n        raise FileNotFoundError(\"Cannot find match_cfg_* under /kaggle/working/recodai_luc/cache. Run Robust Matching stage first.\")\n    cands = sorted(cands, key=lambda p: (p/\"cfg.json\").stat().st_mtime, reverse=True)\n    return cands[0]\n\nMATCH_ROOT = pick_latest_match_root()\nMATCH_CFG = json.loads((MATCH_ROOT/\"cfg.json\").read_text())\nPATCH = int(MATCH_CFG.get(\"patch\", MATCH_CFG.get(\"patch_size\", 14)))\nHTOK  = int(MATCH_CFG.get(\"Ht\", MATCH_CFG.get(\"htok\", 37)))\nWTOK  = int(MATCH_CFG.get(\"Wt\", MATCH_CFG.get(\"wtok\", 37)))\n\ndef build_match_map(pq: Path):\n    if not pq.exists():\n        return {}\n    dfm = pd.read_parquet(pq).copy()\n    dfm[\"case_id\"] = dfm[\"case_id\"].astype(int)\n    if \"match_npz\" not in dfm.columns:\n        return {}\n    dfm = dfm[dfm[\"match_npz\"].notna()].copy()\n    if len(dfm) == 0:\n        return {}\n    score_cols = [c for c in [\"best_peak_score\",\"peak_score_max\",\"max_peak_score\",\"score_max\",\"best_score\"] if c in dfm.columns]\n    if score_cols:\n        sc = score_cols[0]\n        dfm[sc] = pd.to_numeric(dfm[sc], errors=\"coerce\").fillna(-1)\n        dfm = dfm.sort_values([\"case_id\", sc], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\n    else:\n        def _mtime(p):\n            try: return Path(p).stat().st_mtime\n            except Exception: return -1\n        dfm[\"_mtime\"] = dfm[\"match_npz\"].map(_mtime)\n        dfm = dfm.sort_values([\"case_id\",\"_mtime\"], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\n    return dfm.set_index(\"case_id\")[\"match_npz\"].to_dict()\n\nmatch_map_tr = build_match_map(MATCH_ROOT / \"match_manifest_train.parquet\")\nmatch_map_te = build_match_map(MATCH_ROOT / \"match_manifest_test.parquet\")\n\nprint(\"MATCH_ROOT:\", MATCH_ROOT)\nprint(\"TOK GRID  :\", (HTOK, WTOK), \"| PATCH:\", PATCH)\nprint(\"match_map_tr:\", len(match_map_tr), \"| match_map_te:\", len(match_map_te))\n\n# ----------------------------\n# Optional SciPy\n# ----------------------------\ntry:\n    import scipy.ndimage as ndi\n    _HAS_SCIPY = True\nexcept Exception:\n    _HAS_SCIPY = False\n\ndef dilate_tok(x_bool, it=1):\n    if it <= 0: return x_bool\n    x = x_bool.astype(bool)\n    if _HAS_SCIPY:\n        return ndi.binary_dilation(x, iterations=it)\n    for _ in range(it):\n        xp = np.pad(x, 1, mode=\"constant\", constant_values=False)\n        y = np.zeros_like(x, dtype=bool)\n        for dy in (-1,0,1):\n            for dx in (-1,0,1):\n                y |= xp[1+dy:1+dy+x.shape[0], 1+dx:1+dx+x.shape[1]]\n        x = y\n    return x\n\ndef label_cc(x_bool):\n    x = x_bool.astype(bool)\n    if _HAS_SCIPY:\n        lab, n = ndi.label(x, structure=np.ones((3,3), dtype=np.uint8))\n        return lab, int(n)\n    H,W = x.shape\n    lab = np.zeros((H,W), dtype=np.int32); cur=0\n    for y in range(H):\n        for x0 in range(W):\n            if (not x[y,x0]) or lab[y,x0]!=0: continue\n            cur += 1\n            st=[(y,x0)]; lab[y,x0]=cur\n            while st:\n                yy,xx=st.pop()\n                for dy in (-1,0,1):\n                    for dx in (-1,0,1):\n                        if dy==0 and dx==0: continue\n                        ny,nx=yy+dy,xx+dx\n                        if 0<=ny<H and 0<=nx<W and x[ny,nx] and lab[ny,nx]==0:\n                            lab[ny,nx]=cur; st.append((ny,nx))\n    return lab, int(cur)\n\ndef inst_split_union_tok(mask_bool, min_area=2, max_area_frac=0.8, max_keep=8):\n    H,W = mask_bool.shape\n    lab,n = label_cc(mask_bool)\n    if n<=0:\n        return np.zeros((H,W), dtype=bool), 0\n    insts=[]; areas=[]\n    for k in range(1,n+1):\n        m = (lab==k)\n        a = int(m.sum())\n        if a < min_area: continue\n        if a / float(H*W) > max_area_frac: continue\n        insts.append(m); areas.append(a)\n    if not insts:\n        return np.zeros((H,W), dtype=bool), 0\n    order = np.argsort(np.asarray(areas))[::-1][:max_keep]\n    uni = np.zeros((H,W), dtype=bool)\n    for i in order:\n        uni |= insts[i]\n    return uni, int(len(order))\n\ndef dice(pr_bool, gt_bool):\n    a=int(pr_bool.sum()); b=int(gt_bool.sum())\n    if a==0 and b==0: return 1.0\n    if a==0 or b==0: return 0.0\n    inter=int((pr_bool & gt_bool).sum())\n    return float((2.0*inter)/(a+b))\n\n# ----------------------------\n# GT union loader -> token GT\n# ----------------------------\ndef _find_mask_files(mask_dir: Path, case_id: int):\n    if mask_dir is None or (not mask_dir.exists()):\n        return []\n    cid = str(int(case_id))\n    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n    pats = [f\"{cid}*.png\", f\"{cid}*.jpg\", f\"{cid}*.jpeg\", f\"{cid}*.tif\", f\"{cid}*.tiff\", f\"{cid}*.bmp\",\n            f\"{cid}__*.png\", f\"{cid}_*.png\"]\n    out, seen = [], set()\n    for pat in pats:\n        for p in mask_dir.glob(pat):\n            if p.suffix.lower() in exts:\n                s = str(p)\n                if s not in seen:\n                    out.append(p); seen.add(s)\n    return sorted(out)\n\ndef load_gt_union_full(case_id: int):\n    # npy union if exists\n    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n        if d is None or (not d.exists()):\n            continue\n        npy = d / f\"{int(case_id)}.npy\"\n        if npy.exists():\n            a = np.load(npy, mmap_mode=\"r\")\n            if a.ndim == 2:\n                return (np.asarray(a) > 0)\n            if a.ndim == 3:\n                return (np.asarray(a) > 0).any(axis=0)\n    # png union\n    files = []\n    if TRAIN_MASK_DIR is not None: files += _find_mask_files(TRAIN_MASK_DIR, case_id)\n    if SUP_MASK_DIR is not None:   files += _find_mask_files(SUP_MASK_DIR, case_id)\n    if not files:\n        return None\n    m = None\n    for p in files:\n        try:\n            im = Image.open(p).convert(\"L\")\n            a = (np.asarray(im) > 0)\n            m = a if m is None else (m | a)\n        except Exception:\n            continue\n    return m\n\ndef downsample_bool_to_tok(mask_bool: np.ndarray):\n    if mask_bool is None:\n        return np.zeros((HTOK,WTOK), dtype=np.float32)\n    im = Image.fromarray((mask_bool.astype(np.uint8)*255))\n    im = im.resize((WTOK, HTOK), resample=Image.NEAREST)\n    return (np.asarray(im) > 127).astype(np.float32)\n\n# ----------------------------\n# Seed from match_npz (token union) + best_score\n# ----------------------------\ndef load_seed_tok(case_id: int, is_test=False, topk=8):\n    mm = match_map_te if is_test else match_map_tr\n    p = mm.get(int(case_id), None)\n    if p is None or (not Path(p).exists()):\n        return np.zeros((HTOK,WTOK), dtype=np.float32), 0\n    z = np.load(p)\n    scores = z[\"peak_score\"] if \"peak_score\" in z.files else np.zeros((0,), np.int32)\n    src = z[\"src_masks\"] if \"src_masks\" in z.files else np.zeros((0,HTOK,WTOK), np.uint8)\n    tgt = z[\"tgt_masks\"] if \"tgt_masks\" in z.files else np.zeros((0,HTOK,WTOK), np.uint8)\n    if src.ndim != 3 or tgt.ndim != 3 or src.shape[0] == 0:\n        return np.zeros((HTOK,WTOK), dtype=np.float32), int(scores.max()) if len(scores) else 0\n    best_score = int(scores.max()) if len(scores) else 0\n    if topk is not None and src.shape[0] > topk:\n        idx = np.argsort(scores)[::-1][:topk]\n        src = src[idx]; tgt = tgt[idx]\n    seed = ((src>0) | (tgt>0)).any(axis=0).astype(np.float32)\n    return seed, best_score\n\n# ----------------------------\n# Token loader (npz/npy)\n# ----------------------------\ndef load_tok_any(path_str: str):\n    p = Path(path_str)\n    if p.suffix.lower() == \".npy\":\n        a = np.load(p, mmap_mode=\"r\")\n    else:\n        z = np.load(p)\n        for k in [\"tok\",\"tokens\",\"grid\",\"token_grid\",\"x\",\"feat\",\"emb\",\"f\"]:\n            if k in z.files:\n                a = z[k]; break\n        else:\n            keys = list(z.files)\n            if not keys: raise ValueError(\"empty npz\")\n            a = z[keys[0]]\n    a = np.asarray(a)\n    # normalize to (Ht,Wt,D)\n    if a.ndim == 3 and a.shape[0] == HTOK and a.shape[1] == WTOK:\n        return a.astype(np.float32)\n    if a.ndim == 3 and a.shape[-2] == HTOK and a.shape[-1] == WTOK:\n        return np.transpose(a, (1,2,0)).astype(np.float32)  # (D,Ht,Wt)->(Ht,Wt,D)\n    if a.ndim == 3:\n        # resize spatial to tok (slow fallback)\n        H,W,D = a.shape\n        out = np.zeros((HTOK,WTOK,D), np.float32)\n        for d in range(D):\n            im = Image.fromarray(a[:,:,d].astype(np.float32))\n            im = im.resize((WTOK,HTOK), resample=Image.BILINEAR)\n            out[:,:,d] = np.asarray(im).astype(np.float32)\n        return out\n    raise ValueError(f\"Unknown token array shape: {a.shape}\")\n\n# infer input dim\ntmp = load_tok_any(df_tr.iloc[0][tok_col])\nDIN = int(tmp.shape[-1])\nCIN = DIN + 1\nprint(\"Token D:\", DIN, \"| Input C:\", CIN)\n\n# ----------------------------\n# Dataset / loaders\n# ----------------------------\nclass TrainDS(Dataset):\n    def __init__(self, df_in: pd.DataFrame, tok_col: str):\n        self.df = df_in.reset_index(drop=True)\n        self.tok_col = tok_col\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        r = self.df.iloc[i]\n        cid = int(r[\"case_id\"])\n        y = float(r[\"y\"])\n        tok = load_tok_any(r[self.tok_col])          # (Ht,Wt,D)\n        seed, best_score = load_seed_tok(cid, is_test=False, topk=8)\n        gt_full = load_gt_union_full(cid)\n        gt_tok = downsample_bool_to_tok(gt_full)     # (Ht,Wt) float\n        tok_ch = np.transpose(tok, (2,0,1))          # (D,Ht,Wt)\n        x = np.concatenate([tok_ch, seed[None,:,:]], axis=0).astype(np.float32)\n        return {\n            \"x\": torch.from_numpy(x),\n            \"gt\": torch.from_numpy(gt_tok[None,:,:].astype(np.float32)),\n            \"y\": torch.tensor([y], dtype=torch.float32),\n            \"best_score\": torch.tensor([float(best_score)], dtype=torch.float32),\n            \"case_id\": torch.tensor([cid], dtype=torch.int64),\n        }\n\nclass InferDS(Dataset):\n    def __init__(self, df_in: pd.DataFrame, tok_col: str, is_test: bool):\n        self.df = df_in.reset_index(drop=True)\n        self.tok_col = tok_col\n        self.is_test = is_test\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        r = self.df.iloc[i]\n        cid = int(r[\"case_id\"])\n        tok = load_tok_any(r[self.tok_col])\n        seed, best_score = load_seed_tok(cid, is_test=self.is_test, topk=8)\n        tok_ch = np.transpose(tok, (2,0,1))\n        x = np.concatenate([tok_ch, seed[None,:,:]], axis=0).astype(np.float32)\n        return {\n            \"x\": torch.from_numpy(x),\n            \"best_score\": torch.tensor([float(best_score)], dtype=torch.float32),\n            \"case_id\": torch.tensor([cid], dtype=torch.int64),\n        }\n\n# ----------------------------\n# Model (odd-size safe: pool ceil_mode + interpolate up)\n# ----------------------------\nclass ConvBNAct(nn.Module):\n    def __init__(self, c_in, c_out, k=3, s=1, p=1, drop=0.0):\n        super().__init__()\n        self.conv = nn.Conv2d(c_in, c_out, k, s, p, bias=False)\n        self.bn = nn.BatchNorm2d(c_out)\n        self.act = nn.SiLU(inplace=True)\n        self.drop = nn.Dropout2d(drop) if drop > 0 else nn.Identity()\n    def forward(self, x):\n        return self.drop(self.act(self.bn(self.conv(x))))\n\nclass ASPP(nn.Module):\n    def __init__(self, c_in, c_out, rates=(1,2,4)):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(c_in, c_out, 3, padding=r, dilation=r, bias=False),\n                nn.BatchNorm2d(c_out),\n                nn.SiLU(inplace=True),\n            ) for r in rates\n        ])\n        self.proj = nn.Sequential(\n            nn.Conv2d(len(rates)*c_out, c_out, 1, bias=False),\n            nn.BatchNorm2d(c_out),\n            nn.SiLU(inplace=True),\n        )\n    def forward(self, x):\n        y = torch.cat([b(x) for b in self.blocks], dim=1)\n        return self.proj(y)\n\nclass HybridUNet(nn.Module):\n    def __init__(self, c_in, base_ch=96, drop=0.1):\n        super().__init__()\n        c1, c2, c3 = base_ch, base_ch*2, base_ch*3\n        self.e1 = nn.Sequential(ConvBNAct(c_in, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n        self.pool = nn.MaxPool2d(2, 2, ceil_mode=True)\n        self.e2 = nn.Sequential(ConvBNAct(c1, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n        self.e3 = nn.Sequential(ConvBNAct(c2, c3, drop=drop), ConvBNAct(c3, c3, drop=drop))\n        self.aspp = ASPP(c3, c3, rates=(1,2,4))\n\n        self.d2 = nn.Sequential(ConvBNAct(c3+c2, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n        self.d1 = nn.Sequential(ConvBNAct(c2+c1, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n\n        self.seg_head = nn.Conv2d(c1, 1, 1)\n        self.cls_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(c3, c3//2),\n            nn.SiLU(inplace=True),\n            nn.Dropout(drop if drop>0 else 0.0),\n            nn.Linear(c3//2, 1),\n        )\n\n    def forward(self, x):\n        e1 = self.e1(x)                       # (B,c1,H,W)\n        p1 = self.pool(e1)                    # ceil\n        e2 = self.e2(p1)                      # (B,c2,~H/2,~W/2)\n        p2 = self.pool(e2)\n        e3 = self.e3(p2)\n        b  = self.aspp(e3)\n\n        cls_logit = self.cls_head(b)\n\n        u2 = F.interpolate(b, size=e2.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d2 = self.d2(torch.cat([u2, e2], dim=1))\n\n        u1 = F.interpolate(d2, size=e1.shape[-2:], mode=\"bilinear\", align_corners=False)\n        d1 = self.d1(torch.cat([u1, e1], dim=1))\n\n        seg_logit = self.seg_head(d1)\n        return seg_logit, cls_logit\n\n# ----------------------------\n# Loss\n# ----------------------------\ndef dice_loss_from_logits(logits, target, eps=1e-6):\n    p = torch.sigmoid(logits)\n    inter = (p * target).sum(dim=(2,3))\n    den = (p + target).sum(dim=(2,3)) + eps\n    d = (2.0 * inter) / den\n    return 1.0 - d.mean()\n\ndef bce_focal_from_logits(logits, target, gamma=2.0):\n    bce = F.binary_cross_entropy_with_logits(logits, target, reduction=\"none\")\n    if gamma <= 0:\n        return bce.mean()\n    p = torch.sigmoid(logits)\n    pt = target * p + (1-target) * (1-p)\n    w = (1-pt).pow(gamma)\n    return (w * bce).mean()\n\n# ----------------------------\n# Train\n# ----------------------------\nEPOCHS = int(os.environ.get(\"EPOCHS_FINAL\", str(max(12, int(BEST.get(\"trial_epochs\", 0) or 0) + 10))))\nBATCH  = int(os.environ.get(\"BATCH_SIZE\", \"32\"))\nNUM_WORKERS = int(os.environ.get(\"NUM_WORKERS\", \"2\"))\nACCUM = int(os.environ.get(\"ACCUM_STEPS\", \"1\"))\n\nbase_ch = int(BEST[\"base_ch\"])\ndropout = float(BEST[\"dropout\"])\nlr = float(BEST[\"lr\"])\nwd = float(BEST[\"weight_decay\"])\nlam_seg = float(BEST[\"lambda_seg\"])\nlam_cls = float(BEST[\"lambda_cls\"])\nfocal_gamma = float(BEST[\"focal_gamma\"])\n\npos = int(df_tr[\"y\"].sum())\nneg = int(len(df_tr) - pos)\npos_weight = float(neg / max(1, pos))\nbce_cls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], dtype=torch.float32, device=device))\n\nds = TrainDS(df_tr, tok_col=tok_col)\ndl = DataLoader(\n    ds, batch_size=BATCH, shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=(device.type==\"cuda\"),\n    drop_last=True\n)\n\nmodel = HybridUNet(c_in=CIN, base_ch=base_ch, drop=dropout).to(device)\nopt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, EPOCHS))\nscaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n\nprint(\"-\"*60)\nprint(\"FINAL TRAIN:\", {\"epochs\": EPOCHS, \"batch\": BATCH, \"accum\": ACCUM, \"lr\": lr, \"wd\": wd,\n                      \"base_ch\": base_ch, \"dropout\": dropout, \"pos_weight\": pos_weight,\n                      \"lam_seg\": lam_seg, \"lam_cls\": lam_cls, \"focal_gamma\": focal_gamma})\nprint(\"-\"*60)\n\nt0 = time.time()\nmodel.train()\nopt.zero_grad(set_to_none=True)\n\nfor ep in range(1, EPOCHS+1):\n    loss_meter = 0.0\n    nsteps = 0\n    for step, batch in enumerate(dl, start=1):\n        x  = batch[\"x\"].to(device, non_blocking=True)\n        gt = batch[\"gt\"].to(device, non_blocking=True)\n        yb = batch[\"y\"].to(device, non_blocking=True)\n\n        with torch.cuda.amp.autocast(enabled=USE_AMP):\n            seg_logit, cls_logit = model(x)\n            l_seg = bce_focal_from_logits(seg_logit, gt, gamma=focal_gamma) + dice_loss_from_logits(seg_logit, gt)\n            l_cls = bce_cls(cls_logit, yb)\n            loss = lam_seg * l_seg + lam_cls * l_cls\n            loss = loss / ACCUM\n\n        scaler.scale(loss).backward()\n        if step % ACCUM == 0:\n            scaler.step(opt); scaler.update()\n            opt.zero_grad(set_to_none=True)\n\n        loss_meter += float(loss.item()) * ACCUM\n        nsteps += 1\n\n    sched.step()\n    if ep == 1 or ep % 2 == 0 or ep == EPOCHS:\n        print(f\"[ep {ep:02d}/{EPOCHS}] loss={loss_meter/max(1,nsteps):.4f} lr={sched.get_last_lr()[0]:.6g} | {time.time()-t0:.1f}s\")\n\nprint(\"TRAIN DONE |\", f\"{time.time()-t0:.1f}s\")\n\n# ----------------------------\n# Export mask-prob cache (TRAIN+TEST) + p_gate\n# ----------------------------\ndef cfg_hash(d):\n    s = json.dumps(d, sort_keys=True).encode()\n    return hashlib.md5(s).hexdigest()[:10]\n\nCFG_EXPORT = {\n    \"hybrid\": True,\n    \"best_config\": BEST,\n    \"match_root\": str(MATCH_ROOT),\n    \"tok_grid\": {\"HTOK\": HTOK, \"WTOK\": WTOK, \"PATCH\": PATCH},\n    \"train_table\": str(TRAIN_TABLE),\n    \"test_table\": str(TEST_TABLE) if TEST_TABLE else None,\n}\nCFG_ID = cfg_hash(CFG_EXPORT)\nMASKPROB_DIR = CACHE_DIR / f\"mask_prob_hybrid_{CFG_ID}\"\nMASKPROB_DIR.mkdir(parents=True, exist_ok=True)\n\n@torch.no_grad()\ndef export_probs(df_in: pd.DataFrame, tok_col_use: str, is_test: bool, tag: str):\n    ds_inf = InferDS(df_in, tok_col=tok_col_use, is_test=is_test)\n    dl_inf = DataLoader(ds_inf, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS,\n                        pin_memory=(device.type==\"cuda\"), drop_last=False)\n    model.eval()\n    t1 = time.time()\n    wrote = 0\n    for j, batch in enumerate(dl_inf, start=1):\n        x = batch[\"x\"].to(device, non_blocking=True)\n        seg_logit, cls_logit = model(x)\n        prob_tok = torch.sigmoid(seg_logit).detach().cpu().numpy()[:,0]  # (B,Ht,Wt)\n        p_gate   = torch.sigmoid(cls_logit).detach().cpu().numpy().reshape(-1)  # (B,)\n        cids = batch[\"case_id\"].cpu().numpy().reshape(-1)\n\n        for i in range(len(cids)):\n            cid = int(cids[i])\n            np.savez_compressed(\n                MASKPROB_DIR / f\"{cid}.npz\",\n                prob_tok=prob_tok[i].astype(np.float16),\n                p_gate=np.float16(p_gate[i]),\n            )\n            wrote += 1\n\n        if j % 100 == 0:\n            print(f\"[export {tag}] {wrote}/{len(ds_inf)} | {time.time()-t1:.1f}s\")\n\n    print(f\"[export {tag}] done | wrote={wrote} | {time.time()-t1:.1f}s\")\n\nexport_probs(df_tr[[\"case_id\", tok_col]], tok_col, is_test=False, tag=\"train\")\nif df_te is not None:\n    export_probs(df_te[[\"case_id\", tok_col_te]], tok_col_te, is_test=True, tag=\"test\")\n\n# ----------------------------\n# Threshold sweep on TRAIN using Dice-proxy (token-space)\n# ----------------------------\nT1 = float(BEST[\"T1\"]); T0 = float(BEST[\"T0\"]); dil_it = int(BEST[\"seed_dilate_it\"])\nmin_tok_area = int(BEST[\"min_tok_area\"])\nmax_tok_area_frac = float(BEST[\"max_tok_area_frac\"])\nmax_inst_keep = int(BEST[\"max_inst_keep\"])\nmin_peak_keep = int(BEST[\"min_peak_score_keep\"])\nmin_area_frac_keep = float(BEST[\"min_area_frac_keep\"])\n\n@torch.no_grad()\ndef build_dice_arrays_train():\n    ds_inf = TrainDS(df_tr[[\"case_id\",\"y\",tok_col]].copy(), tok_col=tok_col)\n    dl_inf = DataLoader(ds_inf, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS,\n                        pin_memory=(device.type==\"cuda\"), drop_last=False)\n    model.eval()\n\n    p_gate_all = np.zeros(len(ds_inf), np.float32)\n    dice_use   = np.zeros(len(ds_inf), np.float32)\n    dice_empty = np.zeros(len(ds_inf), np.float32)\n\n    k0 = 0\n    t2 = time.time()\n    for batch in dl_inf:\n        x = batch[\"x\"].to(device, non_blocking=True)\n        gt = batch[\"gt\"].cpu().numpy()[:,0] > 0.5\n        best_score = batch[\"best_score\"].cpu().numpy().reshape(-1)\n        seg_logit, cls_logit = model(x)\n        prob = torch.sigmoid(seg_logit).cpu().numpy()[:,0]\n        pg   = torch.sigmoid(cls_logit).cpu().numpy().reshape(-1)\n        seed = batch[\"x\"].cpu().numpy()[:,-1]  # last channel (B,Ht,Wt)\n\n        B = prob.shape[0]\n        for i in range(B):\n            idx = k0 + i\n            p_gate_all[idx] = pg[i]\n\n            gt_empty = (gt[i].sum() == 0)\n            dice_empty[idx] = 1.0 if gt_empty else 0.0\n\n            hard = prob[i] >= T1\n            soft = prob[i] >= T0\n            sd = dilate_tok(seed[i] > 0.5, dil_it)\n            fused = hard | (sd & soft)\n\n            uni, _ = inst_split_union_tok(fused, min_area=min_tok_area, max_area_frac=max_tok_area_frac, max_keep=max_inst_keep)\n            area_frac = float(uni.mean())\n            if (best_score[i] < min_peak_keep) and (area_frac < min_area_frac_keep):\n                uni = np.zeros((HTOK,WTOK), dtype=bool)\n\n            dice_use[idx] = dice(uni, gt[i])\n\n        k0 += B\n        if k0 % 800 == 0:\n            print(f\"[dice-proxy] {k0}/{len(ds_inf)} | {time.time()-t2:.1f}s\")\n\n    return p_gate_all, dice_use, dice_empty\n\np_gate_all, dice_use, dice_empty = build_dice_arrays_train()\n\nthr_grid = np.linspace(0.0, 1.0, 201, dtype=np.float32)\nrows = []\nbest_i = 0\nbest_score = -1.0\n\nfor i, thr in enumerate(thr_grid):\n    use = (p_gate_all >= thr)\n    score = float(np.where(use, dice_use, dice_empty).mean())\n    rows.append({\"thr\": float(thr), \"score_dice_proxy\": score})\n    if score > best_score:\n        best_score = score\n        best_i = i\n\ndf_thr = pd.DataFrame(rows)\nbest_thr = float(df_thr.loc[best_i, \"thr\"])\n\n(df_thr).to_csv(OUT_DIR / \"threshold_table.csv\", index=False)\n(OUT_DIR / \"best_threshold.json\").write_text(json.dumps({\n    \"recommended_thr\": best_thr,\n    \"best_score_dice_proxy\": float(best_score),\n    \"row\": df_thr.loc[best_i].to_dict(),\n    \"pp\": {\n        \"T1\": T1, \"T0\": T0, \"seed_dilate_it\": dil_it,\n        \"min_tok_area\": min_tok_area, \"max_tok_area_frac\": max_tok_area_frac, \"max_inst_keep\": max_inst_keep,\n        \"min_peak_score_keep\": min_peak_keep, \"min_area_frac_keep\": min_area_frac_keep,\n    }\n}, indent=2))\n\nprint(\"BEST_THR:\", best_thr, \"| best_score:\", best_score)\n\n# ----------------------------\n# Save final model bundle\n# ----------------------------\npack = {\n    \"model_type\": \"HybridUNet\",\n    \"state_dict\": {k: v.detach().cpu() for k,v in model.state_dict().items()},\n    \"best_config\": BEST,\n    \"recommended_thr\": best_thr,\n    \"cfg_export\": CFG_EXPORT,\n    \"cfg_id\": CFG_ID,\n    \"maskprob_dir\": str(MASKPROB_DIR),\n    \"tok_grid\": {\"HTOK\": HTOK, \"WTOK\": WTOK, \"PATCH\": PATCH},\n    \"input_channels\": CIN,\n    \"token_dim\": DIN,\n}\ntorch.save(pack, OUT_DIR / \"final_hybrid_model.pt\")\n\nprint(\"-\"*60)\nprint(\"SAVED:\")\nprint(\" -\", OUT_DIR / \"final_hybrid_model.pt\")\nprint(\" -\", OUT_DIR / \"threshold_table.csv\")\nprint(\" -\", OUT_DIR / \"best_threshold.json\")\nprint(\"MASKPROB_DIR:\", MASKPROB_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finalize & Save Model Bundle (Reproducible)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE — Finalize & Save Model Bundle (Reproducible) (ONE CELL) — HYBRID (OPSI-1)\n# Bundle (portable):\n# - final_hybrid_model.pt\n# - best_threshold.json + threshold_table.csv\n# - best_config.json (if exists)\n# - paths.json + match cfg.json (if resolvable)\n# - manifest.json (sha256 + env + metadata)\n# - ZIP: hybrid_model_bundle_<cfg_id>.zip\n# ============================================================\n\nimport os, json, time, hashlib, shutil, platform\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\n\n# ----------------------------\n# Locate inputs\n# ----------------------------\nOPT_DIR  = Path(\"/kaggle/working/recodai_luc_hybrid_opt\")\nOUT_DIR  = Path(\"/kaggle/working/recodai_luc_hybrid_artifacts\")\nPROF_DIR = Path(\"/kaggle/working/recodai_luc_prof\")\n\nFINAL_PT = OUT_DIR / \"final_hybrid_model.pt\"\nBEST_THR = OUT_DIR / \"best_threshold.json\"\nTHR_TAB  = OUT_DIR / \"threshold_table.csv\"\nBEST_CFG = OPT_DIR / \"best_config.json\"\nPATHS_JS = PROF_DIR / \"paths.json\"\n\nfor p in [FINAL_PT, BEST_THR]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {p}. Run Final Training stage first.\")\n\n# ----------------------------\n# Read model pack for metadata\n# ----------------------------\npack = torch.load(FINAL_PT, map_location=\"cpu\")\ncfg_id = str(pack.get(\"cfg_id\", \"unknown\"))\nmaskprob_dir = str(pack.get(\"maskprob_dir\", \"\"))\n\n# resolve match cfg.json if possible\nmatch_cfg_json = None\ntry:\n    match_root = pack.get(\"cfg_export\", {}).get(\"match_root\", None)\n    if match_root and Path(match_root).exists() and (Path(match_root) / \"cfg.json\").exists():\n        match_cfg_json = Path(match_root) / \"cfg.json\"\nexcept Exception:\n    match_cfg_json = None\n\nstamp = time.strftime(\"%Y%m%d_%H%M%S\", time.gmtime())\nBUNDLE_DIR = Path(f\"/kaggle/working/recodai_luc_hybrid_bundle_{cfg_id}_{stamp}\")\nBUNDLE_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef sha256_file(p: Path, chunk=1<<20):\n    h = hashlib.sha256()\n    with open(p, \"rb\") as f:\n        while True:\n            b = f.read(chunk)\n            if not b:\n                break\n            h.update(b)\n    return h.hexdigest()\n\ndef safe_copy(src: Path, dst_dir: Path, new_name: str = None):\n    if src is None or (not Path(src).exists()):\n        return None\n    src = Path(src)\n    dst = dst_dir / (new_name if new_name else src.name)\n    shutil.copy2(src, dst)\n    return dst\n\n# ----------------------------\n# Copy artifacts\n# ----------------------------\ncopied = {}\n\ncopied[\"final_hybrid_model.pt\"] = str(safe_copy(FINAL_PT, BUNDLE_DIR, \"final_hybrid_model.pt\"))\ncopied[\"best_threshold.json\"]   = str(safe_copy(BEST_THR, BUNDLE_DIR, \"best_threshold.json\"))\n\nif THR_TAB.exists():\n    copied[\"threshold_table.csv\"] = str(safe_copy(THR_TAB, BUNDLE_DIR, \"threshold_table.csv\"))\n\nif BEST_CFG.exists():\n    copied[\"best_config.json\"] = str(safe_copy(BEST_CFG, BUNDLE_DIR, \"best_config.json\"))\n\nif PATHS_JS.exists():\n    copied[\"paths.json\"] = str(safe_copy(PATHS_JS, BUNDLE_DIR, \"paths.json\"))\n\nif match_cfg_json is not None and Path(match_cfg_json).exists():\n    copied[\"match_cfg.json\"] = str(safe_copy(match_cfg_json, BUNDLE_DIR, \"match_cfg.json\"))\n\n# lightweight loader hint\nreadme = BUNDLE_DIR / \"README.txt\"\nreadme.write_text(\n    \"Hybrid (OPSI-1) bundle contents:\\n\"\n    \"- final_hybrid_model.pt : Torch state_dict + cfg + recommended_thr + maskprob_dir\\n\"\n    \"- best_threshold.json   : recommended_thr + postprocess params\\n\"\n    \"- threshold_table.csv   : sweep table (optional)\\n\"\n    \"- best_config.json      : HPO best trial (optional)\\n\"\n    \"- paths.json            : dataset paths used in build (optional)\\n\"\n    \"- match_cfg.json        : robust matching cfg used (optional)\\n\\n\"\n    \"Load example:\\n\"\n    \"  import torch, json\\n\"\n    \"  pack = torch.load('final_hybrid_model.pt', map_location='cpu')\\n\"\n    \"  thr  = json.load(open('best_threshold.json'))['recommended_thr']\\n\"\n)\n\n# ----------------------------\n# Manifest (sha256 + env + metadata)\n# ----------------------------\nfiles = sorted([p for p in BUNDLE_DIR.glob(\"*\") if p.is_file() and p.name != \"manifest.json\"])\nhashes = {p.name: sha256_file(p) for p in files}\n\nmeta = {\n    \"bundle_dir\": str(BUNDLE_DIR),\n    \"created_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n    \"cfg_id\": cfg_id,\n    \"recommended_thr\": float(pack.get(\"recommended_thr\", json.loads(BEST_THR.read_text()).get(\"recommended_thr\", 0.5))),\n    \"tok_grid\": pack.get(\"tok_grid\", {}),\n    \"input_channels\": int(pack.get(\"input_channels\", -1)),\n    \"token_dim\": int(pack.get(\"token_dim\", -1)),\n    \"maskprob_dir\": maskprob_dir,\n    \"env\": {\n        \"python\": platform.python_version(),\n        \"platform\": platform.platform(),\n        \"torch\": torch.__version__,\n        \"cuda_available\": bool(torch.cuda.is_available()),\n        \"cuda_version\": torch.version.cuda,\n        \"numpy\": np.__version__,\n        \"pandas\": pd.__version__,\n    },\n    \"files_copied\": copied,\n    \"sha256\": hashes,\n}\n\nmanifest_path = BUNDLE_DIR / \"manifest.json\"\nmanifest_path.write_text(json.dumps(meta, indent=2))\n\n# ----------------------------\n# ZIP bundle\n# ----------------------------\nimport zipfile\nZIP_PATH = Path(f\"/kaggle/working/hybrid_model_bundle_{cfg_id}_{stamp}.zip\")\nwith zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    for p in sorted(BUNDLE_DIR.glob(\"*\")):\n        if p.is_file():\n            z.write(p, arcname=p.name)\n\nprint(\"BUNDLE_DIR:\", BUNDLE_DIR)\nprint(\"ZIP_PATH  :\", ZIP_PATH)\nprint(\"Files:\", [p.name for p in sorted(BUNDLE_DIR.glob('*')) if p.is_file()])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}