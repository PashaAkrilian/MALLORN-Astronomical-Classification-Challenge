{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113558,"databundleVersionId":14878066,"sourceType":"competition"},{"sourceId":14387840,"sourceType":"datasetVersion","datasetId":9171323},{"sourceId":4535,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3327,"modelId":986},{"sourceId":4537,"sourceType":"modelInstanceVersion","modelInstanceId":3329,"modelId":986}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Paths & Select Config (CFG)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Set Paths & Select Config (CFG) (Kaggle-ready, offline)\n# REVISI FULL v3.0 (lebih kuat + siap MULTI-CFG + lebih aman anti-error)\n#\n# Fokus upgrade v3.0 (sesuai strategi naik score):\n# - Multi-CFG support: pilih TOP-K CFG kandidat (bukan cuma 1) untuk MATCH & PRED\n#   -> enabling: stacking/selector di stage training (anti overfit satu CFG)\n# - Scoring CFG lebih kaya:\n#   * wajib: feat_train ada & rows > 0\n#   * prefer: feat_test ada\n#   * prefer: manifest_test ada, pred_summary.json ada\n#   * prefer: ada folder npz test/train_all (untuk submission / mask features)\n#   * tie-break: rows train/test + modified time\n# - Cache/artifacts root independen:\n#   * artifacts bisa dari input, cache bisa dari working (atau sebaliknya)\n# - DINO cache cfg autodetect multi-backbone (large/giant/base) dari cache\n# - Sanity guard lebih informatif, file opsional tidak bikin crash\n#\n# Output globals (TETAP, JANGAN diganti):\n# - COMP_ROOT, OUT_DS_ROOT, OUT_ROOT\n# - PATHS (dict)\n# - MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR\n#\n# Extra globals (aman, membantu training lanjutan):\n# - MATCH_CFG_DIRS, PRED_CFG_DIRS (list[Path] TOP-K)\n# - MATCH_CFG_INFO, PRED_CFG_INFO (list[dict] detail skor)\n# - CACHE_ROOTS (list[Path]), SELECTED (dict), TRAIN_PLAN (dict)\n# ============================================================\n\nimport os, re, json, time\nfrom pathlib import Path\nimport pandas as pd\n\n# ----------------------------\n# Config knobs (boleh diubah)\n# ----------------------------\nTOPK_MATCH_CFGS = int(os.environ.get(\"TOPK_MATCH_CFGS\", \"5\"))\nTOPK_PRED_CFGS  = int(os.environ.get(\"TOPK_PRED_CFGS\",  \"8\"))\n\n# Kalau kamu mau paksa pakai dataset input tertentu:\n# export LUC_OUT_DS_ROOT=\"/kaggle/input/<nama_dataset_output>\"\nENV_OUT_DS_ROOT = os.environ.get(\"LUC_OUT_DS_ROOT\", \"\").strip()\n\n# ----------------------------\n# Helper: fast count CSV rows (binary newline count)\n# ----------------------------\ndef _fast_count_rows_csv(path: Path, assume_header: bool = True) -> int:\n    \"\"\"\n    Count rows quickly by counting '\\n' in binary.\n    Returns -1 if error.\n    \"\"\"\n    try:\n        if not path.exists() or not path.is_file():\n            return -1\n        # count newlines\n        nl = 0\n        with path.open(\"rb\") as f:\n            while True:\n                b = f.read(1024 * 1024)\n                if not b:\n                    break\n                nl += b.count(b\"\\n\")\n        # if file ends without newline, nl still ok for line count approximation\n        # rows = lines - header\n        rows = nl - (1 if assume_header else 0)\n        return int(max(rows, 0))\n    except Exception:\n        return -1\n\ndef _safe_mtime(p: Path) -> float:\n    try:\n        return float(p.stat().st_mtime)\n    except Exception:\n        return 0.0\n\ndef _is_nonempty_file(p: Path) -> bool:\n    try:\n        return p is not None and p.exists() and p.is_file() and p.stat().st_size > 0\n    except Exception:\n        return False\n\ndef _dir_has_any_npz(d: Path) -> bool:\n    try:\n        if d is None or (not d.exists()) or (not d.is_dir()):\n            return False\n        # cepat: cek 1 file npz saja\n        for _ in d.glob(\"*.npz\"):\n            return True\n        return False\n    except Exception:\n        return False\n\n# ----------------------------\n# Helper: find competition root\n# ----------------------------\ndef find_comp_root(preferred: str = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\") -> Path:\n    p = Path(preferred)\n    if p.exists():\n        return p\n\n    base = Path(\"/kaggle/input\")\n    if not base.exists():\n        raise FileNotFoundError(\"/kaggle/input tidak ditemukan (pastikan kamu di Kaggle Notebook).\")\n\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"sample_submission.csv\").exists() and ((d / \"train_images\").exists() or (d / \"test_images\").exists()):\n            cands.append(d)\n\n    if not cands:\n        for d in base.iterdir():\n            if not d.is_dir():\n                continue\n            for x in d.iterdir():\n                if not x.is_dir():\n                    continue\n                if (x / \"sample_submission.csv\").exists() and ((x / \"train_images\").exists() or (x / \"test_images\").exists()):\n                    cands.append(x)\n\n    if not cands:\n        raise FileNotFoundError(\n            \"COMP_ROOT tidak ditemukan. Harus ada folder yang memuat sample_submission.csv dan train_images/test_images.\"\n        )\n\n    cands.sort(key=lambda x: ((\"recodai\" not in x.name.lower()),\n                              (\"forgery\" not in x.name.lower()),\n                              x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: find output dataset root (hasil PREP)\n# ----------------------------\ndef find_output_dataset_root(preferred_names=(\n    \"recod-ailuc-dinov2-base\",\n    \"recod-ai-luc-dinov2-base\",\n    \"recodai-luc-dinov2-base\",\n    \"recodai-luc-dinov2\",\n    \"recodai-luc-dinov2-prep\",\n)) -> Path:\n    base = Path(\"/kaggle/input\")\n\n    # env override\n    if ENV_OUT_DS_ROOT:\n        p = Path(ENV_OUT_DS_ROOT)\n        if p.exists():\n            return p\n        else:\n            print(f\"WARNING: ENV LUC_OUT_DS_ROOT tidak ditemukan: {p}\")\n\n    for nm in preferred_names:\n        p = base / nm\n        if p.exists():\n            return p\n\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"recodai_luc\" / \"artifacts\").exists() or (d / \"recodai_luc\" / \"cache\").exists():\n            cands.append(d)\n            continue\n        inner = list(d.glob(\"*/recodai_luc/artifacts\"))\n        if inner:\n            cands.append(d)\n\n    if not cands:\n        raise FileNotFoundError(\"OUT_DS_ROOT tidak ditemukan. Harus ada /kaggle/input/<...>/recodai_luc/(artifacts|cache)/\")\n\n    cands.sort(key=lambda x: ((\"dinov2\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: resolve OUT_ROOT = <dataset>/recodai_luc\n# ----------------------------\ndef resolve_out_root(out_ds_root: Path) -> Path:\n    direct = out_ds_root / \"recodai_luc\"\n    if direct.exists():\n        return direct\n    hits = list(out_ds_root.glob(\"*/recodai_luc\"))\n    if hits:\n        return hits[0]\n    raise FileNotFoundError(f\"Folder recodai_luc tidak ditemukan di bawah {out_ds_root}\")\n\n# ----------------------------\n# Helper: pick TOP-K cfg directories by multi-criteria scoring\n# ----------------------------\ndef pick_top_cfgs(\n    cache_roots,\n    prefixes,\n    required_train_file: str,\n    prefer_files=(),\n    extra_prefer_dirs=(),\n    max_return: int = 5,\n) -> list:\n    \"\"\"\n    Return list of candidate dicts sorted by score desc.\n    Each candidate dict: {dir, score, train_rows, prefer_hits, mt, root, debug...}\n    \"\"\"\n    if isinstance(prefixes, str):\n        prefixes = [prefixes]\n    prefixes = list(prefixes)\n\n    prefer_files = list(prefer_files or [])\n    extra_prefer_dirs = list(extra_prefer_dirs or [])\n\n    cands = []\n    for root in cache_roots:\n        root = Path(root)\n        if not root.exists():\n            continue\n\n        for d in root.iterdir():\n            if not d.is_dir():\n                continue\n            if not any(d.name.startswith(px) for px in prefixes):\n                continue\n\n            train_fp = d / required_train_file\n            if not train_fp.exists():\n                continue\n\n            train_n = _fast_count_rows_csv(train_fp)\n            if train_n <= 0:\n                continue\n\n            # prefer files\n            pref_hit = 0\n            pref_rows_sum = 0\n            pref_detail = {}\n            for fn in prefer_files:\n                fp = d / fn\n                ok = _is_nonempty_file(fp)\n                pref_detail[fn] = bool(ok)\n                if ok:\n                    pref_hit += 1\n                    pref_rows_sum += max(_fast_count_rows_csv(fp), 0)\n\n            # prefer dirs (npz availability or other dirs)\n            dir_hit = 0\n            dir_detail = {}\n            for dn in extra_prefer_dirs:\n                dd = d / dn\n                ok = dd.exists() and dd.is_dir()\n                # special: if expecting npz, check any npz\n                if ok and dn in (\"test\", \"train_all\"):\n                    ok = _dir_has_any_npz(dd)\n                dir_detail[dn] = bool(ok)\n                if ok:\n                    dir_hit += 1\n\n            # mtime: consider dir + train file + any prefer file\n            mt = max(_safe_mtime(d), _safe_mtime(train_fp))\n            for fn in prefer_files:\n                mt = max(mt, _safe_mtime(d / fn))\n\n            # score design:\n            # - strong prefer: prefer files hits\n            # - prefer dirs hits (mask availability)\n            # - then train rows, then prefer rows\n            # - then newest mtime\n            score = 0.0\n            score += 1e6 * float(pref_hit)            # prefer having test/manifest/summary\n            score += 2e5 * float(dir_hit)             # prefer having npz dirs\n            score += 1.0  * float(train_n)             # size of train features\n            score += 0.05 * float(pref_rows_sum)       # size of prefer files\n            score += 1e-6 * float(mt)                  # newest\n\n            cands.append({\n                \"dir\": d,\n                \"root\": root,\n                \"score\": score,\n                \"train_file\": str(train_fp),\n                \"train_rows\": int(train_n),\n                \"prefer_hits\": int(pref_hit),\n                \"prefer_rows_sum\": int(pref_rows_sum),\n                \"prefer_detail\": pref_detail,\n                \"dir_hits\": int(dir_hit),\n                \"dir_detail\": dir_detail,\n                \"mtime\": float(mt),\n            })\n\n    cands.sort(key=lambda x: (-x[\"score\"], x[\"dir\"].name))\n    return cands[:max_return]\n\n# ----------------------------\n# Helper: detect DINO model dir (offline)\n# ----------------------------\ndef detect_dino_dir() -> Path:\n    # try standard Kaggle dinov2 dataset structure\n    base = Path(\"/kaggle/input/dinov2/pytorch\")\n    if base.exists():\n        # prefer large -> giant -> base\n        for name in [\"large\", \"giant\", \"base\"]:\n            p = base / name / \"1\"\n            if p.exists():\n                return p\n    # fallback (might be missing; warning only)\n    return Path(\"/kaggle/input/dinov2/pytorch/large/1\")\n\ndef detect_dino_cache_cfg(cache_dirs: list) -> Path:\n    \"\"\"\n    Find best cfg under cache/dino_v2_*/cfg_*/manifest_train_all.csv\n    Prefer large then giant then base, but choose whichever has best manifest size.\n    \"\"\"\n    best = None\n    best_key = None  # tuple for sorting\n\n    # priority map (lower is better)\n    prio = {\"dino_v2_large\": 0, \"dino_v2_giant\": 1, \"dino_v2_base\": 2}\n\n    for root in cache_dirs:\n        root = Path(root)\n        if not root.exists():\n            continue\n\n        for dino_name in [\"dino_v2_large\", \"dino_v2_giant\", \"dino_v2_base\"]:\n            dino_root = root / dino_name\n            if not dino_root.exists():\n                continue\n\n            for cfg in dino_root.iterdir():\n                if not (cfg.is_dir() and cfg.name.startswith(\"cfg_\")):\n                    continue\n                mf = cfg / \"manifest_train_all.csv\"\n                if not mf.exists():\n                    continue\n\n                n = _fast_count_rows_csv(mf)\n                mt = _safe_mtime(cfg)\n                key = (prio.get(dino_name, 9), -n, -mt, cfg.name)\n                # choose best: lowest prio, largest n, newest mt\n                if best is None or key < best_key:\n                    best = cfg\n                    best_key = key\n\n    return best  # can be None\n\n# ============================================================\n# 0) Locate roots\n# ============================================================\nCOMP_ROOT = find_comp_root(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\nOUT_DS_ROOT = find_output_dataset_root()\nOUT_ROOT = resolve_out_root(OUT_DS_ROOT)  # dataset input: .../recodai_luc\n\nWORK_OUT_ROOT = Path(\"/kaggle/working/recodai_luc\")\n\n# Cache roots: prefer working if exists, plus input\nCACHE_ROOTS = []\nif (WORK_OUT_ROOT / \"cache\").exists():\n    CACHE_ROOTS.append(WORK_OUT_ROOT / \"cache\")\nif (OUT_ROOT / \"cache\").exists():\n    CACHE_ROOTS.append(OUT_ROOT / \"cache\")\n\n# Artifact roots: prefer working if exists, plus input\nART_ROOTS = []\nif (WORK_OUT_ROOT / \"artifacts\").exists():\n    ART_ROOTS.append(WORK_OUT_ROOT / \"artifacts\")\nif (OUT_ROOT / \"artifacts\").exists():\n    ART_ROOTS.append(OUT_ROOT / \"artifacts\")\n\n# choose first existing artifact root\nART_DIR = None\nfor a in ART_ROOTS:\n    if a.exists():\n        ART_DIR = a\n        break\nif ART_DIR is None:\n    raise FileNotFoundError(\"ART_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n\n# cache dirs that exist\nCACHE_DIRS = [p for p in CACHE_ROOTS if Path(p).exists()]\nif not CACHE_DIRS:\n    raise FileNotFoundError(\"CACHE_DIR tidak ditemukan di /kaggle/working maupun dataset input.\")\n\n# ============================================================\n# 1) Competition paths (raw images/masks)\n# ============================================================\nPATHS = {}\nPATHS[\"COMP_ROOT\"] = str(COMP_ROOT)\nPATHS[\"SAMPLE_SUB\"] = str(COMP_ROOT / \"sample_submission.csv\")\n\nPATHS[\"TRAIN_IMAGES\"] = str(COMP_ROOT / \"train_images\")\nPATHS[\"TEST_IMAGES\"]  = str(COMP_ROOT / \"test_images\")\nPATHS[\"TRAIN_MASKS\"]  = str(COMP_ROOT / \"train_masks\")\nPATHS[\"SUPP_IMAGES\"]  = str(COMP_ROOT / \"supplemental_images\")\nPATHS[\"SUPP_MASKS\"]   = str(COMP_ROOT / \"supplemental_masks\")\n\nPATHS[\"TRAIN_AUTH_DIR\"] = str(COMP_ROOT / \"train_images\" / \"authentic\")\nPATHS[\"TRAIN_FORG_DIR\"] = str(COMP_ROOT / \"train_images\" / \"forged\")\n\n# ============================================================\n# 2) Output dataset paths (clean artifacts + cache)\n# ============================================================\nPATHS[\"OUT_DS_ROOT\"] = str(OUT_DS_ROOT)\nPATHS[\"OUT_ROOT\"]    = str(OUT_ROOT)\n\nPATHS[\"ART_DIR\"]     = str(ART_DIR)\nPATHS[\"CACHE_DIRS\"]  = [str(x) for x in CACHE_DIRS]  # list\n\n# artifacts utama\nPATHS[\"DF_TRAIN_ALL\"] = str(Path(ART_DIR) / \"df_train_all.parquet\")\nPATHS[\"DF_TRAIN_CLS\"] = str(Path(ART_DIR) / \"df_train_cls.parquet\")\nPATHS[\"DF_TRAIN_SEG\"] = str(Path(ART_DIR) / \"df_train_seg.parquet\")\nPATHS[\"DF_TEST\"]      = str(Path(ART_DIR) / \"df_test.parquet\")\n\nPATHS[\"CV_CASE_FOLDS\"]   = str(Path(ART_DIR) / \"cv_case_folds.csv\")\nPATHS[\"CV_SAMPLE_FOLDS\"] = str(Path(ART_DIR) / \"cv_sample_folds.csv\")\n\nPATHS[\"IMG_PROFILE_TRAIN\"] = str(Path(ART_DIR) / \"image_profile_train.parquet\")\nPATHS[\"IMG_PROFILE_TEST\"]  = str(Path(ART_DIR) / \"image_profile_test.parquet\")\nPATHS[\"MASK_PROFILE\"]      = str(Path(ART_DIR) / \"mask_profile.parquet\")\nPATHS[\"CASE_SUMMARY\"]      = str(Path(ART_DIR) / \"case_summary.parquet\")\n\n# ============================================================\n# 3) Select best MATCH/PRED CFG dirs automatically (TOP-K + scoring)\n# ============================================================\n# MATCH candidates: must have match_features_train_all.csv\nMATCH_CFG_INFO = pick_top_cfgs(\n    CACHE_DIRS,\n    prefixes=[\"match_base_cfg_\"],\n    required_train_file=\"match_features_train_all.csv\",\n    prefer_files=[\n        \"match_features_test.csv\",\n        \"manifest_match_test.csv\",\n        \"manifest_match_train_all.csv\",\n    ],\n    extra_prefer_dirs=[],  # biasanya match cfg tidak punya npz\n    max_return=max(1, TOPK_MATCH_CFGS),\n)\n\nif not MATCH_CFG_INFO:\n    raise FileNotFoundError(\"Tidak menemukan match cfg folder yang valid (match_features_train_all.csv).\")\n\nMATCH_CFG_DIRS = [x[\"dir\"] for x in MATCH_CFG_INFO]\nMATCH_CFG_DIR  = MATCH_CFG_DIRS[0]  # primary (kompatibilitas stage lanjut)\n\n# PRED candidates: must have pred_features_train_all.csv\n# Prefer: pred_features_test, manifests, summary, and availability of npz dirs\nPRED_CFG_INFO = pick_top_cfgs(\n    CACHE_DIRS,\n    prefixes=[\"pred_base\"],\n    required_train_file=\"pred_features_train_all.csv\",\n    prefer_files=[\n        \"pred_features_test.csv\",\n        \"manifest_pred_test.csv\",\n        \"manifest_pred_train_all.csv\",\n        \"pred_summary.json\",\n    ],\n    extra_prefer_dirs=[\"test\", \"train_all\"],  # prefer having npz predictions\n    max_return=max(1, TOPK_PRED_CFGS),\n)\n\nif not PRED_CFG_INFO:\n    raise FileNotFoundError(\"Tidak menemukan pred cfg folder yang valid (pred_features_train_all.csv).\")\n\nPRED_CFG_DIRS = [x[\"dir\"] for x in PRED_CFG_INFO]\nPRED_CFG_DIR  = PRED_CFG_DIRS[0]  # primary (kompatibilitas stage lanjut)\n\n# DINO cache cfg (opsional)\nDINO_CFG_DIR = detect_dino_cache_cfg(CACHE_DIRS)\n\n# simpan cfg dir ke PATHS\nPATHS[\"MATCH_CFG_DIR\"]    = str(MATCH_CFG_DIR)\nPATHS[\"PRED_CFG_DIR\"]     = str(PRED_CFG_DIR)\nPATHS[\"DINO_CFG_DIR\"]     = str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\"\n\n# multi-cfg (baru, opsional)\nPATHS[\"MATCH_CFG_DIRS\"]   = [str(x) for x in MATCH_CFG_DIRS]\nPATHS[\"PRED_CFG_DIRS\"]    = [str(x) for x in PRED_CFG_DIRS]\n\n# feature paths dari cfg primary\nPATHS[\"MATCH_FEAT_TRAIN\"] = str(MATCH_CFG_DIR / \"match_features_train_all.csv\")\nPATHS[\"MATCH_FEAT_TEST\"]  = str(MATCH_CFG_DIR / \"match_features_test.csv\")\n\nPATHS[\"PRED_FEAT_TRAIN\"]  = str(PRED_CFG_DIR / \"pred_features_train_all.csv\")\nPATHS[\"PRED_FEAT_TEST\"]   = str(PRED_CFG_DIR / \"pred_features_test.csv\")  # bisa missing (warning)\n\n# pred manifests (sering kepakai untuk infer/submission)\nPATHS[\"PRED_MAN_TRAIN\"] = str(PRED_CFG_DIR / \"manifest_pred_train_all.csv\")\nPATHS[\"PRED_MAN_TEST\"]  = str(PRED_CFG_DIR / \"manifest_pred_test.csv\")\nPATHS[\"PRED_SUMMARY\"]   = str(PRED_CFG_DIR / \"pred_summary.json\")\n\n# match manifests (kalau ada)\nPATHS[\"MATCH_MAN_TRAIN\"] = str(MATCH_CFG_DIR / \"manifest_match_train_all.csv\")\nPATHS[\"MATCH_MAN_TEST\"]  = str(MATCH_CFG_DIR / \"manifest_match_test.csv\")\n\n# ============================================================\n# 4) DINO model dir (offline)\n# ============================================================\nDINO_DIR = detect_dino_dir()\nPATHS[\"DINO_DIR\"] = str(DINO_DIR)\n\n# ============================================================\n# 5) Sanity checks (wajib ada untuk training)\n# ============================================================\nmust_exist = [\n    (\"sample_submission.csv\", PATHS[\"SAMPLE_SUB\"]),\n    (\"df_train_all.parquet\",  PATHS[\"DF_TRAIN_ALL\"]),\n    (\"cv_case_folds.csv\",     PATHS[\"CV_CASE_FOLDS\"]),\n    (\"match_features_train_all.csv\", PATHS[\"MATCH_FEAT_TRAIN\"]),\n    (\"pred_features_train_all.csv\",  PATHS[\"PRED_FEAT_TRAIN\"]),\n]\nmissing = [name for name, p in must_exist if not Path(p).exists()]\nif missing:\n    raise FileNotFoundError(\"Missing required files: \" + \", \".join(missing))\n\n# opsional tapi penting untuk inference: test feature files\nopt_warn = []\nif not Path(PATHS[\"MATCH_FEAT_TEST\"]).exists():\n    opt_warn.append(\"match_features_test.csv (MATCH_FEAT_TEST)\")\nif not Path(PATHS[\"PRED_FEAT_TEST\"]).exists():\n    opt_warn.append(\"pred_features_test.csv (PRED_FEAT_TEST)\")\nif opt_warn:\n    print(\"WARNING: File opsional untuk inference belum ada:\")\n    for w in opt_warn:\n        print(\" -\", w)\n    print(\"Catatan: training masih aman (pakai *_train_all). Untuk inference gate ke test, file ini biasanya dibutuhkan.\")\n\n# DINO model dir opsional (warning saja)\nif not Path(PATHS[\"DINO_DIR\"]).exists():\n    print(f\"WARNING: DINO dir tidak ditemukan: {PATHS['DINO_DIR']}\")\n\n# ============================================================\n# 6) Print summary + export helpers\n# ============================================================\nSELECTED = {\n    \"ART_DIR\": str(ART_DIR),\n    \"CACHE_DIRS\": [str(x) for x in CACHE_DIRS],\n    \"MATCH_CFG_DIR\": str(MATCH_CFG_DIR),\n    \"PRED_CFG_DIR\": str(PRED_CFG_DIR),\n    \"DINO_CFG_DIR\": str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\",\n    \"DINO_DIR\": str(DINO_DIR),\n    \"TOPK_MATCH_CFGS\": TOPK_MATCH_CFGS,\n    \"TOPK_PRED_CFGS\": TOPK_PRED_CFGS,\n}\n\n# Training plan: disiapkan untuk upgrade lanjutan (stacking + calibration + threshold tune)\nTRAIN_PLAN = {\n    \"seed\": 2025,\n    \"group_col\": \"case_id\",\n    \"target_col\": \"y_forged\",\n    \"n_folds\": 5,\n    \"use_calibration\": True,\n    \"calibration\": \"isotonic\",  # robust untuk tabular probs (OOF)\n    \"tune_threshold_on_oof\": True,\n    \"multi_cfg\": {\n        \"enabled\": True,\n        \"topk_match_cfgs\": TOPK_MATCH_CFGS,\n        \"topk_pred_cfgs\": TOPK_PRED_CFGS,\n        \"primary_match\": MATCH_CFG_DIR.name,\n        \"primary_pred\": PRED_CFG_DIR.name,\n    },\n}\n\ndef _print_top_cfg_table(title: str, info_list: list, max_rows: int = 5):\n    print(title)\n    if not info_list:\n        print(\"  (none)\")\n        return\n    show = info_list[:max_rows]\n    for i, x in enumerate(show, 1):\n        d = x[\"dir\"]\n        ph = x[\"prefer_hits\"]\n        dh = x[\"dir_hits\"]\n        tr = x[\"train_rows\"]\n        print(f\"  #{i:02d} {d.name} | score={x['score']:.1f} | train_rows={tr} | prefer_hits={ph} | dir_hits={dh}\")\n\nprint(\"OK — Roots\")\nprint(\"  COMP_ROOT   :\", COMP_ROOT)\nprint(\"  OUT_DS_ROOT :\", OUT_DS_ROOT)\nprint(\"  OUT_ROOT    :\", OUT_ROOT)\nprint(\"  ART_DIR(use):\", ART_DIR)\nprint(\"  CACHE_DIRS  :\", [str(x) for x in CACHE_DIRS])\n\nprint(\"\\nOK — Selected CFG (PRIMARY)\")\nprint(\"  MATCH_CFG_DIR:\", MATCH_CFG_DIR.name)\nprint(\"  PRED_CFG_DIR :\", PRED_CFG_DIR.name)\nprint(\"  DINO_CFG_DIR :\", (DINO_CFG_DIR.name if DINO_CFG_DIR else \"(not found / optional)\"))\n\nprint(\"\\nOK — Top candidates (for MULTI-CFG training)\")\n_print_top_cfg_table(\"MATCH CFG TOP:\", MATCH_CFG_INFO, max_rows=min(5, len(MATCH_CFG_INFO)))\n_print_top_cfg_table(\"PRED  CFG TOP:\", PRED_CFG_INFO,  max_rows=min(5, len(PRED_CFG_INFO)))\n\nprint(\"\\nOK — Key files (train)\")\nfor k in [\"DF_TRAIN_ALL\", \"CV_CASE_FOLDS\", \"MATCH_FEAT_TRAIN\", \"PRED_FEAT_TRAIN\", \"IMG_PROFILE_TRAIN\"]:\n    p = Path(PATHS[k])\n    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing/optional)'}\")\n\nprint(\"\\nOK — Key files (test/infer, optional)\")\nfor k in [\"MATCH_FEAT_TEST\", \"PRED_FEAT_TEST\", \"PRED_MAN_TEST\", \"PRED_SUMMARY\"]:\n    p = Path(PATHS[k])\n    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing)'}\")\n\nprint(\"\\nOK — DINO model dir\")\nprint(\"  DINO_DIR:\", DINO_DIR, \"(exists)\" if DINO_DIR.exists() else \"(missing)\")\n\n# export globals (kompatibilitas)\nglobals().update({\n    \"MATCH_CFG_DIR\": MATCH_CFG_DIR,\n    \"PRED_CFG_DIR\": PRED_CFG_DIR,\n    \"DINO_CFG_DIR\": DINO_CFG_DIR,\n    \"CACHE_ROOTS\": [Path(x) for x in CACHE_DIRS],\n    \"SELECTED\": SELECTED,\n    \"TRAIN_PLAN\": TRAIN_PLAN,\n\n    # extra (multi-cfg)\n    \"MATCH_CFG_DIRS\": MATCH_CFG_DIRS,\n    \"PRED_CFG_DIRS\": PRED_CFG_DIRS,\n    \"MATCH_CFG_INFO\": MATCH_CFG_INFO,\n    \"PRED_CFG_INFO\": PRED_CFG_INFO,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:57:47.290162Z","iopub.execute_input":"2026-01-04T13:57:47.290513Z","iopub.status.idle":"2026-01-04T13:57:47.751165Z","shell.execute_reply.started":"2026-01-04T13:57:47.290482Z","shell.execute_reply":"2026-01-04T13:57:47.750276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build Training Table (X, y, folds)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 2 — Build Training Table (X, y, folds) — REVISI FULL v3.0 (MAX-UPGRADE, robust, anti-error)\n#\n# Upgrade utama v3.0 (sesuai strategi naik score):\n# - MULTI-CFG support (dari STAGE 0 v3.0):\n#     * Primary CFG: load FULL pred_features + (opsional) FULL match_features\n#     * Extra CFGs: load CORE columns saja + buat agregasi row-wise (min/max/mean) -> stabil & kuat\n# - Label hygiene:\n#     * Drop unlabeled (y not in {0,1}) -> menghindari noise supplemental y=-1\n# - Robust parquet read (safe columns)\n# - Feature engineering lebih kuat + aman:\n#     * missing indicators\n#     * clipping caps\n#     * logabs transforms\n#     * interactions (lebih lengkap)\n# - Output tetap kompatibel:\n#   globals: df_train_tabular, FEATURE_COLS, X_train, y_train, folds\n#   save: feature_cols.json, feature_schema.json, df_train_tabular.parquet\n# ============================================================\n\nimport os, json, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# 0) Require PATHS\n# ----------------------------\nif \"PATHS\" not in globals() or not isinstance(PATHS, dict):\n    raise RuntimeError(\"Missing PATHS. Jalankan dulu STAGE 0 — Set Paths & Select Config (CFG).\")\n\n# ----------------------------\n# 1) Feature Engineering Config\n# ----------------------------\nFE_CFG = {\n    # sources\n    \"use_match_features\": True,\n    \"use_image_profile\": True,\n\n    # Multi-CFG (requires STAGE0 v3 which sets PATHS['PRED_CFG_DIRS'], PATHS['MATCH_CFG_DIRS'])\n    \"multi_cfg_enabled\": True,\n    \"multi_cfg_max_pred\": int(os.environ.get(\"MULTI_CFG_MAX_PRED\", \"6\")),   # pakai top-k pred cfg (primary + extra)\n    \"multi_cfg_max_match\": int(os.environ.get(\"MULTI_CFG_MAX_MATCH\", \"3\")), # pakai top-k match cfg (primary + extra)\n    \"multi_cfg_extra_mode\": \"core+agg\",  # \"core\" | \"core+agg\" (recommended)\n\n    # variant encoding\n    \"encode_variant_onehot\": True,\n    \"variant_min_count\": 1,\n\n    # transforms\n    \"add_log_features\": True,\n    \"add_sqrt_features\": False,\n    \"add_interactions\": True,\n    \"add_missing_indicators\": True,\n\n    # outlier control\n    \"clip_by_quantile\": True,\n    \"clip_q\": 0.999,\n    \"clip_max_fallback\": 1e9,\n\n    # fill\n    \"fillna_value\": 0.0,\n\n    # prune\n    \"drop_constant_features\": True,\n\n    # dtype\n    \"cast_float32\": True,\n\n    # label handling\n    \"drop_unlabeled\": True,          # drop y not in {0,1}\n    \"positive_value\": 1,             # keep standard\n}\n\n# ----------------------------\n# 2) Prefer WORKING features if exist (kalau regen di /kaggle/working)\n# ----------------------------\ndef _prefer_existing(*paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(str(p))\n        if p.exists():\n            return p\n    return Path(str(paths[0])) if paths and paths[0] is not None else Path(\"\")\n\ndef _to_str_series(s: pd.Series) -> pd.Series:\n    return s.astype(str).replace({\"nan\": \"\", \"None\": \"\"})\n\ndef _ensure_uid(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"uid\" not in df.columns:\n        for alt in [\"sample_id\", \"id\", \"key\"]:\n            if alt in df.columns:\n                df = df.rename(columns={alt: \"uid\"})\n                break\n    if \"uid\" not in df.columns:\n        raise ValueError(\"Cannot find uid column. Expected 'uid' or 'sample_id'.\")\n    df[\"uid\"] = _to_str_series(df[\"uid\"])\n    return df\n\ndef _parse_case_variant_from_uid(uid_s: pd.Series) -> pd.DataFrame:\n    uid = _to_str_series(uid_s)\n    case1 = uid.str.extract(r\"^(\\d+)__\")[0]\n    var1  = uid.str.extract(r\"__(.+)$\")[0]\n    case2 = uid.str.extract(r\"^(\\d+)_\")[0]\n    var2  = uid.str.extract(r\"_(\\w+)$\")[0]\n    case = case1.fillna(case2)\n    var  = var1.fillna(var2).fillna(\"unk\")\n    return pd.DataFrame({\"case_id\": case, \"variant\": var})\n\ndef _ensure_case_variant(df: pd.DataFrame, df_base_map: pd.DataFrame = None) -> pd.DataFrame:\n    df = _ensure_uid(df)\n\n    if \"case_id\" in df.columns:\n        df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n    if \"variant\" in df.columns:\n        df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n\n    # merge from base map preferred\n    if df_base_map is not None and {\"uid\", \"case_id\", \"variant\"}.issubset(df_base_map.columns):\n        need_merge = (\"case_id\" not in df.columns) or (\"variant\" not in df.columns) or df[\"case_id\"].isna().any()\n        if need_merge:\n            df = df.merge(df_base_map[[\"uid\", \"case_id\", \"variant\"]], on=\"uid\", how=\"left\", suffixes=(\"\", \"_base\"))\n            if \"case_id_base\" in df.columns:\n                df[\"case_id\"] = df[\"case_id\"].fillna(df[\"case_id_base\"])\n                df = df.drop(columns=[\"case_id_base\"])\n            if \"variant_base\" in df.columns:\n                df[\"variant\"] = df[\"variant\"].where(df[\"variant\"].astype(str).str.len() > 0, df[\"variant_base\"])\n                df = df.drop(columns=[\"variant_base\"])\n\n    if (\"case_id\" not in df.columns) or (\"variant\" not in df.columns) or df[\"case_id\"].isna().any():\n        pv = _parse_case_variant_from_uid(df[\"uid\"])\n        if \"case_id\" not in df.columns:\n            df[\"case_id\"] = pd.to_numeric(pv[\"case_id\"], errors=\"coerce\")\n        else:\n            df[\"case_id\"] = df[\"case_id\"].fillna(pd.to_numeric(pv[\"case_id\"], errors=\"coerce\"))\n        if \"variant\" not in df.columns:\n            df[\"variant\"] = pv[\"variant\"]\n        else:\n            v = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n            df[\"variant\"] = v.where(v.str.len() > 0, pv[\"variant\"])\n\n    df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n    df[\"variant\"] = df[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n    return df\n\ndef _pick_label_col(df: pd.DataFrame) -> str:\n    for cand in [\"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]:\n        if cand in df.columns:\n            return cand\n    return \"\"\n\n# parquet safe columns\ndef _read_parquet_cols_safe(path: Path, desired_cols: list) -> pd.DataFrame:\n    path = Path(path)\n    try:\n        import pyarrow.parquet as pq\n        cols = pq.ParquetFile(path).schema.names\n        use = [c for c in desired_cols if c in cols]\n        if not use:\n            return pd.read_parquet(path)\n        return pd.read_parquet(path, columns=use)\n    except Exception:\n        # fallback: read full then subset if possible\n        df = pd.read_parquet(path)\n        use = [c for c in desired_cols if c in df.columns]\n        return df[use].copy() if use else df\n\n# ----------------------------\n# Resolve primary paths\n# ----------------------------\nmatch_cfg_name = Path(PATHS.get(\"MATCH_CFG_DIR\", \"\")).name if PATHS.get(\"MATCH_CFG_DIR\") else \"\"\npred_cfg_name  = Path(PATHS.get(\"PRED_CFG_DIR\", \"\")).name  if PATHS.get(\"PRED_CFG_DIR\") else \"\"\n\nWORK_CACHE_ROOT = Path(\"/kaggle/working/recodai_luc/cache\")\nmatch_feat_work = (WORK_CACHE_ROOT / match_cfg_name / \"match_features_train_all.csv\") if match_cfg_name else None\npred_feat_work  = (WORK_CACHE_ROOT / pred_cfg_name  / \"pred_features_train_all.csv\")  if pred_cfg_name  else None\n\nPRED_FEAT_TRAIN  = _prefer_existing(pred_feat_work,  PATHS.get(\"PRED_FEAT_TRAIN\", \"\"))\nMATCH_FEAT_TRAIN = _prefer_existing(match_feat_work, PATHS.get(\"MATCH_FEAT_TRAIN\", \"\"))\n\nDF_TRAIN_ALL      = Path(PATHS[\"DF_TRAIN_ALL\"])\nCV_CASE_FOLDS     = Path(PATHS[\"CV_CASE_FOLDS\"])\nIMG_PROFILE_TRAIN = Path(PATHS.get(\"IMG_PROFILE_TRAIN\", \"\"))\n\nfor need_name, need_path in [\n    (\"df_train_all.parquet\", DF_TRAIN_ALL),\n    (\"cv_case_folds.csv\", CV_CASE_FOLDS),\n    (\"pred_features_train_all.csv\", PRED_FEAT_TRAIN),\n]:\n    if not Path(need_path).exists():\n        raise FileNotFoundError(f\"Missing required file: {need_name} -> {need_path}\")\n\nprint(\"Using (PRIMARY):\")\nprint(\"  DF_TRAIN_ALL     :\", DF_TRAIN_ALL)\nprint(\"  CV_CASE_FOLDS    :\", CV_CASE_FOLDS)\nprint(\"  PRED_FEAT_TRAIN  :\", PRED_FEAT_TRAIN)\nprint(\"  MATCH_FEAT_TRAIN :\", MATCH_FEAT_TRAIN, \"(optional)\" if Path(MATCH_FEAT_TRAIN).exists() else \"(missing/skip)\")\nprint(\"  IMG_PROFILE_TRAIN:\", IMG_PROFILE_TRAIN, \"(optional)\" if Path(IMG_PROFILE_TRAIN).exists() else \"(missing/skip)\")\n\n# ----------------------------\n# 3) Load minimal inputs\n# ----------------------------\n# df_train_all: ambil kolom minimal (safe)\nbase_cols_want = [\"sample_id\", \"uid\", \"case_id\", \"variant\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]\ndf_base = _read_parquet_cols_safe(DF_TRAIN_ALL, base_cols_want)\n\ndf_cv   = pd.read_csv(CV_CASE_FOLDS)\n\n# primary pred features (full)\ndf_pred_primary = pd.read_csv(PRED_FEAT_TRAIN, low_memory=False)\n\n# primary match features (optional, full)\ndf_match_primary = None\nif FE_CFG[\"use_match_features\"] and Path(MATCH_FEAT_TRAIN).exists():\n    try:\n        df_match_primary = pd.read_csv(MATCH_FEAT_TRAIN, low_memory=False)\n    except Exception:\n        df_match_primary = None\n\n# image profile (optional)\ndf_prof = None\nif FE_CFG[\"use_image_profile\"] and Path(IMG_PROFILE_TRAIN).exists():\n    try:\n        df_prof = pd.read_parquet(IMG_PROFILE_TRAIN)\n    except Exception:\n        df_prof = None\n\n# ----------------------------\n# 4) Prepare base mapping from df_train_all\n# ----------------------------\ndf_base = df_base.copy()\nif \"uid\" not in df_base.columns:\n    if \"sample_id\" in df_base.columns:\n        df_base = df_base.rename(columns={\"sample_id\": \"uid\"})\n    elif (\"case_id\" in df_base.columns and \"variant\" in df_base.columns):\n        df_base[\"uid\"] = _to_str_series(df_base[\"case_id\"]) + \"__\" + _to_str_series(df_base[\"variant\"])\n\ndf_base = _ensure_uid(df_base)\n\nif \"case_id\" in df_base.columns:\n    df_base[\"case_id\"] = pd.to_numeric(df_base[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\nif \"variant\" in df_base.columns:\n    df_base[\"variant\"] = df_base[\"variant\"].astype(str).replace({\"nan\": \"unk\", \"None\": \"unk\"})\n\nlabel_col = _pick_label_col(df_base)\nif not label_col:\n    raise ValueError(\"Cannot find label column in df_train_all (y_forged/has_mask/is_forged/forged).\")\n\ndf_base_map = df_base.drop_duplicates(subset=[\"uid\"], keep=\"first\").copy()\n\n# ----------------------------\n# 5) Prepare folds\n# ----------------------------\nif \"case_id\" not in df_cv.columns or \"fold\" not in df_cv.columns:\n    raise ValueError(\"cv_case_folds.csv must contain columns: case_id, fold\")\n\ndf_cv = df_cv[[\"case_id\", \"fold\"]].copy()\ndf_cv[\"case_id\"] = pd.to_numeric(df_cv[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\ndf_cv[\"fold\"]    = pd.to_numeric(df_cv[\"fold\"], errors=\"coerce\").astype(\"Int64\")\ndf_cv = df_cv.dropna().astype({\"case_id\": int, \"fold\": int}).drop_duplicates(\"case_id\")\n\n# ----------------------------\n# 6) Start from PRIMARY pred features (1 row per uid)\n# ----------------------------\ndf_pred_primary = _ensure_case_variant(df_pred_primary, df_base_map=df_base_map)\nif df_pred_primary[\"uid\"].duplicated().any():\n    df_pred_primary = df_pred_primary.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n\ndf_train = df_pred_primary.copy()\n\n# attach label from base map (sumber paling aman)\ndf_train = df_train.merge(\n    df_base_map[[\"uid\", label_col]].rename(columns={label_col: \"y\"}),\n    on=\"uid\", how=\"left\"\n)\n\nif df_train[\"y\"].isna().any():\n    miss = int(df_train[\"y\"].isna().sum())\n    raise ValueError(f\"Label merge produced NaN in y: {miss} rows. Check df_train_all vs pred_features alignment.\")\n\ndf_train[\"y\"] = pd.to_numeric(df_train[\"y\"], errors=\"coerce\")\n\n# drop unlabeled y not in {0,1} (recommended)\nif FE_CFG[\"drop_unlabeled\"]:\n    before = len(df_train)\n    df_train = df_train[df_train[\"y\"].isin([0, 1])].copy()\n    after = len(df_train)\n    if before != after:\n        print(f\"NOTE: Dropped unlabeled rows (y not in {{0,1}}): {before-after} rows\")\n\ndf_train[\"y\"] = df_train[\"y\"].astype(int)\n\n# attach folds by case_id\ndf_train = df_train.drop(columns=[\"fold\"], errors=\"ignore\").merge(df_cv, on=\"case_id\", how=\"left\")\nif df_train[\"fold\"].isna().any():\n    miss = int(df_train[\"fold\"].isna().sum())\n    raise ValueError(f\"Missing fold after merging cv_case_folds.csv: {miss} rows.\")\ndf_train[\"fold\"] = df_train[\"fold\"].astype(int)\n\n# ----------------------------\n# 7) Optional merge PRIMARY match features (new cols only)\n# ----------------------------\nif df_match_primary is not None:\n    dfm = _ensure_case_variant(df_match_primary, df_base_map=df_base_map)\n    if dfm[\"uid\"].duplicated().any():\n        dfm = dfm.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n\n    base_cols = set(df_train.columns)\n    new_cols = [c for c in dfm.columns if c not in base_cols and c not in [\"case_id\", \"variant\"]]\n    if new_cols:\n        df_train = df_train.merge(dfm[[\"uid\"] + new_cols], on=\"uid\", how=\"left\")\n\n# ----------------------------\n# 8) MULTI-CFG: load EXTRA core features from additional CFG dirs (PRED + MATCH)\n# ----------------------------\ndef _short_cfg_tag(cfg_dir: Path, idx: int) -> str:\n    # tag pendek agar kolom tidak kepanjangan\n    nm = cfg_dir.name\n    # ambil hash kalau ada\n    m = re.search(r\"(cfg_[0-9a-f]{6,})\", nm)\n    tag = m.group(1) if m else nm\n    tag = tag.replace(\"pred_base_\", \"p_\").replace(\"match_base_cfg_\", \"m_\")\n    tag = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", tag)\n    return f\"{idx:02d}_{tag[:24]}\"\n\n# core cols yang paling berpengaruh untuk gate (stabil across cfg)\nCORE_PRED_COLS = [\n    \"area_frac\", \"grid_area_frac\", \"log_pred_area\", \"pred_area_frac\",\n    \"best_count\", \"best_mean_sim\", \"peak_ratio\", \"best_weight\", \"best_weight_frac\",\n    \"inlier_ratio\", \"n_pairs_thr\", \"n_pairs_mnn\", \"overmask_tighten_steps\",\n    \"grid_h\", \"grid_w\", \"has_peak\",\n]\nCORE_MATCH_COLS = [\n    \"best_count\", \"best_mean_sim\", \"peak_ratio\", \"best_weight\", \"best_weight_frac\",\n    \"inlier_ratio\", \"n_pairs_thr\", \"n_pairs_mnn\", \"grid_h\", \"grid_w\", \"has_peak\",\n]\n\ndef _load_csv_core(csv_path: Path, core_cols: list) -> pd.DataFrame:\n    df = pd.read_csv(csv_path, low_memory=False)\n    df = _ensure_uid(df)\n    keep = [\"uid\"] + [c for c in core_cols if c in df.columns]\n    df = df[keep].copy()\n    return df\n\n# get cfg dirs list from PATHS (stage0 v3)\npred_cfg_dirs = [Path(x) for x in PATHS.get(\"PRED_CFG_DIRS\", [])] if FE_CFG[\"multi_cfg_enabled\"] else []\nmatch_cfg_dirs = [Path(x) for x in PATHS.get(\"MATCH_CFG_DIRS\", [])] if FE_CFG[\"multi_cfg_enabled\"] else []\n\n# ensure primary is first (and exists)\nif pred_cfg_dirs:\n    # keep only existing dirs\n    pred_cfg_dirs = [d for d in pred_cfg_dirs if d.exists()]\n    # cap to max\n    pred_cfg_dirs = pred_cfg_dirs[:max(1, FE_CFG[\"multi_cfg_max_pred\"])]\nelse:\n    pred_cfg_dirs = [Path(PATHS[\"PRED_CFG_DIR\"])]\n\nif match_cfg_dirs:\n    match_cfg_dirs = [d for d in match_cfg_dirs if d.exists()]\n    match_cfg_dirs = match_cfg_dirs[:max(1, FE_CFG[\"multi_cfg_max_match\"])]\nelse:\n    match_cfg_dirs = [Path(PATHS[\"MATCH_CFG_DIR\"])] if FE_CFG[\"use_match_features\"] else []\n\n# EXTRA PRED cfgs (exclude primary already loaded)\nextra_pred_cfgs = [d for d in pred_cfg_dirs if str(d) != str(Path(PATHS[\"PRED_CFG_DIR\"]))]\n\npred_core_cols_added = []\npred_core_matrix_cols = {}  # base_name -> list of suffixed cols\n\nfor i, cfg_dir in enumerate(extra_pred_cfgs, 1):\n    fp = cfg_dir / \"pred_features_train_all.csv\"\n    if not fp.exists():\n        continue\n    try:\n        tag = _short_cfg_tag(cfg_dir, i)\n        df_extra = _load_csv_core(fp, CORE_PRED_COLS)\n        # ensure joinable to df_train uids\n        df_extra = _ensure_case_variant(df_extra, df_base_map=df_base_map)\n        # rename core columns with suffix tag\n        ren = {}\n        for c in df_extra.columns:\n            if c == \"uid\":\n                continue\n            ren[c] = f\"{c}__{tag}\"\n            pred_core_matrix_cols.setdefault(c, []).append(ren[c])\n        df_extra = df_extra.rename(columns=ren)\n        # merge\n        df_train = df_train.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n        pred_core_cols_added.extend(list(ren.values()))\n    except Exception as e:\n        print(f\"WARNING: skip extra pred cfg {cfg_dir.name} due to error: {e}\")\n\n# Aggregate across CFGs for core pred metrics (recommended)\n# include primary columns if exist\nif FE_CFG[\"multi_cfg_extra_mode\"] == \"core+agg\":\n    for base_name, cols_suff in pred_core_matrix_cols.items():\n        # build list: [primary_col(if exists)] + extra cols\n        cols_all = []\n        if base_name in df_train.columns and pd.api.types.is_numeric_dtype(df_train[base_name]):\n            cols_all.append(base_name)\n        cols_all.extend([c for c in cols_suff if c in df_train.columns])\n        cols_all = [c for c in cols_all if c in df_train.columns]\n        if len(cols_all) < 2:\n            continue\n\n        # rowwise stats (NaN-safe)\n        mat = df_train[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n        df_train[f\"cfg_mean_{base_name}\"] = mat.mean(axis=1, skipna=True)\n        df_train[f\"cfg_max_{base_name}\"]  = mat.max(axis=1, skipna=True)\n        df_train[f\"cfg_min_{base_name}\"]  = mat.min(axis=1, skipna=True)\n        df_train[f\"cfg_std_{base_name}\"]  = mat.std(axis=1, skipna=True)\n\n# EXTRA MATCH cfgs (optional)\nmatch_core_cols_added = []\nmatch_core_matrix_cols = {}\n\nif FE_CFG[\"use_match_features\"]:\n    extra_match_cfgs = [d for d in match_cfg_dirs if str(d) != str(Path(PATHS[\"MATCH_CFG_DIR\"]))]\n    for i, cfg_dir in enumerate(extra_match_cfgs, 1):\n        fp = cfg_dir / \"match_features_train_all.csv\"\n        if not fp.exists():\n            continue\n        try:\n            tag = _short_cfg_tag(cfg_dir, i)\n            df_extra = _load_csv_core(fp, CORE_MATCH_COLS)\n            df_extra = _ensure_case_variant(df_extra, df_base_map=df_base_map)\n            ren = {}\n            for c in df_extra.columns:\n                if c == \"uid\":\n                    continue\n                ren[c] = f\"{c}__{tag}\"\n                match_core_matrix_cols.setdefault(c, []).append(ren[c])\n            df_extra = df_extra.rename(columns=ren)\n            df_train = df_train.merge(df_extra[[\"uid\"] + list(ren.values())], on=\"uid\", how=\"left\")\n            match_core_cols_added.extend(list(ren.values()))\n        except Exception as e:\n            print(f\"WARNING: skip extra match cfg {cfg_dir.name} due to error: {e}\")\n\n    if FE_CFG[\"multi_cfg_extra_mode\"] == \"core+agg\":\n        for base_name, cols_suff in match_core_matrix_cols.items():\n            cols_all = []\n            if base_name in df_train.columns and pd.api.types.is_numeric_dtype(df_train[base_name]):\n                cols_all.append(base_name)\n            cols_all.extend([c for c in cols_suff if c in df_train.columns])\n            cols_all = [c for c in cols_all if c in df_train.columns]\n            if len(cols_all) < 2:\n                continue\n            mat = df_train[cols_all].apply(pd.to_numeric, errors=\"coerce\")\n            df_train[f\"cfg_mean_match_{base_name}\"] = mat.mean(axis=1, skipna=True)\n            df_train[f\"cfg_max_match_{base_name}\"]  = mat.max(axis=1, skipna=True)\n            df_train[f\"cfg_min_match_{base_name}\"]  = mat.min(axis=1, skipna=True)\n            df_train[f\"cfg_std_match_{base_name}\"]  = mat.std(axis=1, skipna=True)\n\n# ----------------------------\n# 9) Optional merge image profile by case_id (numeric only, prefixed)\n# ----------------------------\nif df_prof is not None and \"case_id\" in df_prof.columns:\n    df_prof2 = df_prof.copy()\n    df_prof2[\"case_id\"] = pd.to_numeric(df_prof2[\"case_id\"], errors=\"coerce\").astype(\"Int64\")\n    df_prof2 = df_prof2.dropna(subset=[\"case_id\"]).astype({\"case_id\": int})\n    df_prof2 = df_prof2.drop_duplicates(\"case_id\")\n\n    prof_num = [\"case_id\"] + [\n        c for c in df_prof2.columns\n        if c != \"case_id\" and pd.api.types.is_numeric_dtype(df_prof2[c])\n    ]\n    df_prof2 = df_prof2[prof_num].copy()\n\n    ren = {c: f\"profile_{c}\" for c in df_prof2.columns if c != \"case_id\"}\n    df_prof2 = df_prof2.rename(columns=ren)\n    df_train = df_train.merge(df_prof2, on=\"case_id\", how=\"left\")\n\n# ----------------------------\n# 10) Feature engineering helpers\n# ----------------------------\ndef _num(s):\n    return pd.to_numeric(s, errors=\"coerce\")\n\ndef safe_log1p_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.log1p(x)\n\ndef safe_sqrt_nonneg(x):\n    x = np.asarray(x, dtype=np.float64)\n    x = np.where(np.isfinite(x), x, 0.0)\n    x = np.clip(x, 0.0, None)\n    return np.sqrt(x)\n\ndef get_clip_cap(series: pd.Series, q: float, fallback: float):\n    s = _num(series).astype(float).replace([np.inf, -np.inf], np.nan).dropna()\n    if len(s) == 0:\n        return float(fallback)\n    s = s[np.isfinite(s)]\n    cap = float(np.quantile(np.abs(s.values), q))\n    if (not np.isfinite(cap)) or (cap <= 0):\n        return float(fallback)\n    return float(cap)\n\n# ----------------------------\n# 11) Candidate numeric feature list (pre-fill)\n# ----------------------------\nTARGET_COLS = {\"y\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"}\nSPLIT_COLS  = {\"fold\"}\nID_DROP_NUM = {\"case_id\"}  # jangan jadi feature\n\n# Replace inf -> NaN for numeric cols\nfor c in df_train.columns:\n    if pd.api.types.is_numeric_dtype(df_train[c]):\n        df_train[c] = df_train[c].replace([np.inf, -np.inf], np.nan)\n\n# Missing indicators (sebelum fill)\nmissing_ind_cols = []\nif FE_CFG[\"add_missing_indicators\"]:\n    for c in df_train.columns:\n        if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n            continue\n        if pd.api.types.is_numeric_dtype(df_train[c]) and df_train[c].isna().any():\n            ind = f\"isna_{c}\"\n            df_train[ind] = df_train[c].isna().astype(np.uint8)\n            missing_ind_cols.append(ind)\n\n# Heavy-tail candidates\nheavy_candidates = set([\n    \"peak_ratio\", \"best_weight\", \"best_count\", \"best_weight_frac\",\n    \"pair_count\", \"n_pairs_thr\", \"n_pairs_mnn\", \"overmask_tighten_steps\",\n    \"largest_comp\", \"n_comp\", \"grid_h\", \"grid_w\",\n    \"grid_area_frac\", \"area_frac\", \"inlier_ratio\",\n])\nfor c in df_train.columns:\n    cl = c.lower()\n    if any(k in cl for k in [\"count\", \"pairs\", \"weight\", \"ratio\", \"area\", \"comp\", \"std_\"]):\n        if pd.api.types.is_numeric_dtype(df_train[c]):\n            heavy_candidates.add(c)\n\nclip_caps = {}\nif FE_CFG[\"clip_by_quantile\"]:\n    for c in sorted(list(heavy_candidates)):\n        if c in df_train.columns and pd.api.types.is_numeric_dtype(df_train[c]):\n            clip_caps[c] = get_clip_cap(df_train[c], FE_CFG[\"clip_q\"], FE_CFG[\"clip_max_fallback\"])\n\n# Clipped + log/sqrt transforms\nif FE_CFG[\"add_log_features\"] or FE_CFG[\"add_sqrt_features\"]:\n    for c, cap in clip_caps.items():\n        x = _num(df_train[c]).fillna(0.0).astype(float).values\n        x = np.clip(x, -cap, cap)\n        df_train[f\"{c}_cap\"] = x.astype(np.float32)\n\n        if FE_CFG[\"add_log_features\"]:\n            df_train[f\"logabs_{c}\"] = safe_log1p_nonneg(np.abs(x)).astype(np.float32)\n        if FE_CFG[\"add_sqrt_features\"]:\n            df_train[f\"sqrtabs_{c}\"] = safe_sqrt_nonneg(np.abs(x)).astype(np.float32)\n\n# Interactions (lebih kaya + aman)\nif FE_CFG[\"add_interactions\"]:\n    def getf(col, default=0.0):\n        if col in df_train.columns:\n            return _num(df_train[col]).fillna(default).astype(float).values\n        return np.full(len(df_train), default, dtype=np.float64)\n\n    best_mean_sim = getf(\"best_mean_sim\", 0.0)\n    best_count    = getf(\"best_count\", 0.0)\n    peak_ratio    = getf(\"peak_ratio\", 0.0)\n    has_peak      = getf(\"has_peak\", 0.0)\n    grid_area     = getf(\"grid_area_frac\", 0.0)\n    area_frac     = getf(\"area_frac\", 0.0)\n    n_pairs_thr   = getf(\"n_pairs_thr\", 0.0)\n    n_pairs_mnn   = getf(\"n_pairs_mnn\", 0.0)\n    inlier_ratio  = getf(\"inlier_ratio\", 0.0)\n    gh = getf(\"grid_h\", 0.0)\n    gw = getf(\"grid_w\", 0.0)\n\n    gridN = np.clip(gh * gw, 0.0, None)\n\n    df_train[\"sim_x_count\"]      = (best_mean_sim * best_count).astype(np.float32)\n    df_train[\"peak_x_sim\"]       = (peak_ratio * best_mean_sim).astype(np.float32)\n    df_train[\"haspeak_x_sim\"]    = (has_peak * best_mean_sim).astype(np.float32)\n    df_train[\"area_x_sim\"]       = (grid_area * best_mean_sim).astype(np.float32)\n    df_train[\"area_x_count\"]     = (grid_area * best_count).astype(np.float32)\n    df_train[\"mask_grid_ratio\"]  = (area_frac / (1e-6 + grid_area)).astype(np.float32)\n    df_train[\"mnn_ratio\"]        = (n_pairs_mnn / (1.0 + n_pairs_thr)).astype(np.float32)\n    df_train[\"pairs_per_cell\"]   = (n_pairs_thr / (1.0 + gridN)).astype(np.float32)\n    df_train[\"inlier_x_pairs\"]   = (inlier_ratio * n_pairs_thr).astype(np.float32)\n\n    # tambahan yang sering bantu stabil:\n    df_train[\"log1p_pairs_thr\"]  = safe_log1p_nonneg(n_pairs_thr).astype(np.float32)\n    df_train[\"log1p_best_count\"] = safe_log1p_nonneg(best_count).astype(np.float32)\n    df_train[\"log1p_area_frac\"]  = safe_log1p_nonneg(np.clip(area_frac, 0, None)).astype(np.float32)\n\n# Fill NaN numeric -> 0\nnum_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\ndf_train[num_cols] = df_train[num_cols].fillna(FE_CFG[\"fillna_value\"])\n\n# ----------------------------\n# 12) Variant encoding (optional one-hot)\n# ----------------------------\nvariant_dummy_cols = []\nif FE_CFG[\"encode_variant_onehot\"]:\n    vc = df_train[\"variant\"].astype(str).fillna(\"unk\")\n    counts = vc.value_counts()\n    keep = set(counts[counts >= int(FE_CFG[\"variant_min_count\"])].index.tolist())\n    vc = vc.where(vc.isin(keep), other=\"rare\")\n\n    dummies = pd.get_dummies(vc, prefix=\"v\", dummy_na=False).astype(np.uint8)\n    variant_dummy_cols = dummies.columns.tolist()\n    df_train = pd.concat([df_train, dummies], axis=1)\n\n# ----------------------------\n# 13) Select final feature columns (numeric only)\n# ----------------------------\nfeature_cols = []\nfor c in df_train.columns:\n    if not pd.api.types.is_numeric_dtype(df_train[c]):\n        continue\n    if c in TARGET_COLS or c in SPLIT_COLS or c in ID_DROP_NUM:\n        continue\n    feature_cols.append(c)\n\n# Drop constant features\ndropped_constant = []\nif FE_CFG[\"drop_constant_features\"]:\n    nun = df_train[feature_cols].nunique(dropna=False)\n    nonconst = nun[nun > 1].index.tolist()\n    dropped_constant = sorted(set(feature_cols) - set(nonconst))\n    feature_cols = nonconst\n\n# Cast float32 for numeric features\nif FE_CFG[\"cast_float32\"]:\n    df_train[feature_cols] = df_train[feature_cols].astype(np.float32)\n\nFEATURE_COLS = list(feature_cols)\n\n# ----------------------------\n# 14) Final outputs\n# ----------------------------\nbase_out_cols = [\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]\ndf_train_tabular = df_train[base_out_cols + FEATURE_COLS].copy()\n\nX_train = df_train_tabular[FEATURE_COLS]\ny_train = df_train_tabular[\"y\"].astype(int)\nfolds   = df_train_tabular[\"fold\"].astype(int)\n\nprint(\"\\nOK — Training table built\")\nprint(\"  df_train_tabular:\", df_train_tabular.shape)\nprint(\"  X_train:\", X_train.shape, \"| y pos%:\", float(y_train.mean()) * 100.0)\nprint(\"  folds:\", int(folds.nunique()), \"unique folds\")\nprint(\"  feature_cols:\", int(len(FEATURE_COLS)))\nif dropped_constant:\n    print(\"  dropped_constant_features:\", len(dropped_constant))\nif variant_dummy_cols:\n    print(\"  variant_dummies:\", len(variant_dummy_cols))\nif missing_ind_cols:\n    print(\"  missing_indicators:\", len(missing_ind_cols))\nif pred_core_cols_added:\n    print(\"  extra_pred_core_cols:\", len(pred_core_cols_added))\nif match_core_cols_added:\n    print(\"  extra_match_core_cols:\", len(match_core_cols_added))\n\n# hard sanity\nif X_train.shape[0] != y_train.shape[0]:\n    raise RuntimeError(\"X_train and y_train row mismatch\")\nif y_train.isna().any():\n    raise RuntimeError(\"y_train contains NaN\")\nif folds.isna().any():\n    raise RuntimeError(\"folds contains NaN\")\n\nprint(\"\\nFeature head:\", FEATURE_COLS[:25])\nprint(\"Feature tail:\", FEATURE_COLS[-15:])\n\n# ----------------------------\n# 15) Save reproducible schema\n# ----------------------------\nOUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_ART.mkdir(parents=True, exist_ok=True)\n\nwith open(OUT_ART / \"feature_cols.json\", \"w\") as f:\n    json.dump(FEATURE_COLS, f, indent=2)\n\nschema = {\n    \"fe_cfg\": FE_CFG,\n    \"label_col_source\": label_col,\n    \"clip_caps\": clip_caps,\n    \"dropped_constant_features\": dropped_constant,\n    \"variant_dummy_cols\": variant_dummy_cols,\n    \"missing_indicator_cols\": missing_ind_cols,\n    \"multi_cfg\": {\n        \"pred_cfg_dirs_used\": [str(Path(PATHS[\"PRED_CFG_DIR\"]))] + [str(d) for d in extra_pred_cfgs],\n        \"match_cfg_dirs_used\": [str(Path(PATHS[\"MATCH_CFG_DIR\"]))] + ([str(d) for d in match_cfg_dirs if str(d) != str(Path(PATHS[\"MATCH_CFG_DIR\"]))] if FE_CFG[\"use_match_features\"] else []),\n        \"extra_pred_core_cols_added\": pred_core_cols_added[:200],\n        \"extra_match_core_cols_added\": match_core_cols_added[:200],\n    },\n    \"n_features\": int(len(FEATURE_COLS)),\n    \"example_feature_head\": FEATURE_COLS[:30],\n}\nwith open(OUT_ART / \"feature_schema.json\", \"w\") as f:\n    json.dump(schema, f, indent=2)\n\ndf_train_tabular.to_parquet(OUT_ART / \"df_train_tabular.parquet\", index=False)\n\nprint(f\"\\nSaved -> {OUT_ART/'feature_cols.json'}\")\nprint(f\"Saved -> {OUT_ART/'feature_schema.json'}\")\nprint(f\"Saved -> {OUT_ART/'df_train_tabular.parquet'}\")\n\n# export globals\nglobals().update({\n    \"df_train_tabular\": df_train_tabular,\n    \"FEATURE_COLS\": FEATURE_COLS,\n    \"X_train\": X_train,\n    \"y_train\": y_train,\n    \"folds\": folds,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:57:47.752914Z","iopub.execute_input":"2026-01-04T13:57:47.753453Z","iopub.status.idle":"2026-01-04T13:57:48.398903Z","shell.execute_reply.started":"2026-01-04T13:57:47.753413Z","shell.execute_reply":"2026-01-04T13:57:48.398102Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Baseline Model (Leakage-Safe CV)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 3 — Train Stronger Model (Leakage-Safe CV) — REVISI FULL v3.0 (MAX-UPGRADE, robust)\n#\n# Upgrade v3.0 (sesuai strategi naik score & anti error):\n# - Multi-SEED bagging (auto: 1 seed CPU/small GPU, 2 seed untuk GPU besar; bisa override ENV N_SEEDS)\n# - Leakage-safe PER-FOLD calibration (isotonic/sigmoid/none) -> oof_cal lebih stabil untuk thresholding\n# - Simpan OOF RAW + OOF CAL (csv + npy) + fold packs lengkap (mu/sig + calibrator + cfg + metrics)\n# - Threshold search lebih kaya: F1 + F0.5 + F2 (default pilih F0.5 utk menekan FP)\n# - Better early stopping: monitor val_logloss (EMA-eval) + simpan best_state (EMA weights)\n# - Guard kuat: NaN/Inf, fold sanity, amp safe, workers safe\n#\n# Output globals:\n# - OOF_PRED_MHC_RAW, OOF_PRED_MHC_CAL (np.ndarray)\n# - BASELINE_MHC_TF_OVERALL, BASELINE_MHC_TF_FOLD_REPORTS, BASELINE_MHC_TF_BEST_EPOCHS\n# - FULL_PACK_PATH (str)\n# ============================================================\n\nimport json, gc, math, time, os, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, log_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# 0) Require outputs from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nrequired_cols = {\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"}\nmissing_cols = [c for c in required_cols if c not in df_train_tabular.columns]\nif missing_cols:\n    raise ValueError(f\"df_train_tabular missing columns: {missing_cols}.\")\n\n# ----------------------------\n# 1) CFG (AUTO: CPU/SAFE/STRONG)\n# ----------------------------\nCFG_CPU = {\n    \"seed\": 2025,\n\n    \"n_streams\": 2,\n    \"sinkhorn_tmax\": 10,\n    \"alpha_init\": 0.01,\n\n    \"d_model\": 192,\n    \"n_layers\": 4,\n    \"n_heads\": 6,\n    \"ffn_mult\": 4,\n    \"dropout\": 0.12,\n    \"attn_dropout\": 0.08,\n\n    \"feat_token_drop_p\": 0.05,\n    \"input_noise_std\": 0.008,\n    \"label_smoothing\": 0.00,\n    \"focal_gamma\": 1.2,\n\n    \"batch_size\": 256,\n    \"accum_steps\": 2,\n    \"epochs\": 45,\n\n    \"lr\": 3e-4,\n    \"betas\": (0.9, 0.95),\n    \"eps\": 1e-8,\n    \"weight_decay\": 5e-2,\n\n    \"warmup_frac\": 0.08,\n    \"lr_decay_milestones\": (0.80, 0.90),\n    \"lr_decay_values\": (0.316, 0.10),\n\n    \"grad_clip\": 1.0,\n\n    \"early_stop_patience\": 10,\n    \"early_stop_min_delta\": 1e-4,\n\n    \"use_ema\": True,\n    \"ema_decay\": 0.999,\n\n    # calibration (fold-wise; leakage-safe)\n    \"use_calibration\": True,\n    \"calibration\": None,  # auto from TRAIN_PLAN if available else \"isotonic\"\n    \"calib_min_samples\": 200,  # isotonic butuh cukup sample\n\n    # reporting/thresholding\n    \"search_best_thr\": True,\n    \"thr_grid\": 801,          # lebih rapat, tetap aman\n    \"thr_objective\": \"f0.5\",  # \"f1\" | \"f0.5\" | \"f2\"  (f0.5 menekan FP)\n    \"report_thr\": 0.5,\n}\n\nCFG_SAFE = {\n    **CFG_CPU,\n    \"n_streams\": 4,\n    \"sinkhorn_tmax\": 20,\n\n    \"d_model\": 256,\n    \"n_layers\": 6,\n    \"n_heads\": 8,\n\n    \"epochs\": 60,\n    \"lr\": 3e-4,\n    \"weight_decay\": 5e-2,\n\n    \"feat_token_drop_p\": 0.05,\n    \"input_noise_std\": 0.01,\n    \"focal_gamma\": 1.5,\n\n    \"batch_size\": 256,\n    \"accum_steps\": 2,\n}\n\nCFG_STRONG = {\n    **CFG_SAFE,\n    \"d_model\": 384,\n    \"n_layers\": 8,\n    \"dropout\": 0.16,\n    \"epochs\": 80,\n    \"lr\": 2e-4,\n    \"weight_decay\": 7e-2,\n    \"feat_token_drop_p\": 0.06,\n    \"input_noise_std\": 0.012,\n    \"focal_gamma\": 1.5,\n}\n\n# auto select by device/memory\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\n\nCFG = dict(CFG_SAFE)\nCFG_NAME = \"SAFE\"\nif device.type != \"cuda\":\n    CFG = dict(CFG_CPU)\n    CFG_NAME = \"CPU\"\nelse:\n    try:\n        mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n        if mem_gb >= 30:\n            CFG = dict(CFG_STRONG)\n            CFG_NAME = \"STRONG\"\n    except Exception:\n        pass\n\n# calibration preference from TRAIN_PLAN if available\nif \"TRAIN_PLAN\" in globals() and isinstance(TRAIN_PLAN, dict):\n    if CFG.get(\"use_calibration\", True):\n        CFG[\"use_calibration\"] = bool(TRAIN_PLAN.get(\"use_calibration\", True))\n        CFG[\"calibration\"] = TRAIN_PLAN.get(\"calibration\", CFG.get(\"calibration\", None))\n# default calibration\nif CFG.get(\"use_calibration\", True) and (CFG.get(\"calibration\") is None):\n    CFG[\"calibration\"] = \"isotonic\"  # default terbaik untuk tabular probs\n\n# ----------------------------\n# 2) Seed + device optim\n# ----------------------------\ndef seed_everything(seed: int = 2025):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(int(CFG[\"seed\"]))\n\nprint(\"Device:\", device, \"| AMP:\", use_amp, \"| CFG:\", CFG_NAME)\nif device.type == \"cuda\":\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\n# ----------------------------\n# 3) Build arrays + guard\n# ----------------------------\nX = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\nfolds = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\n\nif not np.isfinite(X).all():\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\nn = len(df_train_tabular)\nunique_folds = sorted(pd.Series(folds).unique().tolist())\nn_folds = len(unique_folds)\nn_features = X.shape[1]\n\nprint(\"Setup:\")\nprint(\"  rows      :\", n)\nprint(\"  folds     :\", n_folds, \"|\", unique_folds)\nprint(\"  pos%      :\", float(y.mean()) * 100.0)\nprint(\"  n_features:\", n_features)\nif n_folds < 2:\n    raise ValueError(\"Need >=2 folds. Check cv_case_folds.\")\n\n# ----------------------------\n# 4) Dataset\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\n# ----------------------------\n# 5) Normalization (leakage-safe)\n# ----------------------------\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 6) Metrics helpers\n# ----------------------------\ndef safe_auc(y_true, p):\n    y_true = np.asarray(y_true)\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-6, 1-1e-6)\n    return float(log_loss(y_true, p, labels=[0, 1]))\n\ndef fbeta_np(y_true, yhat, beta=1.0):\n    # y_true,yhat in {0,1}\n    y_true = np.asarray(y_true).astype(int)\n    yhat = np.asarray(yhat).astype(int)\n    tp = int(((y_true == 1) & (yhat == 1)).sum())\n    fp = int(((y_true == 0) & (yhat == 1)).sum())\n    fn = int(((y_true == 1) & (yhat == 0)).sum())\n    if tp == 0:\n        return 0.0\n    b2 = beta * beta\n    return float((1 + b2) * tp / max(1e-12, (1 + b2) * tp + b2 * fn + fp))\n\ndef find_best_threshold(y_true, p, n_grid=801, objective=\"f0.5\"):\n    \"\"\"\n    objective: f1 / f0.5 / f2\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int)\n    p = np.asarray(p).astype(np.float64)\n    if objective == \"f2\":\n        beta = 2.0\n    elif objective == \"f1\":\n        beta = 1.0\n    else:\n        beta = 0.5\n\n    best = {\"thr\": 0.5, \"score\": -1.0, \"f1\": 0.0, \"f05\": 0.0, \"f2\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n    for thr in np.linspace(0.0, 1.0, int(n_grid)):\n        yh = (p >= thr).astype(int)\n        sc = fbeta_np(y_true, yh, beta=beta)\n        if sc > best[\"score\"]:\n            best[\"thr\"] = float(thr)\n            best[\"score\"] = float(sc)\n            best[\"f1\"] = float(f1_score(y_true, yh, zero_division=0))\n            best[\"f05\"] = float(fbeta_np(y_true, yh, beta=0.5))\n            best[\"f2\"] = float(fbeta_np(y_true, yh, beta=2.0))\n            best[\"precision\"] = float(precision_score(y_true, yh, zero_division=0))\n            best[\"recall\"] = float(recall_score(y_true, yh, zero_division=0))\n    best[\"objective\"] = str(objective)\n    return best\n\n# ----------------------------\n# 7) Sinkhorn projection\n# ----------------------------\ndef sinkhorn_doubly_stochastic(logits: torch.Tensor, tmax: int = 20, eps: float = 1e-6):\n    z = logits - logits.max()\n    M = torch.exp(z)\n    for _ in range(int(tmax)):\n        M = M / (M.sum(dim=-1, keepdim=True) + eps)\n        M = M / (M.sum(dim=-2, keepdim=True) + eps)\n    return M\n\n# ----------------------------\n# 8) RMSNorm\n# ----------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, d: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d))\n\n    def forward(self, x):\n        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()\n        return x * rms * self.weight\n\n# ----------------------------\n# 9) EMA helper\n# ----------------------------\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[name] = p.detach().clone()\n            p.copy_(self.shadow[name])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[name])\n        self.backup = {}\n\n# ----------------------------\n# 10) Model blocks (feature token dropout)\n# ----------------------------\nclass MHCAttnBlock(nn.Module):\n    def __init__(self, d_model, n_heads, ffn_mult, dropout, attn_dropout,\n                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01):\n        super().__init__()\n        self.n_streams = int(n_streams)\n        self.sinkhorn_tmax = int(sinkhorn_tmax)\n\n        self.norm1 = RMSNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=attn_dropout, batch_first=True)\n        self.drop1 = nn.Dropout(dropout)\n\n        self.norm2 = RMSNorm(d_model)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, ffn_mult * d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(ffn_mult * d_model, d_model),\n        )\n        self.drop2 = nn.Dropout(dropout)\n\n        self.h_logits = nn.Parameter(torch.zeros(self.n_streams, self.n_streams))\n        nn.init.zeros_(self.h_logits)\n\n        a0 = float(alpha_init)\n        a0 = min(max(a0, 1e-4), 1 - 1e-4)\n        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n\n    def forward(self, streams):\n        # streams: (B, n_streams, S, D)\n        B, nS, S, D = streams.shape\n\n        x = streams[:, 0]  # (B,S,D)\n\n        x0 = x\n        q = self.norm1(x)\n        attn_out, _ = self.attn(q, q, q, need_weights=False)\n        x = x0 + self.drop1(attn_out)\n\n        x1 = x\n        h = self.norm2(x)\n        h = self.ffn(h)\n        x = x1 + self.drop2(h)\n\n        # minimal clone (avoid in-place on view)\n        streams = streams.clone()\n        streams[:, 0] = x\n\n        H = sinkhorn_doubly_stochastic(self.h_logits, tmax=self.sinkhorn_tmax)\n        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype)\n        I = torch.eye(self.n_streams, device=streams.device, dtype=streams.dtype)\n        Hres = (1.0 - alpha) * I + alpha * H.to(dtype=streams.dtype)\n\n        mixed = torch.einsum(\"ij,bjtd->bitd\", Hres, streams)\n        return mixed\n\nclass MHCFTTransformer(nn.Module):\n    def __init__(self, n_features,\n                 d_model=256, n_heads=8, n_layers=6, ffn_mult=4,\n                 dropout=0.12, attn_dropout=0.08,\n                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01,\n                 feat_token_drop_p=0.0):\n        super().__init__()\n        self.n_features = int(n_features)\n        self.d_model = int(d_model)\n        self.feat_token_drop_p = float(feat_token_drop_p)\n\n        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n        self.in_drop = nn.Dropout(dropout)\n\n        self.blocks = nn.ModuleList([\n            MHCAttnBlock(\n                d_model=self.d_model,\n                n_heads=n_heads,\n                ffn_mult=ffn_mult,\n                dropout=dropout,\n                attn_dropout=attn_dropout,\n                n_streams=n_streams,\n                sinkhorn_tmax=sinkhorn_tmax,\n                alpha_init=alpha_init\n            )\n            for _ in range(int(n_layers))\n        ])\n\n        self.out_norm = RMSNorm(self.d_model)\n        self.head = nn.Sequential(\n            nn.Linear(self.d_model, self.d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(self.d_model, 1),\n        )\n\n        self.n_streams = int(n_streams)\n\n    def forward(self, x):\n        # x: (B,F)\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)   # (B,F,D)\n        tok = tok + self.feat_emb.unsqueeze(0)\n\n        # feature-token dropout (do not drop CLS)\n        if self.training and self.feat_token_drop_p > 0:\n            B, F, D = tok.shape\n            keep = (torch.rand(B, F, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n            tok = tok * keep.unsqueeze(-1)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)                                    # (B,1,D)\n        seq = torch.cat([cls, tok], dim=1)                                  # (B,1+F,D)\n        seq = self.in_drop(seq)\n\n        streams = seq.unsqueeze(1).repeat(1, self.n_streams, 1, 1)          # (B,nS,S,D)\n        for blk in self.blocks:\n            streams = blk(streams)\n\n        z = streams[:, 0, 0]\n        z = self.out_norm(z)\n        logit = self.head(z).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 11) LR Scheduler\n# ----------------------------\ndef make_warmup_step_scheduler(optimizer, total_steps: int, warmup_steps: int,\n                              milestones_frac=(0.8, 0.9), decay_values=(0.316, 0.1)):\n    m1 = int(float(milestones_frac[0]) * total_steps)\n    m2 = int(float(milestones_frac[1]) * total_steps)\n    d1 = float(decay_values[0])\n    d2 = float(decay_values[1])\n\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        if step < m1:\n            return 1.0\n        elif step < m2:\n            return d1\n        else:\n            return d2\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# ----------------------------\n# 12) Loss: BCE / Focal-BCE\n# ----------------------------\ndef focal_bce_with_logits(logits, targets, pos_weight=None, gamma=0.0):\n    bce = F.binary_cross_entropy_with_logits(\n        logits, targets, reduction=\"none\", pos_weight=pos_weight\n    )\n    if gamma and gamma > 0:\n        p = torch.sigmoid(logits)\n        p_t = p * targets + (1 - p) * (1 - targets)\n        mod = (1.0 - p_t).clamp_min(0.0).pow(gamma)\n        bce = bce * mod\n    return bce.mean()\n\n# ----------------------------\n# 13) Predict helper (optionally with EMA)\n# ----------------------------\n@torch.no_grad()\ndef predict_proba(model, loader, ema: EMA = None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n    probs = []\n    for batch in loader:\n        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n        xb = xb.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            logits = model(xb)\n            p = torch.sigmoid(logits)\n        probs.append(p.detach().cpu().numpy())\n    out = np.concatenate(probs, axis=0).astype(np.float32)\n    if ema is not None:\n        ema.restore(model)\n    return out\n\n# ----------------------------\n# 14) Calibration (fold-wise; leakage-safe)\n#   - isotonic: store (x,y) knots -> apply via np.interp\n#   - sigmoid: store (a,b) for calibrated_logit = a*logit(p)+b\n# ----------------------------\ndef _fit_isotonic(p, y, min_samples=200):\n    from sklearn.isotonic import IsotonicRegression\n    p = np.asarray(p, dtype=np.float64)\n    y = np.asarray(y, dtype=np.int32)\n    ok = np.isfinite(p)\n    p = p[ok]; y = y[ok]\n    if len(p) < int(min_samples) or len(np.unique(y)) < 2:\n        return None\n    iso = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True, out_of_bounds=\"clip\")\n    iso.fit(p, y)\n    # store knots for portable inference\n    return {\"type\": \"isotonic\", \"x\": iso.X_thresholds_.astype(np.float32).tolist(),\n            \"y\": iso.y_thresholds_.astype(np.float32).tolist()}\n\ndef _apply_isotonic(cal, p):\n    x = np.asarray(cal[\"x\"], dtype=np.float32)\n    yk = np.asarray(cal[\"y\"], dtype=np.float32)\n    p = np.asarray(p, dtype=np.float32)\n    p = np.clip(p, 0.0, 1.0)\n    return np.interp(p, x, yk).astype(np.float32)\n\ndef _fit_sigmoid(p, y, min_samples=200):\n    from sklearn.linear_model import LogisticRegression\n    p = np.asarray(p, dtype=np.float64)\n    y = np.asarray(y, dtype=np.int32)\n    ok = np.isfinite(p)\n    p = p[ok]; y = y[ok]\n    if len(p) < int(min_samples) or len(np.unique(y)) < 2:\n        return None\n    p = np.clip(p, 1e-6, 1-1e-6)\n    logit = np.log(p / (1 - p)).reshape(-1, 1)\n    lr = LogisticRegression(C=1000.0, solver=\"lbfgs\", max_iter=200)\n    lr.fit(logit, y)\n    a = float(lr.coef_.ravel()[0])\n    b = float(lr.intercept_.ravel()[0])\n    return {\"type\": \"sigmoid\", \"a\": a, \"b\": b}\n\ndef _apply_sigmoid(cal, p):\n    p = np.asarray(p, dtype=np.float64)\n    p = np.clip(p, 1e-6, 1-1e-6)\n    logit = np.log(p / (1 - p))\n    z = cal[\"a\"] * logit + cal[\"b\"]\n    out = 1.0 / (1.0 + np.exp(-z))\n    return out.astype(np.float32)\n\ndef fit_calibrator(calib_type, p_tr, y_tr, min_samples=200):\n    if calib_type is None or str(calib_type).lower() in [\"none\", \"off\", \"false\"]:\n        return None\n    calib_type = str(calib_type).lower()\n    if calib_type == \"isotonic\":\n        return _fit_isotonic(p_tr, y_tr, min_samples=min_samples)\n    elif calib_type in [\"sigmoid\", \"platt\"]:\n        return _fit_sigmoid(p_tr, y_tr, min_samples=min_samples)\n    else:\n        return None\n\ndef apply_calibrator(cal, p):\n    if cal is None:\n        return np.asarray(p, dtype=np.float32)\n    if cal[\"type\"] == \"isotonic\":\n        return _apply_isotonic(cal, p)\n    if cal[\"type\"] == \"sigmoid\":\n        return _apply_sigmoid(cal, p)\n    return np.asarray(p, dtype=np.float32)\n\n# ----------------------------\n# 15) Train one fold (EMA + focal + noise + best_state + fold calibration)\n# ----------------------------\ndef train_one_fold(X_tr, y_tr, X_va, y_va, cfg):\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    # deterministic-ish loader config\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n\n    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n    # for calibration: need train preds with shuffle False\n    dl_tr_eval = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                            num_workers=nw, pin_memory=pin, drop_last=False,\n                            persistent_workers=(nw > 0))\n\n    model = MHCFTTransformer(\n        n_features=n_features,\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    # imbalance pos_weight\n    pos = int(y_tr.sum())\n    neg = int(len(y_tr) - pos)\n    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        betas=tuple(cfg[\"betas\"]),\n        eps=float(cfg[\"eps\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    optim_steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n    total_optim_steps = int(cfg[\"epochs\"]) * max(1, optim_steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt,\n        total_steps=total_optim_steps,\n        warmup_steps=warmup_steps,\n        milestones_frac=cfg[\"lr_decay_milestones\"],\n        decay_values=cfg[\"lr_decay_values\"],\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n\n    best = {\"val_logloss\": 1e9, \"val_auc\": None, \"epoch\": -1}\n    best_state = None\n    bad = 0\n\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n    label_smooth = float(cfg.get(\"label_smoothing\", 0.0))\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        t0 = time.time()\n        loss_sum = 0.0\n        n_sum = 0\n\n        opt.zero_grad(set_to_none=True)\n        micro_step = 0\n        optim_step_in_epoch = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            if label_smooth and label_smooth > 0:\n                yb = yb * (1.0 - label_smooth) + 0.5 * label_smooth\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = focal_bce_with_logits(logits, yb, pos_weight=pos_weight, gamma=focal_gamma)\n                loss = loss / accum_steps\n\n            scaler.scale(loss).backward()\n            micro_step += 1\n\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro_step % accum_steps) == 0:\n                if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                optim_step_in_epoch += 1\n\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro_step % accum_steps) != 0:\n            if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            optim_step_in_epoch += 1\n            if ema is not None:\n                ema.update(model)\n\n        # validate (EMA-eval)\n        p_va_raw = predict_proba(model, dl_va, ema=ema)\n        vll = safe_logloss(y_va, p_va_raw)\n        vauc = safe_auc(y_va, p_va_raw)\n\n        tr_loss = loss_sum / max(1, n_sum)\n        dt = time.time() - t0\n        print(f\"  epoch {epoch+1:03d}/{cfg['epochs']} | train_loss={tr_loss:.5f} | val_logloss={vll:.5f} | val_auc={vauc} | opt_steps={optim_step_in_epoch} | dt={dt:.1f}s\")\n\n        improved = (best[\"val_logloss\"] - vll) > float(cfg[\"early_stop_min_delta\"])\n        if improved:\n            best[\"val_logloss\"] = float(vll)\n            best[\"val_auc\"] = vauc\n            best[\"epoch\"] = int(epoch)\n\n            # save best EMA-weight (since eval used EMA)\n            if ema is not None:\n                ema.apply_shadow(model)\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n                ema.restore(model)\n            else:\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= int(cfg[\"early_stop_patience\"]):\n                print(f\"  early stop at epoch {epoch+1}, best_epoch={best['epoch']+1}, best_val_logloss={best['val_logloss']:.5f}\")\n                break\n\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n        gc.collect()\n\n    # load best_state (already EMA weights if used)\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    # predict RAW on train & val (best weights)\n    p_tr_raw = predict_proba(model, dl_tr_eval, ema=None)\n    p_va_raw = predict_proba(model, dl_va, ema=None)\n\n    # fit fold calibrator on TRAIN preds only (leakage-safe)\n    cal = None\n    p_va_cal = p_va_raw\n    if bool(cfg.get(\"use_calibration\", True)):\n        cal_type = cfg.get(\"calibration\", None)\n        cal = fit_calibrator(cal_type, p_tr_raw, y_tr, min_samples=int(cfg.get(\"calib_min_samples\", 200)))\n        if cal is not None:\n            p_va_cal = apply_calibrator(cal, p_va_raw)\n\n    pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n        \"best\": best,\n        \"calibrator\": cal,  # portable dict or None\n    }\n    return pack, p_va_raw, p_va_cal, best\n\n# ----------------------------\n# 16) Multi-seed CV loop\n# ----------------------------\n# auto N_SEEDS\ntry:\n    mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3) if device.type == \"cuda\" else 0\nexcept Exception:\n    mem_gb = 0\n\nN_SEEDS_ENV = os.environ.get(\"N_SEEDS\", \"\").strip()\nif N_SEEDS_ENV:\n    N_SEEDS = int(N_SEEDS_ENV)\nelse:\n    N_SEEDS = 2 if (device.type == \"cuda\" and mem_gb >= 30) else 1\n\nSEEDS = [int(CFG[\"seed\"]) + i * 17 for i in range(max(1, N_SEEDS))]\nprint(\"\\nSEED plan:\", SEEDS)\n\nout_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\nall_seed_reports = []\nall_seed_oof_raw = []\nall_seed_oof_cal = []\nbest_epochs_all = []\n\nfor seed_i, seed in enumerate(SEEDS):\n    print(f\"\\n==============================\")\n    print(f\"== SEED {seed} ({seed_i+1}/{len(SEEDS)})\")\n    print(f\"==============================\")\n    seed_everything(seed)\n\n    oof_raw = np.zeros(n, dtype=np.float32)\n    oof_cal = np.zeros(n, dtype=np.float32)\n\n    fold_reports = []\n    best_epochs = []\n\n    models_dir = out_dir / f\"mhc_transformer_folds_seed_{seed}\"\n    models_dir.mkdir(parents=True, exist_ok=True)\n\n    for f in unique_folds:\n        print(f\"\\n[Seed {seed} | Fold {f}]\")\n        tr_idx = np.where(folds != f)[0]\n        va_idx = np.where(folds == f)[0]\n\n        X_tr, y_tr = X[tr_idx], y[tr_idx]\n        X_va, y_va = X[va_idx], y[va_idx]\n\n        pack, p_va_raw, p_va_cal, best = train_one_fold(X_tr, y_tr, X_va, y_va, CFG)\n\n        oof_raw[va_idx] = p_va_raw\n        oof_cal[va_idx] = p_va_cal\n        best_epochs.append(int(best[\"epoch\"] + 1))\n\n        # metrics raw\n        auc_raw = safe_auc(y_va, p_va_raw)\n        ll_raw  = safe_logloss(y_va, p_va_raw)\n\n        # metrics calibrated\n        auc_cal = safe_auc(y_va, p_va_cal)\n        ll_cal  = safe_logloss(y_va, p_va_cal)\n\n        # best threshold search for calibrated (recommended)\n        if bool(CFG.get(\"search_best_thr\", True)):\n            bt = find_best_threshold(y_va, p_va_cal, n_grid=int(CFG.get(\"thr_grid\", 801)),\n                                     objective=str(CFG.get(\"thr_objective\", \"f0.5\")).lower())\n            thr_use = float(bt[\"thr\"])\n        else:\n            thr_use = float(CFG.get(\"report_thr\", 0.5))\n            bt = None\n\n        yhat = (p_va_cal >= thr_use).astype(np.int32)\n\n        rep = {\n            \"seed\": int(seed),\n            \"fold\": int(f),\n            \"n_val\": int(len(va_idx)),\n            \"pos_val\": int(y_va.sum()),\n            \"auc_raw\": auc_raw,\n            \"logloss_raw\": ll_raw,\n            \"auc_cal\": auc_cal,\n            \"logloss_cal\": ll_cal,\n            \"thr_used\": thr_use,\n            \"f1@thr\": float(f1_score(y_va, yhat, zero_division=0)),\n            \"f0.5@thr\": float(fbeta_np(y_va, yhat, beta=0.5)),\n            \"f2@thr\": float(fbeta_np(y_va, yhat, beta=2.0)),\n            \"precision@thr\": float(precision_score(y_va, yhat, zero_division=0)),\n            \"recall@thr\": float(recall_score(y_va, yhat, zero_division=0)),\n            \"best_val_logloss\": float(best[\"val_logloss\"]),\n            \"best_val_auc\": best[\"val_auc\"],\n            \"best_epoch\": int(best[\"epoch\"] + 1),\n            \"thr_search\": bt,\n            \"used_calibration\": bool(pack.get(\"calibrator\") is not None),\n            \"calibration_type\": (pack[\"calibrator\"][\"type\"] if pack.get(\"calibrator\") else None),\n        }\n        fold_reports.append(rep)\n\n        torch.save(\n            {\"pack\": pack, \"feature_cols\": FEATURE_COLS, \"seed\": int(seed), \"fold\": int(f)},\n            models_dir / f\"mhc_transformer_fold_{f}.pt\"\n        )\n\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n        gc.collect()\n\n    # overall metrics for seed\n    oof_auc_raw = safe_auc(y, oof_raw)\n    oof_ll_raw  = safe_logloss(y, oof_raw)\n    oof_auc_cal = safe_auc(y, oof_cal)\n    oof_ll_cal  = safe_logloss(y, oof_cal)\n\n    bt_oof = None\n    best_thr = None\n    if bool(CFG.get(\"search_best_thr\", True)):\n        bt_oof = find_best_threshold(y, oof_cal, n_grid=int(CFG.get(\"thr_grid\", 801)),\n                                     objective=str(CFG.get(\"thr_objective\", \"f0.5\")).lower())\n        best_thr = float(bt_oof[\"thr\"])\n\n    # fixed thr baseline\n    thr_fixed = float(CFG.get(\"report_thr\", 0.5))\n    yhat_fixed = (oof_cal >= thr_fixed).astype(np.int32)\n\n    overall = {\n        \"seed\": int(seed),\n        \"rows\": int(n),\n        \"folds\": int(n_folds),\n        \"pos_total\": int(y.sum()),\n        \"pos_rate\": float(y.mean()),\n        \"oof_auc_raw\": oof_auc_raw,\n        \"oof_logloss_raw\": oof_ll_raw,\n        \"oof_auc_cal\": oof_auc_cal,\n        \"oof_logloss_cal\": oof_ll_cal,\n        f\"oof_f1@{thr_fixed}\": float(f1_score(y, yhat_fixed, zero_division=0)),\n        f\"oof_f0.5@{thr_fixed}\": float(fbeta_np(y, yhat_fixed, beta=0.5)),\n        f\"oof_precision@{thr_fixed}\": float(precision_score(y, yhat_fixed, zero_division=0)),\n        f\"oof_recall@{thr_fixed}\": float(recall_score(y, yhat_fixed, zero_division=0)),\n        \"oof_best_thr_cal\": best_thr,\n        \"oof_best_thr_detail\": bt_oof,\n        \"best_epochs\": best_epochs,\n    }\n\n    df_rep = pd.DataFrame(fold_reports).sort_values([\"seed\", \"fold\"]).reset_index(drop=True)\n    print(\"\\nPer-fold report:\")\n    display(df_rep)\n    print(\"\\nOOF overall (seed):\")\n    print(overall)\n\n    # save per-seed report\n    with open(out_dir / f\"mhc_transformer_cv_report_seed_{seed}.json\", \"w\") as f:\n        json.dump({\"cfg_name\": CFG_NAME, \"cfg\": CFG, \"fold_reports\": fold_reports, \"overall\": overall}, f, indent=2)\n\n    all_seed_reports.append({\"seed\": seed, \"fold_reports\": fold_reports, \"overall\": overall})\n    all_seed_oof_raw.append(oof_raw)\n    all_seed_oof_cal.append(oof_cal)\n    best_epochs_all.append(best_epochs)\n\n# ----------------------------\n# 17) Seed-ensemble OOF (avg)\n# ----------------------------\nOOF_PRED_MHC_RAW = np.mean(np.stack(all_seed_oof_raw, axis=0), axis=0).astype(np.float32)\nOOF_PRED_MHC_CAL = np.mean(np.stack(all_seed_oof_cal, axis=0), axis=0).astype(np.float32)\n\nens_auc_raw = safe_auc(y, OOF_PRED_MHC_RAW)\nens_ll_raw  = safe_logloss(y, OOF_PRED_MHC_RAW)\nens_auc_cal = safe_auc(y, OOF_PRED_MHC_CAL)\nens_ll_cal  = safe_logloss(y, OOF_PRED_MHC_CAL)\n\nthr_fixed = float(CFG.get(\"report_thr\", 0.5))\nyhat_fixed = (OOF_PRED_MHC_CAL >= thr_fixed).astype(np.int32)\n\nbt_ens = None\nens_best_thr = None\nif bool(CFG.get(\"search_best_thr\", True)):\n    bt_ens = find_best_threshold(y, OOF_PRED_MHC_CAL, n_grid=int(CFG.get(\"thr_grid\", 801)),\n                                 objective=str(CFG.get(\"thr_objective\", \"f0.5\")).lower())\n    ens_best_thr = float(bt_ens[\"thr\"])\n\nBASELINE_MHC_TF_OVERALL = {\n    \"model\": \"mHC-FTTransformer (tabular gate) v3.0\",\n    \"cfg_name\": CFG_NAME,\n    \"seeds\": SEEDS,\n    \"feature_count\": int(len(FEATURE_COLS)),\n    \"oof_auc_raw\": ens_auc_raw,\n    \"oof_logloss_raw\": ens_ll_raw,\n    \"oof_auc_cal\": ens_auc_cal,\n    \"oof_logloss_cal\": ens_ll_cal,\n    f\"oof_f1@{thr_fixed}\": float(f1_score(y, yhat_fixed, zero_division=0)),\n    f\"oof_f0.5@{thr_fixed}\": float(fbeta_np(y, yhat_fixed, beta=0.5)),\n    f\"oof_precision@{thr_fixed}\": float(precision_score(y, yhat_fixed, zero_division=0)),\n    f\"oof_recall@{thr_fixed}\": float(recall_score(y, yhat_fixed, zero_division=0)),\n    \"oof_best_thr_cal\": ens_best_thr,\n    \"oof_best_thr_detail\": bt_ens,\n}\n\nprint(\"\\n==============================\")\nprint(\"OOF ENSEMBLE overall:\")\nprint(BASELINE_MHC_TF_OVERALL)\nprint(\"==============================\\n\")\n\n# flatten fold reports for export\nBASELINE_MHC_TF_FOLD_REPORTS = []\nfor sr in all_seed_reports:\n    BASELINE_MHC_TF_FOLD_REPORTS.extend(sr[\"fold_reports\"])\n\nBASELINE_MHC_TF_BEST_EPOCHS = [int(x) for xs in best_epochs_all for x in xs]\n\n# ----------------------------\n# 18) Train FULL model (epochs = median(best_epoch) * 1.15)\n#    (pakai seed pertama; fold model sudah disimpan per seed)\n# ----------------------------\ndef train_full_fixed(X_full_raw, y_full, cfg, epochs_full: int, seed: int):\n    seed_everything(seed)\n    mu, sig = fit_standardizer(X_full_raw)\n    X_full = apply_standardizer(X_full_raw, mu, sig)\n\n    ds_full = TabDataset(X_full, y_full)\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n\n    dl_full = DataLoader(ds_full, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                         num_workers=nw, pin_memory=pin, drop_last=False,\n                         persistent_workers=(nw > 0))\n\n    model = MHCFTTransformer(\n        n_features=n_features,\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    pos = int(y_full.sum())\n    neg = int(len(y_full) - pos)\n    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        betas=tuple(cfg[\"betas\"]),\n        eps=float(cfg[\"eps\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    optim_steps_per_epoch = int(math.ceil(len(dl_full) / accum_steps))\n    total_optim_steps = int(epochs_full) * max(1, optim_steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt,\n        total_steps=total_optim_steps,\n        warmup_steps=warmup_steps,\n        milestones_frac=cfg[\"lr_decay_milestones\"],\n        decay_values=cfg[\"lr_decay_values\"],\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    ema = EMA(model, decay=float(cfg[\"ema_decay\"])) if bool(cfg.get(\"use_ema\", True)) else None\n\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n    label_smooth = float(cfg.get(\"label_smoothing\", 0.0))\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n\n    print(f\"\\nTraining FULL mHC transformer for {epochs_full} epochs | seed={seed} ...\")\n\n    for epoch in range(int(epochs_full)):\n        model.train()\n        loss_sum = 0.0\n        n_sum = 0\n\n        opt.zero_grad(set_to_none=True)\n        micro_step = 0\n\n        for xb, yb in dl_full:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            if label_smooth and label_smooth > 0:\n                yb = yb * (1.0 - label_smooth) + 0.5 * label_smooth\n\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = focal_bce_with_logits(logits, yb, pos_weight=pos_weight, gamma=focal_gamma)\n                loss = loss / accum_steps\n\n            scaler.scale(loss).backward()\n            micro_step += 1\n\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro_step % accum_steps) == 0:\n                if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n                    scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        if (micro_step % accum_steps) != 0:\n            if cfg.get(\"grad_clip\", 0) and float(cfg[\"grad_clip\"]) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        print(f\"  full epoch {epoch+1:03d}/{epochs_full} | loss={loss_sum/max(1,n_sum):.5f}\")\n\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n        gc.collect()\n\n    # save EMA weights if used\n    used_ema = bool(ema is not None)\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    full_pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n        \"epochs_full\": int(epochs_full),\n        \"used_ema\": used_ema,\n        \"seed\": int(seed),\n        \"recommended_thr\": BASELINE_MHC_TF_OVERALL.get(\"oof_best_thr_cal\", None),\n        \"recommended_thr_detail\": BASELINE_MHC_TF_OVERALL.get(\"oof_best_thr_detail\", None),\n    }\n\n    if ema is not None:\n        ema.restore(model)\n\n    return full_pack\n\n# decide epochs_full from all folds, all seeds (median)\nflat_best_epochs = np.array(BASELINE_MHC_TF_BEST_EPOCHS, dtype=np.int32)\nmed_best = int(np.median(flat_best_epochs)) if len(flat_best_epochs) else int(max(12, CFG[\"epochs\"] * 0.7))\nepochs_full = int(max(12, round(med_best * 1.15)))\nepochs_full = int(min(epochs_full, int(CFG[\"epochs\"])))  # safety\n\nfull_pack = train_full_fixed(X, y, CFG, epochs_full=epochs_full, seed=int(SEEDS[0]))\n\nFULL_PACK_PATH = str(out_dir / \"mhc_transformer_model_full.pt\")\ntorch.save({\"pack\": full_pack, \"feature_cols\": FEATURE_COLS}, FULL_PACK_PATH)\n\n# ----------------------------\n# 19) Save OOF + report\n# ----------------------------\ndf_oof = df_train_tabular[[\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]].copy()\ndf_oof[\"oof_pred_mhc_raw\"] = OOF_PRED_MHC_RAW\ndf_oof[\"oof_pred_mhc_cal\"] = OOF_PRED_MHC_CAL\ndf_oof.to_csv(out_dir / \"oof_mhc_transformer.csv\", index=False)\n\nnp.save(out_dir / \"oof_pred_mhc_raw.npy\", OOF_PRED_MHC_RAW)\nnp.save(out_dir / \"oof_pred_mhc_cal.npy\", OOF_PRED_MHC_CAL)\n\nreport = {\n    \"model\": \"mHC-FTTransformer (numeric tabular gate) v3.0\",\n    \"cfg_name\": CFG_NAME,\n    \"cfg\": CFG,\n    \"feature_count\": int(len(FEATURE_COLS)),\n    \"seeds\": SEEDS,\n    \"seed_reports\": all_seed_reports,\n    \"ensemble_overall\": BASELINE_MHC_TF_OVERALL,\n    \"epochs_full\": int(epochs_full),\n    \"full_pack_path\": FULL_PACK_PATH,\n}\nwith open(out_dir / \"mhc_transformer_cv_report.json\", \"w\") as f:\n    json.dump(report, f, indent=2)\n\nprint(\"\\nSaved artifacts:\")\nprint(\"  fold models  ->\", out_dir, \"(mhc_transformer_folds_seed_*/)\")\nprint(\"  full model   ->\", FULL_PACK_PATH)\nprint(\"  oof preds    ->\", out_dir / \"oof_mhc_transformer.csv\")\nprint(\"  oof npy      ->\", out_dir / \"oof_pred_mhc_raw.npy\", \"and\", out_dir / \"oof_pred_mhc_cal.npy\")\nprint(\"  cv report    ->\", out_dir / \"mhc_transformer_cv_report.json\")\n\n# Export globals\nglobals().update({\n    \"OOF_PRED_MHC_RAW\": OOF_PRED_MHC_RAW,\n    \"OOF_PRED_MHC_CAL\": OOF_PRED_MHC_CAL,\n    \"BASELINE_MHC_TF_OVERALL\": BASELINE_MHC_TF_OVERALL,\n    \"BASELINE_MHC_TF_FOLD_REPORTS\": BASELINE_MHC_TF_FOLD_REPORTS,\n    \"BASELINE_MHC_TF_BEST_EPOCHS\": BASELINE_MHC_TF_BEST_EPOCHS,\n    \"FULL_PACK_PATH\": FULL_PACK_PATH,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:57:48.400236Z","iopub.execute_input":"2026-01-04T13:57:48.400525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optimize Model & Hyperparameters (Iterative)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 4 — Optimize Model & Hyperparameters (Iterative) — TRANSFORMER ONLY\n# REVISI FULL v3.2 (mHC-lite PDF-inspired, differentiable + EMA + accum + reg)\n#\n# Perbaikan v3.2 (dibanding draft kamu):\n# - Stage-1 folds subset dipilih benar-benar “evenly spaced” (bukan slicing raw)\n# - AMP/GradScaler aman: otomatis OFF di CPU (hindari warning/bug)\n# - Scheduler benar-benar on optimizer-steps (sudah), + total_steps guard\n# - Early-stop logging lebih jelas + cleanup lebih agresif\n# - Save best_gate_model.pt sekarang juga menyimpan recommended_thr + best_oof_score\n# - Save stage1_results.csv + bisa skip kandidat yang sudah ada (resume-safe)\n#\n# Primary score: OOF best F-beta (beta=0.5)\n#\n# Output:\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/stage1_results.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.json\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/oof_preds_<cfg_name>.csv (top configs)\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt\n# ============================================================\n\nimport os, json, gc, math, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# 0) Require data from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nX_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\nfolds_all = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\nuids_all = df_train_tabular[\"uid\"].astype(str).to_numpy()\n\nif not np.isfinite(X_all).all():\n    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n\nunique_folds = sorted(pd.Series(folds_all).unique().tolist())\nn = len(y_all)\npos_rate = float(y_all.mean())\nn_features = X_all.shape[1]\n\nprint(\"Optimize setup (Transformer-only, mHC-lite):\")\nprint(f\"  rows={n} | folds={len(unique_folds)} | pos%={pos_rate*100:.2f} | n_features={n_features}\")\n\n# ----------------------------\n# 1) Global settings\n# ----------------------------\nSEED = 2025\nBETA = 0.5\nTHR_GRID = 201\n\n# 2-stage runtime controls\nSTAGE1_FOLDS = min(3, len(unique_folds))\nSTAGE1_EPOCH_CAP = 35\nSTAGE1_PAT_CAP = 6\n\nSTAGE2_TOPM = 3\nREPORT_TOPK_OOF = 3\n\n# Optional time budget (0 = off)\nTIME_BUDGET_SEC = 0  # contoh: 2.5*60*60\n\ndef seed_everything(seed=2025):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\nprint(\"Device:\", device, \"| AMP:\", use_amp)\n\nif device.type == \"cuda\":\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\ndef get_mem_gb():\n    if not torch.cuda.is_available():\n        return 0.0\n    try:\n        return float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n    except Exception:\n        return 0.0\n\nMEM_GB = get_mem_gb()\nprint(\"GPU mem GB:\", MEM_GB)\n\n# ----------------------------\n# 2) Metrics helpers (F-beta primary)\n# ----------------------------\ndef best_fbeta_fast(y_true, p, beta=0.5, grid=201):\n    y = (np.asarray(y_true).astype(np.int32) == 1)\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1.0 - 1e-8)\n\n    thrs = np.linspace(0.01, 0.99, int(grid), dtype=np.float64)\n    pred = (p[:, None] >= thrs[None, :])\n\n    y1 = y[:, None]\n    tp = (pred & y1).sum(axis=0).astype(np.float64)\n    fp = (pred & (~y1)).sum(axis=0).astype(np.float64)\n    fn = (y.sum().astype(np.float64) - tp)\n\n    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n    recall    = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n\n    b2 = beta * beta\n    denom = (b2 * precision + recall)\n    fbeta = np.divide((1.0 + b2) * precision * recall, denom, out=np.zeros_like(precision), where=denom > 0)\n\n    j = int(np.argmax(fbeta))\n    return {\n        \"fbeta\": float(fbeta[j]),\n        \"thr\": float(thrs[j]),\n        \"precision\": float(precision[j]),\n        \"recall\": float(recall[j]),\n    }\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n    return float(log_loss(y_true, p, labels=[0, 1]))\n\n# ----------------------------\n# 3) Dataset + Standardizer (no leakage)\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n\n    def __len__(self): return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 4) RMSNorm + Differentiable Sinkhorn + EMA\n# ----------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, d, eps=1e-6):\n        super().__init__()\n        self.eps = float(eps)\n        self.weight = nn.Parameter(torch.ones(d))\n\n    def forward(self, x):\n        rms = torch.mean(x * x, dim=-1, keepdim=True)\n        x = x * torch.rsqrt(rms + self.eps)\n        return x * self.weight\n\ndef sinkhorn_knopp(P, tmax=20, eps=1e-6):\n    \"\"\"\n    Differentiable Sinkhorn-Knopp\n    P: (B,n,n) non-negative\n    \"\"\"\n    M = P.clamp_min(eps)\n    for _ in range(int(tmax)):\n        M = M / (M.sum(dim=-1, keepdim=True).clamp_min(eps))  # row\n        M = M / (M.sum(dim=-2, keepdim=True).clamp_min(eps))  # col\n    return M\n\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[name] = p.detach().clone()\n            p.copy_(self.shadow[name])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[name])\n        self.backup = {}\n\n# ----------------------------\n# 5) mHC-lite on CLS streams\n# ----------------------------\nclass MHCLite(nn.Module):\n    \"\"\"\n    Maintain n_streams for CLS only.\n    Build per-sample mixing matrix -> Sinkhorn -> residual blend with Identity.\n    \"\"\"\n    def __init__(self, d_model, n_streams=4, alpha_init=0.01, tmax=20, dropout=0.0):\n        super().__init__()\n        self.n = int(n_streams)\n        self.tmax = int(tmax)\n        self.drop = nn.Dropout(float(dropout))\n\n        self.norm = RMSNorm(d_model, eps=1e-6)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Linear(d_model, self.n * self.n),\n        )\n        self.softplus = nn.Softplus()\n\n        a0 = float(alpha_init)\n        a0 = min(max(a0, 1e-4), 1.0 - 1e-4)\n        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n\n    def forward(self, streams, cls_vec):\n        # streams: (B,n,D), cls_vec: (B,D)\n        B, n, D = streams.shape\n        h = self.norm(cls_vec)\n        logits = self.mlp(h).view(B, n, n)  # (B,n,n)\n\n        P = self.softplus(logits)           # non-negative\n        M = sinkhorn_knopp(P, tmax=self.tmax, eps=1e-6)\n\n        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype, device=streams.device)\n        I = torch.eye(n, device=streams.device, dtype=streams.dtype).unsqueeze(0).expand(B, -1, -1)\n        H = (1.0 - alpha) * I + alpha * M\n\n        mixed = torch.einsum(\"bij,bjd->bid\", H, streams)\n        injected = mixed + cls_vec.unsqueeze(1)\n        return self.drop(injected)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n        super().__init__()\n        self.norm1 = RMSNorm(d_model, eps=1e-6)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=d_model, num_heads=int(n_heads),\n            dropout=float(attn_dropout), batch_first=True\n        )\n        self.drop1 = nn.Dropout(float(dropout))\n\n        self.norm2 = RMSNorm(d_model, eps=1e-6)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, int(ffn_mult) * d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(int(ffn_mult) * d_model, d_model),\n        )\n        self.drop2 = nn.Dropout(float(dropout))\n\n    def forward(self, x):\n        h = self.norm1(x)\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\n        x = x + self.drop1(attn_out)\n\n        h = self.norm2(x)\n        x = x + self.drop2(self.ffn(h))\n        return x\n\nclass FTTransformer_MHCLite(nn.Module):\n    \"\"\"\n    Numeric FT-Transformer + CLS-stream mHC-lite between blocks.\n    Regularizer: feature-token drop (zero some feature tokens, not CLS).\n    \"\"\"\n    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n                 dropout=0.2, attn_dropout=0.1,\n                 n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n                 feat_token_drop_p=0.0):\n        super().__init__()\n        self.n_features = int(n_features)\n        self.d_model = int(d_model)\n        self.n_layers = int(n_layers)\n        self.feat_token_drop_p = float(feat_token_drop_p)\n\n        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n        self.in_drop = nn.Dropout(float(dropout))\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                d_model=self.d_model,\n                n_heads=n_heads,\n                ffn_mult=ffn_mult,\n                dropout=dropout,\n                attn_dropout=attn_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.mhc = nn.ModuleList([\n            MHCLite(\n                d_model=self.d_model,\n                n_streams=n_streams,\n                alpha_init=alpha_init,\n                tmax=sinkhorn_tmax,\n                dropout=mhc_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.out_norm = RMSNorm(self.d_model, eps=1e-6)\n        self.head = nn.Sequential(\n            nn.Linear(self.d_model, self.d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(self.d_model, 1),\n        )\n\n    def forward(self, x):\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n        tok = tok + self.feat_emb.unsqueeze(0)\n\n        if self.training and self.feat_token_drop_p > 0:\n            B, F_, D = tok.shape\n            keep = (torch.rand(B, F_, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n            tok = tok * keep.unsqueeze(-1)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)\n        seq = torch.cat([cls, tok], dim=1)\n        seq = self.in_drop(seq)\n\n        nS = self.mhc[0].n\n        streams = seq[:, 0, :].unsqueeze(1).expand(B, nS, self.d_model).contiguous()\n\n        for l, blk in enumerate(self.blocks):\n            cls_in = streams.mean(dim=1).unsqueeze(1)\n            seq = torch.cat([cls_in, seq[:, 1:, :]], dim=1)\n\n            seq = blk(seq)\n            cls_vec = seq[:, 0, :]\n\n            streams = self.mhc[l](streams, cls_vec)\n\n        out = self.out_norm(streams.mean(dim=1))\n        logit = self.head(out).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 6) Scheduler (optimizer-steps)\n# ----------------------------\ndef make_warmup_step_scheduler(optimizer, total_steps, warmup_steps,\n                              r1=0.8, r2=0.9, d1=0.316, d2=0.1):\n    total_steps = int(max(1, total_steps))\n    warmup_steps = int(max(0, min(warmup_steps, total_steps)))\n\n    m1 = int(float(r1) * total_steps)\n    m2 = int(float(r2) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        mult = 1.0\n        if step >= m1:\n            mult *= float(d1)\n        if step >= m2:\n            mult *= float(d2)\n        return mult\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# ----------------------------\n# 7) Predict helper\n# ----------------------------\n@torch.no_grad()\ndef predict_proba(model, loader, ema: EMA = None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    probs = []\n    for batch in loader:\n        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n        xb = xb.to(device, non_blocking=True)\n        if use_amp:\n            with torch.cuda.amp.autocast(enabled=True):\n                logits = model(xb)\n                p = torch.sigmoid(logits)\n        else:\n            logits = model(xb)\n            p = torch.sigmoid(logits)\n        probs.append(p.detach().cpu().numpy())\n\n    out = np.concatenate(probs, axis=0).astype(np.float32)\n    if ema is not None:\n        ema.restore(model)\n    return out\n\n# ----------------------------\n# 8) Train one fold\n# ----------------------------\ndef train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg):\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n\n    dl_tr = DataLoader(ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n    dl_va = DataLoader(ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n                       num_workers=nw, pin_memory=pin, drop_last=False,\n                       persistent_workers=(nw > 0))\n\n    model = FTTransformer_MHCLite(\n        n_features=n_features,\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\n    pos = int(y_tr.sum())\n    neg = int(len(y_tr) - pos)\n    pos_weight = torch.tensor([float(neg / max(1, pos))], device=device, dtype=torch.float32)\n\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n    label_smoothing = float(cfg.get(\"label_smoothing\", 0.0))\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    def loss_fn(logits, targets):\n        if label_smoothing and label_smoothing > 0:\n            targets = targets * (1.0 - label_smoothing) + 0.5 * label_smoothing\n        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\", pos_weight=pos_weight)\n        if focal_gamma and focal_gamma > 0:\n            p = torch.sigmoid(logits)\n            p_t = p * targets + (1 - p) * (1 - targets)\n            mod = (1.0 - p_t).clamp_min(0.0).pow(focal_gamma)\n            bce = bce * mod\n        return bce.mean()\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n        eps=float(cfg[\"adam_eps\"]),\n    )\n\n    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n    total_steps = int(cfg[\"epochs\"]) * max(1, steps_per_epoch)\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n\n    sch = make_warmup_step_scheduler(\n        opt,\n        total_steps=total_steps,\n        warmup_steps=warmup_steps,\n        r1=float(cfg[\"lr_decay_ratio1\"]),\n        r2=float(cfg[\"lr_decay_ratio2\"]),\n        d1=float(cfg[\"lr_decay_rate1\"]),\n        d2=float(cfg[\"lr_decay_rate2\"]),\n    )\n\n    if use_amp:\n        scaler = torch.cuda.amp.GradScaler(enabled=True)\n    else:\n        scaler = None\n\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n\n    best_val = 1e18\n    best_state = None\n    best_epoch = -1\n    bad = 0\n    opt_step = 0\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum = 0.0\n        n_sum = 0\n        micro = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            if use_amp:\n                with torch.cuda.amp.autocast(enabled=True):\n                    logits = model(xb)\n                    loss = loss_fn(logits, yb) / accum_steps\n                scaler.scale(loss).backward()\n            else:\n                logits = model(xb)\n                loss = loss_fn(logits, yb) / accum_steps\n                loss.backward()\n\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n            n_sum += xb.size(0)\n\n            if (micro % accum_steps) == 0:\n                if float(cfg[\"grad_clip\"]) and float(cfg[\"grad_clip\"]) > 0:\n                    if use_amp:\n                        scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n                if use_amp:\n                    scaler.step(opt)\n                    scaler.update()\n                else:\n                    opt.step()\n\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                opt_step += 1\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro % accum_steps) != 0:\n            if float(cfg[\"grad_clip\"]) and float(cfg[\"grad_clip\"]) > 0:\n                if use_amp:\n                    scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n            if use_amp:\n                scaler.step(opt)\n                scaler.update()\n            else:\n                opt.step()\n\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            opt_step += 1\n            if ema is not None:\n                ema.update(model)\n\n        # validate with EMA (if enabled)\n        p_va = predict_proba(model, dl_va, ema=ema)\n        vll = safe_logloss(y_va, p_va)\n\n        improved = (best_val - vll) > float(cfg[\"min_delta\"])\n        if improved:\n            best_val = float(vll)\n            best_epoch = int(epoch)\n\n            if ema is not None:\n                ema.apply_shadow(model)\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n                ema.restore(model)\n            else:\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n\n            bad = 0\n        else:\n            bad += 1\n            if bad >= int(cfg[\"patience\"]):\n                break\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    p_va = predict_proba(model, dl_va, ema=None)\n\n    pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": dict(cfg),\n        \"best_epoch\": int(best_epoch + 1),\n        \"best_val_logloss\": float(best_val),\n    }\n    return pack, p_va\n\n# ----------------------------\n# 9) CV evaluator for a config\n# ----------------------------\ndef run_cv_config(cfg, cfg_name, folds_subset=None, beta=0.5, thr_grid=201):\n    oof = np.zeros(n, dtype=np.float32)\n    fold_rows = []\n    fold_packs = []\n\n    use_folds = unique_folds if folds_subset is None else list(folds_subset)\n\n    for f in use_folds:\n        tr = np.where(folds_all != f)[0]\n        va = np.where(folds_all == f)[0]\n\n        X_tr, y_tr = X_all[tr], y_all[tr]\n        X_va, y_va = X_all[va], y_all[va]\n\n        pack, p_va = train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg)\n        oof[va] = p_va\n\n        fold_auc = safe_auc(y_va, p_va)\n        fold_ll  = safe_logloss(y_va, p_va)\n        best_fold = best_fbeta_fast(y_va, p_va, beta=beta, grid=max(81, thr_grid//2))\n\n        fold_rows.append({\n            \"cfg\": cfg_name,\n            \"fold\": int(f),\n            \"n_val\": int(len(va)),\n            \"pos_val\": int(y_va.sum()),\n            \"auc\": fold_auc,\n            \"logloss\": fold_ll,\n            \"best_fbeta\": best_fold[\"fbeta\"],\n            \"best_thr\": best_fold[\"thr\"],\n            \"best_prec\": best_fold[\"precision\"],\n            \"best_rec\": best_fold[\"recall\"],\n            \"best_val_logloss\": float(pack[\"best_val_logloss\"]),\n            \"best_epoch\": int(pack[\"best_epoch\"]),\n        })\n\n        pack2 = dict(pack)\n        pack2[\"fold\"] = int(f)\n        fold_packs.append(pack2)\n\n        del pack\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if folds_subset is None:\n        idx_eval = np.arange(n)\n    else:\n        idx_eval = np.where(np.isin(folds_all, np.array(use_folds)))[0]\n\n    oof_eval = oof[idx_eval]\n    y_eval = y_all[idx_eval]\n\n    oof_auc = safe_auc(y_eval, oof_eval)\n    oof_ll  = safe_logloss(y_eval, oof_eval)\n    best_oof = best_fbeta_fast(y_eval, oof_eval, beta=beta, grid=thr_grid)\n\n    summary = {\n        \"cfg\": cfg_name,\n        \"stage\": \"full\" if folds_subset is None else f\"subset{len(use_folds)}\",\n        \"oof_auc\": oof_auc,\n        \"oof_logloss\": oof_ll,\n        \"oof_best_fbeta\": best_oof[\"fbeta\"],\n        \"oof_best_thr\": best_oof[\"thr\"],\n        \"oof_best_prec\": best_oof[\"precision\"],\n        \"oof_best_rec\": best_oof[\"recall\"],\n\n        \"d_model\": cfg[\"d_model\"],\n        \"n_layers\": cfg[\"n_layers\"],\n        \"n_heads\": cfg[\"n_heads\"],\n        \"ffn_mult\": cfg[\"ffn_mult\"],\n        \"dropout\": cfg[\"dropout\"],\n        \"attn_dropout\": cfg[\"attn_dropout\"],\n\n        \"n_streams\": cfg[\"n_streams\"],\n        \"alpha_init\": cfg[\"alpha_init\"],\n        \"sinkhorn_tmax\": cfg[\"sinkhorn_tmax\"],\n        \"mhc_dropout\": cfg[\"mhc_dropout\"],\n\n        \"feat_token_drop_p\": cfg.get(\"feat_token_drop_p\", 0.0),\n        \"input_noise_std\": cfg.get(\"input_noise_std\", 0.0),\n        \"focal_gamma\": cfg.get(\"focal_gamma\", 0.0),\n        \"label_smoothing\": cfg.get(\"label_smoothing\", 0.0),\n\n        \"batch_size\": cfg[\"batch_size\"],\n        \"accum_steps\": cfg.get(\"accum_steps\", 1),\n        \"epochs\": cfg[\"epochs\"],\n        \"lr\": cfg[\"lr\"],\n        \"weight_decay\": cfg[\"weight_decay\"],\n        \"warmup_frac\": cfg[\"warmup_frac\"],\n        \"beta1\": cfg[\"beta1\"],\n        \"beta2\": cfg[\"beta2\"],\n        \"adam_eps\": cfg[\"adam_eps\"],\n        \"lr_decay_ratio1\": cfg[\"lr_decay_ratio1\"],\n        \"lr_decay_ratio2\": cfg[\"lr_decay_ratio2\"],\n        \"lr_decay_rate1\": cfg[\"lr_decay_rate1\"],\n        \"lr_decay_rate2\": cfg[\"lr_decay_rate2\"],\n        \"patience\": cfg[\"patience\"],\n        \"min_delta\": cfg[\"min_delta\"],\n        \"grad_clip\": cfg[\"grad_clip\"],\n        \"use_ema\": cfg.get(\"use_ema\", True),\n        \"ema_decay\": cfg.get(\"ema_decay\", 0.999),\n    }\n    return summary, fold_rows, oof, fold_packs\n\n# ----------------------------\n# 10) Candidate configs\n# ----------------------------\ndef make_base():\n    if device.type == \"cuda\":\n        if MEM_GB >= 30:\n            bs, acc = 512, 2\n        elif MEM_GB >= 16:\n            bs, acc = 384, 2\n        else:\n            bs, acc = 256, 2\n    else:\n        bs, acc = 256, 1\n\n    return dict(\n        batch_size=bs,\n        accum_steps=acc,\n        epochs=75 if device.type == \"cuda\" else 40,\n        lr=2e-4,\n        weight_decay=1.0e-2,\n        warmup_frac=0.10,\n        grad_clip=1.0,\n        patience=10,\n        min_delta=1e-4,\n\n        beta1=0.9,\n        beta2=0.95,\n        adam_eps=1e-8,\n\n        lr_decay_ratio1=0.8,\n        lr_decay_ratio2=0.9,\n        lr_decay_rate1=0.316,\n        lr_decay_rate2=0.1,\n\n        n_streams=4,\n        alpha_init=0.01,\n        sinkhorn_tmax=20,\n        mhc_dropout=0.00,\n\n        feat_token_drop_p=0.05,\n        input_noise_std=0.01,\n        focal_gamma=1.5,\n        label_smoothing=0.00,\n\n        use_ema=True,\n        ema_decay=0.999,\n    )\n\nBASE = make_base()\n\ncandidates = []\ncandidates.append((\"mhc_384x8_main\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=4, dropout=0.18, attn_dropout=0.10)))\ncandidates.append((\"mhc_384x10_reg\", dict(BASE, d_model=384, n_layers=10, n_heads=8,  ffn_mult=4, dropout=0.24, attn_dropout=0.12,\n                                         lr=1.6e-4, weight_decay=1.5e-2, patience=12,\n                                         feat_token_drop_p=0.06, input_noise_std=0.012)))\ncandidates.append((\"mhc_384x8_ffn2\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=2, dropout=0.16, attn_dropout=0.10,\n                                         lr=2.2e-4, weight_decay=8e-3)))\ncandidates.append((\"mhc_256x6_fast\", dict(BASE, d_model=256, n_layers=6,  n_heads=8,  ffn_mult=4, dropout=0.16, attn_dropout=0.08,\n                                         lr=3e-4, weight_decay=6e-3, epochs=min(int(BASE[\"epochs\"]), 60), patience=9,\n                                         feat_token_drop_p=0.04, input_noise_std=0.010)))\n\nif device.type == \"cuda\" and MEM_GB >= 20:\n    candidates.append((\"mhc_512x10_big\", dict(BASE, d_model=512, n_layers=10, n_heads=16, ffn_mult=4, dropout=0.26, attn_dropout=0.14,\n                                             lr=1.2e-4, weight_decay=2.0e-2, epochs=max(int(BASE[\"epochs\"]), 85), patience=12,\n                                             mhc_dropout=0.05, feat_token_drop_p=0.06)))\n\nprint(f\"\\nTotal Transformer candidates: {len(candidates)}\")\nprint(\"Primary score: OOF best F-beta (beta=0.5)\")\n\n# ----------------------------\n# 11) Run 2-stage search\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOPT_DIR = OUT_DIR / \"opt_search\"\nOPT_DIR.mkdir(parents=True, exist_ok=True)\n\nSTAGE1_PATH = OPT_DIR / \"stage1_results.csv\"\n\n# stage-1 folds subset: evenly spaced indices\nif STAGE1_FOLDS >= len(unique_folds):\n    folds_subset = unique_folds\nelse:\n    idxs = np.linspace(0, len(unique_folds) - 1, STAGE1_FOLDS)\n    idxs = np.round(idxs).astype(int)\n    idxs = np.unique(idxs).tolist()\n    folds_subset = [unique_folds[i] for i in idxs]\n    if len(folds_subset) < STAGE1_FOLDS:\n        # pad if collision happened\n        for f in unique_folds:\n            if f not in folds_subset:\n                folds_subset.append(f)\n            if len(folds_subset) >= STAGE1_FOLDS:\n                break\n\nprint(\"\\nStage-1 folds subset:\", folds_subset)\n\n# resume-safe: load already evaluated stage1 cfg names\ndone_stage1 = set()\nif STAGE1_PATH.exists():\n    try:\n        df_prev = pd.read_csv(STAGE1_PATH)\n        if \"cfg\" in df_prev.columns:\n            done_stage1 = set(df_prev[\"cfg\"].astype(str).tolist())\n            print(f\"Resume: found {len(done_stage1)} configs already in stage1_results.csv\")\n    except Exception:\n        pass\n\nt0 = time.time()\nstage1_rows = []\nstage1_fold_rows = []\n\nfor i, (name, cfg) in enumerate(candidates, 1):\n    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n        print(\"Time budget reached. Stop search.\")\n        break\n\n    if name in done_stage1:\n        print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] SKIP (already done) -> {name}\")\n        continue\n\n    cfg1 = dict(cfg)\n    cfg1[\"epochs\"] = int(min(int(cfg1[\"epochs\"]), int(STAGE1_EPOCH_CAP)))\n    cfg1[\"patience\"] = int(min(int(cfg1[\"patience\"]), int(STAGE1_PAT_CAP)))\n\n    print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] CV(subset) -> {name}\")\n    summ, fold_rows, _, _ = run_cv_config(cfg1, name, folds_subset=folds_subset, beta=BETA, thr_grid=101)\n\n    stage1_rows.append(summ)\n    stage1_fold_rows.extend(fold_rows)\n\n    print(f\"  stage1 best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f} | logloss: {summ['oof_logloss']:.6f}\")\n\n    # append-progress to disk (resume-safe)\n    try:\n        df_append = pd.DataFrame([summ])\n        if STAGE1_PATH.exists():\n            df_old = pd.read_csv(STAGE1_PATH)\n            df_new = pd.concat([df_old, df_append], axis=0, ignore_index=True)\n        else:\n            df_new = df_append\n        df_new.to_csv(STAGE1_PATH, index=False)\n    except Exception:\n        pass\n\n# build stage1 ranking from disk (preferred) + current in-memory\nif STAGE1_PATH.exists():\n    df_s1 = pd.read_csv(STAGE1_PATH)\nelse:\n    df_s1 = pd.DataFrame(stage1_rows)\n\nif len(df_s1) == 0:\n    raise RuntimeError(\"Stage-1 menghasilkan 0 hasil. Cek runtime/VRAM atau kecilkan kandidat/epochs.\")\n\ndf_s1 = df_s1.sort_values([\"oof_best_fbeta\",\"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\nprint(\"\\nStage-1 ranking (top):\")\ndisplay(df_s1.head(10))\n\ntopM = min(int(STAGE2_TOPM), len(df_s1))\nstage2_names = df_s1[\"cfg\"].head(topM).astype(str).tolist()\nprint(\"\\nStage-2 will run full CV for:\", stage2_names)\n\nall_summaries = []\nall_fold_rows = []\noof_store = {}\npack_store = {}\n\nfor j, nm in enumerate(stage2_names, 1):\n    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n        print(\"Time budget reached. Stop stage-2.\")\n        break\n\n    cfg = None\n    for (nname, ccfg) in candidates:\n        if nname == nm:\n            cfg = ccfg\n            break\n    if cfg is None:\n        continue\n\n    print(f\"\\n[Stage-2 {j:02d}/{len(stage2_names)}] CV(full) -> {nm}\")\n    summ, fold_rows, oof, fold_packs = run_cv_config(cfg, nm, folds_subset=None, beta=BETA, thr_grid=THR_GRID)\n\n    all_summaries.append(summ)\n    all_fold_rows.extend(fold_rows)\n    oof_store[nm] = oof\n    pack_store[nm] = fold_packs\n\n    print(f\"  OOF best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f}\"\n          f\" | auc: {(summ['oof_auc'] if summ['oof_auc'] is not None else float('nan')):.6f}\"\n          f\" | logloss: {summ['oof_logloss']:.6f}\")\n\ndf_sum = pd.DataFrame(all_summaries)\ndf_fold = pd.DataFrame(all_fold_rows)\n\nif len(df_sum) == 0:\n    raise RuntimeError(\"Stage-2 produced no results. Turunkan kandidat/epochs atau cek device/VRAM.\")\n\ndf_sum = df_sum.sort_values([\"oof_best_fbeta\", \"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n\nprint(\"\\nStage-2 top candidates (full CV):\")\ndisplay(df_sum)\n\n# save search results\ndf_sum.to_csv(OPT_DIR / \"opt_results.csv\", index=False)\nwith open(OPT_DIR / \"opt_results.json\", \"w\") as f:\n    json.dump(df_sum.to_dict(orient=\"records\"), f, indent=2)\ndf_fold.to_csv(OPT_DIR / \"opt_fold_details.csv\", index=False)\n\n# save OOF preds for top configs (debug)\ntop_names = df_sum[\"cfg\"].head(min(REPORT_TOPK_OOF, len(df_sum))).astype(str).tolist()\nfor nm in top_names:\n    df_o = pd.DataFrame({\n        \"uid\": uids_all,\n        \"y\": y_all,\n        \"fold\": folds_all,\n        f\"oof_pred_{nm}\": oof_store[nm]\n    })\n    df_o.to_csv(OPT_DIR / f\"oof_preds_{nm}.csv\", index=False)\n\n# ----------------------------\n# 12) Choose best config + save BEST fold packs (tanpa retrain ulang)\n# ----------------------------\nbest_single = df_sum.iloc[0].to_dict()\nbest_cfg_name = str(best_single[\"cfg\"])\n\nbest_cfg = None\nfor nm, cfg in candidates:\n    if nm == best_cfg_name:\n        best_cfg = cfg\n        break\nif best_cfg is None:\n    raise RuntimeError(\"Best cfg not found in candidates list (unexpected).\")\n\nbest_fold_packs = pack_store[best_cfg_name]\nbest_oof = oof_store[best_cfg_name]\nbest_oof_best = best_fbeta_fast(y_all, best_oof, beta=BETA, grid=THR_GRID)\n\nbest_model_path = OUT_DIR / \"best_gate_model.pt\"\ntorch.save(\n    {\n        \"type\": \"mhc_lite_ft_transformer_v3\",\n        \"feature_cols\": FEATURE_COLS,\n        \"fold_packs\": best_fold_packs,\n        \"cfg_name\": best_cfg_name,\n        \"cfg\": best_cfg,\n        \"seed\": SEED,\n        \"beta_for_tuning\": BETA,\n        \"recommended_thr\": best_oof_best[\"thr\"],\n        \"recommended_score\": best_oof_best[\"fbeta\"],\n    },\n    best_model_path\n)\n\nbest_bundle = {\n    \"type\": \"mhc_lite_ft_transformer_v3\",\n    \"model_name\": best_cfg_name,\n    \"members\": [best_cfg_name],\n    \"random_seed\": SEED,\n    \"beta_for_tuning\": BETA,\n\n    \"feature_cols\": FEATURE_COLS,\n    \"cfg\": best_cfg,\n\n    \"oof_best_thr\": best_oof_best[\"thr\"],\n    \"oof_best_fbeta\": best_oof_best[\"fbeta\"],\n    \"oof_best_prec\": best_oof_best[\"precision\"],\n    \"oof_best_rec\": best_oof_best[\"recall\"],\n    \"oof_auc\": safe_auc(y_all, best_oof),\n    \"oof_logloss\": safe_logloss(y_all, best_oof),\n\n    \"notes\": \"Best config from Step 4 (Transformer-only, mHC-lite differentiable + EMA + accum + reg). Best model saved as fold_packs (no retrain).\",\n}\n\nwith open(OUT_DIR / \"best_gate_config.json\", \"w\") as f:\n    json.dump(best_bundle, f, indent=2)\n\nprint(\"\\nSaved best artifacts:\")\nprint(\"  best model (fold packs) ->\", best_model_path)\nprint(\"  best config             ->\", OUT_DIR / \"best_gate_config.json\")\nprint(\"  opt results             ->\", OPT_DIR / \"opt_results.csv\")\nprint(\"  fold detail             ->\", OPT_DIR / \"opt_fold_details.csv\")\nprint(\"  stage1 cache            ->\", STAGE1_PATH)\n\n# Export globals for Step 5\nBEST_GATE_BUNDLE = best_bundle\nBEST_TF_CFG_NAME = best_cfg_name\nBEST_TF_CFG = best_cfg\nOPT_RESULTS_DF = df_sum\nBEST_TF_OOF = best_oof\nBEST_TF_OOF_METRIC = best_oof_best\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Training (Train on Full Data)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 5 — Final Training (Train on Full Data) — TRANSFORMER ONLY\n# REVISI FULL v4.1 (match Step 4 v3.2: mHC-lite differentiable + EMA + accum)\n#\n# Fix v4.1:\n# - STRICT match best cfg dari Step 4 (tanpa auto “bigger model” kecuali kamu ON-kan)\n# - AMP/GradScaler aman (CPU = no-amp, no-scaler)\n# - CFG guard: d_model harus divisible oleh n_heads (auto-fix saat fallback/OOM)\n# - Internal val case-level: robust fallback kalau val kosong / 1-class\n# - OOM fallback lebih aman: turunkan batch -> d_model -> n_layers -> n_heads\n# - Scheduler total_steps guard (>=1) + warmup clamp\n#\n# Output:\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n# ============================================================\n\nimport os, json, gc, math, time, warnings\nfrom pathlib import Path\nfrom contextlib import nullcontext\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import log_loss, roc_auc_score\n\n# ----------------------------\n# 0) REQUIRE\n# ----------------------------\nif \"df_train_tabular\" not in globals():\n    raise RuntimeError(\"Missing `df_train_tabular`. Jalankan Step 2 dulu.\")\nif \"FEATURE_COLS\" not in globals():\n    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 dulu.\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nneed_cols = {\"uid\", \"case_id\", \"y\"}\nmiss = [c for c in need_cols if c not in df_train_tabular.columns]\nif miss:\n    raise ValueError(f\"df_train_tabular missing columns: {miss}\")\n\nX_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n\nif not np.isfinite(X_all).all():\n    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Final training data:\")\nprint(f\"  rows={len(y_all)} | pos%={float(y_all.mean())*100:.2f} | n_features={X_all.shape[1]}\")\n\n# ----------------------------\n# 1) Load best cfg (Step 4 output)\n# ----------------------------\nbest_bundle = None\nsource = None\n\ncfg_path = OUT_DIR / \"best_gate_config.json\"\nbest_model_candidates = [\n    OUT_DIR / \"best_gate_model.pt\",\n    OUT_DIR / \"best_gate_model.pth\",\n]\n\nif \"BEST_GATE_BUNDLE\" in globals() and isinstance(BEST_GATE_BUNDLE, dict):\n    best_bundle = BEST_GATE_BUNDLE\n    source = \"memory(BEST_GATE_BUNDLE)\"\nelif cfg_path.exists():\n    best_bundle = json.loads(cfg_path.read_text())\n    source = str(cfg_path)\n\nif best_bundle is not None and isinstance(best_bundle, dict) and isinstance(best_bundle.get(\"cfg\", None), dict):\n    base_cfg = dict(best_bundle[\"cfg\"])\n    print(\"\\nLoaded cfg from:\", source)\nelse:\n    base_cfg = {}\n    print(\"\\nNo best_gate_config found. Using strong default cfg.\")\n\n# optional: load fold_packs dari best_gate_model.pt (biar final file bisa bawa ensemble fold juga)\nfold_packs_from_step4 = None\nbest_gate_model_path = None\nfor p in best_model_candidates:\n    if p.exists():\n        best_gate_model_path = p\n        break\n\nif best_gate_model_path is not None:\n    try:\n        obj = torch.load(best_gate_model_path, map_location=\"cpu\")\n        if isinstance(obj, dict) and isinstance(obj.get(\"fold_packs\", None), list):\n            fold_packs_from_step4 = obj[\"fold_packs\"]\n            print(\"Loaded fold_packs from:\", str(best_gate_model_path))\n    except Exception as e:\n        print(\"Warning: failed to load best_gate_model.* fold_packs:\", repr(e))\n\n# ----------------------------\n# 2) Device + seed\n# ----------------------------\ndef seed_everything(seed: int):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nFINAL_SEED = 2025\nif isinstance(best_bundle, dict):\n    FINAL_SEED = int(best_bundle.get(\"seed\", best_bundle.get(\"random_seed\", 2025)))\n\nseed_everything(FINAL_SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\n\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\nvram_gb = None\nif device.type == \"cuda\":\n    vram_gb = float(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n\nprint(\"\\nDevice:\", device, \"| AMP:\", use_amp, \"| VRAM_GB:\", (f\"{vram_gb:.1f}\" if vram_gb else \"CPU\"))\n\n# ----------------------------\n# 3) Training policy\n# ----------------------------\n# IMPORTANT: defaultnya STRICT match Step 4 best cfg (tidak dibesarkan otomatis)\nALLOW_UPSCALE = False           # set True kalau kamu mau auto-besar sesuai VRAM\nUSE_INTERNAL_VAL = True         # cari best_epoch dari val (case-level)\nVAL_FRAC_CASE = 0.08            # 8% case untuk val\nEARLY_STOP = True\n\n# runtime: 1 seed default\nN_SEEDS = 1\n\n# target effective batch\nTARGET_EFF_BATCH = 1024 if device.type == \"cuda\" else 256\n\n# ----------------------------\n# 4) Dataset + Standardizer\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.float32))\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 5) Metrics helpers\n# ----------------------------\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n    return float(log_loss(y_true, p, labels=[0, 1]))\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\n# ----------------------------\n# 6) Model: FTTransformer_MHCLite (MATCH Step 4)\n# ----------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, d, eps=1e-6):\n        super().__init__()\n        self.eps = float(eps)\n        self.weight = nn.Parameter(torch.ones(d))\n    def forward(self, x):\n        rms = torch.mean(x * x, dim=-1, keepdim=True)\n        x = x * torch.rsqrt(rms + self.eps)\n        return x * self.weight\n\ndef sinkhorn_knopp(P, tmax=20, eps=1e-6):\n    M = P.clamp_min(eps)\n    for _ in range(int(tmax)):\n        M = M / (M.sum(dim=-1, keepdim=True).clamp_min(eps))  # row\n        M = M / (M.sum(dim=-2, keepdim=True).clamp_min(eps))  # col\n    return M\n\nclass MHCLite(nn.Module):\n    def __init__(self, d_model, n_streams=4, alpha_init=0.01, tmax=20, dropout=0.0):\n        super().__init__()\n        self.n = int(n_streams)\n        self.tmax = int(tmax)\n        self.drop = nn.Dropout(float(dropout))\n\n        self.norm = RMSNorm(d_model, eps=1e-6)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Linear(d_model, self.n * self.n),\n        )\n        self.softplus = nn.Softplus()\n\n        a0 = float(alpha_init)\n        a0 = min(max(a0, 1e-4), 1.0 - 1e-4)\n        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n\n    def forward(self, streams, cls_vec):\n        B, n, D = streams.shape\n        h = self.norm(cls_vec)\n        logits = self.mlp(h).view(B, n, n)\n        P = self.softplus(logits)\n        M = sinkhorn_knopp(P, tmax=self.tmax, eps=1e-6)\n\n        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype, device=streams.device)\n        I = torch.eye(n, device=streams.device, dtype=streams.dtype).unsqueeze(0).expand(B, -1, -1)\n        H = (1.0 - alpha) * I + alpha * M\n\n        mixed = torch.einsum(\"bij,bjd->bid\", H, streams)\n        injected = mixed + cls_vec.unsqueeze(1)\n        return self.drop(injected)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n        super().__init__()\n        self.norm1 = RMSNorm(d_model, eps=1e-6)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=d_model, num_heads=int(n_heads),\n            dropout=float(attn_dropout), batch_first=True\n        )\n        self.drop1 = nn.Dropout(float(dropout))\n\n        self.norm2 = RMSNorm(d_model, eps=1e-6)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, int(ffn_mult) * d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(int(ffn_mult) * d_model, d_model),\n        )\n        self.drop2 = nn.Dropout(float(dropout))\n\n    def forward(self, x):\n        h = self.norm1(x)\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\n        x = x + self.drop1(attn_out)\n        h = self.norm2(x)\n        x = x + self.drop2(self.ffn(h))\n        return x\n\nclass FTTransformer_MHCLite(nn.Module):\n    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n                 dropout=0.2, attn_dropout=0.1,\n                 n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n                 feat_token_drop_p=0.0):\n        super().__init__()\n        self.n_features = int(n_features)\n        self.d_model = int(d_model)\n        self.n_layers = int(n_layers)\n        self.feat_token_drop_p = float(feat_token_drop_p)\n\n        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n        self.in_drop = nn.Dropout(float(dropout))\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                d_model=self.d_model,\n                n_heads=n_heads,\n                ffn_mult=ffn_mult,\n                dropout=dropout,\n                attn_dropout=attn_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.mhc = nn.ModuleList([\n            MHCLite(\n                d_model=self.d_model,\n                n_streams=n_streams,\n                alpha_init=alpha_init,\n                tmax=sinkhorn_tmax,\n                dropout=mhc_dropout\n            ) for _ in range(self.n_layers)\n        ])\n\n        self.out_norm = RMSNorm(self.d_model, eps=1e-6)\n        self.head = nn.Sequential(\n            nn.Linear(self.d_model, self.d_model),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n            nn.Linear(self.d_model, 1),\n        )\n\n    def forward(self, x):\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n        tok = tok + self.feat_emb.unsqueeze(0)\n\n        if self.training and self.feat_token_drop_p > 0:\n            B, F_, D = tok.shape\n            keep = (torch.rand(B, F_, device=tok.device) > self.feat_token_drop_p).to(tok.dtype)\n            tok = tok * keep.unsqueeze(-1)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)\n        seq = torch.cat([cls, tok], dim=1)\n        seq = self.in_drop(seq)\n\n        nS = self.mhc[0].n\n        streams = seq[:, 0, :].unsqueeze(1).expand(B, nS, self.d_model).contiguous()\n\n        for l, blk in enumerate(self.blocks):\n            cls_in = streams.mean(dim=1).unsqueeze(1)\n            seq = torch.cat([cls_in, seq[:, 1:, :]], dim=1)\n            seq = blk(seq)\n            cls_vec = seq[:, 0, :]\n            streams = self.mhc[l](streams, cls_vec)\n\n        out = self.out_norm(streams.mean(dim=1))\n        logit = self.head(out).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 7) EMA + scheduler (optimizer-steps)\n# ----------------------------\nclass EMA:\n    def __init__(self, model: nn.Module, decay: float = 0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model: nn.Module):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    @torch.no_grad()\n    def apply_shadow(self, model: nn.Module):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.backup[name] = p.detach().clone()\n            p.copy_(self.shadow[name])\n\n    @torch.no_grad()\n    def restore(self, model: nn.Module):\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            p.copy_(self.backup[name])\n        self.backup = {}\n\ndef make_warmup_step_scheduler(optimizer, total_steps, warmup_steps,\n                              r1=0.8, r2=0.9, d1=0.316, d2=0.1):\n    total_steps = int(max(1, total_steps))\n    warmup_steps = int(max(0, min(int(warmup_steps), total_steps)))\n\n    m1 = int(float(r1) * total_steps)\n    m2 = int(float(r2) * total_steps)\n\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        mult = 1.0\n        if step >= m1:\n            mult *= float(d1)\n        if step >= m2:\n            mult *= float(d2)\n        return mult\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n@torch.no_grad()\ndef predict_proba(model, loader, ema: EMA = None):\n    model.eval()\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n    ps = []\n    for batch in loader:\n        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n        xb = xb.to(device, non_blocking=True)\n        with ctx:\n            logits = model(xb)\n            p = torch.sigmoid(logits)\n        ps.append(p.detach().cpu().numpy())\n\n    out = np.concatenate(ps, axis=0).astype(np.float32)\n    if ema is not None:\n        ema.restore(model)\n    return out\n\n# ----------------------------\n# 8) CFG merge + guards\n# ----------------------------\nCFG = dict(\n    # arch\n    d_model=384, n_layers=8, n_heads=8, ffn_mult=4,\n    dropout=0.20, attn_dropout=0.10,\n\n    # mHC-lite\n    n_streams=4, alpha_init=0.01, sinkhorn_tmax=20, mhc_dropout=0.0,\n\n    # regularization\n    feat_token_drop_p=0.05,\n    input_noise_std=0.01,\n    focal_gamma=1.5,\n    label_smoothing=0.00,\n\n    # optim\n    lr=2e-4,\n    weight_decay=1.0e-2,\n    beta1=0.9,\n    beta2=0.95,\n    adam_eps=1e-8,\n\n    # sched\n    warmup_frac=0.10,\n    lr_decay_ratio1=0.8,\n    lr_decay_ratio2=0.9,\n    lr_decay_rate1=0.316,\n    lr_decay_rate2=0.1,\n\n    # train\n    batch_size=512 if device.type == \"cuda\" else 256,\n    epochs=75 if device.type == \"cuda\" else 35,\n    accum_steps=2 if device.type == \"cuda\" else 1,\n    grad_clip=1.0,\n    patience=10,\n    min_delta=1e-4,\n\n    # EMA\n    use_ema=True,\n    ema_decay=0.999,\n)\n\n# merge best cfg -> CFG (strict match Step 4)\nfor k, v in base_cfg.items():\n    if k in CFG:\n        CFG[k] = v\n\ndef cpu_safe_cfg(cfg: dict):\n    cfg = dict(cfg)\n    if device.type != \"cuda\":\n        # keep stable + fast on CPU\n        cfg[\"d_model\"] = min(int(cfg[\"d_model\"]), 256)\n        cfg[\"n_layers\"] = min(int(cfg[\"n_layers\"]), 6)\n        cfg[\"n_heads\"]  = min(int(cfg[\"n_heads\"]), 8)\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 256)\n        cfg[\"epochs\"] = min(int(cfg[\"epochs\"]), 35)\n        cfg[\"accum_steps\"] = 1\n    return cfg\n\ndef maybe_upscale_cfg(cfg: dict):\n    if not (device.type == \"cuda\" and ALLOW_UPSCALE and vram_gb is not None):\n        return dict(cfg)\n    cfg = dict(cfg)\n    # mild upscale only (optional)\n    if vram_gb >= 24:\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 512)\n        cfg[\"accum_steps\"] = max(int(cfg.get(\"accum_steps\", 2)), 2)\n    elif vram_gb >= 16:\n        cfg[\"batch_size\"] = min(int(cfg[\"batch_size\"]), 384)\n        cfg[\"accum_steps\"] = max(int(cfg.get(\"accum_steps\", 2)), 2)\n    return cfg\n\ndef fix_heads_divisibility(cfg: dict):\n    cfg = dict(cfg)\n    d = int(cfg[\"d_model\"])\n    h = int(cfg[\"n_heads\"])\n    if h < 1:\n        h = 1\n    if d % h == 0:\n        cfg[\"n_heads\"] = h\n        return cfg\n    # find largest divisor <= h\n    cand = []\n    for k in range(h, 0, -1):\n        if d % k == 0:\n            cand.append(k)\n            break\n    cfg[\"n_heads\"] = cand[0] if cand else 1\n    return cfg\n\nCFG = cpu_safe_cfg(CFG)\nCFG = maybe_upscale_cfg(CFG)\n\n# effective batch close to target\nCFG[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(CFG[\"batch_size\"]))))\n\n# final divisibility guard\nCFG = fix_heads_divisibility(CFG)\n\nprint(\"\\nCFG (final):\")\nfor k in [\n    \"d_model\",\"n_layers\",\"n_heads\",\"ffn_mult\",\"dropout\",\"attn_dropout\",\n    \"n_streams\",\"alpha_init\",\"sinkhorn_tmax\",\"mhc_dropout\",\n    \"batch_size\",\"accum_steps\",\"epochs\",\"lr\",\"weight_decay\",\"warmup_frac\",\"patience\"\n]:\n    print(f\"  {k}: {CFG[k]}\")\n\n# ----------------------------\n# 9) Internal val split (case_id-safe) + robust fallback\n# ----------------------------\ndef make_case_split(df: pd.DataFrame, val_frac=0.08, seed=2025):\n    g = df.groupby(\"case_id\")[\"y\"].max().reset_index().rename(columns={\"y\": \"case_y\"})\n    pos_cases = g.loc[g[\"case_y\"] == 1, \"case_id\"].astype(str).to_numpy()\n    neg_cases = g.loc[g[\"case_y\"] == 0, \"case_id\"].astype(str).to_numpy()\n\n    rng = np.random.RandomState(int(seed))\n    rng.shuffle(pos_cases)\n    rng.shuffle(neg_cases)\n\n    if len(pos_cases) == 0 or len(neg_cases) == 0:\n        # extreme case: cannot stratify by case; fallback to row stratified\n        idx = np.arange(len(df))\n        rng.shuffle(idx)\n        n_val = max(1, int(len(df) * float(val_frac)))\n        val_idx = idx[:n_val]\n        is_val = np.zeros(len(df), dtype=bool)\n        is_val[val_idx] = True\n        return is_val\n\n    n_val_pos = max(1, int(len(pos_cases) * float(val_frac)))\n    n_val_neg = max(1, int(len(neg_cases) * float(val_frac)))\n\n    val_cases = np.concatenate([pos_cases[:n_val_pos], neg_cases[:n_val_neg]])\n    val_set = set(val_cases.tolist())\n    is_val = df[\"case_id\"].astype(str).isin(val_set).to_numpy(dtype=bool)\n\n    # if too small / degenerate, fallback to row stratified\n    va_idx = np.where(is_val)[0]\n    if len(va_idx) < 32 or len(np.unique(df.loc[is_val, \"y\"].values)) < 2:\n        idx_pos = np.where(df[\"y\"].values == 1)[0]\n        idx_neg = np.where(df[\"y\"].values == 0)[0]\n        rng.shuffle(idx_pos); rng.shuffle(idx_neg)\n        n_val = max(32, int(len(df) * float(val_frac)))\n        n_val_pos = max(1, int(n_val * float(df[\"y\"].mean())))\n        n_val_pos = min(n_val_pos, len(idx_pos))\n        n_val_neg = min(n_val - n_val_pos, len(idx_neg))\n        val_idx = np.concatenate([idx_pos[:n_val_pos], idx_neg[:n_val_neg]])\n        is_val = np.zeros(len(df), dtype=bool)\n        is_val[val_idx] = True\n\n    return is_val\n\n# ----------------------------\n# 10) Training helpers (workers/amp/scaler)\n# ----------------------------\ndef make_loader(ds, batch_size, shuffle):\n    cpu_cnt = os.cpu_count() or 2\n    nw = 2 if cpu_cnt >= 4 else 0\n    pin = (device.type == \"cuda\")\n    return DataLoader(\n        ds,\n        batch_size=int(batch_size),\n        shuffle=bool(shuffle),\n        num_workers=nw,\n        pin_memory=pin,\n        drop_last=False,\n        persistent_workers=(nw > 0),\n    )\n\ndef build_loss_fn(y_for_pos_weight, cfg):\n    pos = float(np.sum(y_for_pos_weight))\n    neg = float(len(y_for_pos_weight) - pos)\n    pos_weight = torch.tensor([neg / max(1.0, pos)], device=device, dtype=torch.float32)\n\n    focal_gamma = float(cfg.get(\"focal_gamma\", 0.0))\n    label_smoothing = float(cfg.get(\"label_smoothing\", 0.0))\n\n    def loss_fn(logits, targets):\n        if label_smoothing and label_smoothing > 0:\n            targets = targets * (1.0 - label_smoothing) + 0.5 * label_smoothing\n        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\", pos_weight=pos_weight)\n        if focal_gamma and focal_gamma > 0:\n            p = torch.sigmoid(logits)\n            p_t = p * targets + (1 - p) * (1 - targets)\n            mod = (1.0 - p_t).clamp_min(0.0).pow(focal_gamma)\n            bce = bce * mod\n        return bce.mean()\n\n    return loss_fn\n\ndef build_model(cfg, n_features):\n    cfg = fix_heads_divisibility(cfg)\n    return FTTransformer_MHCLite(\n        n_features=int(n_features),\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n        n_streams=int(cfg[\"n_streams\"]),\n        alpha_init=float(cfg[\"alpha_init\"]),\n        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n        feat_token_drop_p=float(cfg.get(\"feat_token_drop_p\", 0.0)),\n    ).to(device)\n\ndef build_opt_and_sch(model, cfg, steps_per_epoch, epochs):\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n        eps=float(cfg[\"adam_eps\"]),\n    )\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    total_steps = int(max(1, int(epochs) * int(max(1, steps_per_epoch))))\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n    sch = make_warmup_step_scheduler(\n        opt,\n        total_steps=total_steps,\n        warmup_steps=warmup_steps,\n        r1=float(cfg[\"lr_decay_ratio1\"]),\n        r2=float(cfg[\"lr_decay_ratio2\"]),\n        d1=float(cfg[\"lr_decay_rate1\"]),\n        d2=float(cfg[\"lr_decay_rate2\"]),\n    )\n    return opt, sch\n\n# ----------------------------\n# 11) Train with internal val to get best_epoch\n# ----------------------------\ndef train_with_internal_val_get_best_epoch(X_raw, y_raw, cfg, seed=2025):\n    seed_everything(int(seed))\n\n    is_val = make_case_split(df_train_tabular, val_frac=float(VAL_FRAC_CASE), seed=int(seed))\n    tr_idx = np.where(~is_val)[0]\n    va_idx = np.where(is_val)[0]\n\n    X_tr, y_tr = X_raw[tr_idx], y_raw[tr_idx]\n    X_va, y_va = X_raw[va_idx], y_raw[va_idx]\n\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    dl_tr = make_loader(ds_tr, cfg[\"batch_size\"], shuffle=True)\n    dl_va = make_loader(ds_va, cfg[\"batch_size\"], shuffle=False)\n\n    model = build_model(cfg, n_features=X_raw.shape[1])\n\n    loss_fn = build_loss_fn(y_tr, cfg)\n\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl_tr) / accum))\n    opt, sch = build_opt_and_sch(model, cfg, steps_per_epoch=steps_per_epoch, epochs=int(cfg[\"epochs\"]))\n\n    scaler = torch.cuda.amp.GradScaler(enabled=True) if use_amp else None\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    best_val = 1e18\n    best_epoch = -1\n    bad = 0\n\n    print(f\"\\nInternal val split: train={len(tr_idx)} | val={len(va_idx)} | val_pos%={float(y_va.mean())*100:.2f}\")\n\n    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum = 0.0\n        n_sum = 0\n        micro = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            with ctx:\n                logits = model(xb)\n                loss = loss_fn(logits, yb) / float(accum)\n\n            if use_amp:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n            n_sum += xb.size(0)\n\n            if (micro % accum) == 0:\n                if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                    if use_amp:\n                        scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n\n                if use_amp:\n                    scaler.step(opt)\n                    scaler.update()\n                else:\n                    opt.step()\n\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro % accum) != 0:\n            if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                if use_amp:\n                    scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n\n            if use_amp:\n                scaler.step(opt)\n                scaler.update()\n            else:\n                opt.step()\n\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        p_va = predict_proba(model, dl_va, ema=ema)\n        vll = safe_logloss(y_va, p_va)\n        vauc = safe_auc(y_va, p_va)\n\n        improved = (best_val - vll) > float(cfg[\"min_delta\"])\n        if improved:\n            best_val = float(vll)\n            best_epoch = int(epoch) + 1\n            bad = 0\n        else:\n            bad += 1\n\n        print(f\"  epoch {epoch+1:03d}/{int(cfg['epochs'])} | tr_loss={loss_sum/max(1,n_sum):.5f} | val_ll={vll:.5f} | val_auc={(vauc if vauc is not None else float('nan')):.5f} | bad={bad}\")\n\n        if EARLY_STOP and bad >= int(cfg[\"patience\"]):\n            break\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    if best_epoch < 1:\n        best_epoch = max(12, int(cfg[\"epochs\"] * 0.6))\n\n    return {\n        \"best_epoch\": int(best_epoch),\n        \"best_val_logloss\": float(best_val) if best_val < 1e18 else None,\n    }\n\n# ----------------------------\n# 12) Train FULL data for fixed epochs (best_epoch)\n# ----------------------------\ndef train_full_fixed_epochs(X_raw, y_raw, cfg, epochs_fixed, seed=2025):\n    seed_everything(int(seed))\n\n    mu, sig = fit_standardizer(X_raw)\n    Xn = apply_standardizer(X_raw, mu, sig)\n\n    ds = TabDataset(Xn, y_raw)\n    dl = make_loader(ds, cfg[\"batch_size\"], shuffle=True)\n\n    model = build_model(cfg, n_features=X_raw.shape[1])\n    loss_fn = build_loss_fn(y_raw, cfg)\n\n    accum = max(1, int(cfg.get(\"accum_steps\", 1)))\n    steps_per_epoch = int(math.ceil(len(dl) / accum))\n    opt, sch = build_opt_and_sch(model, cfg, steps_per_epoch=steps_per_epoch, epochs=int(epochs_fixed))\n\n    scaler = torch.cuda.amp.GradScaler(enabled=True) if use_amp else None\n    ema = EMA(model, decay=float(cfg.get(\"ema_decay\", 0.999))) if bool(cfg.get(\"use_ema\", True)) else None\n    input_noise_std = float(cfg.get(\"input_noise_std\", 0.0))\n\n    t0 = time.time()\n    ctx = torch.cuda.amp.autocast(enabled=True) if use_amp else nullcontext()\n\n    for epoch in range(int(epochs_fixed)):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        loss_sum = 0.0\n        n_sum = 0\n        micro = 0\n\n        for xb, yb in dl:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            if input_noise_std and input_noise_std > 0:\n                xb = xb + torch.randn_like(xb) * input_noise_std\n\n            with ctx:\n                logits = model(xb)\n                loss = loss_fn(logits, yb) / float(accum)\n\n            if use_amp:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n\n            micro += 1\n            loss_sum += float(loss.item()) * xb.size(0) * float(accum)\n            n_sum += xb.size(0)\n\n            if (micro % accum) == 0:\n                if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                    if use_amp:\n                        scaler.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n\n                if use_amp:\n                    scaler.step(opt)\n                    scaler.update()\n                else:\n                    opt.step()\n\n                opt.zero_grad(set_to_none=True)\n                sch.step()\n                if ema is not None:\n                    ema.update(model)\n\n        # flush last partial\n        if (micro % accum) != 0:\n            if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                if use_amp:\n                    scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n\n            if use_amp:\n                scaler.step(opt)\n                scaler.update()\n            else:\n                opt.step()\n\n            opt.zero_grad(set_to_none=True)\n            sch.step()\n            if ema is not None:\n                ema.update(model)\n\n        print(f\"  full epoch {epoch+1:03d}/{int(epochs_fixed)} | loss={loss_sum/max(1,n_sum):.5f}\")\n\n        gc.collect()\n        if device.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n    # save EMA weights if enabled\n    if ema is not None:\n        ema.apply_shadow(model)\n\n    pack = {\n        \"type\": \"mhc_lite_ft_transformer_full_v4\",\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": dict(cfg),\n        \"seed\": int(seed),\n        \"train_rows\": int(len(y_raw)),\n        \"pos_rate\": float(np.mean(y_raw)),\n        \"epochs_fixed\": int(epochs_fixed),\n        \"accum_steps\": int(accum),\n        \"train_time_s\": float(time.time() - t0),\n        \"used_ema_weights\": bool(ema is not None),\n    }\n    return pack\n\n# ----------------------------\n# 13) OOM fallback policy\n# ----------------------------\ndef apply_oom_fallback(cfg: dict):\n    cfg = dict(cfg)\n\n    # 1) reduce batch first\n    cfg[\"batch_size\"] = max(64, int(cfg[\"batch_size\"]) // 2)\n\n    # recompute accum to keep eff batch\n    cfg[\"accum_steps\"] = max(1, int(math.ceil(TARGET_EFF_BATCH / int(cfg[\"batch_size\"]))))\n\n    # 2) reduce width/depth gradually\n    cfg[\"d_model\"] = max(256, int(cfg[\"d_model\"]) - 64)\n    cfg[\"n_layers\"] = max(6, int(cfg[\"n_layers\"]) - 2)\n\n    # 3) fix heads divisibility (also may reduce heads)\n    cfg = fix_heads_divisibility(cfg)\n\n    return cfg\n\n# ----------------------------\n# 14) Train final (OOM-safe)\n# ----------------------------\nfinal_full_packs = []\ninternal_val_infos = []\n\nfor s in range(int(N_SEEDS)):\n    seed_i = FINAL_SEED + s\n    print(f\"\\n[Final Train v4.1] seed={seed_i}\")\n\n    cfg_run = dict(CFG)\n\n    # retry loop for OOM\n    for attempt in range(6):\n        try:\n            # Phase A: get best_epoch from internal val\n            if USE_INTERNAL_VAL:\n                info = train_with_internal_val_get_best_epoch(X_all, y_all, cfg_run, seed=seed_i)\n                best_epoch = int(info[\"best_epoch\"])\n                internal_val_infos.append({\"seed\": seed_i, **info, \"cfg_used\": dict(cfg_run)})\n\n                # retrain full data (slight +5% for stability, capped by cfg_run[\"epochs\"])\n                E_FULL = int(min(int(cfg_run[\"epochs\"]), max(12, round(best_epoch * 1.05))))\n                print(f\"\\nBest_epoch(from internal val)={best_epoch} -> Retrain FULL for E_FULL={E_FULL}\")\n            else:\n                E_FULL = int(cfg_run[\"epochs\"])\n\n            full_pack = train_full_fixed_epochs(X_all, y_all, cfg_run, epochs_fixed=E_FULL, seed=seed_i)\n            final_full_packs.append(full_pack)\n            break\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n            if (\"out of memory\" in msg) and device.type == \"cuda\":\n                print(f\"  OOM detected (attempt {attempt+1}). Applying fallback.\")\n                torch.cuda.empty_cache()\n                cfg_run = apply_oom_fallback(cfg_run)\n                print(\"  New CFG after fallback:\")\n                for k in [\"d_model\",\"n_layers\",\"n_heads\",\"batch_size\",\"accum_steps\",\"epochs\"]:\n                    print(f\"    {k}: {cfg_run[k]}\")\n                continue\n            raise\n\n    gc.collect()\n    if device.type == \"cuda\":\n        torch.cuda.empty_cache()\n\nif len(final_full_packs) == 0:\n    raise RuntimeError(\"Final training failed: no full_packs produced.\")\n\n# ----------------------------\n# 15) Save artifacts\n# ----------------------------\nfinal_model_path = OUT_DIR / \"final_gate_model.pt\"\n\n# threshold recommended from Step 4 (OOF)\nbest_thr = None\nif isinstance(best_bundle, dict):\n    best_thr = best_bundle.get(\"oof_best_thr\", None)\n\ntorch.save(\n    {\n        \"type\": \"final_gate_v4_1\",\n        \"feature_cols\": FEATURE_COLS,\n\n        # keep fold ensemble from Step 4 if available\n        \"fold_packs\": fold_packs_from_step4,\n\n        # full-data trained packs (list; usually 1 seed)\n        \"full_packs\": final_full_packs,\n\n        # training meta\n        \"internal_val_infos\": internal_val_infos,\n        \"recommended_thr\": best_thr,\n        \"bundle_source\": source,\n        \"seed_base\": int(FINAL_SEED),\n    },\n    final_model_path\n)\n\nfinal_bundle = {\n    \"type\": \"final_gate_v4_1\",\n    \"feature_cols\": FEATURE_COLS,\n    \"n_seeds\": int(len(final_full_packs)),\n    \"seeds\": [int(p[\"seed\"]) for p in final_full_packs],\n    \"cfg_final\": dict(CFG),\n\n    \"use_internal_val\": bool(USE_INTERNAL_VAL),\n    \"val_frac_case\": float(VAL_FRAC_CASE) if USE_INTERNAL_VAL else 0.0,\n    \"early_stop\": bool(EARLY_STOP) if USE_INTERNAL_VAL else False,\n\n    \"train_rows\": int(len(y_all)),\n    \"pos_rate\": float(np.mean(y_all)),\n\n    \"recommended_thr\": best_thr,\n    \"has_fold_packs_from_step4\": bool(fold_packs_from_step4 is not None),\n    \"best_cfg_source\": source,\n    \"notes\": \"Final model uses Step4-matched mHC-lite FTTransformer with EMA+accum; best_epoch estimated via internal case-level val then retrain full.\",\n}\n\nfinal_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\nfinal_bundle_path.write_text(json.dumps(final_bundle, indent=2))\n\nprint(\"\\nSaved final training artifacts:\")\nprint(\"  model  ->\", final_model_path)\nprint(\"  bundle ->\", final_bundle_path)\n\n# Export globals\nFINAL_GATE_MODEL_PT = str(final_model_path)\nFINAL_GATE_BUNDLE = final_bundle\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finalize & Save Model Bundle (Reproducible)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 6 — Finalize & Save Model Bundle (Notebook-3 / Inference Notebook)\n# REVISI FULL v5.1 (portable + robust source discovery + copies core files)\n#\n# Goals:\n# - READ artifacts from /kaggle/input/... (read-only) OR /kaggle/working/... (if exists)\n# - AUTO-find the *actual* bundle folder (supports nested model_bundle_v*/ etc.)\n# - COPY core files into writable bundle folder:\n#     /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v5_notebook3/\n# - WRITE thresholds/manifest/pack + ZIP to that bundle folder\n# - Compatible with Step 5 v4+:\n#     final_gate_model.pt may contain {fold_packs, full_packs, recommended_thr}\n# ============================================================\n\nimport os, json, time, platform, warnings, zipfile, hashlib, shutil\nfrom pathlib import Path\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# 0) IO roots\n# ----------------------------\nOUT_ROOT = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_ROOT.mkdir(parents=True, exist_ok=True)\n\nBUNDLE_VERSION = \"v5_notebook3\"\nOUT_DIR = OUT_ROOT / f\"model_bundle_{BUNDLE_VERSION}\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"OUTPUT (write):\", OUT_DIR)\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef read_json_safe(p: Path, default=None):\n    try:\n        return json.loads(Path(p).read_text())\n    except Exception:\n        return default\n\ndef sha256_file(p: Path, chunk=1024 * 1024):\n    p = Path(p)\n    h = hashlib.sha256()\n    with p.open(\"rb\") as f:\n        while True:\n            b = f.read(chunk)\n            if not b:\n                break\n            h.update(b)\n    return h.hexdigest()\n\ndef file_meta(p: Path):\n    p = Path(p)\n    if not p.exists() or not p.is_file():\n        return None\n    st = p.stat()\n    return {\n        \"path\": str(p),\n        \"name\": p.name,\n        \"bytes\": int(st.st_size),\n        \"sha256\": sha256_file(p),\n        \"mtime_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(st.st_mtime)),\n    }\n\ndef safe_add(zf: zipfile.ZipFile, p: Path, arcname: str):\n    if p is None:\n        return\n    p = Path(p)\n    if p.exists() and p.is_file():\n        zf.write(p, arcname=arcname)\n\ndef pick_first_existing(paths):\n    for p in paths:\n        if p is None:\n            continue\n        p = Path(p)\n        if p.exists() and p.is_file():\n            return p\n    return None\n\ndef find_file_near(root: Path, filename: str, max_depth: int = 3):\n    \"\"\"\n    Search root, then 1..max_depth levels deep for a file named filename.\n    Cheap bounded search (no full rglob).\n    \"\"\"\n    root = Path(root)\n    if (root / filename).exists():\n        return root / filename\n\n    # depth 1\n    for p1 in root.glob(\"*\"):\n        if p1.is_dir() and (p1 / filename).exists():\n            return p1 / filename\n\n    if max_depth >= 2:\n        for p2 in root.glob(\"*/*\"):\n            if p2.is_dir() and (p2 / filename).exists():\n                return p2 / filename\n\n    if max_depth >= 3:\n        for p3 in root.glob(\"*/*/*\"):\n            if p3.is_dir() and (p3 / filename).exists():\n                return p3 / filename\n\n    return None\n\ndef copy_if_needed(src: Path, dst: Path, verbose=True):\n    src, dst = Path(src), Path(dst)\n    dst.parent.mkdir(parents=True, exist_ok=True)\n    if not src.exists():\n        raise FileNotFoundError(f\"Missing src: {src}\")\n\n    if dst.exists() and dst.is_file():\n        try:\n            if src.stat().st_size == dst.stat().st_size:\n                # optional fast check: same size; verify by sha if you want strict\n                # strict sha check (safe):\n                if sha256_file(src) == sha256_file(dst):\n                    if verbose:\n                        print(\"  [skip copy] already identical:\", dst.name)\n                    return dst\n        except Exception:\n            pass\n\n    shutil.copy2(src, dst)\n    if verbose:\n        print(\"  [copied] ->\", dst)\n    return dst\n\ndef _score_dir(p: Path):\n    \"\"\"\n    Score candidate directory based on presence of key artifacts directly inside.\n    \"\"\"\n    p = Path(p)\n    score = 0\n    if (p / \"final_gate_model.pt\").exists(): score += 100\n    if (p / \"final_gate_bundle.json\").exists(): score += 30\n    if (p / \"feature_cols.json\").exists(): score += 20\n    if (p / \"thresholds.json\").exists(): score += 5\n    if (p / \"best_gate_config.json\").exists(): score += 3\n    if (p / \"best_gate_model.pt\").exists(): score += 3\n    if (p / \"opt_search\" / \"opt_results.csv\").exists(): score += 1\n    return score\n\ndef _gather_candidate_dirs():\n    \"\"\"\n    Collect candidate directories that *might* contain final_gate_model.pt.\n    Supports nested structures like:\n      /kaggle/input/<ds>/recodai_luc_gate_artifacts/model_bundle_v3/final_gate_model.pt\n    \"\"\"\n    cands = []\n\n    # Working roots (if you ran training in same notebook)\n    for base in [\n        Path(\"/kaggle/working/recodai_luc_gate_artifacts\"),\n        OUT_ROOT,  # current bundle root too\n    ]:\n        if base.exists():\n            cands.append(base)\n            # common nested\n            for sub in base.glob(\"model_bundle_*\"):\n                if sub.is_dir():\n                    cands.append(sub)\n\n    # Inputs (read-only datasets)\n    inp = Path(\"/kaggle/input\")\n    if inp.exists():\n        for ds in inp.iterdir():\n            if not ds.is_dir():\n                continue\n\n            # bounded-depth directory walk: ds, ds/*, ds/*/*, ds/*/*/*\n            level0 = [ds]\n            level1 = [p for p in ds.glob(\"*\") if p.is_dir()]\n            level2 = [p for p in ds.glob(\"*/*\") if p.is_dir()]\n            level3 = [p for p in ds.glob(\"*/*/*\") if p.is_dir()]\n            for d in (level0 + level1 + level2 + level3):\n                # if directory name hints artifacts, include it\n                if d.name == \"recodai_luc_gate_artifacts\" or \"recodai\" in d.name.lower() or \"bundle\" in d.name.lower():\n                    cands.append(d)\n                # if it directly contains final model, include it\n                if (d / \"final_gate_model.pt\").exists():\n                    cands.append(d)\n                # if it's the artifacts root, also include common nested bundles\n                if d.name == \"recodai_luc_gate_artifacts\":\n                    for sub in d.glob(\"model_bundle_*\"):\n                        if sub.is_dir():\n                            cands.append(sub)\n\n    # de-dup preserve order\n    seen = set()\n    out = []\n    for p in cands:\n        p = Path(p)\n        sp = str(p)\n        if sp not in seen and p.exists() and p.is_dir():\n            seen.add(sp)\n            out.append(p)\n    return out\n\n# ----------------------------\n# 1) Pick best SRC_DIR\n# ----------------------------\nSRC_CANDS = _gather_candidate_dirs()\nif len(SRC_CANDS) == 0:\n    raise FileNotFoundError(\n        \"Tidak menemukan kandidat folder artifacts di /kaggle/working maupun /kaggle/input.\\n\"\n        \"Pastikan kamu sudah add dataset output training ke notebook ini.\"\n    )\n\n# If none has high score, we still pick best and do a deeper file search below.\nSRC_DIR = max(SRC_CANDS, key=_score_dir)\n\nprint(\"\\nSOURCE candidate picked:\", SRC_DIR)\nprint(\"Score:\", _score_dir(SRC_DIR))\n\n# ----------------------------\n# 2) Locate required artifacts (robust)\n# ----------------------------\nfinal_model_pt = find_file_near(SRC_DIR, \"final_gate_model.pt\", max_depth=3)\nif final_model_pt is None:\n    raise FileNotFoundError(\n        \"Missing final_gate_model.pt in selected SRC_DIR (even after nested search).\\n\"\n        f\"SRC_DIR={SRC_DIR}\\n\"\n        \"Cek struktur dataset input kamu (mungkin bundle ada lebih dalam).\"\n    )\n\n# For bundle json and feature cols: search near final_model folder first (more reliable)\nMODEL_DIR = final_model_pt.parent\n\nfinal_bundle_json = find_file_near(MODEL_DIR, \"final_gate_bundle.json\", max_depth=2)\nfeature_cols_path = find_file_near(MODEL_DIR, \"feature_cols.json\", max_depth=2)\n\n# Fallback: search near SRC_DIR if still missing\nif final_bundle_json is None:\n    final_bundle_json = find_file_near(SRC_DIR, \"final_gate_bundle.json\", max_depth=3)\nif feature_cols_path is None:\n    feature_cols_path = find_file_near(SRC_DIR, \"feature_cols.json\", max_depth=3)\n\nif feature_cols_path is None:\n    raise FileNotFoundError(\n        \"Missing feature_cols.json (jalankan Step 2 dulu di training notebook / pastikan ikut dibundle).\"\n    )\n\n# Optional / extras\nbaseline_report_path = pick_first_existing([\n    find_file_near(MODEL_DIR, \"baseline_mhc_transformer_cv_report.json\", max_depth=2),\n    find_file_near(MODEL_DIR, \"baseline_transformer_cv_report.json\", max_depth=2),\n    find_file_near(MODEL_DIR, \"baseline_cv_report.json\", max_depth=2),\n    find_file_near(SRC_DIR,   \"baseline_mhc_transformer_cv_report.json\", max_depth=3),\n    find_file_near(SRC_DIR,   \"baseline_transformer_cv_report.json\", max_depth=3),\n    find_file_near(SRC_DIR,   \"baseline_cv_report.json\", max_depth=3),\n])\n\nbest_gate_config_path = pick_first_existing([\n    find_file_near(MODEL_DIR, \"best_gate_config.json\", max_depth=2),\n    find_file_near(SRC_DIR,   \"best_gate_config.json\", max_depth=3),\n])\n\nbest_gate_model_path = pick_first_existing([\n    find_file_near(MODEL_DIR, \"best_gate_model.pt\", max_depth=2),\n    find_file_near(SRC_DIR,   \"best_gate_model.pt\", max_depth=3),\n])\n\nopt_results_csv = pick_first_existing([\n    find_file_near(MODEL_DIR, \"opt_results.csv\", max_depth=3),\n    find_file_near(SRC_DIR,   \"opt_results.csv\", max_depth=3),\n    (MODEL_DIR / \"opt_search\" / \"opt_results.csv\") if (MODEL_DIR / \"opt_search\" / \"opt_results.csv\").exists() else None,\n    (SRC_DIR   / \"opt_search\" / \"opt_results.csv\") if (SRC_DIR   / \"opt_search\" / \"opt_results.csv\").exists() else None,\n])\n\nopt_fold_csv = pick_first_existing([\n    find_file_near(MODEL_DIR, \"opt_fold_details.csv\", max_depth=3),\n    find_file_near(SRC_DIR,   \"opt_fold_details.csv\", max_depth=3),\n    (MODEL_DIR / \"opt_search\" / \"opt_fold_details.csv\") if (MODEL_DIR / \"opt_search\" / \"opt_fold_details.csv\").exists() else None,\n    (SRC_DIR   / \"opt_search\" / \"opt_fold_details.csv\") if (SRC_DIR   / \"opt_search\" / \"opt_fold_details.csv\").exists() else None,\n])\n\noof_baseline_csv = pick_first_existing([\n    find_file_near(MODEL_DIR, \"oof_baseline_mhc_transformer.csv\", max_depth=2),\n    find_file_near(MODEL_DIR, \"oof_baseline_transformer.csv\", max_depth=2),\n    find_file_near(MODEL_DIR, \"oof_baseline.csv\", max_depth=2),\n    find_file_near(SRC_DIR,   \"oof_baseline_mhc_transformer.csv\", max_depth=3),\n    find_file_near(SRC_DIR,   \"oof_baseline_transformer.csv\", max_depth=3),\n    find_file_near(SRC_DIR,   \"oof_baseline.csv\", max_depth=3),\n])\n\nprint(\"\\nFound artifacts (read):\")\nprint(\"  final_model        :\", final_model_pt)\nprint(\"  final_bundle       :\", final_bundle_json if (final_bundle_json and final_bundle_json.exists()) else \"(missing/skip)\")\nprint(\"  feature_cols       :\", feature_cols_path)\nprint(\"  best_gate_config   :\", best_gate_config_path if best_gate_config_path else \"(missing/skip)\")\nprint(\"  best_gate_model    :\", best_gate_model_path if best_gate_model_path else \"(missing/skip)\")\nprint(\"  baseline_report    :\", baseline_report_path if baseline_report_path else \"(missing/skip)\")\nprint(\"  opt_results_csv    :\", opt_results_csv if opt_results_csv else \"(missing/skip)\")\nprint(\"  opt_fold_csv       :\", opt_fold_csv if opt_fold_csv else \"(missing/skip)\")\nprint(\"  oof_baseline_csv   :\", oof_baseline_csv if oof_baseline_csv else \"(missing/skip)\")\n\n# ----------------------------\n# 3) COPY core artifacts into OUT_DIR (portable bundle)\n# ----------------------------\nprint(\"\\nCopying core files into OUT_DIR (portable):\")\nfinal_model_dst = copy_if_needed(final_model_pt, OUT_DIR / \"final_gate_model.pt\")\nfinal_bundle_dst = None\nif final_bundle_json is not None and final_bundle_json.exists():\n    final_bundle_dst = copy_if_needed(final_bundle_json, OUT_DIR / \"final_gate_bundle.json\")\nfeature_cols_dst = copy_if_needed(feature_cols_path, OUT_DIR / \"feature_cols.json\")\n\n# Copy extras (optional, but useful)\nextras_dir = OUT_DIR / \"extras\"\nextras_dir.mkdir(parents=True, exist_ok=True)\n\nbaseline_report_dst = None\nif baseline_report_path:\n    baseline_report_dst = copy_if_needed(baseline_report_path, extras_dir / Path(baseline_report_path).name, verbose=False)\n\nbest_gate_config_dst = None\nif best_gate_config_path:\n    best_gate_config_dst = copy_if_needed(best_gate_config_path, extras_dir / Path(best_gate_config_path).name, verbose=False)\n\nbest_gate_model_dst = None\nif best_gate_model_path:\n    best_gate_model_dst = copy_if_needed(best_gate_model_path, extras_dir / Path(best_gate_model_path).name, verbose=False)\n\nopt_dir = OUT_DIR / \"opt_search\"\nopt_dir.mkdir(parents=True, exist_ok=True)\nopt_results_dst = None\nif opt_results_csv:\n    opt_results_dst = copy_if_needed(opt_results_csv, opt_dir / Path(opt_results_csv).name, verbose=False)\n\nopt_fold_dst = None\nif opt_fold_csv:\n    opt_fold_dst = copy_if_needed(opt_fold_csv, opt_dir / Path(opt_fold_csv).name, verbose=False)\n\noof_dir = OUT_DIR / \"oof\"\noof_dir.mkdir(parents=True, exist_ok=True)\noof_baseline_dst = None\nif oof_baseline_csv:\n    oof_baseline_dst = copy_if_needed(oof_baseline_csv, oof_dir / Path(oof_baseline_csv).name, verbose=False)\n\n# ----------------------------\n# 4) Load metadata (from copied files)\n# ----------------------------\nfeature_cols = read_json_safe(feature_cols_dst, default=[])\nif not isinstance(feature_cols, list) or len(feature_cols) == 0:\n    raise ValueError(f\"feature_cols invalid/empty: {feature_cols_dst}\")\n\nfinal_bundle = read_json_safe(final_bundle_dst, default={}) if final_bundle_dst else {}\nbaseline_report = read_json_safe(baseline_report_dst, default=None) if baseline_report_dst else None\nbest_gate_config = read_json_safe(best_gate_config_dst, default=None) if best_gate_config_dst else None\n\n# recommended_thr from final_gate_model.pt\nrecommended_thr_from_pt = None\ntry:\n    import torch\n    obj = torch.load(final_model_dst, map_location=\"cpu\")\n    if isinstance(obj, dict):\n        recommended_thr_from_pt = obj.get(\"recommended_thr\", None)\nexcept Exception as e:\n    print(\"Warning: failed to read final_gate_model.pt for recommended_thr:\", repr(e))\n\n# ----------------------------\n# 5) Thresholds resolve (robust priority)\n# Priority:\n#   (a) SRC thresholds.json near original model dir\n#   (b) existing OUT thresholds.json (if rerun)\n#   (c) final_gate_bundle.json -> recommended_thr\n#   (d) final_gate_model.pt -> recommended_thr\n#   (e) best_gate_config.json -> oof_best_thr OR selection.oof_best_thr (legacy)\n#   (f) fallback 0.5\n# ----------------------------\ndef extract_thr_from_best_gate_config(cfg: dict):\n    if not isinstance(cfg, dict):\n        return None\n    if \"oof_best_thr\" in cfg:\n        try:\n            return float(cfg[\"oof_best_thr\"])\n        except Exception:\n            pass\n    sel = cfg.get(\"selection\", None)\n    if isinstance(sel, dict) and (\"oof_best_thr\" in sel):\n        try:\n            return float(sel[\"oof_best_thr\"])\n        except Exception:\n            pass\n    return None\n\nT_gate = None\nsrc_thresh = find_file_near(MODEL_DIR, \"thresholds.json\", max_depth=2) or find_file_near(SRC_DIR, \"thresholds.json\", max_depth=3)\nout_thresh = OUT_DIR / \"thresholds.json\"\n\n# (a) SRC thresholds.json\nif src_thresh and src_thresh.exists():\n    tj = read_json_safe(src_thresh, default={})\n    if isinstance(tj, dict) and tj.get(\"T_gate\", None) is not None:\n        try:\n            T_gate = float(tj[\"T_gate\"])\n        except Exception:\n            T_gate = None\n\n# (b) OUT thresholds.json (existing)\nif T_gate is None and out_thresh.exists():\n    tj = read_json_safe(out_thresh, default={})\n    if isinstance(tj, dict) and tj.get(\"T_gate\", None) is not None:\n        try:\n            T_gate = float(tj[\"T_gate\"])\n        except Exception:\n            T_gate = None\n\n# (c) final_bundle recommended_thr\nif T_gate is None and isinstance(final_bundle, dict):\n    try:\n        if final_bundle.get(\"recommended_thr\", None) is not None:\n            T_gate = float(final_bundle[\"recommended_thr\"])\n    except Exception:\n        T_gate = None\n\n# (d) final_model.pt recommended_thr\nif T_gate is None and recommended_thr_from_pt is not None:\n    try:\n        T_gate = float(recommended_thr_from_pt)\n    except Exception:\n        T_gate = None\n\n# (e) best_gate_config oof_best_thr\nif T_gate is None and isinstance(best_gate_config, dict):\n    T_gate = extract_thr_from_best_gate_config(best_gate_config)\n\n# (f) fallback\nif T_gate is None:\n    T_gate = 0.5\n\nthresholds = {\n    \"T_gate\": float(T_gate),\n    \"beta_for_tuning\": float(best_gate_config.get(\"beta_for_tuning\", 0.5)) if isinstance(best_gate_config, dict) else 0.5,\n    \"guards\": {\"min_area_frac\": None, \"max_area_frac\": None, \"max_components\": None},\n    \"source_priority\": [\n        \"SRC thresholds.json (near model)\",\n        \"OUT_DIR/thresholds.json (existing)\",\n        \"final_gate_bundle.json.recommended_thr\",\n        \"final_gate_model.pt.recommended_thr\",\n        \"best_gate_config.json.oof_best_thr (or selection.oof_best_thr legacy)\",\n        \"fallback 0.5\",\n    ],\n    \"notes\": \"Gate threshold used for binary decision. Update after calibration/OOF tuning if needed.\",\n}\n\nthresholds_path = OUT_DIR / \"thresholds.json\"\nthresholds_path.write_text(json.dumps(thresholds, indent=2))\n\nprint(\"\\nThreshold resolved:\")\nprint(\"  T_gate:\", thresholds[\"T_gate\"])\nprint(\"  thresholds.json ->\", thresholds_path)\n\n# ----------------------------\n# 6) cfg_meta (optional)\n# ----------------------------\ncfg_meta = {}\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    cfg_meta = {k: PATHS.get(k, None) for k in [\n        \"COMP_ROOT\",\"OUT_DS_ROOT\",\"OUT_ROOT\",\"MATCH_CFG_DIR\",\"PRED_CFG_DIR\",\"DINO_CFG_DIR\",\"DINO_LARGE_DIR\",\n        \"PRED_FEAT_TRAIN\",\"MATCH_FEAT_TRAIN\",\"DF_TRAIN_ALL\",\"CV_CASE_FOLDS\",\"IMG_PROFILE_TRAIN\"\n    ]}\n\n# ----------------------------\n# 7) Manifest (reproducible) + sha256 index (for OUT_DIR files)\n# ----------------------------\ntask_str = \"Recod.ai/LUC — Gate Model — DINOv2 features + Transformer gate (.pt)\"\nmodel_format = \"torch_pt\"\n\nartifact_paths = [\n    final_model_dst,\n    final_bundle_dst if final_bundle_dst else None,\n    feature_cols_dst,\n    thresholds_path,\n    baseline_report_dst,\n    best_gate_config_dst,\n    best_gate_model_dst,\n    opt_results_dst,\n    opt_fold_dst,\n    oof_baseline_dst,\n]\nartifact_paths = [p for p in artifact_paths if p is not None]\n\nartifacts_meta = {}\nfor p in artifact_paths:\n    m = file_meta(p)\n    if m is not None:\n        artifacts_meta[m[\"name\"]] = m\n\nopt_summary = None\nif isinstance(best_gate_config, dict):\n    opt_summary = best_gate_config.get(\"selection\", None)\n    if opt_summary is None:\n        opt_summary = {\n            \"model_name\": best_gate_config.get(\"model_name\", None),\n            \"oof_best_thr\": best_gate_config.get(\"oof_best_thr\", None),\n            \"oof_best_fbeta\": best_gate_config.get(\"oof_best_fbeta\", None),\n            \"oof_auc\": best_gate_config.get(\"oof_auc\", None),\n            \"oof_logloss\": best_gate_config.get(\"oof_logloss\", None),\n        }\n\nmanifest = {\n    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"python\": platform.python_version(),\n    \"platform\": platform.platform(),\n    \"bundle_version\": BUNDLE_VERSION,\n    \"task\": task_str,\n    \"model_format\": model_format,\n    \"source_dir_selected\": str(SRC_DIR),\n    \"source_model_dir\": str(MODEL_DIR),\n    \"output_dir\": str(OUT_DIR),\n    \"artifacts_index\": artifacts_meta,\n    \"cfg_meta\": cfg_meta,\n    \"model_summary\": {\n        \"type\": (final_bundle.get(\"type\") if isinstance(final_bundle, dict) else None),\n        \"n_seeds\": (final_bundle.get(\"n_seeds\") if isinstance(final_bundle, dict) else None),\n        \"seeds\": (final_bundle.get(\"seeds\") if isinstance(final_bundle, dict) else None),\n        \"train_rows\": (final_bundle.get(\"train_rows\") if isinstance(final_bundle, dict) else None),\n        \"pos_rate\": (final_bundle.get(\"pos_rate\") if isinstance(final_bundle, dict) else None),\n        \"feature_count\": int(len(feature_cols)),\n        \"T_gate\": float(thresholds.get(\"T_gate\", 0.5)),\n        \"recommended_thr_from_pt\": recommended_thr_from_pt,\n    },\n    \"baseline_summary\": (baseline_report.get(\"overall\") if isinstance(baseline_report, dict) else None),\n    \"opt_summary\": opt_summary,\n}\n\nmanifest_path = OUT_DIR / \"model_bundle_manifest.json\"\nmanifest_path.write_text(json.dumps(manifest, indent=2))\n\n# ----------------------------\n# 8) Bundle pack (portable JSON) + optional joblib\n# ----------------------------\nbundle_pack = {\n    \"bundle_version\": BUNDLE_VERSION,\n    \"model_format\": model_format,\n    \"bundle_files\": {\n        \"final_gate_model.pt\": \"final_gate_model.pt\",\n        \"final_gate_bundle.json\": \"final_gate_bundle.json\" if final_bundle_dst else None,\n        \"feature_cols.json\": \"feature_cols.json\",\n        \"thresholds.json\": \"thresholds.json\",\n        \"model_bundle_manifest.json\": \"model_bundle_manifest.json\",\n        \"model_bundle_pack.json\": \"model_bundle_pack.json\",\n        \"model_bundle_pack.joblib\": \"model_bundle_pack.joblib\",\n        \"model_bundle_v5_notebook3.zip\": \"model_bundle_v5_notebook3.zip\",\n    },\n    \"thresholds\": thresholds,\n    \"feature_cols\": feature_cols,\n    \"cfg_meta\": cfg_meta,\n    \"manifest\": manifest,\n}\n\nbundle_pack_json = OUT_DIR / \"model_bundle_pack.json\"\nbundle_pack_json.write_text(json.dumps(bundle_pack, indent=2))\n\nbundle_pack_joblib = OUT_DIR / \"model_bundle_pack.joblib\"\njoblib_ok = False\ntry:\n    import joblib\n    joblib.dump(bundle_pack, bundle_pack_joblib)\n    joblib_ok = True\nexcept Exception:\n    joblib_ok = False\n\n# ----------------------------\n# 9) Create portable ZIP (writes to OUT_DIR)\n# ----------------------------\nzip_path = OUT_DIR / \"model_bundle_v5_notebook3.zip\"\n\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n    # core files (from OUT_DIR)\n    safe_add(zf, final_model_dst, \"final_gate_model.pt\")\n    safe_add(zf, final_bundle_dst, \"final_gate_bundle.json\")\n    safe_add(zf, feature_cols_dst, \"feature_cols.json\")\n    safe_add(zf, thresholds_path, \"thresholds.json\")\n    safe_add(zf, manifest_path, \"model_bundle_manifest.json\")\n    safe_add(zf, bundle_pack_json, \"model_bundle_pack.json\")\n    if joblib_ok:\n        safe_add(zf, bundle_pack_joblib, \"model_bundle_pack.joblib\")\n\n    # extras (already copied into OUT_DIR)\n    for p in (extras_dir.glob(\"*\") if extras_dir.exists() else []):\n        safe_add(zf, p, f\"extras/{p.name}\")\n\n    # opt_search\n    if opt_dir.exists():\n        for p in opt_dir.glob(\"*\"):\n            safe_add(zf, p, f\"opt_search/{p.name}\")\n\n    # oof\n    if oof_dir.exists():\n        for p in oof_dir.glob(\"*\"):\n            safe_add(zf, p, f\"oof/{p.name}\")\n\nprint(\"\\nOK — Model bundle finalized (Notebook-3 compatible)\")\nprint(\"  SRC_DIR selected ->\", SRC_DIR)\nprint(\"  MODEL_DIR        ->\", MODEL_DIR)\nprint(\"  OUT_DIR          ->\", OUT_DIR)\nprint(\"  manifest         ->\", manifest_path)\nprint(\"  pack (json)      ->\", bundle_pack_json)\nprint(\"  pack (joblib)    ->\", (bundle_pack_joblib if joblib_ok else \"(skip; joblib not available)\"))\nprint(\"  thresholds       ->\", thresholds_path)\nprint(\"  zip              ->\", zip_path)\n\nprint(\"\\nBundle summary:\")\nprint(\"  bundle_version:\", BUNDLE_VERSION)\nprint(\"  model_format  :\", model_format)\nprint(\"  feature_cnt   :\", len(feature_cols))\nprint(\"  T_gate        :\", thresholds.get(\"T_gate\"))\nprint(\"  task          :\", task_str)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}