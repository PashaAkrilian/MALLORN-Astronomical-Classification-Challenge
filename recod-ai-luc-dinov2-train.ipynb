{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d93d6d",
   "metadata": {
    "papermill": {
     "duration": 0.005249,
     "end_time": "2026-01-04T13:58:35.975018",
     "exception": false,
     "start_time": "2026-01-04T13:58:35.969769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set Paths & Select Config (CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24252550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:58:35.985020Z",
     "iopub.status.busy": "2026-01-04T13:58:35.984526Z",
     "iopub.status.idle": "2026-01-04T13:58:36.769223Z",
     "shell.execute_reply": "2026-01-04T13:58:36.768328Z"
    },
    "papermill": {
     "duration": 0.791729,
     "end_time": "2026-01-04T13:58:36.770880",
     "exception": false,
     "start_time": "2026-01-04T13:58:35.979151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK — Roots\n",
      "  COMP_ROOT   : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n",
      "  OUT_DS_ROOT : /kaggle/input/recod-ailuc-dinov2-base\n",
      "  OUT_ROOT    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc\n",
      "\n",
      "OK — Selected CFG\n",
      "  MATCH_CFG_DIR: match_base_cfg_f9f7ea3a65c5\n",
      "  PRED_CFG_DIR : pred_base_v3_v7_cfg_5dbf0aa165\n",
      "  DINO_CFG_DIR : cfg_3246fd54aab0\n",
      "\n",
      "OK — Key files\n",
      "  DF_TRAIN_ALL    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet  (exists)\n",
      "  CV_CASE_FOLDS   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv  (exists)\n",
      "  MATCH_FEAT_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv  (exists)\n",
      "  PRED_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv  (exists)\n",
      "  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet  (exists)\n",
      "\n",
      "OK — DINO model dir\n",
      "  DINO_LARGE_DIR: /kaggle/input/dinov2/pytorch/large/1 (exists)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 0 — Set Paths & Select Config (CFG) (Kaggle-ready, offline) — REVISI FULL (anti-error)\n",
    "# - English step name, Indonesian explanations.\n",
    "#\n",
    "# Tujuan:\n",
    "# - Deteksi root kompetisi (COMP_ROOT)\n",
    "# - Deteksi root output dataset hasil PREP (OUT_DS_ROOT) + OUT_ROOT (= .../recodai_luc)\n",
    "# - Auto-pilih CFG terbaik untuk MATCH + PRED (berdasarkan coverage rows features train)\n",
    "# - Deteksi CFG DINO cache (opsional) + simpan path model DINOv2-LARGE offline\n",
    "#\n",
    "# Output globals (dipakai step berikutnya, JANGAN diganti namanya):\n",
    "# - COMP_ROOT, OUT_DS_ROOT, OUT_ROOT\n",
    "# - PATHS (dict jalur penting)\n",
    "# - MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR\n",
    "# ============================================================\n",
    "\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: find competition root\n",
    "# ----------------------------\n",
    "def find_comp_root(preferred: str = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\") -> Path:\n",
    "    p = Path(preferred)\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(\"/kaggle/input tidak ditemukan (pastikan kamu di Kaggle Notebook).\")\n",
    "\n",
    "    cands = []\n",
    "    for d in base.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        # Heuristic kompetisi: ada sample_submission.csv dan folder train/test images\n",
    "        if (d / \"sample_submission.csv\").exists() and ((d / \"train_images\").exists() or (d / \"test_images\").exists()):\n",
    "            cands.append(d)\n",
    "\n",
    "    if not cands:\n",
    "        # fallback: cari yang nested satu-level\n",
    "        for d in base.iterdir():\n",
    "            if not d.is_dir():\n",
    "                continue\n",
    "            inner = [x for x in d.iterdir() if x.is_dir()]\n",
    "            for x in inner:\n",
    "                if (x / \"sample_submission.csv\").exists() and ((x / \"train_images\").exists() or (x / \"test_images\").exists()):\n",
    "                    cands.append(x)\n",
    "\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\n",
    "            \"COMP_ROOT tidak ditemukan. Harus ada folder yang memuat sample_submission.csv dan train_images/test_images.\"\n",
    "        )\n",
    "\n",
    "    # prefer yang mengandung kata kunci\n",
    "    cands.sort(key=lambda x: ((\"recodai\" not in x.name.lower()), (\"forgery\" not in x.name.lower()), x.name))\n",
    "    return cands[0]\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: find output dataset root (hasil PREP)\n",
    "# ----------------------------\n",
    "def find_output_dataset_root(preferred_names=(\n",
    "    \"recod-ailuc-dinov2-base\",\n",
    "    \"recod-ai-luc-dinov2-base\",\n",
    "    \"recodai-luc-dinov2-base\",\n",
    "    \"recodai-luc-dinov2\",\n",
    "    \"recodai-luc-dinov2-prep\",\n",
    ")) -> Path:\n",
    "    base = Path(\"/kaggle/input\")\n",
    "\n",
    "    # direct hit\n",
    "    for nm in preferred_names:\n",
    "        p = base / nm\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # scan: cari yang punya recodai_luc/artifacts\n",
    "    cands = []\n",
    "    for d in base.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if (d / \"recodai_luc\" / \"artifacts\").exists():\n",
    "            cands.append(d)\n",
    "            continue\n",
    "        inner = list(d.glob(\"*/recodai_luc/artifacts\"))\n",
    "        if inner:\n",
    "            cands.append(d)\n",
    "\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\n",
    "            \"OUT_DS_ROOT tidak ditemukan. Harus ada /kaggle/input/<...>/recodai_luc/artifacts/\"\n",
    "        )\n",
    "\n",
    "    # prefer yang mengandung 'dinov2'\n",
    "    cands.sort(key=lambda x: ((\"dinov2\" not in x.name.lower()), x.name))\n",
    "    return cands[0]\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: resolve OUT_ROOT = <dataset>/recodai_luc\n",
    "# ----------------------------\n",
    "def resolve_out_root(out_ds_root: Path) -> Path:\n",
    "    direct = out_ds_root / \"recodai_luc\"\n",
    "    if direct.exists():\n",
    "        return direct\n",
    "    hits = list(out_ds_root.glob(\"*/recodai_luc\"))\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "    raise FileNotFoundError(f\"Folder recodai_luc tidak ditemukan di bawah {out_ds_root}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: pick best cfg directory by train feature coverage (row count)\n",
    "# ----------------------------\n",
    "def _fast_count_rows_csv(path: Path) -> int:\n",
    "    # hitung rows CSV (tanpa pandas) supaya cepat & hemat memori\n",
    "    try:\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            # -1 untuk header\n",
    "            n = sum(1 for _ in f) - 1\n",
    "        return int(max(n, 0))\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def pick_best_cfg(cache_root: Path, prefix: str, feat_train_filename: str) -> Path:\n",
    "    \"\"\"\n",
    "    cache_root: .../recodai_luc/cache\n",
    "    prefix: contoh 'match_base_cfg_' atau 'pred_base'\n",
    "    feat_train_filename: contoh 'match_features_train_all.csv'\n",
    "    \"\"\"\n",
    "    if not cache_root.exists():\n",
    "        raise FileNotFoundError(f\"cache_root tidak ditemukan: {cache_root}\")\n",
    "\n",
    "    cands = []\n",
    "    for d in cache_root.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if not d.name.startswith(prefix):\n",
    "            continue\n",
    "        feat_path = d / feat_train_filename\n",
    "        if not feat_path.exists():\n",
    "            continue\n",
    "        n = _fast_count_rows_csv(feat_path)\n",
    "        cands.append((n, d, feat_path))\n",
    "\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Tidak ada CFG folder di {cache_root} dengan prefix='{prefix}' dan file '{feat_train_filename}'.\"\n",
    "        )\n",
    "\n",
    "    # pilih coverage terbesar, tie-break nama\n",
    "    cands.sort(key=lambda x: (-x[0], x[1].name))\n",
    "    best_n, best_dir, best_feat = cands[0]\n",
    "    return best_dir\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Locate roots\n",
    "# ----------------------------\n",
    "COMP_ROOT = find_comp_root(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "OUT_DS_ROOT = find_output_dataset_root()\n",
    "OUT_ROOT = resolve_out_root(OUT_DS_ROOT)  # .../recodai_luc\n",
    "\n",
    "ART_DIR = OUT_ROOT / \"artifacts\"\n",
    "CACHE_DIR = OUT_ROOT / \"cache\"\n",
    "\n",
    "if not ART_DIR.exists():\n",
    "    raise FileNotFoundError(f\"ART_DIR tidak ditemukan: {ART_DIR}\")\n",
    "if not CACHE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"CACHE_DIR tidak ditemukan: {CACHE_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Competition paths (raw images/masks)\n",
    "# ----------------------------\n",
    "PATHS = {}\n",
    "PATHS[\"COMP_ROOT\"] = str(COMP_ROOT)\n",
    "PATHS[\"SAMPLE_SUB\"] = str(COMP_ROOT / \"sample_submission.csv\")\n",
    "\n",
    "PATHS[\"TRAIN_IMAGES\"] = str(COMP_ROOT / \"train_images\")\n",
    "PATHS[\"TEST_IMAGES\"]  = str(COMP_ROOT / \"test_images\")\n",
    "PATHS[\"TRAIN_MASKS\"]  = str(COMP_ROOT / \"train_masks\")\n",
    "PATHS[\"SUPP_IMAGES\"]  = str(COMP_ROOT / \"supplemental_images\")\n",
    "PATHS[\"SUPP_MASKS\"]   = str(COMP_ROOT / \"supplemental_masks\")\n",
    "\n",
    "# opsional: jika train_images dibagi authentic/forged\n",
    "PATHS[\"TRAIN_AUTH_DIR\"] = str(COMP_ROOT / \"train_images\" / \"authentic\")\n",
    "PATHS[\"TRAIN_FORG_DIR\"] = str(COMP_ROOT / \"train_images\" / \"forged\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Output dataset paths (clean artifacts + cache)\n",
    "# ----------------------------\n",
    "PATHS[\"OUT_DS_ROOT\"] = str(OUT_DS_ROOT)\n",
    "PATHS[\"OUT_ROOT\"]    = str(OUT_ROOT)\n",
    "PATHS[\"ART_DIR\"]     = str(ART_DIR)\n",
    "PATHS[\"CACHE_DIR\"]   = str(CACHE_DIR)\n",
    "\n",
    "# artifacts utama\n",
    "PATHS[\"DF_TRAIN_ALL\"] = str(ART_DIR / \"df_train_all.parquet\")\n",
    "PATHS[\"DF_TRAIN_CLS\"] = str(ART_DIR / \"df_train_cls.parquet\")\n",
    "PATHS[\"DF_TRAIN_SEG\"] = str(ART_DIR / \"df_train_seg.parquet\")\n",
    "PATHS[\"DF_TEST\"]      = str(ART_DIR / \"df_test.parquet\")\n",
    "\n",
    "PATHS[\"CV_CASE_FOLDS\"]   = str(ART_DIR / \"cv_case_folds.csv\")\n",
    "PATHS[\"CV_SAMPLE_FOLDS\"] = str(ART_DIR / \"cv_sample_folds.csv\")\n",
    "\n",
    "PATHS[\"IMG_PROFILE_TRAIN\"] = str(ART_DIR / \"image_profile_train.parquet\")\n",
    "PATHS[\"IMG_PROFILE_TEST\"]  = str(ART_DIR / \"image_profile_test.parquet\")\n",
    "PATHS[\"MASK_PROFILE\"]      = str(ART_DIR / \"mask_profile.parquet\")\n",
    "PATHS[\"CASE_SUMMARY\"]      = str(ART_DIR / \"case_summary.parquet\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Select best MATCH/PRED CFG dirs automatically\n",
    "# ----------------------------\n",
    "# MATCH: match_base_cfg_<hash>/match_features_train_all.csv\n",
    "MATCH_CFG_DIR = pick_best_cfg(\n",
    "    CACHE_DIR,\n",
    "    prefix=\"match_base_cfg_\",\n",
    "    feat_train_filename=\"match_features_train_all.csv\",\n",
    ")\n",
    "\n",
    "# PRED: pred_base.../pred_features_train_all.csv\n",
    "PRED_CFG_DIR = pick_best_cfg(\n",
    "    CACHE_DIR,\n",
    "    prefix=\"pred_base\",\n",
    "    feat_train_filename=\"pred_features_train_all.csv\",\n",
    ")\n",
    "\n",
    "# DINO cache cfg (opsional): cache/dino_v2_large/cfg_*/manifest_train_all.csv\n",
    "DINO_CFG_DIR = None\n",
    "dino_root = CACHE_DIR / \"dino_v2_large\"\n",
    "if dino_root.exists():\n",
    "    dino_cands = []\n",
    "    for d in dino_root.iterdir():\n",
    "        if d.is_dir() and d.name.startswith(\"cfg_\") and (d / \"manifest_train_all.csv\").exists():\n",
    "            dino_cands.append(d)\n",
    "    if dino_cands:\n",
    "        # pilih yang paling \"lengkap\" berdasarkan rows manifest_train_all\n",
    "        scored = []\n",
    "        for d in dino_cands:\n",
    "            mf = d / \"manifest_train_all.csv\"\n",
    "            scored.append((_fast_count_rows_csv(mf), d))\n",
    "        scored.sort(key=lambda x: (-x[0], x[1].name))\n",
    "        DINO_CFG_DIR = scored[0][1]\n",
    "\n",
    "# simpan cfg dir ke PATHS\n",
    "PATHS[\"MATCH_CFG_DIR\"] = str(MATCH_CFG_DIR)\n",
    "PATHS[\"PRED_CFG_DIR\"]  = str(PRED_CFG_DIR)\n",
    "PATHS[\"DINO_CFG_DIR\"]  = str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\"\n",
    "\n",
    "# feature paths dari cfg terpilih\n",
    "PATHS[\"MATCH_FEAT_TRAIN\"] = str(MATCH_CFG_DIR / \"match_features_train_all.csv\")\n",
    "PATHS[\"MATCH_FEAT_TEST\"]  = str(MATCH_CFG_DIR / \"match_features_test.csv\")\n",
    "PATHS[\"PRED_FEAT_TRAIN\"]  = str(PRED_CFG_DIR / \"pred_features_train_all.csv\")\n",
    "PATHS[\"PRED_FEAT_TEST\"]   = str(PRED_CFG_DIR / \"pred_features_test.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) DINOv2-LARGE model path (offline) — dipakai step training/infer berikutnya\n",
    "# ----------------------------\n",
    "DINO_LARGE_DIR = Path(\"/kaggle/input/dinov2/pytorch/large/1\")\n",
    "PATHS[\"DINO_LARGE_DIR\"] = str(DINO_LARGE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Sanity checks (wajib ada)\n",
    "# ----------------------------\n",
    "must_exist = [\n",
    "    (\"sample_submission.csv\", PATHS[\"SAMPLE_SUB\"]),\n",
    "    (\"df_train_all.parquet\",  PATHS[\"DF_TRAIN_ALL\"]),\n",
    "    (\"cv_case_folds.csv\",     PATHS[\"CV_CASE_FOLDS\"]),\n",
    "    (\"match_features_train_all.csv\", PATHS[\"MATCH_FEAT_TRAIN\"]),\n",
    "    (\"pred_features_train_all.csv\",  PATHS[\"PRED_FEAT_TRAIN\"]),\n",
    "]\n",
    "missing = [name for name, p in must_exist if not Path(p).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\"Missing required files: \" + \", \".join(missing))\n",
    "\n",
    "# DINO model dir opsional tapi biasanya kamu butuh; jadi hanya warning (tidak hard fail)\n",
    "if not DINO_LARGE_DIR.exists():\n",
    "    print(f\"WARNING: DINOv2-Large dir tidak ditemukan: {DINO_LARGE_DIR} (kalau butuh backbone, pastikan input ada)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Print summary (konsisten dengan step-step kamu)\n",
    "# ----------------------------\n",
    "print(\"OK — Roots\")\n",
    "print(\"  COMP_ROOT   :\", COMP_ROOT)\n",
    "print(\"  OUT_DS_ROOT :\", OUT_DS_ROOT)\n",
    "print(\"  OUT_ROOT    :\", OUT_ROOT)\n",
    "\n",
    "print(\"\\nOK — Selected CFG\")\n",
    "print(\"  MATCH_CFG_DIR:\", MATCH_CFG_DIR.name)\n",
    "print(\"  PRED_CFG_DIR :\", PRED_CFG_DIR.name)\n",
    "print(\"  DINO_CFG_DIR :\", (DINO_CFG_DIR.name if DINO_CFG_DIR else \"(not found / optional)\"))\n",
    "\n",
    "print(\"\\nOK — Key files\")\n",
    "for k in [\"DF_TRAIN_ALL\", \"CV_CASE_FOLDS\", \"MATCH_FEAT_TRAIN\", \"PRED_FEAT_TRAIN\", \"IMG_PROFILE_TRAIN\"]:\n",
    "    p = Path(PATHS[k])\n",
    "    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing/optional)'}\")\n",
    "\n",
    "print(\"\\nOK — DINO model dir\")\n",
    "print(\"  DINO_LARGE_DIR:\", DINO_LARGE_DIR, \"(exists)\" if DINO_LARGE_DIR.exists() else \"(missing)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cb0a9",
   "metadata": {
    "papermill": {
     "duration": 0.004052,
     "end_time": "2026-01-04T13:58:36.779492",
     "exception": false,
     "start_time": "2026-01-04T13:58:36.775440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Training Table (X, y, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea8cead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:58:36.790120Z",
     "iopub.status.busy": "2026-01-04T13:58:36.789609Z",
     "iopub.status.idle": "2026-01-04T13:58:37.222985Z",
     "shell.execute_reply": "2026-01-04T13:58:37.221727Z"
    },
    "papermill": {
     "duration": 0.440885,
     "end_time": "2026-01-04T13:58:37.224645",
     "exception": false,
     "start_time": "2026-01-04T13:58:36.783760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "  DF_TRAIN_ALL      : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet\n",
      "  CV_CASE_FOLDS     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv\n",
      "  PRED_FEAT_TRAIN   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv\n",
      "  MATCH_FEAT_TRAIN  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv (optional)\n",
      "  IMG_PROFILE_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet (optional)\n",
      "  DINO_LARGE_DIR    : /kaggle/input/dinov2/pytorch/large/1\n",
      "\n",
      "OK — Training table built\n",
      "  df_train_tabular: (5176, 67)\n",
      "  X_train: (5176, 62) | y pos%: 54.07650695517774\n",
      "  folds: 5 unique folds\n",
      "  feature_cols: 62\n",
      "  dropped_constant_features: 6\n",
      "\n",
      "Feature head: ['has_peak', 'peak_ratio', 'best_weight', 'best_count', 'best_mean_sim', 'n_pairs_thr', 'n_pairs_mnn', 'best_inlier_ratio', 'best_weight_frac', 'inlier_ratio', 'pair_count', 'uniq_src', 'uniq_dst', 'mean_sim', 'thr_used', 'cnt_thr_used', 'relaxed_used', 'min_pairs_used', 'area_frac', 'n_comp']\n",
      "Feature tail: ['log_largest_comp', 'grid_area_frac_cap', 'log_grid_area_frac', 'sim_x_count', 'area_x_sim', 'area_x_count', 'comp_density', 'comp_inv', 'mnn_ratio', 'has_peak_x_logpeak']\n",
      "\n",
      "Saved -> /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      "Saved -> /kaggle/working/recodai_luc_gate_artifacts/feature_schema.json\n",
      "Saved -> /kaggle/working/recodai_luc_gate_artifacts/df_train_tabular.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2 — Build Training Table (X, y, folds) — REVISI FULL (Transformer-ready, robust)\n",
    "# - Fokus: siapkan df_train_tabular + FEATURE_COLS\n",
    "# - Sumber utama: pred_features + (opsional) match_features + (opsional) image_profile\n",
    "# - Split: gunakan cv_case_folds.csv (anti leakage, by case_id)\n",
    "# - Tidak ada submission di sini\n",
    "#\n",
    "# Output globals:\n",
    "# - df_train_tabular, FEATURE_COLS\n",
    "# - (opsional) X_train, y_train, folds (pandas series/df)\n",
    "#\n",
    "# Saved:\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/feature_schema.json\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/df_train_tabular.parquet\n",
    "# ============================================================\n",
    "\n",
    "import os, json, math, gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require PATHS\n",
    "# ----------------------------\n",
    "if \"PATHS\" not in globals() or not isinstance(PATHS, dict):\n",
    "    raise RuntimeError(\"Missing PATHS. Jalankan dulu STAGE 0 — Set Paths & Select Config (CFG).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Check DINOv2 Large local path (offline) (hanya cek exist)\n",
    "# ----------------------------\n",
    "DINO_LARGE_DIR = Path(PATHS.get(\"DINO_LARGE_DIR\", \"/kaggle/input/dinov2/pytorch/large/1\"))\n",
    "if not DINO_LARGE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"DINOv2-Large path not found: {DINO_LARGE_DIR}\")\n",
    "PATHS[\"DINO_LARGE_DIR\"] = str(DINO_LARGE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Feature Engineering Config (fleksibel)\n",
    "# ----------------------------\n",
    "FE_CFG = {\n",
    "    \"use_match_features\": True,\n",
    "    \"use_image_profile\": True,\n",
    "\n",
    "    \"add_log_features\": True,\n",
    "    \"add_interactions\": True,\n",
    "    \"drop_constant_features\": True,\n",
    "\n",
    "    # outlier control\n",
    "    \"clip_by_quantile\": True,\n",
    "    \"clip_q\": 0.999,              # p99.9 cap\n",
    "    \"clip_max_fallback\": 1e9,      # fallback cap jika quantile gagal\n",
    "\n",
    "    # fill\n",
    "    \"fillna_value\": 0.0,\n",
    "\n",
    "    # dtype\n",
    "    \"cast_float32\": True,\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Prefer WORKING features if exist (kalau kamu regen di /kaggle/working)\n",
    "# ----------------------------\n",
    "def prefer_working(input_path: str, working_candidate: str | None = None) -> Path:\n",
    "    p_in = Path(input_path)\n",
    "    if working_candidate is not None:\n",
    "        p_w = Path(working_candidate)\n",
    "        if p_w.exists():\n",
    "            return p_w\n",
    "    return p_in\n",
    "\n",
    "match_cfg_name = Path(PATHS[\"MATCH_CFG_DIR\"]).name if PATHS.get(\"MATCH_CFG_DIR\") else \"\"\n",
    "pred_cfg_name  = Path(PATHS[\"PRED_CFG_DIR\"]).name  if PATHS.get(\"PRED_CFG_DIR\") else \"\"\n",
    "\n",
    "WORK_ROOT = Path(\"/kaggle/working/recodai_luc/cache\")\n",
    "match_feat_work = WORK_ROOT / match_cfg_name / \"match_features_train_all.csv\"\n",
    "pred_feat_work  = WORK_ROOT / pred_cfg_name  / \"pred_features_train_all.csv\"\n",
    "\n",
    "PRED_FEAT_TRAIN  = prefer_working(PATHS[\"PRED_FEAT_TRAIN\"],  str(pred_feat_work))\n",
    "MATCH_FEAT_TRAIN = prefer_working(PATHS[\"MATCH_FEAT_TRAIN\"], str(match_feat_work))\n",
    "\n",
    "DF_TRAIN_ALL      = Path(PATHS[\"DF_TRAIN_ALL\"])\n",
    "CV_CASE_FOLDS     = Path(PATHS[\"CV_CASE_FOLDS\"])\n",
    "IMG_PROFILE_TRAIN = Path(PATHS.get(\"IMG_PROFILE_TRAIN\", \"\"))\n",
    "\n",
    "for need_name, need_path in [\n",
    "    (\"df_train_all.parquet\", DF_TRAIN_ALL),\n",
    "    (\"cv_case_folds.csv\", CV_CASE_FOLDS),\n",
    "    (\"pred_features_train_all.csv\", PRED_FEAT_TRAIN),\n",
    "]:\n",
    "    if not need_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {need_name} -> {need_path}\")\n",
    "\n",
    "print(\"Using:\")\n",
    "print(\"  DF_TRAIN_ALL      :\", DF_TRAIN_ALL)\n",
    "print(\"  CV_CASE_FOLDS     :\", CV_CASE_FOLDS)\n",
    "print(\"  PRED_FEAT_TRAIN   :\", PRED_FEAT_TRAIN)\n",
    "print(\"  MATCH_FEAT_TRAIN  :\", MATCH_FEAT_TRAIN, \"(optional)\" if MATCH_FEAT_TRAIN.exists() else \"(missing/skip)\")\n",
    "print(\"  IMG_PROFILE_TRAIN :\", IMG_PROFILE_TRAIN, \"(optional)\" if IMG_PROFILE_TRAIN.exists() else \"(missing/skip)\")\n",
    "print(\"  DINO_LARGE_DIR    :\", DINO_LARGE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Load minimal inputs\n",
    "# ----------------------------\n",
    "df_base = pd.read_parquet(DF_TRAIN_ALL)\n",
    "df_cv   = pd.read_csv(CV_CASE_FOLDS)\n",
    "df_pred = pd.read_csv(PRED_FEAT_TRAIN)\n",
    "\n",
    "df_match = None\n",
    "if FE_CFG[\"use_match_features\"] and MATCH_FEAT_TRAIN.exists():\n",
    "    try:\n",
    "        df_match = pd.read_csv(MATCH_FEAT_TRAIN)\n",
    "    except Exception:\n",
    "        df_match = None\n",
    "\n",
    "df_prof = None\n",
    "if FE_CFG[\"use_image_profile\"] and IMG_PROFILE_TRAIN.exists():\n",
    "    try:\n",
    "        df_prof = pd.read_parquet(IMG_PROFILE_TRAIN)\n",
    "    except Exception:\n",
    "        df_prof = None\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Normalize keys: uid/sample_id, case_id, variant\n",
    "# ----------------------------\n",
    "def ensure_uid_case_variant(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"uid\" not in df.columns:\n",
    "        for alt in [\"sample_id\", \"id\", \"key\"]:\n",
    "            if alt in df.columns:\n",
    "                df = df.rename(columns={alt: \"uid\"})\n",
    "                break\n",
    "    if \"uid\" not in df.columns:\n",
    "        raise ValueError(\"Cannot find uid/sample_id column. Expected 'uid' or 'sample_id'.\")\n",
    "\n",
    "    if (\"case_id\" not in df.columns) or (\"variant\" not in df.columns):\n",
    "        uid = df[\"uid\"].astype(str)\n",
    "        if \"case_id\" not in df.columns:\n",
    "            df[\"case_id\"] = uid.str.extract(r\"^(\\d+)\")[0].astype(\"Int64\")\n",
    "        if \"variant\" not in df.columns:\n",
    "            v  = uid.str.extract(r\"__(\\w+)$\")[0]\n",
    "            v2 = uid.str.extract(r\"_(\\w+)$\")[0]\n",
    "            df[\"variant\"] = v.fillna(v2).fillna(\"unk\")\n",
    "\n",
    "    # guard missing extraction\n",
    "    if df[\"case_id\"].isna().any():\n",
    "        raise ValueError(\"Failed to parse case_id from uid for some rows. Check uid format.\")\n",
    "\n",
    "    df[\"case_id\"] = df[\"case_id\"].astype(int)\n",
    "    df[\"variant\"] = df[\"variant\"].astype(str)\n",
    "    df[\"uid\"]     = df[\"uid\"].astype(str)\n",
    "    return df\n",
    "\n",
    "df_pred = ensure_uid_case_variant(df_pred)\n",
    "\n",
    "df_base2 = df_base.copy()\n",
    "if \"uid\" not in df_base2.columns:\n",
    "    if \"sample_id\" in df_base2.columns:\n",
    "        df_base2 = df_base2.rename(columns={\"sample_id\": \"uid\"})\n",
    "    elif (\"case_id\" in df_base2.columns and \"variant\" in df_base2.columns):\n",
    "        df_base2[\"uid\"] = df_base2[\"case_id\"].astype(str) + \"__\" + df_base2[\"variant\"].astype(str)\n",
    "\n",
    "# label detection\n",
    "label_col = None\n",
    "for cand in [\"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]:\n",
    "    if cand in df_base2.columns:\n",
    "        label_col = cand\n",
    "        break\n",
    "if label_col is None and \"y_forged\" in df_pred.columns:\n",
    "    label_col = \"y_forged\"\n",
    "if label_col is None:\n",
    "    raise ValueError(\"Cannot find label column in df_train_all/pred_features (y_forged/has_mask/is_forged/forged).\")\n",
    "\n",
    "# folds sanity\n",
    "if \"case_id\" not in df_cv.columns or \"fold\" not in df_cv.columns:\n",
    "    raise ValueError(\"cv_case_folds.csv must contain columns: case_id, fold\")\n",
    "df_cv[\"case_id\"] = df_cv[\"case_id\"].astype(int)\n",
    "df_cv[\"fold\"]    = df_cv[\"fold\"].astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Merge: start from df_pred (1 row per uid)\n",
    "# ----------------------------\n",
    "df_train = df_pred.copy()\n",
    "\n",
    "# attach label\n",
    "if \"y_forged\" in df_train.columns:\n",
    "    df_train[\"y\"] = pd.to_numeric(df_train[\"y_forged\"], errors=\"coerce\")\n",
    "else:\n",
    "    if \"uid\" in df_base2.columns:\n",
    "        df_train = df_train.merge(\n",
    "            df_base2[[\"uid\", label_col]].rename(columns={label_col: \"y\"}),\n",
    "            on=\"uid\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    else:\n",
    "        if {\"case_id\", \"variant\", label_col}.issubset(df_base2.columns):\n",
    "            df_train = df_train.merge(\n",
    "                df_base2[[\"case_id\", \"variant\", label_col]].rename(columns={label_col: \"y\"}),\n",
    "                on=[\"case_id\", \"variant\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Could not merge label from df_train_all (missing uid or case_id+variant).\")\n",
    "\n",
    "# guard y missing\n",
    "if df_train[\"y\"].isna().any():\n",
    "    miss = int(df_train[\"y\"].isna().sum())\n",
    "    raise ValueError(f\"Label merge produced NaN in y: {miss} rows. Check df_train_all vs pred_features alignment.\")\n",
    "df_train[\"y\"] = df_train[\"y\"].astype(int)\n",
    "\n",
    "# attach folds\n",
    "df_train = df_train.drop(columns=[\"fold\"], errors=\"ignore\").merge(df_cv[[\"case_id\",\"fold\"]], on=\"case_id\", how=\"left\")\n",
    "if df_train[\"fold\"].isna().any():\n",
    "    miss = int(df_train[\"fold\"].isna().sum())\n",
    "    raise ValueError(f\"Missing fold after merging cv_case_folds.csv: {miss} rows.\")\n",
    "df_train[\"fold\"] = df_train[\"fold\"].astype(int)\n",
    "\n",
    "# optional: merge match features (only new cols)\n",
    "if df_match is not None:\n",
    "    df_match = ensure_uid_case_variant(df_match)\n",
    "    base_cols = set(df_train.columns)\n",
    "    new_cols = [c for c in df_match.columns if c not in base_cols]\n",
    "    keep_cols = [\"uid\"] + [c for c in new_cols if c not in [\"case_id\", \"variant\"]]\n",
    "    if len(keep_cols) > 1:\n",
    "        df_train = df_train.merge(df_match[keep_cols], on=\"uid\", how=\"left\")\n",
    "\n",
    "# optional: merge image profile by case_id\n",
    "if df_prof is not None and \"case_id\" in df_prof.columns:\n",
    "    df_prof2 = df_prof.copy()\n",
    "    df_prof2[\"case_id\"] = df_prof2[\"case_id\"].astype(int)\n",
    "    df_prof2 = df_prof2.drop_duplicates(\"case_id\")\n",
    "    clash = set(df_prof2.columns).intersection(df_train.columns)\n",
    "    clash -= {\"case_id\"}\n",
    "    if clash:\n",
    "        df_prof2 = df_prof2.rename(columns={c: f\"profile_{c}\" for c in clash})\n",
    "    df_train = df_train.merge(df_prof2, on=\"case_id\", how=\"left\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Feature engineering (lebih kaya + stabil untuk Transformer)\n",
    "# ----------------------------\n",
    "def safe_log1p(arr):\n",
    "    arr = np.asarray(arr, dtype=np.float64)\n",
    "    arr = np.where(np.isfinite(arr), arr, 0.0)\n",
    "    arr = np.clip(arr, 0.0, None)\n",
    "    return np.log1p(arr)\n",
    "\n",
    "def get_clip_cap(series: pd.Series, q: float, fallback: float):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").astype(float)\n",
    "    s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if len(s) == 0:\n",
    "        return float(fallback)\n",
    "    s = s[s >= 0]\n",
    "    if len(s) == 0:\n",
    "        return float(fallback)\n",
    "    cap = float(s.quantile(q))\n",
    "    if (not np.isfinite(cap)) or (cap <= 0):\n",
    "        return float(fallback)\n",
    "    return float(cap)\n",
    "\n",
    "# heavy-tail candidates (akan dipakai kalau ada)\n",
    "HEAVY_COLS = [\n",
    "    \"peak_ratio\", \"best_weight\", \"best_count\",\n",
    "    \"n_pairs_thr\", \"n_pairs_mnn\", \"n_pairs\",\n",
    "    \"n_comp\", \"largest_comp\",\n",
    "    \"grid_area_frac\", \"mask_area_frac\", \"pred_area_frac\",\n",
    "    \"overmask_tighten_steps\",\n",
    "]\n",
    "\n",
    "clip_caps = {}\n",
    "if FE_CFG[\"clip_by_quantile\"]:\n",
    "    for c in HEAVY_COLS:\n",
    "        if c in df_train.columns:\n",
    "            clip_caps[c] = get_clip_cap(df_train[c], FE_CFG[\"clip_q\"], FE_CFG[\"clip_max_fallback\"])\n",
    "\n",
    "# log+cap features\n",
    "if FE_CFG[\"add_log_features\"]:\n",
    "    for c in HEAVY_COLS:\n",
    "        if c in df_train.columns:\n",
    "            cap = clip_caps.get(c, FE_CFG[\"clip_max_fallback\"])\n",
    "            x = pd.to_numeric(df_train[c], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "            x = np.clip(x, 0.0, cap)\n",
    "            df_train[f\"{c}_cap\"] = x.astype(np.float32)\n",
    "            df_train[f\"log_{c}\"] = safe_log1p(x).astype(np.float32)\n",
    "\n",
    "# interaction features\n",
    "if FE_CFG[\"add_interactions\"]:\n",
    "    def getf(col, default=0.0):\n",
    "        if col in df_train.columns:\n",
    "            return pd.to_numeric(df_train[col], errors=\"coerce\").fillna(default).astype(float).values\n",
    "        return np.full(len(df_train), default, dtype=np.float64)\n",
    "\n",
    "    best_mean_sim = getf(\"best_mean_sim\", 0.0)\n",
    "    best_count    = getf(\"best_count\", 0.0)\n",
    "    grid_area     = getf(\"grid_area_frac\", 0.0)\n",
    "    has_peak      = getf(\"has_peak\", 0.0)\n",
    "    n_comp        = getf(\"n_comp\", 0.0)\n",
    "    largest_comp  = getf(\"largest_comp\", 0.0)\n",
    "\n",
    "    df_train[\"sim_x_count\"]   = (best_mean_sim * best_count).astype(np.float32)\n",
    "    df_train[\"area_x_sim\"]    = (grid_area * best_mean_sim).astype(np.float32)\n",
    "    df_train[\"area_x_count\"]  = (grid_area * best_count).astype(np.float32)\n",
    "    df_train[\"comp_density\"]  = (largest_comp / (1.0 + n_comp)).astype(np.float32)\n",
    "    df_train[\"comp_inv\"]      = (1.0 / (1.0 + n_comp)).astype(np.float32)\n",
    "\n",
    "    # mnn ratio\n",
    "    n_pairs_thr = getf(\"n_pairs_thr\", 0.0)\n",
    "    n_pairs_mnn = getf(\"n_pairs_mnn\", 0.0)\n",
    "    df_train[\"mnn_ratio\"] = (n_pairs_mnn / (1.0 + n_pairs_thr)).astype(np.float32)\n",
    "\n",
    "    # peak gating\n",
    "    if \"log_peak_ratio\" in df_train.columns:\n",
    "        df_train[\"has_peak_x_logpeak\"] = (has_peak * getf(\"log_peak_ratio\", 0.0)).astype(np.float32)\n",
    "    elif \"log_peak_ratio_cap\" in df_train.columns:\n",
    "        df_train[\"has_peak_x_logpeak\"] = (has_peak * getf(\"log_peak_ratio_cap\", 0.0)).astype(np.float32)\n",
    "    else:\n",
    "        df_train[\"has_peak_x_logpeak\"] = (has_peak * 0.0).astype(np.float32)\n",
    "\n",
    "# replace inf -> NaN (numeric)\n",
    "for c in df_train.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_train[c]):\n",
    "        df_train[c] = df_train[c].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Select feature columns (numeric only; exclude identifiers/labels/split)\n",
    "# ----------------------------\n",
    "TARGET_COLS = {\"y\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"}\n",
    "SPLIT_COLS  = {\"fold\"}\n",
    "ID_NUM_DROP = {\"case_id\"}  # numeric id jangan dipakai sebagai feature\n",
    "\n",
    "num_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
    "feature_cols = [c for c in num_cols if c not in TARGET_COLS and c not in SPLIT_COLS and c not in ID_NUM_DROP]\n",
    "\n",
    "# fill NaN\n",
    "df_train[feature_cols] = df_train[feature_cols].fillna(FE_CFG[\"fillna_value\"])\n",
    "\n",
    "# drop constant cols (stabil untuk Transformer)\n",
    "if FE_CFG[\"drop_constant_features\"]:\n",
    "    nun = df_train[feature_cols].nunique(dropna=False)\n",
    "    nonconst = nun[nun > 1].index.tolist()\n",
    "    dropped = sorted(set(feature_cols) - set(nonconst))\n",
    "    feature_cols = nonconst\n",
    "else:\n",
    "    dropped = []\n",
    "\n",
    "# cast float32\n",
    "if FE_CFG[\"cast_float32\"]:\n",
    "    df_train[feature_cols] = df_train[feature_cols].astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Final outputs\n",
    "# ----------------------------\n",
    "df_train_tabular = df_train[[\"uid\",\"case_id\",\"variant\",\"fold\",\"y\"] + feature_cols].copy()\n",
    "\n",
    "X_train = df_train_tabular[feature_cols]\n",
    "y_train = df_train_tabular[\"y\"].astype(int)\n",
    "folds   = df_train_tabular[\"fold\"].astype(int)\n",
    "\n",
    "print(\"\\nOK — Training table built\")\n",
    "print(\"  df_train_tabular:\", df_train_tabular.shape)\n",
    "print(\"  X_train:\", X_train.shape, \"| y pos%:\", float(y_train.mean())*100.0)\n",
    "print(\"  folds:\", int(folds.nunique()), \"unique folds\")\n",
    "print(\"  feature_cols:\", int(len(feature_cols)))\n",
    "if dropped:\n",
    "    print(\"  dropped_constant_features:\", len(dropped))\n",
    "\n",
    "# quick sanity\n",
    "if X_train.shape[0] != y_train.shape[0]:\n",
    "    raise RuntimeError(\"X_train and y_train row mismatch\")\n",
    "if y_train.isna().any():\n",
    "    raise RuntimeError(\"y_train contains NaN\")\n",
    "if folds.isna().any():\n",
    "    raise RuntimeError(\"folds contains NaN\")\n",
    "\n",
    "FEATURE_COLS = list(feature_cols)\n",
    "print(\"\\nFeature head:\", FEATURE_COLS[:20])\n",
    "print(\"Feature tail:\", FEATURE_COLS[-10:])\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Save reproducible schema (feature list + FE config + clip caps)\n",
    "# ----------------------------\n",
    "OUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OUT_ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(OUT_ART / \"feature_cols.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_COLS, f, indent=2)\n",
    "\n",
    "schema = {\n",
    "    \"fe_cfg\": FE_CFG,\n",
    "    \"clip_caps\": clip_caps,\n",
    "    \"dropped_constant_features\": dropped,\n",
    "    \"n_features\": int(len(FEATURE_COLS)),\n",
    "    \"example_feature_head\": FEATURE_COLS[:25],\n",
    "}\n",
    "with open(OUT_ART / \"feature_schema.json\", \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "# opsional: simpan table agar gampang resume\n",
    "df_train_tabular.to_parquet(OUT_ART / \"df_train_tabular.parquet\", index=False)\n",
    "\n",
    "print(f\"\\nSaved -> {OUT_ART/'feature_cols.json'}\")\n",
    "print(f\"Saved -> {OUT_ART/'feature_schema.json'}\")\n",
    "print(f\"Saved -> {OUT_ART/'df_train_tabular.parquet'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d63a96",
   "metadata": {
    "papermill": {
     "duration": 0.004418,
     "end_time": "2026-01-04T13:58:37.233658",
     "exception": false,
     "start_time": "2026-01-04T13:58:37.229240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Baseline Model (Leakage-Safe CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c216cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:58:37.245365Z",
     "iopub.status.busy": "2026-01-04T13:58:37.245051Z",
     "iopub.status.idle": "2026-01-04T14:08:31.382262Z",
     "shell.execute_reply": "2026-01-04T14:08:31.381296Z"
    },
    "papermill": {
     "duration": 594.145836,
     "end_time": "2026-01-04T14:08:31.384099",
     "exception": false,
     "start_time": "2026-01-04T13:58:37.238263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | AMP: True | CFG: SAFE\n",
      "Setup:\n",
      "  rows      : 5176\n",
      "  folds     : 5 | [0, 1, 2, 3, 4]\n",
      "  pos%      : 54.07650695517774\n",
      "  n_features: 62\n",
      "\n",
      "[Fold 0]\n",
      "  epoch 001/50 | train_loss=0.64058 | val_logloss=0.69460 | opt_steps=9 | dt=4.3s\n",
      "  epoch 002/50 | train_loss=0.63702 | val_logloss=0.69371 | opt_steps=9 | dt=3.3s\n",
      "  epoch 003/50 | train_loss=0.63887 | val_logloss=0.68990 | opt_steps=9 | dt=3.3s\n",
      "  epoch 004/50 | train_loss=0.63930 | val_logloss=0.70310 | opt_steps=9 | dt=3.3s\n",
      "  epoch 005/50 | train_loss=0.63816 | val_logloss=0.70115 | opt_steps=9 | dt=3.3s\n",
      "  epoch 006/50 | train_loss=0.63732 | val_logloss=0.71565 | opt_steps=9 | dt=3.3s\n",
      "  epoch 007/50 | train_loss=0.64055 | val_logloss=0.71997 | opt_steps=9 | dt=3.3s\n",
      "  epoch 008/50 | train_loss=0.64192 | val_logloss=0.68971 | opt_steps=9 | dt=3.3s\n",
      "  epoch 009/50 | train_loss=0.63878 | val_logloss=0.69451 | opt_steps=9 | dt=3.3s\n",
      "  epoch 010/50 | train_loss=0.63670 | val_logloss=0.69193 | opt_steps=9 | dt=3.3s\n",
      "  epoch 011/50 | train_loss=0.63695 | val_logloss=0.69502 | opt_steps=9 | dt=3.3s\n",
      "  epoch 012/50 | train_loss=0.63707 | val_logloss=0.68941 | opt_steps=9 | dt=3.3s\n",
      "  epoch 013/50 | train_loss=0.64025 | val_logloss=0.70622 | opt_steps=9 | dt=3.3s\n",
      "  epoch 014/50 | train_loss=0.63783 | val_logloss=0.68964 | opt_steps=9 | dt=3.3s\n",
      "  epoch 015/50 | train_loss=0.63930 | val_logloss=0.69442 | opt_steps=9 | dt=3.3s\n",
      "  epoch 016/50 | train_loss=0.63615 | val_logloss=0.69657 | opt_steps=9 | dt=3.3s\n",
      "  epoch 017/50 | train_loss=0.63700 | val_logloss=0.69481 | opt_steps=9 | dt=3.3s\n",
      "  epoch 018/50 | train_loss=0.63685 | val_logloss=0.69008 | opt_steps=9 | dt=3.3s\n",
      "  epoch 019/50 | train_loss=0.63549 | val_logloss=0.69197 | opt_steps=9 | dt=3.3s\n",
      "  epoch 020/50 | train_loss=0.63499 | val_logloss=0.69714 | opt_steps=9 | dt=3.3s\n",
      "  epoch 021/50 | train_loss=0.63599 | val_logloss=0.69370 | opt_steps=9 | dt=3.3s\n",
      "  epoch 022/50 | train_loss=0.63576 | val_logloss=0.69085 | opt_steps=9 | dt=3.3s\n",
      "  early stop at epoch 22, best_epoch=12, best_val_logloss=0.68941\n",
      "\n",
      "[Fold 1]\n",
      "  epoch 001/50 | train_loss=0.63845 | val_logloss=0.69645 | opt_steps=9 | dt=3.3s\n",
      "  epoch 002/50 | train_loss=0.63782 | val_logloss=0.69927 | opt_steps=9 | dt=3.3s\n",
      "  epoch 003/50 | train_loss=0.63658 | val_logloss=0.69062 | opt_steps=9 | dt=3.3s\n",
      "  epoch 004/50 | train_loss=0.63739 | val_logloss=0.69907 | opt_steps=9 | dt=3.3s\n",
      "  epoch 005/50 | train_loss=0.63540 | val_logloss=0.70413 | opt_steps=9 | dt=3.3s\n",
      "  epoch 006/50 | train_loss=0.63917 | val_logloss=0.70077 | opt_steps=9 | dt=3.3s\n",
      "  epoch 007/50 | train_loss=0.63732 | val_logloss=0.69219 | opt_steps=9 | dt=3.3s\n",
      "  epoch 008/50 | train_loss=0.63540 | val_logloss=0.69014 | opt_steps=9 | dt=3.3s\n",
      "  epoch 009/50 | train_loss=0.63437 | val_logloss=0.69051 | opt_steps=9 | dt=3.3s\n",
      "  epoch 010/50 | train_loss=0.63702 | val_logloss=0.69048 | opt_steps=9 | dt=3.3s\n",
      "  epoch 011/50 | train_loss=0.63402 | val_logloss=0.69139 | opt_steps=9 | dt=3.3s\n",
      "  epoch 012/50 | train_loss=0.63420 | val_logloss=0.68826 | opt_steps=9 | dt=3.3s\n",
      "  epoch 013/50 | train_loss=0.63533 | val_logloss=0.70340 | opt_steps=9 | dt=3.3s\n",
      "  epoch 014/50 | train_loss=0.63858 | val_logloss=0.69351 | opt_steps=9 | dt=3.3s\n",
      "  epoch 015/50 | train_loss=0.63467 | val_logloss=0.68943 | opt_steps=9 | dt=3.3s\n",
      "  epoch 016/50 | train_loss=0.63346 | val_logloss=0.69248 | opt_steps=9 | dt=3.3s\n",
      "  epoch 017/50 | train_loss=0.63370 | val_logloss=0.68971 | opt_steps=9 | dt=3.3s\n",
      "  epoch 018/50 | train_loss=0.63241 | val_logloss=0.69161 | opt_steps=9 | dt=3.3s\n",
      "  epoch 019/50 | train_loss=0.63281 | val_logloss=0.70169 | opt_steps=9 | dt=3.3s\n",
      "  epoch 020/50 | train_loss=0.63358 | val_logloss=0.68935 | opt_steps=9 | dt=3.3s\n",
      "  epoch 021/50 | train_loss=0.63369 | val_logloss=0.69638 | opt_steps=9 | dt=3.3s\n",
      "  epoch 022/50 | train_loss=0.63217 | val_logloss=0.68970 | opt_steps=9 | dt=3.3s\n",
      "  early stop at epoch 22, best_epoch=12, best_val_logloss=0.68826\n",
      "\n",
      "[Fold 2]\n",
      "  epoch 001/50 | train_loss=0.64042 | val_logloss=0.69470 | opt_steps=9 | dt=3.3s\n",
      "  epoch 002/50 | train_loss=0.63802 | val_logloss=0.69773 | opt_steps=9 | dt=3.3s\n",
      "  epoch 003/50 | train_loss=0.63641 | val_logloss=0.68876 | opt_steps=9 | dt=3.3s\n",
      "  epoch 004/50 | train_loss=0.64224 | val_logloss=0.68976 | opt_steps=9 | dt=3.3s\n",
      "  epoch 005/50 | train_loss=0.63932 | val_logloss=0.69397 | opt_steps=9 | dt=3.3s\n",
      "  epoch 006/50 | train_loss=0.63907 | val_logloss=0.69314 | opt_steps=9 | dt=3.3s\n",
      "  epoch 007/50 | train_loss=0.63767 | val_logloss=0.69722 | opt_steps=9 | dt=3.3s\n",
      "  epoch 008/50 | train_loss=0.63844 | val_logloss=0.69776 | opt_steps=9 | dt=3.3s\n",
      "  epoch 009/50 | train_loss=0.63912 | val_logloss=0.68896 | opt_steps=9 | dt=3.3s\n",
      "  epoch 010/50 | train_loss=0.63753 | val_logloss=0.69578 | opt_steps=9 | dt=3.3s\n",
      "  epoch 011/50 | train_loss=0.63710 | val_logloss=0.69069 | opt_steps=9 | dt=3.3s\n",
      "  epoch 012/50 | train_loss=0.63704 | val_logloss=0.69341 | opt_steps=9 | dt=3.3s\n",
      "  epoch 013/50 | train_loss=0.63668 | val_logloss=0.68849 | opt_steps=9 | dt=3.3s\n",
      "  epoch 014/50 | train_loss=0.63814 | val_logloss=0.70605 | opt_steps=9 | dt=3.3s\n",
      "  epoch 015/50 | train_loss=0.63861 | val_logloss=0.69272 | opt_steps=9 | dt=3.3s\n",
      "  epoch 016/50 | train_loss=0.63551 | val_logloss=0.69498 | opt_steps=9 | dt=3.3s\n",
      "  epoch 017/50 | train_loss=0.63557 | val_logloss=0.69514 | opt_steps=9 | dt=3.3s\n",
      "  epoch 018/50 | train_loss=0.63674 | val_logloss=0.68863 | opt_steps=9 | dt=3.3s\n",
      "  epoch 019/50 | train_loss=0.63573 | val_logloss=0.69243 | opt_steps=9 | dt=3.3s\n",
      "  epoch 020/50 | train_loss=0.63635 | val_logloss=0.68878 | opt_steps=9 | dt=3.3s\n",
      "  epoch 021/50 | train_loss=0.64144 | val_logloss=0.69922 | opt_steps=9 | dt=3.3s\n",
      "  epoch 022/50 | train_loss=0.63669 | val_logloss=0.68882 | opt_steps=9 | dt=3.3s\n",
      "  epoch 023/50 | train_loss=0.63702 | val_logloss=0.69807 | opt_steps=9 | dt=3.3s\n",
      "  early stop at epoch 23, best_epoch=13, best_val_logloss=0.68849\n",
      "\n",
      "[Fold 3]\n",
      "  epoch 001/50 | train_loss=0.64078 | val_logloss=0.69393 | opt_steps=9 | dt=3.3s\n",
      "  epoch 002/50 | train_loss=0.64139 | val_logloss=0.69187 | opt_steps=9 | dt=3.3s\n",
      "  epoch 003/50 | train_loss=0.63774 | val_logloss=0.69034 | opt_steps=9 | dt=3.3s\n",
      "  epoch 004/50 | train_loss=0.63774 | val_logloss=0.69269 | opt_steps=9 | dt=3.3s\n",
      "  epoch 005/50 | train_loss=0.63686 | val_logloss=0.71266 | opt_steps=9 | dt=3.3s\n",
      "  epoch 006/50 | train_loss=0.64079 | val_logloss=0.69116 | opt_steps=9 | dt=3.3s\n",
      "  epoch 007/50 | train_loss=0.63742 | val_logloss=0.69139 | opt_steps=9 | dt=3.3s\n",
      "  epoch 008/50 | train_loss=0.63919 | val_logloss=0.69189 | opt_steps=9 | dt=3.3s\n",
      "  epoch 009/50 | train_loss=0.63753 | val_logloss=0.70036 | opt_steps=9 | dt=3.3s\n",
      "  epoch 010/50 | train_loss=0.63841 | val_logloss=0.69066 | opt_steps=9 | dt=3.3s\n",
      "  epoch 011/50 | train_loss=0.63688 | val_logloss=0.69059 | opt_steps=9 | dt=3.3s\n",
      "  epoch 012/50 | train_loss=0.63560 | val_logloss=0.69980 | opt_steps=9 | dt=3.3s\n",
      "  epoch 013/50 | train_loss=0.63822 | val_logloss=0.68902 | opt_steps=9 | dt=3.3s\n",
      "  epoch 014/50 | train_loss=0.63726 | val_logloss=0.68831 | opt_steps=9 | dt=3.3s\n",
      "  epoch 015/50 | train_loss=0.63627 | val_logloss=0.69313 | opt_steps=9 | dt=3.3s\n",
      "  epoch 016/50 | train_loss=0.63637 | val_logloss=0.68675 | opt_steps=9 | dt=3.3s\n",
      "  epoch 017/50 | train_loss=0.63521 | val_logloss=0.69533 | opt_steps=9 | dt=3.3s\n",
      "  epoch 018/50 | train_loss=0.63691 | val_logloss=0.69416 | opt_steps=9 | dt=3.3s\n",
      "  epoch 019/50 | train_loss=0.63554 | val_logloss=0.68832 | opt_steps=9 | dt=3.3s\n",
      "  epoch 020/50 | train_loss=0.63581 | val_logloss=0.68604 | opt_steps=9 | dt=3.3s\n",
      "  epoch 021/50 | train_loss=0.63625 | val_logloss=0.69399 | opt_steps=9 | dt=3.3s\n",
      "  epoch 022/50 | train_loss=0.63545 | val_logloss=0.69384 | opt_steps=9 | dt=3.3s\n",
      "  epoch 023/50 | train_loss=0.63508 | val_logloss=0.68674 | opt_steps=9 | dt=3.3s\n",
      "  epoch 024/50 | train_loss=0.63395 | val_logloss=0.68945 | opt_steps=9 | dt=3.3s\n",
      "  epoch 025/50 | train_loss=0.63554 | val_logloss=0.69262 | opt_steps=9 | dt=3.3s\n",
      "  epoch 026/50 | train_loss=0.63474 | val_logloss=0.69085 | opt_steps=9 | dt=3.3s\n",
      "  epoch 027/50 | train_loss=0.63428 | val_logloss=0.68957 | opt_steps=9 | dt=3.3s\n",
      "  epoch 028/50 | train_loss=0.63423 | val_logloss=0.69034 | opt_steps=9 | dt=3.3s\n",
      "  epoch 029/50 | train_loss=0.63394 | val_logloss=0.68805 | opt_steps=9 | dt=3.3s\n",
      "  epoch 030/50 | train_loss=0.63299 | val_logloss=0.69431 | opt_steps=9 | dt=3.3s\n",
      "  early stop at epoch 30, best_epoch=20, best_val_logloss=0.68604\n",
      "\n",
      "[Fold 4]\n",
      "  epoch 001/50 | train_loss=0.63949 | val_logloss=0.68999 | opt_steps=9 | dt=3.3s\n",
      "  epoch 002/50 | train_loss=0.64242 | val_logloss=0.69010 | opt_steps=9 | dt=3.3s\n",
      "  epoch 003/50 | train_loss=0.63767 | val_logloss=0.70069 | opt_steps=9 | dt=3.3s\n",
      "  epoch 004/50 | train_loss=0.64056 | val_logloss=0.69523 | opt_steps=9 | dt=3.3s\n",
      "  epoch 005/50 | train_loss=0.63771 | val_logloss=0.69891 | opt_steps=9 | dt=3.3s\n",
      "  epoch 006/50 | train_loss=0.63867 | val_logloss=0.70486 | opt_steps=9 | dt=3.3s\n",
      "  epoch 007/50 | train_loss=0.64049 | val_logloss=0.69964 | opt_steps=9 | dt=3.3s\n",
      "  epoch 008/50 | train_loss=0.63874 | val_logloss=0.69473 | opt_steps=9 | dt=3.3s\n",
      "  epoch 009/50 | train_loss=0.63863 | val_logloss=0.68891 | opt_steps=9 | dt=3.3s\n",
      "  epoch 010/50 | train_loss=0.63899 | val_logloss=0.69963 | opt_steps=9 | dt=3.3s\n",
      "  epoch 011/50 | train_loss=0.63864 | val_logloss=0.69189 | opt_steps=9 | dt=3.3s\n",
      "  epoch 012/50 | train_loss=0.63685 | val_logloss=0.69219 | opt_steps=9 | dt=3.3s\n",
      "  epoch 013/50 | train_loss=0.63618 | val_logloss=0.69776 | opt_steps=9 | dt=3.3s\n",
      "  epoch 014/50 | train_loss=0.63650 | val_logloss=0.68606 | opt_steps=9 | dt=3.3s\n",
      "  epoch 015/50 | train_loss=0.63631 | val_logloss=0.68803 | opt_steps=9 | dt=3.3s\n",
      "  epoch 016/50 | train_loss=0.63712 | val_logloss=0.68533 | opt_steps=9 | dt=3.3s\n",
      "  epoch 017/50 | train_loss=0.63689 | val_logloss=0.68782 | opt_steps=9 | dt=3.3s\n",
      "  epoch 018/50 | train_loss=0.63601 | val_logloss=0.69908 | opt_steps=9 | dt=3.3s\n",
      "  epoch 019/50 | train_loss=0.63745 | val_logloss=0.68806 | opt_steps=9 | dt=3.3s\n",
      "  epoch 020/50 | train_loss=0.63641 | val_logloss=0.69206 | opt_steps=9 | dt=3.3s\n",
      "  epoch 021/50 | train_loss=0.63704 | val_logloss=0.68954 | opt_steps=9 | dt=3.3s\n",
      "  epoch 022/50 | train_loss=0.63478 | val_logloss=0.68689 | opt_steps=9 | dt=3.3s\n",
      "  epoch 023/50 | train_loss=0.63760 | val_logloss=0.68924 | opt_steps=9 | dt=3.3s\n",
      "  epoch 024/50 | train_loss=0.63631 | val_logloss=0.68756 | opt_steps=9 | dt=3.3s\n",
      "  epoch 025/50 | train_loss=0.63458 | val_logloss=0.69681 | opt_steps=9 | dt=3.3s\n",
      "  epoch 026/50 | train_loss=0.63803 | val_logloss=0.69087 | opt_steps=9 | dt=3.3s\n",
      "  early stop at epoch 26, best_epoch=16, best_val_logloss=0.68533\n",
      "\n",
      "Per-fold report:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>n_val</th>\n",
       "      <th>pos_val</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1@0.5</th>\n",
       "      <th>precision@0.5</th>\n",
       "      <th>recall@0.5</th>\n",
       "      <th>logloss</th>\n",
       "      <th>best_val_logloss</th>\n",
       "      <th>best_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1034</td>\n",
       "      <td>559</td>\n",
       "      <td>0.531931</td>\n",
       "      <td>0.701820</td>\n",
       "      <td>0.540619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.689406</td>\n",
       "      <td>0.689406</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1041</td>\n",
       "      <td>561</td>\n",
       "      <td>0.537325</td>\n",
       "      <td>0.689474</td>\n",
       "      <td>0.546403</td>\n",
       "      <td>0.934046</td>\n",
       "      <td>0.688262</td>\n",
       "      <td>0.688262</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1032</td>\n",
       "      <td>559</td>\n",
       "      <td>0.541792</td>\n",
       "      <td>0.674564</td>\n",
       "      <td>0.552511</td>\n",
       "      <td>0.865832</td>\n",
       "      <td>0.688492</td>\n",
       "      <td>0.688492</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1035</td>\n",
       "      <td>560</td>\n",
       "      <td>0.534132</td>\n",
       "      <td>0.691391</td>\n",
       "      <td>0.549474</td>\n",
       "      <td>0.932143</td>\n",
       "      <td>0.686043</td>\n",
       "      <td>0.686043</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1034</td>\n",
       "      <td>560</td>\n",
       "      <td>0.545153</td>\n",
       "      <td>0.693688</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.932143</td>\n",
       "      <td>0.685333</td>\n",
       "      <td>0.685333</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold  n_val  pos_val       auc    f1@0.5  precision@0.5  recall@0.5  \\\n",
       "0     0   1034      559  0.531931  0.701820       0.540619    1.000000   \n",
       "1     1   1041      561  0.537325  0.689474       0.546403    0.934046   \n",
       "2     2   1032      559  0.541792  0.674564       0.552511    0.865832   \n",
       "3     3   1035      560  0.534132  0.691391       0.549474    0.932143   \n",
       "4     4   1034      560  0.545153  0.693688       0.552381    0.932143   \n",
       "\n",
       "    logloss  best_val_logloss  best_epoch  \n",
       "0  0.689406          0.689406          12  \n",
       "1  0.688262          0.688262          12  \n",
       "2  0.688492          0.688492          13  \n",
       "3  0.686043          0.686043          20  \n",
       "4  0.685333          0.685333          16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF overall:\n",
      "{'rows': 5176, 'folds': 5, 'pos_total': 2799, 'pos_rate': 0.5407650695517774, 'oof_auc': 0.5304809413422638, 'oof_f1@0.5': 0.6904667460002645, 'oof_precision@0.5': 0.5480688497061293, 'oof_recall@0.5': 0.9328331546981065, 'oof_logloss': 0.6875075715645032}\n",
      "\n",
      "Training full mHC transformer for 35 epochs (fixed)...\n",
      "  full epoch 001/35 | loss=0.63884\n",
      "  full epoch 002/35 | loss=0.63956\n",
      "  full epoch 003/35 | loss=0.63930\n",
      "  full epoch 004/35 | loss=0.63830\n",
      "  full epoch 005/35 | loss=0.63730\n",
      "  full epoch 006/35 | loss=0.63711\n",
      "  full epoch 007/35 | loss=0.63663\n",
      "  full epoch 008/35 | loss=0.63689\n",
      "  full epoch 009/35 | loss=0.63603\n",
      "  full epoch 010/35 | loss=0.63704\n",
      "  full epoch 011/35 | loss=0.63472\n",
      "  full epoch 012/35 | loss=0.63743\n",
      "  full epoch 013/35 | loss=0.63758\n",
      "  full epoch 014/35 | loss=0.63682\n",
      "  full epoch 015/35 | loss=0.63730\n",
      "  full epoch 016/35 | loss=0.63645\n",
      "  full epoch 017/35 | loss=0.63543\n",
      "  full epoch 018/35 | loss=0.63354\n",
      "  full epoch 019/35 | loss=0.63524\n",
      "  full epoch 020/35 | loss=0.63357\n",
      "  full epoch 021/35 | loss=0.63379\n",
      "  full epoch 022/35 | loss=0.63265\n",
      "  full epoch 023/35 | loss=0.63484\n",
      "  full epoch 024/35 | loss=0.63581\n",
      "  full epoch 025/35 | loss=0.63544\n",
      "  full epoch 026/35 | loss=0.63541\n",
      "  full epoch 027/35 | loss=0.63456\n",
      "  full epoch 028/35 | loss=0.63315\n",
      "  full epoch 029/35 | loss=0.63231\n",
      "  full epoch 030/35 | loss=0.63271\n",
      "  full epoch 031/35 | loss=0.63200\n",
      "  full epoch 032/35 | loss=0.63309\n",
      "  full epoch 033/35 | loss=0.63187\n",
      "  full epoch 034/35 | loss=0.63263\n",
      "  full epoch 035/35 | loss=0.63171\n",
      "\n",
      "Saved artifacts:\n",
      "  fold models  -> /kaggle/working/recodai_luc_gate_artifacts/baseline_mhc_transformer_folds\n",
      "  full model   -> /kaggle/working/recodai_luc_gate_artifacts/baseline_mhc_transformer_model_full.pt\n",
      "  oof preds    -> /kaggle/working/recodai_luc_gate_artifacts/oof_baseline_mhc_transformer.csv\n",
      "  cv report    -> /kaggle/working/recodai_luc_gate_artifacts/baseline_mhc_transformer_cv_report.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 3 — Train Baseline Model (Leakage-Safe CV)\n",
    "# - Baseline: mHC-FTTransformer (numeric tabular) + Sinkhorn (PDF mHC)\n",
    "# - CV: pakai kolom `fold` (by case_id)\n",
    "# - Output:\n",
    "#   * OOF probabilities\n",
    "#   * CV report (AUC, F1, Precision, Recall, LogLoss)\n",
    "#   * Simpan model per fold + model_full (torch .pt pack: state_dict + scaler + cfg)\n",
    "#\n",
    "# Implementasi ide PDF (mHC):\n",
    "# - Multi-stream residual mixing (n_streams = 4 default)\n",
    "# - H_res per-layer diproyeksikan ke doubly-stochastic (Birkhoff polytope) via Sinkhorn-Knopp (tmax=20)\n",
    "# - Gating factor alpha init 0.01 (awal dekat identity -> stabil)\n",
    "#\n",
    "# Kaggle-safe:\n",
    "# - Update hanya active stream (stream-0) tiap layer -> jauh lebih cepat\n",
    "# - AMP + grad accumulation (effective batch besar tanpa OOM)\n",
    "# - LR scheduler step ala tabel: decay di 0.8 & 0.9 progress\n",
    "# ============================================================\n",
    "\n",
    "import json, gc, math, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require outputs from Step 2\n",
    "# ----------------------------\n",
    "need_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\n",
    "for v in need_vars:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n",
    "\n",
    "df_train_tabular = df_train_tabular.copy()\n",
    "FEATURE_COLS = list(FEATURE_COLS)\n",
    "\n",
    "required_cols = {\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"}\n",
    "missing_cols = [c for c in required_cols if c not in df_train_tabular.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"df_train_tabular missing columns: {missing_cols}.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CFG (dua preset: SAFE vs STRONG)\n",
    "# ----------------------------\n",
    "# SAFE: cocok T4/P100; STRONG: kalau A100/H100 (atau kamu yakin VRAM kuat)\n",
    "CFG_SAFE = {\n",
    "    \"seed\": 2025,\n",
    "\n",
    "    # mHC (PDF)\n",
    "    \"n_streams\": 4,         # mHC/HC expansion rate n = 4 (sesuai tabel)\n",
    "    \"sinkhorn_tmax\": 20,    # sesuai PDF (praktikal)\n",
    "    \"alpha_init\": 0.01,     # sesuai tabel (awal dekat identity)\n",
    "\n",
    "    # model capacity (SAFE)\n",
    "    \"d_model\": 256,\n",
    "    \"n_layers\": 6,\n",
    "    \"n_heads\": 8,\n",
    "    \"ffn_mult\": 4,\n",
    "    \"dropout\": 0.15,\n",
    "    \"attn_dropout\": 0.10,\n",
    "\n",
    "    # training (effective batch besar via accum)\n",
    "    \"batch_size\": 256,      # micro-batch\n",
    "    \"accum_steps\": 2,       # effective batch = 512\n",
    "    \"epochs\": 50,\n",
    "\n",
    "    # AdamW ala tabel (yang relevan)\n",
    "    \"lr\": 3e-4,\n",
    "    \"betas\": (0.9, 0.95),\n",
    "    \"eps\": 1e-8,            # eps tabel 1e-20 terlalu ekstrem utk tabular kecil; ini lebih aman\n",
    "    \"weight_decay\": 5e-2,\n",
    "\n",
    "    # warmup + step decay (mirip tabel: step @ 0.8 & 0.9)\n",
    "    \"warmup_frac\": 0.10,\n",
    "    \"lr_decay_milestones\": (0.80, 0.90),\n",
    "    \"lr_decay_values\": (0.316, 0.10),  # multiplier piecewise setelah milestone\n",
    "\n",
    "    \"grad_clip\": 1.0,\n",
    "\n",
    "    # early stopping\n",
    "    \"early_stop_patience\": 10,\n",
    "    \"early_stop_min_delta\": 1e-4,\n",
    "\n",
    "    # report threshold (hanya report baseline)\n",
    "    \"report_thr\": 0.5,\n",
    "}\n",
    "\n",
    "CFG_STRONG = {\n",
    "    **CFG_SAFE,\n",
    "    \"d_model\": 384,\n",
    "    \"n_layers\": 8,\n",
    "    \"dropout\": 0.20,\n",
    "    \"epochs\": 70,\n",
    "    \"lr\": 2e-4,\n",
    "    \"weight_decay\": 7e-2,\n",
    "    \"batch_size\": 256,\n",
    "    \"accum_steps\": 2,\n",
    "}\n",
    "\n",
    "# pilih otomatis berdasarkan GPU memory (kalau ada)\n",
    "CFG = dict(CFG_SAFE)\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        if mem_gb >= 30:\n",
    "            CFG = dict(CFG_STRONG)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Seed + device\n",
    "# ----------------------------\n",
    "def seed_everything(seed: int = 2025):\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(int(CFG[\"seed\"]))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "print(\"Device:\", device, \"| AMP:\", use_amp, \"| CFG:\", (\"STRONG\" if CFG is CFG_STRONG else \"SAFE\"))\n",
    "\n",
    "# speed knobs\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build arrays + guard\n",
    "# ----------------------------\n",
    "X = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "y = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n",
    "folds = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\n",
    "\n",
    "if not np.isfinite(X).all():\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "n = len(df_train_tabular)\n",
    "unique_folds = sorted(pd.Series(folds).unique().tolist())\n",
    "n_folds = len(unique_folds)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "print(\"Setup:\")\n",
    "print(\"  rows      :\", n)\n",
    "print(\"  folds     :\", n_folds, \"|\", unique_folds)\n",
    "print(\"  pos%      :\", float(y.mean()) * 100.0)\n",
    "print(\"  n_features:\", n_features)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset\n",
    "# ----------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Normalization (leakage-safe: fit on train fold only)\n",
    "# ----------------------------\n",
    "def fit_standardizer(X_tr: np.ndarray):\n",
    "    mu = X_tr.mean(axis=0, dtype=np.float64)\n",
    "    sig = X_tr.std(axis=0, dtype=np.float64)\n",
    "    sig = np.where(sig < 1e-8, 1.0, sig)\n",
    "    return mu.astype(np.float32), sig.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n",
    "    return ((X_in - mu) / sig).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Metrics helpers\n",
    "# ----------------------------\n",
    "def safe_auc(y_true, p):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return None\n",
    "    return float(roc_auc_score(y_true, p))\n",
    "\n",
    "def safe_logloss(y_true, p):\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-6, 1-1e-6)\n",
    "    return float(log_loss(y_true, p, labels=[0, 1]))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) PDF mHC: Sinkhorn projection (doubly-stochastic)\n",
    "# ----------------------------\n",
    "def sinkhorn_doubly_stochastic(logits: torch.Tensor, tmax: int = 20, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    logits: (n, n) unconstrained\n",
    "    return: (n, n) ~ doubly-stochastic via Sinkhorn-Knopp on exp(logits)\n",
    "    \"\"\"\n",
    "    # stabilize exp\n",
    "    z = logits - logits.max()\n",
    "    M = torch.exp(z)\n",
    "    for _ in range(int(tmax)):\n",
    "        M = M / (M.sum(dim=-1, keepdim=True) + eps)  # row norm\n",
    "        M = M / (M.sum(dim=-2, keepdim=True) + eps)  # col norm\n",
    "    return M\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Model: RMSNorm (PDF uses RMSNorm in big transformers)\n",
    "# ----------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., d)\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()\n",
    "        return x * rms * self.weight\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Transformer block + mHC mixing (multi-stream)\n",
    "# ----------------------------\n",
    "class MHCAttnBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Update only active stream-0, then mix all streams with H_res (mHC).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, ffn_mult, dropout, attn_dropout,\n",
    "                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01):\n",
    "        super().__init__()\n",
    "        self.n_streams = int(n_streams)\n",
    "        self.sinkhorn_tmax = int(sinkhorn_tmax)\n",
    "\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=attn_dropout, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ffn_mult * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_mult * d_model, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        # mHC params (PDF):\n",
    "        # raw logits -> Sinkhorn -> doubly-stochastic; then convex combine with Identity using alpha\n",
    "        self.h_logits = nn.Parameter(torch.zeros(self.n_streams, self.n_streams))\n",
    "        nn.init.zeros_(self.h_logits)\n",
    "\n",
    "        # alpha in (0,1), init alpha_init ~ 0.01\n",
    "        a0 = float(alpha_init)\n",
    "        a0 = min(max(a0, 1e-4), 1 - 1e-4)\n",
    "        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(a0 / (1 - a0), dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, streams):\n",
    "        # streams: (B, n_streams, S, D)\n",
    "        B, nS, S, D = streams.shape\n",
    "        assert nS == self.n_streams\n",
    "\n",
    "        # update only active stream 0\n",
    "        x = streams[:, 0]  # (B,S,D)\n",
    "\n",
    "        # self-attn (pre-norm)\n",
    "        x0 = x\n",
    "        q = self.norm1(x)\n",
    "        attn_out, _ = self.attn(q, q, q, need_weights=False)\n",
    "        x = x0 + self.drop1(attn_out)\n",
    "\n",
    "        # FFN (pre-norm)\n",
    "        x1 = x\n",
    "        h = self.norm2(x)\n",
    "        h = self.ffn(h)\n",
    "        x = x1 + self.drop2(h)\n",
    "\n",
    "        # put back\n",
    "        streams = streams.clone()\n",
    "        streams[:, 0] = x\n",
    "\n",
    "        # mHC mixing\n",
    "        H = sinkhorn_doubly_stochastic(self.h_logits, tmax=self.sinkhorn_tmax)  # (n,n)\n",
    "        alpha = torch.sigmoid(self.alpha_logit).to(dtype=streams.dtype)\n",
    "        I = torch.eye(self.n_streams, device=streams.device, dtype=streams.dtype)\n",
    "        Hres = (1.0 - alpha) * I + alpha * H.to(dtype=streams.dtype)\n",
    "\n",
    "        # mix across stream dimension\n",
    "        mixed = torch.einsum(\"ij,bjtd->bitd\", Hres, streams)\n",
    "        return mixed\n",
    "\n",
    "class MHCFTTransformer(nn.Module):\n",
    "    def __init__(self, n_features,\n",
    "                 d_model=256, n_heads=8, n_layers=6, ffn_mult=4,\n",
    "                 dropout=0.15, attn_dropout=0.10,\n",
    "                 n_streams=4, sinkhorn_tmax=20, alpha_init=0.01):\n",
    "        super().__init__()\n",
    "        self.n_features = int(n_features)\n",
    "        self.d_model = int(d_model)\n",
    "\n",
    "        # numeric tokenization: x_f -> x_f * W_f + b_f\n",
    "        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n",
    "        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n",
    "        self.in_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MHCAttnBlock(\n",
    "                d_model=self.d_model,\n",
    "                n_heads=n_heads,\n",
    "                ffn_mult=ffn_mult,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout,\n",
    "                n_streams=n_streams,\n",
    "                sinkhorn_tmax=sinkhorn_tmax,\n",
    "                alpha_init=alpha_init\n",
    "            )\n",
    "            for _ in range(int(n_layers))\n",
    "        ])\n",
    "\n",
    "        self.out_norm = RMSNorm(self.d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.d_model, 1),\n",
    "        )\n",
    "\n",
    "        self.n_streams = int(n_streams)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,F)\n",
    "        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)   # (B,F,D)\n",
    "        tok = tok + self.feat_emb.unsqueeze(0)\n",
    "        B = tok.size(0)\n",
    "        cls = self.cls.expand(B, -1, -1)                                    # (B,1,D)\n",
    "        seq = torch.cat([cls, tok], dim=1)                                  # (B,1+F,D)\n",
    "        seq = self.in_drop(seq)\n",
    "\n",
    "        # init streams: replicate seq\n",
    "        streams = seq.unsqueeze(1).repeat(1, self.n_streams, 1, 1)          # (B,nS,S,D)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            streams = blk(streams)\n",
    "\n",
    "        z = streams[:, 0, 0]  # CLS from active stream\n",
    "        z = self.out_norm(z)\n",
    "        logit = self.head(z).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "# ----------------------------\n",
    "# 10) LR Scheduler: warmup + step decay (0.8 & 0.9)\n",
    "# ----------------------------\n",
    "def make_warmup_step_scheduler(optimizer, total_steps: int, warmup_steps: int,\n",
    "                              milestones_frac=(0.8, 0.9), decay_values=(0.316, 0.1)):\n",
    "    m1 = int(float(milestones_frac[0]) * total_steps)\n",
    "    m2 = int(float(milestones_frac[1]) * total_steps)\n",
    "    d1 = float(decay_values[0])\n",
    "    d2 = float(decay_values[1])\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        # warmup: 0 -> 1\n",
    "        if step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "\n",
    "        # step decay piecewise (mirip tabel)\n",
    "        if step < m1:\n",
    "            mult = 1.0\n",
    "        elif step < m2:\n",
    "            mult = d1\n",
    "        else:\n",
    "            mult = d2\n",
    "        return mult\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Predict helper (handle batch=(xb,yb))\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, loader):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(xb)\n",
    "            p = torch.sigmoid(logits)\n",
    "        probs.append(p.detach().cpu().numpy())\n",
    "    return np.concatenate(probs, axis=0).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Train one fold (AMP + grad accumulation + early stopping)\n",
    "# ----------------------------\n",
    "def train_one_fold(X_tr, y_tr, X_va, y_va, cfg):\n",
    "    mu, sig = fit_standardizer(X_tr)\n",
    "    X_trn = apply_standardizer(X_tr, mu, sig)\n",
    "    X_van = apply_standardizer(X_va, mu, sig)\n",
    "\n",
    "    ds_tr = TabDataset(X_trn, y_tr)\n",
    "    ds_va = TabDataset(X_van, y_va)\n",
    "\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n",
    "        num_workers=2, pin_memory=(device.type == \"cuda\"),\n",
    "        drop_last=False\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n",
    "        num_workers=2, pin_memory=(device.type == \"cuda\"),\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    model = MHCFTTransformer(\n",
    "        n_features=n_features,\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        ffn_mult=int(cfg[\"ffn_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        attn_dropout=float(cfg[\"attn_dropout\"]),\n",
    "        n_streams=int(cfg[\"n_streams\"]),\n",
    "        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n",
    "        alpha_init=float(cfg[\"alpha_init\"]),\n",
    "    ).to(device)\n",
    "\n",
    "    # imbalance weight\n",
    "    pos = int(y_tr.sum())\n",
    "    neg = int(len(y_tr) - pos)\n",
    "    pos_weight = float(neg / max(1, pos))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        betas=tuple(cfg[\"betas\"]),\n",
    "        eps=float(cfg[\"eps\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "    )\n",
    "\n",
    "    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    optim_steps_per_epoch = int(math.ceil(len(dl_tr) / accum_steps))\n",
    "    total_optim_steps = int(cfg[\"epochs\"]) * max(1, optim_steps_per_epoch)\n",
    "    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n",
    "\n",
    "    sch = make_warmup_step_scheduler(\n",
    "        opt,\n",
    "        total_steps=total_optim_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        milestones_frac=cfg[\"lr_decay_milestones\"],\n",
    "        decay_values=cfg[\"lr_decay_values\"],\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    best = {\"val_logloss\": 1e9, \"epoch\": -1}\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(int(cfg[\"epochs\"])):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        micro_step = 0\n",
    "        optim_step_in_epoch = 0\n",
    "\n",
    "        for batch in dl_tr:\n",
    "            xb, yb = batch\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss = loss / accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            micro_step += 1\n",
    "\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * accum_steps  # undo divide for logging\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "            if (micro_step % accum_steps) == 0:\n",
    "                if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "                sch.step()\n",
    "                global_step += 1\n",
    "                optim_step_in_epoch += 1\n",
    "\n",
    "        # flush last partial accumulation (kalau ada)\n",
    "        if (micro_step % accum_steps) != 0:\n",
    "            if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            sch.step()\n",
    "            global_step += 1\n",
    "            optim_step_in_epoch += 1\n",
    "\n",
    "        # validate\n",
    "        p_va = predict_proba(model, dl_va)\n",
    "        vll = safe_logloss(y_va, p_va)\n",
    "\n",
    "        tr_loss = loss_sum / max(1, n_sum)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"  epoch {epoch+1:03d}/{cfg['epochs']} | train_loss={tr_loss:.5f} | val_logloss={vll:.5f} | opt_steps={optim_step_in_epoch} | dt={dt:.1f}s\")\n",
    "\n",
    "        improved = (best[\"val_logloss\"] - vll) > float(cfg[\"early_stop_min_delta\"])\n",
    "        if improved:\n",
    "            best[\"val_logloss\"] = float(vll)\n",
    "            best[\"epoch\"] = int(epoch)\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= int(cfg[\"early_stop_patience\"]):\n",
    "                print(f\"  early stop at epoch {epoch+1}, best_epoch={best['epoch']+1}, best_val_logloss={best['val_logloss']:.5f}\")\n",
    "                break\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "\n",
    "    # final val preds (best)\n",
    "    p_va = predict_proba(model, dl_va)\n",
    "\n",
    "    pack = {\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": cfg,\n",
    "    }\n",
    "    return pack, p_va, best\n",
    "\n",
    "# ----------------------------\n",
    "# 13) CV loop\n",
    "# ----------------------------\n",
    "oof_pred = np.zeros(n, dtype=np.float32)\n",
    "fold_reports = []\n",
    "\n",
    "models_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts/baseline_mhc_transformer_folds\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for f in unique_folds:\n",
    "    print(f\"\\n[Fold {f}]\")\n",
    "    tr_idx = np.where(folds != f)[0]\n",
    "    va_idx = np.where(folds == f)[0]\n",
    "\n",
    "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "    pack, p_va, best = train_one_fold(X_tr, y_tr, X_va, y_va, CFG)\n",
    "    oof_pred[va_idx] = p_va\n",
    "\n",
    "    auc = safe_auc(y_va, p_va)\n",
    "    thr = float(CFG[\"report_thr\"])\n",
    "    yhat = (p_va >= thr).astype(np.int32)\n",
    "\n",
    "    rep = {\n",
    "        \"fold\": int(f),\n",
    "        \"n_val\": int(len(va_idx)),\n",
    "        \"pos_val\": int(y_va.sum()),\n",
    "        \"auc\": auc,\n",
    "        f\"f1@{thr}\": float(f1_score(y_va, yhat, zero_division=0)),\n",
    "        f\"precision@{thr}\": float(precision_score(y_va, yhat, zero_division=0)),\n",
    "        f\"recall@{thr}\": float(recall_score(y_va, yhat, zero_division=0)),\n",
    "        \"logloss\": safe_logloss(y_va, p_va),\n",
    "        \"best_val_logloss\": float(best[\"val_logloss\"]),\n",
    "        \"best_epoch\": int(best[\"epoch\"] + 1),\n",
    "    }\n",
    "    fold_reports.append(rep)\n",
    "\n",
    "    torch.save(\n",
    "        {\"pack\": pack, \"feature_cols\": FEATURE_COLS},\n",
    "        models_dir / f\"baseline_mhc_transformer_fold_{f}.pt\"\n",
    "    )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Overall OOF metrics\n",
    "# ----------------------------\n",
    "oof_auc = safe_auc(y, oof_pred)\n",
    "thr = float(CFG[\"report_thr\"])\n",
    "oof_yhat = (oof_pred >= thr).astype(np.int32)\n",
    "\n",
    "overall = {\n",
    "    \"rows\": int(n),\n",
    "    \"folds\": int(n_folds),\n",
    "    \"pos_total\": int(y.sum()),\n",
    "    \"pos_rate\": float(y.mean()),\n",
    "    \"oof_auc\": oof_auc,\n",
    "    f\"oof_f1@{thr}\": float(f1_score(y, oof_yhat, zero_division=0)),\n",
    "    f\"oof_precision@{thr}\": float(precision_score(y, oof_yhat, zero_division=0)),\n",
    "    f\"oof_recall@{thr}\": float(recall_score(y, oof_yhat, zero_division=0)),\n",
    "    \"oof_logloss\": safe_logloss(y, oof_pred),\n",
    "}\n",
    "\n",
    "df_rep = pd.DataFrame(fold_reports).sort_values(\"fold\").reset_index(drop=True)\n",
    "print(\"\\nPer-fold report:\")\n",
    "display(df_rep)\n",
    "\n",
    "print(\"\\nOOF overall:\")\n",
    "print(overall)\n",
    "\n",
    "# ----------------------------\n",
    "# 15) Train FULL model (fixed epochs = 70% of CV epochs)\n",
    "# ----------------------------\n",
    "def train_full_fixed(X_full_raw, y_full, cfg):\n",
    "    mu, sig = fit_standardizer(X_full_raw)\n",
    "    X_full = apply_standardizer(X_full_raw, mu, sig)\n",
    "\n",
    "    ds_full = TabDataset(X_full, y_full)\n",
    "    dl_full = DataLoader(\n",
    "        ds_full, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n",
    "        num_workers=2, pin_memory=(device.type == \"cuda\"),\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    model = MHCFTTransformer(\n",
    "        n_features=n_features,\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        ffn_mult=int(cfg[\"ffn_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        attn_dropout=float(cfg[\"attn_dropout\"]),\n",
    "        n_streams=int(cfg[\"n_streams\"]),\n",
    "        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n",
    "        alpha_init=float(cfg[\"alpha_init\"]),\n",
    "    ).to(device)\n",
    "\n",
    "    pos = int(y_full.sum())\n",
    "    neg = int(len(y_full) - pos)\n",
    "    pos_weight = float(neg / max(1, pos))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        betas=tuple(cfg[\"betas\"]),\n",
    "        eps=float(cfg[\"eps\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "    )\n",
    "\n",
    "    accum_steps = max(1, int(cfg.get(\"accum_steps\", 1)))\n",
    "    E_FULL = max(12, int(cfg[\"epochs\"] * 0.7))\n",
    "    optim_steps_per_epoch = int(math.ceil(len(dl_full) / accum_steps))\n",
    "    total_optim_steps = E_FULL * max(1, optim_steps_per_epoch)\n",
    "    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_optim_steps)\n",
    "\n",
    "    sch = make_warmup_step_scheduler(\n",
    "        opt,\n",
    "        total_steps=total_optim_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        milestones_frac=cfg[\"lr_decay_milestones\"],\n",
    "        decay_values=cfg[\"lr_decay_values\"],\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    print(f\"\\nTraining full mHC transformer for {E_FULL} epochs (fixed)...\")\n",
    "    for epoch in range(E_FULL):\n",
    "        model.train()\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        micro_step = 0\n",
    "        for xb, yb in dl_full:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss = loss / accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            micro_step += 1\n",
    "\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * accum_steps\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "            if (micro_step % accum_steps) == 0:\n",
    "                if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "\n",
    "        # flush last partial\n",
    "        if (micro_step % accum_steps) != 0:\n",
    "            if cfg[\"grad_clip\"] and float(cfg[\"grad_clip\"]) > 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            sch.step()\n",
    "\n",
    "        print(f\"  full epoch {epoch+1:03d}/{E_FULL} | loss={loss_sum/max(1,n_sum):.5f}\")\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    full_pack = {\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": cfg,\n",
    "    }\n",
    "    return full_pack\n",
    "\n",
    "out_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "full_pack = train_full_fixed(X, y, CFG)\n",
    "torch.save({\"pack\": full_pack, \"feature_cols\": FEATURE_COLS}, out_dir / \"baseline_mhc_transformer_model_full.pt\")\n",
    "\n",
    "# ----------------------------\n",
    "# 16) Save OOF + report\n",
    "# ----------------------------\n",
    "df_oof = df_train_tabular[[\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]].copy()\n",
    "df_oof[\"oof_pred_baseline_mhc_tf\"] = oof_pred\n",
    "df_oof.to_csv(out_dir / \"oof_baseline_mhc_transformer.csv\", index=False)\n",
    "\n",
    "report = {\n",
    "    \"model\": \"mHC-FTTransformer (numeric tabular) — baseline\",\n",
    "    \"cfg\": CFG,\n",
    "    \"feature_count\": int(len(FEATURE_COLS)),\n",
    "    \"fold_reports\": fold_reports,\n",
    "    \"overall\": overall,\n",
    "}\n",
    "with open(out_dir / \"baseline_mhc_transformer_cv_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\"  fold models  ->\", models_dir)\n",
    "print(\"  full model   ->\", out_dir / \"baseline_mhc_transformer_model_full.pt\")\n",
    "print(\"  oof preds    ->\", out_dir / \"oof_baseline_mhc_transformer.csv\")\n",
    "print(\"  cv report    ->\", out_dir / \"baseline_mhc_transformer_cv_report.json\")\n",
    "\n",
    "# Export globals\n",
    "OOF_PRED_BASELINE_MHC_TF = oof_pred\n",
    "BASELINE_MHC_TF_OVERALL = overall\n",
    "BASELINE_MHC_TF_FOLD_REPORTS = fold_reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e57ac",
   "metadata": {
    "papermill": {
     "duration": 0.010636,
     "end_time": "2026-01-04T14:08:31.406016",
     "exception": false,
     "start_time": "2026-01-04T14:08:31.395380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimize Model & Hyperparameters (Iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b26d7ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:08:31.428217Z",
     "iopub.status.busy": "2026-01-04T14:08:31.427628Z",
     "iopub.status.idle": "2026-01-04T15:41:20.494160Z",
     "shell.execute_reply": "2026-01-04T15:41:20.493327Z"
    },
    "papermill": {
     "duration": 5569.091876,
     "end_time": "2026-01-04T15:41:20.507764",
     "exception": false,
     "start_time": "2026-01-04T14:08:31.415888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize setup (Transformer only, mHC-lite):\n",
      "  rows=5176 | folds=5 | pos%=54.08 | n_features=62\n",
      "Device: cuda | AMP: True\n",
      "\n",
      "Total Transformer candidates: 5\n",
      "Primary score: OOF best F-beta (beta=0.5)\n",
      "\n",
      "Stage-1 folds subset: [0, 1, 2]\n",
      "\n",
      "[Stage-1 01/5] CV(subset) -> mhc_384x8\n",
      "  stage1 best_fbeta: 0.596330 | thr: 0.461 | logloss: 0.688900\n",
      "\n",
      "[Stage-1 02/5] CV(subset) -> mhc_384x10_reg\n",
      "  stage1 best_fbeta: 0.597149 | thr: 0.441 | logloss: 0.687816\n",
      "\n",
      "[Stage-1 03/5] CV(subset) -> mhc_384x8_ffn2\n",
      "  stage1 best_fbeta: 0.596601 | thr: 0.451 | logloss: 0.688464\n",
      "\n",
      "[Stage-1 04/5] CV(subset) -> mhc_256x6_fast\n",
      "  stage1 best_fbeta: 0.595978 | thr: 0.510 | logloss: 0.688378\n",
      "\n",
      "[Stage-1 05/5] CV(subset) -> mhc_512x10_big\n",
      "  stage1 best_fbeta: 0.597284 | thr: 0.490 | logloss: 0.689078\n",
      "\n",
      "Stage-1 ranking (top):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cfg</th>\n",
       "      <th>stage</th>\n",
       "      <th>oof_auc</th>\n",
       "      <th>oof_logloss</th>\n",
       "      <th>oof_best_fbeta</th>\n",
       "      <th>oof_best_thr</th>\n",
       "      <th>oof_best_prec</th>\n",
       "      <th>oof_best_rec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>...</th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>adam_eps</th>\n",
       "      <th>lr_decay_ratio1</th>\n",
       "      <th>lr_decay_ratio2</th>\n",
       "      <th>lr_decay_rate1</th>\n",
       "      <th>lr_decay_rate2</th>\n",
       "      <th>patience</th>\n",
       "      <th>min_delta</th>\n",
       "      <th>grad_clip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mhc_512x10_big</td>\n",
       "      <td>subset3</td>\n",
       "      <td>0.528329</td>\n",
       "      <td>0.689078</td>\n",
       "      <td>0.597284</td>\n",
       "      <td>0.4902</td>\n",
       "      <td>0.546703</td>\n",
       "      <td>0.948183</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mhc_384x10_reg</td>\n",
       "      <td>subset3</td>\n",
       "      <td>0.524184</td>\n",
       "      <td>0.687816</td>\n",
       "      <td>0.597149</td>\n",
       "      <td>0.4412</td>\n",
       "      <td>0.543400</td>\n",
       "      <td>0.988088</td>\n",
       "      <td>384</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mhc_384x8_ffn2</td>\n",
       "      <td>subset3</td>\n",
       "      <td>0.526784</td>\n",
       "      <td>0.688464</td>\n",
       "      <td>0.596601</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>0.543883</td>\n",
       "      <td>0.974390</td>\n",
       "      <td>384</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mhc_384x8</td>\n",
       "      <td>subset3</td>\n",
       "      <td>0.523240</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.4608</td>\n",
       "      <td>0.542914</td>\n",
       "      <td>0.983323</td>\n",
       "      <td>384</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mhc_256x6_fast</td>\n",
       "      <td>subset3</td>\n",
       "      <td>0.529948</td>\n",
       "      <td>0.688378</td>\n",
       "      <td>0.595978</td>\n",
       "      <td>0.5098</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>0.970816</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cfg    stage   oof_auc  oof_logloss  oof_best_fbeta  \\\n",
       "0  mhc_512x10_big  subset3  0.528329     0.689078        0.597284   \n",
       "1  mhc_384x10_reg  subset3  0.524184     0.687816        0.597149   \n",
       "2  mhc_384x8_ffn2  subset3  0.526784     0.688464        0.596601   \n",
       "3       mhc_384x8  subset3  0.523240     0.688900        0.596330   \n",
       "4  mhc_256x6_fast  subset3  0.529948     0.688378        0.595978   \n",
       "\n",
       "   oof_best_thr  oof_best_prec  oof_best_rec  d_model  n_layers  ...  beta1  \\\n",
       "0        0.4902       0.546703      0.948183      512        10  ...    0.9   \n",
       "1        0.4412       0.543400      0.988088      384        10  ...    0.9   \n",
       "2        0.4510       0.543883      0.974390      384         8  ...    0.9   \n",
       "3        0.4608       0.542914      0.983323      384         8  ...    0.9   \n",
       "4        0.5098       0.543515      0.970816      256         6  ...    0.9   \n",
       "\n",
       "   beta2      adam_eps  lr_decay_ratio1  lr_decay_ratio2  lr_decay_rate1  \\\n",
       "0   0.95  1.000000e-08              0.8              0.9           0.316   \n",
       "1   0.95  1.000000e-08              0.8              0.9           0.316   \n",
       "2   0.95  1.000000e-08              0.8              0.9           0.316   \n",
       "3   0.95  1.000000e-08              0.8              0.9           0.316   \n",
       "4   0.95  1.000000e-08              0.8              0.9           0.316   \n",
       "\n",
       "   lr_decay_rate2  patience  min_delta  grad_clip  \n",
       "0             0.1         6     0.0001        1.0  \n",
       "1             0.1         6     0.0001        1.0  \n",
       "2             0.1         6     0.0001        1.0  \n",
       "3             0.1         6     0.0001        1.0  \n",
       "4             0.1         6     0.0001        1.0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage-2 will run full CV for: ['mhc_512x10_big', 'mhc_384x10_reg', 'mhc_384x8_ffn2']\n",
      "\n",
      "[Stage-2 01/3] CV(full) -> mhc_512x10_big\n",
      "  OOF best_fbeta: 0.598793 | thr: 0.490 | auc: 0.530975 | logloss: 0.687369\n",
      "\n",
      "[Stage-2 02/3] CV(full) -> mhc_384x10_reg\n",
      "  OOF best_fbeta: 0.599523 | thr: 0.471 | auc: 0.535769 | logloss: 0.685788\n",
      "\n",
      "[Stage-2 03/3] CV(full) -> mhc_384x8_ffn2\n",
      "  OOF best_fbeta: 0.598313 | thr: 0.466 | auc: 0.526982 | logloss: 0.687210\n",
      "\n",
      "Stage-2 top candidates (full CV):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cfg</th>\n",
       "      <th>stage</th>\n",
       "      <th>oof_auc</th>\n",
       "      <th>oof_logloss</th>\n",
       "      <th>oof_best_fbeta</th>\n",
       "      <th>oof_best_thr</th>\n",
       "      <th>oof_best_prec</th>\n",
       "      <th>oof_best_rec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>...</th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>adam_eps</th>\n",
       "      <th>lr_decay_ratio1</th>\n",
       "      <th>lr_decay_ratio2</th>\n",
       "      <th>lr_decay_rate1</th>\n",
       "      <th>lr_decay_rate2</th>\n",
       "      <th>patience</th>\n",
       "      <th>min_delta</th>\n",
       "      <th>grad_clip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mhc_384x10_reg</td>\n",
       "      <td>full</td>\n",
       "      <td>0.535769</td>\n",
       "      <td>0.685788</td>\n",
       "      <td>0.599523</td>\n",
       "      <td>0.4706</td>\n",
       "      <td>0.547269</td>\n",
       "      <td>0.969989</td>\n",
       "      <td>384</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mhc_512x10_big</td>\n",
       "      <td>full</td>\n",
       "      <td>0.530975</td>\n",
       "      <td>0.687369</td>\n",
       "      <td>0.598793</td>\n",
       "      <td>0.4902</td>\n",
       "      <td>0.550598</td>\n",
       "      <td>0.921401</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mhc_384x8_ffn2</td>\n",
       "      <td>full</td>\n",
       "      <td>0.526982</td>\n",
       "      <td>0.687210</td>\n",
       "      <td>0.598313</td>\n",
       "      <td>0.4657</td>\n",
       "      <td>0.546151</td>\n",
       "      <td>0.968203</td>\n",
       "      <td>384</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cfg stage   oof_auc  oof_logloss  oof_best_fbeta  oof_best_thr  \\\n",
       "0  mhc_384x10_reg  full  0.535769     0.685788        0.599523        0.4706   \n",
       "1  mhc_512x10_big  full  0.530975     0.687369        0.598793        0.4902   \n",
       "2  mhc_384x8_ffn2  full  0.526982     0.687210        0.598313        0.4657   \n",
       "\n",
       "   oof_best_prec  oof_best_rec  d_model  n_layers  ...  beta1  beta2  \\\n",
       "0       0.547269      0.969989      384        10  ...    0.9   0.95   \n",
       "1       0.550598      0.921401      512        10  ...    0.9   0.95   \n",
       "2       0.546151      0.968203      384         8  ...    0.9   0.95   \n",
       "\n",
       "       adam_eps  lr_decay_ratio1  lr_decay_ratio2  lr_decay_rate1  \\\n",
       "0  1.000000e-08              0.8              0.9           0.316   \n",
       "1  1.000000e-08              0.8              0.9           0.316   \n",
       "2  1.000000e-08              0.8              0.9           0.316   \n",
       "\n",
       "   lr_decay_rate2  patience  min_delta  grad_clip  \n",
       "0             0.1        12     0.0001        1.0  \n",
       "1             0.1        12     0.0001        1.0  \n",
       "2             0.1        10     0.0001        1.0  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best config: mhc_384x10_reg\n",
      "{'cfg': 'mhc_384x10_reg', 'stage': 'full', 'oof_auc': 0.5357689799364909, 'oof_logloss': 0.6857882784945699, 'oof_best_fbeta': 0.5995230314004328, 'oof_best_thr': 0.4706, 'oof_best_prec': 0.5472686958274542, 'oof_best_rec': 0.969989281886388, 'd_model': 384, 'n_layers': 10, 'n_heads': 8, 'ffn_mult': 4, 'dropout': 0.25, 'attn_dropout': 0.12, 'n_streams': 4, 'mhc_alpha': 0.01, 'sinkhorn_tmax': 20, 'mhc_dropout': 0.0, 'batch_size': 512, 'epochs': 85, 'lr': 0.00016, 'weight_decay': 0.012, 'warmup_frac': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'adam_eps': 1e-08, 'lr_decay_ratio1': 0.8, 'lr_decay_ratio2': 0.9, 'lr_decay_rate1': 0.316, 'lr_decay_rate2': 0.1, 'patience': 12, 'min_delta': 0.0001, 'grad_clip': 1.0}\n",
      "\n",
      "Re-train folds for best config -> mhc_384x10_reg\n",
      "\n",
      "Saved best artifacts:\n",
      "  best model (fold packs) -> /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt\n",
      "  best config             -> /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n",
      "  opt results             -> /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n",
      "  fold detail             -> /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 4 — Optimize Model & Hyperparameters (Iterative) — TRANSFORMER ONLY (REVISI FULL v2, mHC-lite from PDF)\n",
    "# - FIX: predict_proba robust (batch can be xb or (xb,yb) or list/tuple) => NO ERROR\n",
    "# - Implementasi materi PDF (adapted, runtime-safe):\n",
    "#     * mHC-lite (multi-stream residual on CLS) with Sinkhorn-Knopp (tmax=20)\n",
    "#     * n_streams=4, alpha=0.01 (default)\n",
    "#     * AdamW betas=(0.9,0.95)\n",
    "#     * LR schedule: Warmup + Step decay at ratios [0.8,0.9] with rates [0.316, 0.1]\n",
    "# - Validasi: leakage-safe CV pakai `fold` by case_id\n",
    "# - Skor utama: OOF best F-beta (beta=0.5)\n",
    "#\n",
    "# Output:\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.json\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/oof_preds_<cfg_name>.csv (top configs)\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt\n",
    "#\n",
    "# REQUIRE:\n",
    "# - Step 2 sudah jalan: df_train_tabular, FEATURE_COLS\n",
    "# ============================================================\n",
    "\n",
    "import os, json, gc, math, time, warnings, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require data from Step 2\n",
    "# ----------------------------\n",
    "need_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\n",
    "for v in need_vars:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n",
    "\n",
    "df_train_tabular = df_train_tabular.copy()\n",
    "FEATURE_COLS = list(FEATURE_COLS)\n",
    "\n",
    "X_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "y_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n",
    "folds_all = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\n",
    "uids_all = df_train_tabular[\"uid\"].astype(str).to_numpy()\n",
    "\n",
    "# guard\n",
    "if not np.isfinite(X_all).all():\n",
    "    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "unique_folds = sorted(pd.Series(folds_all).unique().tolist())\n",
    "n = len(y_all)\n",
    "pos_rate = float(y_all.mean())\n",
    "n_features = X_all.shape[1]\n",
    "\n",
    "print(\"Optimize setup (Transformer only, mHC-lite):\")\n",
    "print(f\"  rows={n} | folds={len(unique_folds)} | pos%={pos_rate*100:.2f} | n_features={n_features}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Global settings\n",
    "# ----------------------------\n",
    "SEED = 2025\n",
    "BETA = 0.5\n",
    "THR_GRID = 201\n",
    "\n",
    "# runtime control (tanpa mengubah akurasi terlalu banyak)\n",
    "STAGE1_FOLDS = min(3, len(unique_folds))         # stage-1 pakai subset fold (lebih cepat)\n",
    "STAGE1_EPOCH_CAP = 40                            # stage-1 cap epoch\n",
    "STAGE1_PAT_CAP = 6\n",
    "\n",
    "STAGE2_TOPM = min(3, 5)                          # stage-2 full CV hanya top-M config\n",
    "REPORT_TOPK_OOF = 3\n",
    "\n",
    "# optional time budget (biar tidak kebablasan)\n",
    "TIME_BUDGET_SEC = 0\n",
    "# contoh: TIME_BUDGET_SEC = 2.5 * 60 * 60  # 2.5 jam\n",
    "# biarkan 0 jika tidak ingin dihentikan otomatis\n",
    "\n",
    "def seed_everything(seed=2025):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "print(\"Device:\", device, \"| AMP:\", use_amp)\n",
    "\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Helpers: threshold search + safe metrics\n",
    "# ----------------------------\n",
    "def best_fbeta_fast(y_true, p, beta=0.5, grid=201):\n",
    "    y = (np.asarray(y_true).astype(np.int32) == 1)\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1.0 - 1e-8)\n",
    "\n",
    "    thrs = np.linspace(0.01, 0.99, grid, dtype=np.float64)\n",
    "    pred = (p[:, None] >= thrs[None, :])\n",
    "\n",
    "    y1 = y[:, None]\n",
    "    tp = (pred & y1).sum(axis=0).astype(np.float64)\n",
    "    fp = (pred & (~y1)).sum(axis=0).astype(np.float64)\n",
    "    fn = (y.sum().astype(np.float64) - tp)\n",
    "\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n",
    "    recall    = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n",
    "\n",
    "    b2 = beta * beta\n",
    "    denom = (b2 * precision + recall)\n",
    "    fbeta = np.divide((1.0 + b2) * precision * recall, denom, out=np.zeros_like(precision), where=denom > 0)\n",
    "\n",
    "    j = int(np.argmax(fbeta))\n",
    "    return {\n",
    "        \"fbeta\": float(fbeta[j]),\n",
    "        \"thr\": float(thrs[j]),\n",
    "        \"precision\": float(precision[j]),\n",
    "        \"recall\": float(recall[j]),\n",
    "    }\n",
    "\n",
    "def safe_auc(y_true, p):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return None\n",
    "    return float(roc_auc_score(y_true, p))\n",
    "\n",
    "def safe_logloss(y_true, p):\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n",
    "    return float(log_loss(y_true, p, labels=[0, 1]))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset + Standardizer (fit only on train fold => no leakage)\n",
    "# ----------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def fit_standardizer(X_tr: np.ndarray):\n",
    "    mu = X_tr.mean(axis=0, dtype=np.float64)\n",
    "    sig = X_tr.std(axis=0, dtype=np.float64)\n",
    "    sig = np.where(sig < 1e-8, 1.0, sig)\n",
    "    return mu.astype(np.float32), sig.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n",
    "    return ((X_in - mu) / sig).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) PDF-inspired blocks: RMSNorm + Sinkhorn + mHC-lite on CLS streams\n",
    "# ----------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., d)\n",
    "        rms = torch.mean(x * x, dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(rms + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "@torch.no_grad()\n",
    "def sinkhorn_knopp(P, tmax=20, eps=1e-12):\n",
    "    \"\"\"\n",
    "    P: (B, n, n) non-negative\n",
    "    output: doubly-stochastic approx via alternating row/col normalization\n",
    "    \"\"\"\n",
    "    M = P.clamp_min(eps)\n",
    "    for _ in range(int(tmax)):\n",
    "        M = M / (M.sum(dim=-1, keepdim=True).clamp_min(eps))  # row norm\n",
    "        M = M / (M.sum(dim=-2, keepdim=True).clamp_min(eps))  # col norm\n",
    "    return M\n",
    "\n",
    "class MHCLite(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight adaptation of mHC:\n",
    "    - maintain n_streams residual streams only for CLS embedding\n",
    "    - compute non-negative mixing matrix via softplus + Sinkhorn-Knopp\n",
    "    - inject current CLS back to each stream\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_streams=4, alpha=0.01, tmax=20, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n = int(n_streams)\n",
    "        self.alpha = float(alpha)\n",
    "        self.tmax = int(tmax)\n",
    "        self.dropout = nn.Dropout(float(dropout))\n",
    "\n",
    "        # dynamic mapping from CLS -> n*n\n",
    "        # use RMSNorm to stabilize (PDF mentions RMSNorm in infra)\n",
    "        self.norm = RMSNorm(d_model, eps=1e-8)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, self.n * self.n),\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, streams, cls_vec):\n",
    "        # streams: (B, n, D)\n",
    "        # cls_vec: (B, D)\n",
    "        B, n, D = streams.shape\n",
    "        h = self.norm(cls_vec)\n",
    "        logits = self.mlp(h).view(B, n, n) * self.alpha\n",
    "        P = self.softplus(logits)  # non-negative\n",
    "        M = sinkhorn_knopp(P, tmax=self.tmax)  # (B,n,n) doubly-stochastic approx\n",
    "\n",
    "        mixed = torch.einsum(\"bij,bjd->bid\", M, streams)       # mix streams\n",
    "        injected = mixed + cls_vec.unsqueeze(1)                # inject CLS\n",
    "        return self.dropout(injected)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom transformer block with RMSNorm + separate attn_dropout support.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model, eps=1e-8)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, num_heads=int(n_heads),\n",
    "            dropout=float(attn_dropout), batch_first=True\n",
    "        )\n",
    "        self.drop1 = nn.Dropout(float(dropout))\n",
    "\n",
    "        self.norm2 = RMSNorm(d_model, eps=1e-8)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, int(ffn_mult) * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "            nn.Linear(int(ffn_mult) * d_model, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(float(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm (norm_first)\n",
    "        h = self.norm1(x)\n",
    "        attn_out, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = x + self.drop1(attn_out)\n",
    "\n",
    "        h = self.norm2(x)\n",
    "        x = x + self.drop2(self.ffn(h))\n",
    "        return x\n",
    "\n",
    "class FTTransformer_MHCLite(nn.Module):\n",
    "    \"\"\"\n",
    "    Numeric FT-Transformer + mHC-lite on CLS between blocks (runtime-safe).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n",
    "                 dropout=0.2, attn_dropout=0.1,\n",
    "                 n_streams=4, mhc_alpha=0.01, sinkhorn_tmax=20, mhc_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n_features = int(n_features)\n",
    "        self.d_model = int(d_model)\n",
    "        self.n_layers = int(n_layers)\n",
    "\n",
    "        # per-feature linear tokenization\n",
    "        self.w = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "        self.b = nn.Parameter(torch.zeros(self.n_features, self.d_model))\n",
    "        self.feat_emb = nn.Parameter(torch.randn(self.n_features, self.d_model) * 0.02)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, self.d_model) * 0.02)\n",
    "\n",
    "        self.token_dropout = nn.Dropout(float(attn_dropout))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=self.d_model,\n",
    "                n_heads=n_heads,\n",
    "                ffn_mult=ffn_mult,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout\n",
    "            ) for _ in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.mhc = nn.ModuleList([\n",
    "            MHCLite(\n",
    "                d_model=self.d_model,\n",
    "                n_streams=n_streams,\n",
    "                alpha=mhc_alpha,\n",
    "                tmax=sinkhorn_tmax,\n",
    "                dropout=mhc_dropout\n",
    "            ) for _ in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.out_norm = RMSNorm(self.d_model, eps=1e-8)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "            nn.Linear(self.d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, F)\n",
    "        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n",
    "        tok = tok + self.feat_emb.unsqueeze(0)\n",
    "        tok = self.token_dropout(tok)\n",
    "\n",
    "        B = tok.size(0)\n",
    "        cls = self.cls.expand(B, -1, -1)   # (B,1,D)\n",
    "        seq = torch.cat([cls, tok], dim=1) # (B,1+F,D)\n",
    "\n",
    "        # init streams from CLS\n",
    "        streams = seq[:, 0:1, :].expand(B, self.mhc[0].n, self.d_model).contiguous()\n",
    "\n",
    "        for l, blk in enumerate(self.blocks):\n",
    "            # inject mean-stream into CLS before block\n",
    "            seq[:, 0, :] = streams.mean(dim=1)\n",
    "\n",
    "            seq = blk(seq)\n",
    "            cls_vec = seq[:, 0, :]\n",
    "\n",
    "            # mHC-lite update streams using current CLS\n",
    "            streams = self.mhc[l](streams, cls_vec)\n",
    "\n",
    "        out = self.out_norm(streams.mean(dim=1))\n",
    "        logit = self.head(out).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "# ----------------------------\n",
    "# 5) LR schedule from PDF: warmup + step decay at ratios [0.8,0.9] with rates [0.316,0.1]\n",
    "# ----------------------------\n",
    "def make_warmup_step_scheduler(optimizer, total_steps, warmup_steps, r1=0.8, r2=0.9, d1=0.316, d2=0.1):\n",
    "    m1 = int(r1 * total_steps)\n",
    "    m2 = int(r2 * total_steps)\n",
    "    def lr_lambda(step):\n",
    "        if warmup_steps > 0 and step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "        mult = 1.0\n",
    "        if step >= m1:\n",
    "            mult *= float(d1)\n",
    "        if step >= m2:\n",
    "            mult *= float(d2)\n",
    "        return mult\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Predict helper (FIX: handle batch=(xb,yb) / list / tuple)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, loader):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(xb)\n",
    "            p = torch.sigmoid(logits)\n",
    "        probs.append(p.detach().cpu().numpy())\n",
    "    return np.concatenate(probs, axis=0).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Train one fold (AMP + early stopping)\n",
    "# ----------------------------\n",
    "def train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg):\n",
    "    mu, sig = fit_standardizer(X_tr)\n",
    "    X_trn = apply_standardizer(X_tr, mu, sig)\n",
    "    X_van = apply_standardizer(X_va, mu, sig)\n",
    "\n",
    "    ds_tr = TabDataset(X_trn, y_tr)\n",
    "    ds_va = TabDataset(X_van, y_va)\n",
    "\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr, batch_size=int(cfg[\"batch_size\"]), shuffle=True,\n",
    "        num_workers=2, pin_memory=(device.type==\"cuda\"), drop_last=False\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        ds_va, batch_size=int(cfg[\"batch_size\"]), shuffle=False,\n",
    "        num_workers=2, pin_memory=(device.type==\"cuda\"), drop_last=False\n",
    "    )\n",
    "\n",
    "    model = FTTransformer_MHCLite(\n",
    "        n_features=n_features,\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        ffn_mult=int(cfg[\"ffn_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        attn_dropout=float(cfg[\"attn_dropout\"]),\n",
    "        n_streams=int(cfg[\"n_streams\"]),\n",
    "        mhc_alpha=float(cfg[\"mhc_alpha\"]),\n",
    "        sinkhorn_tmax=int(cfg[\"sinkhorn_tmax\"]),\n",
    "        mhc_dropout=float(cfg[\"mhc_dropout\"]),\n",
    "    ).to(device)\n",
    "\n",
    "    # imbalance -> pos_weight for BCEWithLogitsLoss\n",
    "    pos = int(y_tr.sum())\n",
    "    neg = int(len(y_tr) - pos)\n",
    "    pos_weight = float(neg / max(1, pos))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
    "\n",
    "    # PDF-like AdamW betas\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "        betas=(float(cfg[\"beta1\"]), float(cfg[\"beta2\"])),\n",
    "        eps=float(cfg[\"adam_eps\"]),\n",
    "    )\n",
    "\n",
    "    total_steps = int(cfg[\"epochs\"]) * max(1, len(dl_tr))\n",
    "    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n",
    "\n",
    "    sch = make_warmup_step_scheduler(\n",
    "        opt,\n",
    "        total_steps=total_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        r1=float(cfg[\"lr_decay_ratio1\"]),\n",
    "        r2=float(cfg[\"lr_decay_ratio2\"]),\n",
    "        d1=float(cfg[\"lr_decay_rate1\"]),\n",
    "        d2=float(cfg[\"lr_decay_rate2\"]),\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    best_val = 1e9\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    bad = 0\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(int(cfg[\"epochs\"])):\n",
    "        model.train()\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "\n",
    "        for xb, yb in dl_tr:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if float(cfg[\"grad_clip\"]) and float(cfg[\"grad_clip\"]) > 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            sch.step()\n",
    "\n",
    "            loss_sum += float(loss.item()) * xb.size(0)\n",
    "            n_sum += xb.size(0)\n",
    "            step += 1\n",
    "\n",
    "        # val\n",
    "        p_va = predict_proba(model, dl_va)\n",
    "        vll = safe_logloss(y_va, p_va)\n",
    "\n",
    "        improved = (best_val - vll) > float(cfg[\"min_delta\"])\n",
    "        if improved:\n",
    "            best_val = float(vll)\n",
    "            best_epoch = int(epoch)\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= int(cfg[\"patience\"]):\n",
    "                break\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "\n",
    "    p_va = predict_proba(model, dl_va)\n",
    "\n",
    "    pack = {\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": dict(cfg),\n",
    "        \"best_epoch\": int(best_epoch + 1),\n",
    "        \"best_val_logloss\": float(best_val),\n",
    "    }\n",
    "    return pack, p_va\n",
    "\n",
    "# ----------------------------\n",
    "# 8) CV evaluator for a config (optionally limited folds)\n",
    "# ----------------------------\n",
    "def run_cv_config(cfg, cfg_name, folds_subset=None, beta=0.5, thr_grid=201):\n",
    "    oof = np.zeros(n, dtype=np.float32)\n",
    "    fold_rows = []\n",
    "    fold_packs = []\n",
    "\n",
    "    use_folds = unique_folds if folds_subset is None else list(folds_subset)\n",
    "\n",
    "    for f in use_folds:\n",
    "        tr = np.where(folds_all != f)[0]\n",
    "        va = np.where(folds_all == f)[0]\n",
    "\n",
    "        X_tr, y_tr = X_all[tr], y_all[tr]\n",
    "        X_va, y_va = X_all[va], y_all[va]\n",
    "\n",
    "        pack, p_va = train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg)\n",
    "        oof[va] = p_va\n",
    "\n",
    "        fold_auc = safe_auc(y_va, p_va)\n",
    "        fold_ll  = safe_logloss(y_va, p_va)\n",
    "        best_fold = best_fbeta_fast(y_va, p_va, beta=beta, grid=max(51, thr_grid//2))\n",
    "\n",
    "        fold_rows.append({\n",
    "            \"cfg\": cfg_name,\n",
    "            \"fold\": int(f),\n",
    "            \"n_val\": int(len(va)),\n",
    "            \"pos_val\": int(y_va.sum()),\n",
    "            \"auc\": fold_auc,\n",
    "            \"logloss\": fold_ll,\n",
    "            \"best_fbeta\": best_fold[\"fbeta\"],\n",
    "            \"best_thr\": best_fold[\"thr\"],\n",
    "            \"best_prec\": best_fold[\"precision\"],\n",
    "            \"best_rec\": best_fold[\"recall\"],\n",
    "            \"best_val_logloss\": float(pack[\"best_val_logloss\"]),\n",
    "            \"best_epoch\": int(pack[\"best_epoch\"]),\n",
    "        })\n",
    "\n",
    "        fold_packs.append({\"fold\": int(f), \"pack\": pack})\n",
    "\n",
    "        del pack\n",
    "        gc.collect()\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # overall (on filled parts only)\n",
    "    # NOTE: ini tetap OOF \"full-length\", tapi fold yang tidak dihitung akan 0.\n",
    "    # Untuk stage-1 ranking, kita hitung metric hanya pada indeks fold subset.\n",
    "    if folds_subset is None:\n",
    "        idx_eval = np.arange(n)\n",
    "    else:\n",
    "        idx_eval = np.where(np.isin(folds_all, np.array(use_folds)))[0]\n",
    "\n",
    "    oof_eval = oof[idx_eval]\n",
    "    y_eval = y_all[idx_eval]\n",
    "\n",
    "    oof_auc = safe_auc(y_eval, oof_eval)\n",
    "    oof_ll  = safe_logloss(y_eval, oof_eval)\n",
    "    best_oof = best_fbeta_fast(y_eval, oof_eval, beta=beta, grid=thr_grid)\n",
    "\n",
    "    summary = {\n",
    "        \"cfg\": cfg_name,\n",
    "        \"stage\": \"full\" if folds_subset is None else f\"subset{len(use_folds)}\",\n",
    "        \"oof_auc\": oof_auc,\n",
    "        \"oof_logloss\": oof_ll,\n",
    "        \"oof_best_fbeta\": best_oof[\"fbeta\"],\n",
    "        \"oof_best_thr\": best_oof[\"thr\"],\n",
    "        \"oof_best_prec\": best_oof[\"precision\"],\n",
    "        \"oof_best_rec\": best_oof[\"recall\"],\n",
    "\n",
    "        # log cfg\n",
    "        \"d_model\": cfg[\"d_model\"],\n",
    "        \"n_layers\": cfg[\"n_layers\"],\n",
    "        \"n_heads\": cfg[\"n_heads\"],\n",
    "        \"ffn_mult\": cfg[\"ffn_mult\"],\n",
    "        \"dropout\": cfg[\"dropout\"],\n",
    "        \"attn_dropout\": cfg[\"attn_dropout\"],\n",
    "\n",
    "        # mHC-lite params (PDF)\n",
    "        \"n_streams\": cfg[\"n_streams\"],\n",
    "        \"mhc_alpha\": cfg[\"mhc_alpha\"],\n",
    "        \"sinkhorn_tmax\": cfg[\"sinkhorn_tmax\"],\n",
    "        \"mhc_dropout\": cfg[\"mhc_dropout\"],\n",
    "\n",
    "        # train params (PDF-ish)\n",
    "        \"batch_size\": cfg[\"batch_size\"],\n",
    "        \"epochs\": cfg[\"epochs\"],\n",
    "        \"lr\": cfg[\"lr\"],\n",
    "        \"weight_decay\": cfg[\"weight_decay\"],\n",
    "        \"warmup_frac\": cfg[\"warmup_frac\"],\n",
    "        \"beta1\": cfg[\"beta1\"],\n",
    "        \"beta2\": cfg[\"beta2\"],\n",
    "        \"adam_eps\": cfg[\"adam_eps\"],\n",
    "        \"lr_decay_ratio1\": cfg[\"lr_decay_ratio1\"],\n",
    "        \"lr_decay_ratio2\": cfg[\"lr_decay_ratio2\"],\n",
    "        \"lr_decay_rate1\": cfg[\"lr_decay_rate1\"],\n",
    "        \"lr_decay_rate2\": cfg[\"lr_decay_rate2\"],\n",
    "        \"patience\": cfg[\"patience\"],\n",
    "        \"min_delta\": cfg[\"min_delta\"],\n",
    "        \"grad_clip\": cfg[\"grad_clip\"],\n",
    "    }\n",
    "    return summary, fold_rows, oof, fold_packs\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Define candidate configs (big but still Kaggle-safe)\n",
    "# ----------------------------\n",
    "BASE = dict(\n",
    "    # training\n",
    "    batch_size=512 if device.type==\"cuda\" else 256,\n",
    "    epochs=70 if device.type==\"cuda\" else 40,\n",
    "    lr=2e-4,\n",
    "    weight_decay=8e-3,          # tabular biasanya lebih kecil dari 0.1; tapi kita tetap grid\n",
    "    warmup_frac=0.10,\n",
    "    grad_clip=1.0,\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "\n",
    "    # optimizer (PDF)\n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    adam_eps=1e-8,              # PDF table shows 1e-20; 1e-8 lebih aman di Kaggle float32/amp\n",
    "\n",
    "    # LR schedule (PDF)\n",
    "    lr_decay_ratio1=0.8,\n",
    "    lr_decay_ratio2=0.9,\n",
    "    lr_decay_rate1=0.316,\n",
    "    lr_decay_rate2=0.1,\n",
    "\n",
    "    # mHC-lite (PDF)\n",
    "    n_streams=4,\n",
    "    mhc_alpha=0.01,\n",
    "    sinkhorn_tmax=20,\n",
    "    mhc_dropout=0.0,\n",
    ")\n",
    "\n",
    "candidates = []\n",
    "\n",
    "# Strong default (stabil)\n",
    "candidates.append((\"mhc_384x8\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=4, dropout=0.20, attn_dropout=0.10)))\n",
    "\n",
    "# Lebih kuat (sedikit lebih lambat; lebih regularized)\n",
    "candidates.append((\"mhc_384x10_reg\", dict(BASE, d_model=384, n_layers=10, n_heads=8,  ffn_mult=4, dropout=0.25, attn_dropout=0.12,\n",
    "                                         lr=1.6e-4, weight_decay=1.2e-2, epochs=85 if device.type==\"cuda\" else 50, patience=12)))\n",
    "\n",
    "# FFN lebih kecil (kadang lebih tahan noise)\n",
    "candidates.append((\"mhc_384x8_ffn2\", dict(BASE, d_model=384, n_layers=8,  n_heads=8,  ffn_mult=2, dropout=0.18, attn_dropout=0.10,\n",
    "                                         lr=2.2e-4, weight_decay=6e-3)))\n",
    "\n",
    "# Lebih cepat (buat pembanding)\n",
    "candidates.append((\"mhc_256x6_fast\", dict(BASE, d_model=256, n_layers=6,  n_heads=8,  ffn_mult=4, dropout=0.18, attn_dropout=0.08,\n",
    "                                         lr=3e-4, weight_decay=4e-3, epochs=60 if device.type==\"cuda\" else 35, patience=9)))\n",
    "\n",
    "# Extreme (hanya kalau CUDA; kalau CPU skip otomatis)\n",
    "if device.type == \"cuda\":\n",
    "    candidates.append((\"mhc_512x10_big\", dict(BASE, d_model=512, n_layers=10, n_heads=16, ffn_mult=4, dropout=0.28, attn_dropout=0.15,\n",
    "                                             lr=1.2e-4, weight_decay=2e-2, epochs=90, patience=12,\n",
    "                                             mhc_dropout=0.05)))\n",
    "\n",
    "print(f\"\\nTotal Transformer candidates: {len(candidates)}\")\n",
    "print(\"Primary score: OOF best F-beta (beta=0.5)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Run 2-stage search (runtime-safe)\n",
    "# ----------------------------\n",
    "OUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OPT_DIR = OUT_DIR / \"opt_search\"\n",
    "OPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# pick subset folds evenly spaced for stage-1\n",
    "if STAGE1_FOLDS >= len(unique_folds):\n",
    "    folds_subset = unique_folds\n",
    "else:\n",
    "    stepk = max(1, len(unique_folds) // STAGE1_FOLDS)\n",
    "    folds_subset = unique_folds[::stepk][:STAGE1_FOLDS]\n",
    "\n",
    "print(\"\\nStage-1 folds subset:\", folds_subset)\n",
    "\n",
    "t0 = time.time()\n",
    "stage1_rows = []\n",
    "stage1_fold_rows = []\n",
    "stage1_oof_store = {}\n",
    "\n",
    "for i, (name, cfg) in enumerate(candidates, 1):\n",
    "    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n",
    "        print(\"Time budget reached. Stop search.\")\n",
    "        break\n",
    "\n",
    "    cfg1 = dict(cfg)\n",
    "    cfg1[\"epochs\"] = int(min(int(cfg1[\"epochs\"]), int(STAGE1_EPOCH_CAP)))\n",
    "    cfg1[\"patience\"] = int(min(int(cfg1[\"patience\"]), int(STAGE1_PAT_CAP)))\n",
    "\n",
    "    print(f\"\\n[Stage-1 {i:02d}/{len(candidates)}] CV(subset) -> {name}\")\n",
    "    summ, fold_rows, oof, _ = run_cv_config(cfg1, name, folds_subset=folds_subset, beta=BETA, thr_grid=101)\n",
    "\n",
    "    stage1_rows.append(summ)\n",
    "    stage1_fold_rows.extend(fold_rows)\n",
    "    stage1_oof_store[name] = oof\n",
    "\n",
    "    print(f\"  stage1 best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f}\"\n",
    "          f\" | logloss: {summ['oof_logloss']:.6f}\")\n",
    "\n",
    "df_s1 = pd.DataFrame(stage1_rows).sort_values([\"oof_best_fbeta\",\"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nStage-1 ranking (top):\")\n",
    "display(df_s1.head(10))\n",
    "\n",
    "# pick top-M for stage-2 full CV\n",
    "topM = min(int(STAGE2_TOPM), len(df_s1))\n",
    "stage2_names = df_s1[\"cfg\"].head(topM).tolist()\n",
    "print(\"\\nStage-2 will run full CV for:\", stage2_names)\n",
    "\n",
    "all_summaries = []\n",
    "all_fold_rows = []\n",
    "oof_store = {}\n",
    "\n",
    "for j, nm in enumerate(stage2_names, 1):\n",
    "    if TIME_BUDGET_SEC and (time.time() - t0) > TIME_BUDGET_SEC:\n",
    "        print(\"Time budget reached. Stop stage-2.\")\n",
    "        break\n",
    "\n",
    "    cfg = None\n",
    "    for (nname, ccfg) in candidates:\n",
    "        if nname == nm:\n",
    "            cfg = ccfg\n",
    "            break\n",
    "    if cfg is None:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[Stage-2 {j:02d}/{len(stage2_names)}] CV(full) -> {nm}\")\n",
    "    summ, fold_rows, oof, _ = run_cv_config(cfg, nm, folds_subset=None, beta=BETA, thr_grid=THR_GRID)\n",
    "\n",
    "    all_summaries.append(summ)\n",
    "    all_fold_rows.extend(fold_rows)\n",
    "    oof_store[nm] = oof\n",
    "\n",
    "    print(f\"  OOF best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f}\"\n",
    "          f\" | auc: {(summ['oof_auc'] if summ['oof_auc'] is not None else float('nan')):.6f}\"\n",
    "          f\" | logloss: {summ['oof_logloss']:.6f}\")\n",
    "\n",
    "df_sum = pd.DataFrame(all_summaries)\n",
    "df_fold = pd.DataFrame(all_fold_rows)\n",
    "\n",
    "df_sum = df_sum.sort_values([\"oof_best_fbeta\", \"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nStage-2 top candidates (full CV):\")\n",
    "display(df_sum)\n",
    "\n",
    "# save search results\n",
    "df_sum.to_csv(OPT_DIR / \"opt_results.csv\", index=False)\n",
    "with open(OPT_DIR / \"opt_results.json\", \"w\") as f:\n",
    "    json.dump(df_sum.to_dict(orient=\"records\"), f, indent=2)\n",
    "df_fold.to_csv(OPT_DIR / \"opt_fold_details.csv\", index=False)\n",
    "\n",
    "# save OOF preds for top configs (debugging)\n",
    "top_names = df_sum[\"cfg\"].head(min(REPORT_TOPK_OOF, len(df_sum))).tolist()\n",
    "for nm in top_names:\n",
    "    df_o = pd.DataFrame({\n",
    "        \"uid\": uids_all,\n",
    "        \"y\": y_all,\n",
    "        \"fold\": folds_all,\n",
    "        f\"oof_pred_{nm}\": oof_store[nm]\n",
    "    })\n",
    "    df_o.to_csv(OPT_DIR / f\"oof_preds_{nm}.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Choose best config + retrain fold packs for best (full CV)\n",
    "# ----------------------------\n",
    "if len(df_sum) == 0:\n",
    "    raise RuntimeError(\"Stage-2 produced no results. Cek device/VRAM atau turunkan kandidat/epochs.\")\n",
    "\n",
    "best_single = df_sum.iloc[0].to_dict()\n",
    "best_cfg_name = best_single[\"cfg\"]\n",
    "\n",
    "best_cfg = None\n",
    "for nm, cfg in candidates:\n",
    "    if nm == best_cfg_name:\n",
    "        best_cfg = cfg\n",
    "        break\n",
    "if best_cfg is None:\n",
    "    raise RuntimeError(\"Best cfg not found in candidates list (unexpected).\")\n",
    "\n",
    "print(\"\\nBest config:\", best_cfg_name)\n",
    "print(best_single)\n",
    "\n",
    "print(f\"\\nRe-train folds for best config -> {best_cfg_name}\")\n",
    "best_fold_packs = []\n",
    "best_oof = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "for f in unique_folds:\n",
    "    tr = np.where(folds_all != f)[0]\n",
    "    va = np.where(folds_all == f)[0]\n",
    "\n",
    "    X_tr, y_tr = X_all[tr], y_all[tr]\n",
    "    X_va, y_va = X_all[va], y_all[va]\n",
    "\n",
    "    pack, p_va = train_one_fold_transformer(X_tr, y_tr, X_va, y_va, best_cfg)\n",
    "    pack[\"fold\"] = int(f)\n",
    "    best_fold_packs.append(pack)\n",
    "    best_oof[va] = p_va\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "best_model_path = OUT_DIR / \"best_gate_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"type\": \"mhc_lite_ft_transformer\",\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"fold_packs\": best_fold_packs,\n",
    "        \"cfg_name\": best_cfg_name,\n",
    "        \"cfg\": best_cfg,\n",
    "        \"seed\": SEED,\n",
    "    },\n",
    "    best_model_path\n",
    ")\n",
    "\n",
    "best_oof_best = best_fbeta_fast(y_all, best_oof, beta=BETA, grid=THR_GRID)\n",
    "\n",
    "best_bundle = {\n",
    "    # dibuat supaya Step 5 nanti gampang:\n",
    "    \"type\": \"mhc_lite_ft_transformer\",\n",
    "    \"model_name\": best_cfg_name,\n",
    "    \"members\": [best_cfg_name],\n",
    "    \"random_seed\": SEED,\n",
    "    \"beta_for_tuning\": BETA,\n",
    "\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"cfg\": best_cfg,\n",
    "\n",
    "    \"oof_best_thr\": best_oof_best[\"thr\"],\n",
    "    \"oof_best_fbeta\": best_oof_best[\"fbeta\"],\n",
    "    \"oof_best_prec\": best_oof_best[\"precision\"],\n",
    "    \"oof_best_rec\": best_oof_best[\"recall\"],\n",
    "    \"oof_auc\": safe_auc(y_all, best_oof),\n",
    "    \"oof_logloss\": safe_logloss(y_all, best_oof),\n",
    "\n",
    "    \"notes\": \"Best config from Step 4 (Transformer-only, mHC-lite PDF-inspired). Step 5 should train FULL model for inference.\",\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / \"best_gate_config.json\", \"w\") as f:\n",
    "    json.dump(best_bundle, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved best artifacts:\")\n",
    "print(\"  best model (fold packs) ->\", best_model_path)\n",
    "print(\"  best config             ->\", OUT_DIR / \"best_gate_config.json\")\n",
    "print(\"  opt results             ->\", OPT_DIR / \"opt_results.csv\")\n",
    "print(\"  fold detail             ->\", OPT_DIR / \"opt_fold_details.csv\")\n",
    "\n",
    "# Export globals for Step 5\n",
    "BEST_GATE_BUNDLE = best_bundle\n",
    "BEST_TF_CFG_NAME = best_cfg_name\n",
    "BEST_TF_CFG = best_cfg\n",
    "OPT_RESULTS_DF = df_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108a43a",
   "metadata": {
    "papermill": {
     "duration": 0.010639,
     "end_time": "2026-01-04T15:41:20.529042",
     "exception": false,
     "start_time": "2026-01-04T15:41:20.518403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Training (Train on Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b683c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:41:20.552853Z",
     "iopub.status.busy": "2026-01-04T15:41:20.552554Z",
     "iopub.status.idle": "2026-01-04T15:45:42.557908Z",
     "shell.execute_reply": "2026-01-04T15:45:42.556987Z"
    },
    "papermill": {
     "duration": 262.020037,
     "end_time": "2026-01-04T15:45:42.559819",
     "exception": false,
     "start_time": "2026-01-04T15:41:20.539782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data:\n",
      "  rows=5176 | pos%=54.08 | n_features=62\n",
      "\n",
      "Loaded cfg from: memory(BEST_GATE_BUNDLE)\n",
      "\n",
      "Device: cuda | AMP: True | VRAM_GB: 15.9\n",
      "\n",
      "CFG (final):\n",
      "  d_model: 384\n",
      "  n_layers: 10\n",
      "  n_heads: 8\n",
      "  ffn_mult: 4\n",
      "  dropout: 0.25\n",
      "  attn_dropout: 0.12\n",
      "  batch_size: 384\n",
      "  epochs: 85\n",
      "  lr: 0.00016\n",
      "  weight_decay: 0.012\n",
      "  warmup_frac: 0.1\n",
      "  patience: 12\n",
      "\n",
      "GradAccum: 3 (target_eff_batch=1024)\n",
      "\n",
      "[Final Train] seed_offset=0\n",
      "  internal split: train=4767 | val=409 | val_pos%=54.52\n",
      "  epoch 001/85 | tr_loss=0.64814 | val_ll=0.69400 | val_auc=0.49684 | bad=0\n",
      "  epoch 002/85 | tr_loss=0.64287 | val_ll=0.69523 | val_auc=0.50406 | bad=1\n",
      "  epoch 003/85 | tr_loss=0.64509 | val_ll=0.68876 | val_auc=0.51273 | bad=0\n",
      "  epoch 004/85 | tr_loss=0.64128 | val_ll=0.69768 | val_auc=0.52736 | bad=1\n",
      "  epoch 005/85 | tr_loss=0.64028 | val_ll=0.68863 | val_auc=0.53367 | bad=0\n",
      "  epoch 006/85 | tr_loss=0.63989 | val_ll=0.68734 | val_auc=0.53738 | bad=0\n",
      "  epoch 007/85 | tr_loss=0.63965 | val_ll=0.69351 | val_auc=0.54463 | bad=1\n",
      "  epoch 008/85 | tr_loss=0.63728 | val_ll=0.68710 | val_auc=0.54513 | bad=0\n",
      "  epoch 009/85 | tr_loss=0.63799 | val_ll=0.69058 | val_auc=0.54379 | bad=1\n",
      "  epoch 010/85 | tr_loss=0.64097 | val_ll=0.68821 | val_auc=0.54481 | bad=2\n",
      "  epoch 011/85 | tr_loss=0.63712 | val_ll=0.68842 | val_auc=0.54590 | bad=3\n",
      "  epoch 012/85 | tr_loss=0.63630 | val_ll=0.69121 | val_auc=0.54342 | bad=4\n",
      "  epoch 013/85 | tr_loss=0.63701 | val_ll=0.68718 | val_auc=0.54166 | bad=5\n",
      "  epoch 014/85 | tr_loss=0.63547 | val_ll=0.68766 | val_auc=0.54219 | bad=6\n",
      "  epoch 015/85 | tr_loss=0.63800 | val_ll=0.69142 | val_auc=0.54249 | bad=7\n",
      "  epoch 016/85 | tr_loss=0.63719 | val_ll=0.68624 | val_auc=0.54340 | bad=0\n",
      "  epoch 017/85 | tr_loss=0.63600 | val_ll=0.69317 | val_auc=0.53960 | bad=1\n",
      "  epoch 018/85 | tr_loss=0.63577 | val_ll=0.68921 | val_auc=0.53973 | bad=2\n",
      "  epoch 019/85 | tr_loss=0.63774 | val_ll=0.68813 | val_auc=0.54512 | bad=3\n",
      "  epoch 020/85 | tr_loss=0.63697 | val_ll=0.69288 | val_auc=0.54387 | bad=4\n",
      "  epoch 021/85 | tr_loss=0.63575 | val_ll=0.68507 | val_auc=0.54431 | bad=0\n",
      "  epoch 022/85 | tr_loss=0.63512 | val_ll=0.69358 | val_auc=0.54853 | bad=1\n",
      "  epoch 023/85 | tr_loss=0.63630 | val_ll=0.70119 | val_auc=0.55478 | bad=2\n",
      "  epoch 024/85 | tr_loss=0.63654 | val_ll=0.68448 | val_auc=0.54324 | bad=0\n",
      "  epoch 025/85 | tr_loss=0.63671 | val_ll=0.69389 | val_auc=0.55460 | bad=1\n",
      "  epoch 026/85 | tr_loss=0.63460 | val_ll=0.68673 | val_auc=0.55644 | bad=2\n",
      "  epoch 027/85 | tr_loss=0.63471 | val_ll=0.69031 | val_auc=0.54428 | bad=3\n",
      "  epoch 028/85 | tr_loss=0.63387 | val_ll=0.69335 | val_auc=0.54299 | bad=4\n",
      "  epoch 029/85 | tr_loss=0.63260 | val_ll=0.68552 | val_auc=0.55208 | bad=5\n",
      "  epoch 030/85 | tr_loss=0.63597 | val_ll=0.69945 | val_auc=0.55309 | bad=6\n",
      "  epoch 031/85 | tr_loss=0.63383 | val_ll=0.68693 | val_auc=0.51844 | bad=7\n",
      "  epoch 032/85 | tr_loss=0.63374 | val_ll=0.69085 | val_auc=0.55029 | bad=8\n",
      "  epoch 033/85 | tr_loss=0.63184 | val_ll=0.69072 | val_auc=0.54533 | bad=9\n",
      "  epoch 034/85 | tr_loss=0.63290 | val_ll=0.68985 | val_auc=0.53181 | bad=10\n",
      "  epoch 035/85 | tr_loss=0.63161 | val_ll=0.68948 | val_auc=0.54711 | bad=11\n",
      "  epoch 036/85 | tr_loss=0.63140 | val_ll=0.69141 | val_auc=0.55224 | bad=12\n",
      "  early stop at epoch 36, best_epoch=24, best_val_ll=0.68448\n",
      "\n",
      "Saved final training artifacts:\n",
      "  model  -> /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n",
      "  bundle -> /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 5 — Final Training (Train on Full Data) — TRANSFORMER ONLY (REVISI FULL v3)\n",
    "# - FIX: predict_proba aman untuk batch=(xb,yb)/list/tuple\n",
    "# - mHC-style training: AdamW betas(0.9,0.95) + Warmup + Step decay (0.8/0.9)\n",
    "# - Auto \"besar tapi muat\" berdasarkan VRAM (safe untuk runtime Kaggle)\n",
    "#\n",
    "# Output:\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n",
    "# ============================================================\n",
    "\n",
    "import os, json, gc, math, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "# ----------------------------\n",
    "# 0) REQUIRE\n",
    "# ----------------------------\n",
    "if \"df_train_tabular\" not in globals():\n",
    "    raise RuntimeError(\"Missing `df_train_tabular`. Jalankan Step 2 dulu.\")\n",
    "if \"FEATURE_COLS\" not in globals():\n",
    "    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 dulu.\")\n",
    "\n",
    "df_train_tabular = df_train_tabular.copy()\n",
    "FEATURE_COLS = list(FEATURE_COLS)\n",
    "\n",
    "need_cols = {\"uid\",\"case_id\",\"fold\",\"y\"}\n",
    "miss = [c for c in need_cols if c not in df_train_tabular.columns]\n",
    "if miss:\n",
    "    raise ValueError(f\"df_train_tabular missing columns: {miss}\")\n",
    "\n",
    "X_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "y_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n",
    "\n",
    "if not np.isfinite(X_all).all():\n",
    "    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "OUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Final training data:\")\n",
    "print(f\"  rows={len(y_all)} | pos%={float(y_all.mean())*100:.2f} | n_features={X_all.shape[1]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load best cfg (optional)\n",
    "#    - kompatibel: format lama transformer_ft, atau bundle apapun yang punya [\"cfg\"]\n",
    "# ----------------------------\n",
    "cfg_path = OUT_DIR / \"best_gate_config.json\"\n",
    "best_bundle = None\n",
    "source = None\n",
    "\n",
    "if \"BEST_GATE_BUNDLE\" in globals() and isinstance(BEST_GATE_BUNDLE, dict):\n",
    "    best_bundle = BEST_GATE_BUNDLE\n",
    "    source = \"memory(BEST_GATE_BUNDLE)\"\n",
    "elif cfg_path.exists():\n",
    "    best_bundle = json.loads(cfg_path.read_text())\n",
    "    source = str(cfg_path)\n",
    "\n",
    "if best_bundle is not None and isinstance(best_bundle, dict) and isinstance(best_bundle.get(\"cfg\", None), dict):\n",
    "    base_cfg = dict(best_bundle[\"cfg\"])\n",
    "    print(\"\\nLoaded cfg from:\", source)\n",
    "else:\n",
    "    base_cfg = {}\n",
    "    print(\"\\nNo best_gate_config found. Using strong default cfg.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Device + seed\n",
    "# ----------------------------\n",
    "FINAL_SEED = int(best_bundle.get(\"seed\", 2025)) if isinstance(best_bundle, dict) else 2025\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(FINAL_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "\n",
    "# matmul perf (pytorch 2.x)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "vram_gb = None\n",
    "if device.type == \"cuda\":\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "\n",
    "print(\"\\nDevice:\", device, \"| AMP:\", use_amp, \"| VRAM_GB:\", (f\"{vram_gb:.1f}\" if vram_gb else \"CPU\"))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Runtime-safe \"big model\" policy\n",
    "#    - kamu minta model besar tersimpan => default ON jika CUDA\n",
    "#    - kalau CPU: otomatis dikecilkan supaya runtime tidak meledak\n",
    "# ----------------------------\n",
    "WANT_BIG_MODEL = True\n",
    "USE_INTERNAL_VAL = True         # early stopping lebih aman\n",
    "VAL_FRAC_CASE = 0.08            # 8% case-level val (cukup stabil, tidak terlalu buang data)\n",
    "EARLY_STOP = True\n",
    "\n",
    "# multi-seed = lebih kuat tapi lebih lama; keep 1 untuk runtime Kaggle\n",
    "N_SEEDS = 1\n",
    "\n",
    "# default cfg kalau base_cfg kosong\n",
    "CFG = dict(\n",
    "    d_model=384,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    ffn_mult=4,\n",
    "    dropout=0.20,\n",
    "    attn_dropout=0.10,\n",
    "\n",
    "    batch_size=512,\n",
    "    epochs=60,              # cukup, early stop akan potong\n",
    "    lr=2e-4,\n",
    "    weight_decay=1e-2,      # lebih dekat spirit AdamW (paper), tapi tidak seganas 0.1\n",
    "    warmup_frac=0.10,\n",
    "    grad_clip=1.0,\n",
    "\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    ")\n",
    "\n",
    "# merge base_cfg -> CFG\n",
    "for k,v in base_cfg.items():\n",
    "    if k in CFG:\n",
    "        CFG[k] = v\n",
    "\n",
    "# auto upscale (big) jika GPU\n",
    "def autoscale_cfg_for_device(cfg: dict):\n",
    "    cfg = dict(cfg)\n",
    "    if device.type != \"cuda\":\n",
    "        # CPU fallback: lebih kecil biar feasible\n",
    "        cfg[\"d_model\"] = min(int(cfg.get(\"d_model\", 384)), 256)\n",
    "        cfg[\"n_layers\"] = min(int(cfg.get(\"n_layers\", 8)), 6)\n",
    "        cfg[\"n_heads\"]  = min(int(cfg.get(\"n_heads\", 8)), 8)\n",
    "        cfg[\"batch_size\"] = min(int(cfg.get(\"batch_size\", 512)), 256)\n",
    "        cfg[\"epochs\"] = min(int(cfg.get(\"epochs\", 60)), 35)\n",
    "        cfg[\"lr\"] = float(cfg.get(\"lr\", 2e-4))\n",
    "        cfg[\"weight_decay\"] = float(cfg.get(\"weight_decay\", 1e-2))\n",
    "        return cfg\n",
    "\n",
    "    # CUDA\n",
    "    if not WANT_BIG_MODEL:\n",
    "        return cfg\n",
    "\n",
    "    # pilih \"besar tapi aman\" berdasar VRAM\n",
    "    if vram_gb is None:\n",
    "        return cfg\n",
    "\n",
    "    if vram_gb >= 24:\n",
    "        cfg.update(dict(d_model=max(int(cfg[\"d_model\"]), 512),\n",
    "                        n_layers=max(int(cfg[\"n_layers\"]), 12),\n",
    "                        n_heads=max(int(cfg[\"n_heads\"]), 16),\n",
    "                        ffn_mult=max(int(cfg[\"ffn_mult\"]), 4),\n",
    "                        dropout=max(float(cfg[\"dropout\"]), 0.25),\n",
    "                        attn_dropout=max(float(cfg[\"attn_dropout\"]), 0.12),\n",
    "                        lr=min(float(cfg[\"lr\"]), 1.5e-4),\n",
    "                        weight_decay=max(float(cfg[\"weight_decay\"]), 2e-2),\n",
    "                        batch_size=min(int(cfg[\"batch_size\"]), 512),\n",
    "                        epochs=max(int(cfg[\"epochs\"]), 70),\n",
    "                        patience=max(int(cfg[\"patience\"]), 12)))\n",
    "    elif vram_gb >= 16:\n",
    "        cfg.update(dict(d_model=max(int(cfg[\"d_model\"]), 448),\n",
    "                        n_layers=max(int(cfg[\"n_layers\"]), 10),\n",
    "                        n_heads=max(int(cfg[\"n_heads\"]), 8),\n",
    "                        ffn_mult=max(int(cfg[\"ffn_mult\"]), 4),\n",
    "                        dropout=max(float(cfg[\"dropout\"]), 0.24),\n",
    "                        attn_dropout=max(float(cfg[\"attn_dropout\"]), 0.12),\n",
    "                        lr=min(float(cfg[\"lr\"]), 1.8e-4),\n",
    "                        weight_decay=max(float(cfg[\"weight_decay\"]), 1.5e-2),\n",
    "                        batch_size=min(int(cfg[\"batch_size\"]), 512),\n",
    "                        epochs=max(int(cfg[\"epochs\"]), 65),\n",
    "                        patience=max(int(cfg[\"patience\"]), 11)))\n",
    "    else:\n",
    "        # 8–12GB (umum Kaggle)\n",
    "        cfg.update(dict(d_model=max(int(cfg[\"d_model\"]), 384),\n",
    "                        n_layers=max(int(cfg[\"n_layers\"]), 8),\n",
    "                        n_heads=max(int(cfg[\"n_heads\"]), 8),\n",
    "                        ffn_mult=max(int(cfg[\"ffn_mult\"]), 4),\n",
    "                        dropout=max(float(cfg[\"dropout\"]), 0.22),\n",
    "                        attn_dropout=max(float(cfg[\"attn_dropout\"]), 0.10),\n",
    "                        lr=min(float(cfg[\"lr\"]), 2e-4),\n",
    "                        weight_decay=max(float(cfg[\"weight_decay\"]), 1e-2),\n",
    "                        batch_size=min(int(cfg[\"batch_size\"]), 384),   # jaga OOM\n",
    "                        epochs=max(int(cfg[\"epochs\"]), 60),\n",
    "                        patience=max(int(cfg[\"patience\"]), 10)))\n",
    "    return cfg\n",
    "\n",
    "CFG = autoscale_cfg_for_device(CFG)\n",
    "\n",
    "# mHC paper: AdamW betas (0.9,0.95); eps kecil (tetap aman)\n",
    "ADAM_BETAS = (0.9, 0.95)\n",
    "ADAM_EPS   = 1e-8  # paper pakai eps sangat kecil; 1e-8 lebih aman numerik untuk tabular\n",
    "\n",
    "# Step decay ratios (paper): 0.8 & 0.9 epoch fractions\n",
    "STEP_RATIOS = (0.8, 0.9)\n",
    "STEP_GAMMAS = (0.316, 0.1)\n",
    "\n",
    "print(\"\\nCFG (final):\")\n",
    "for k in [\"d_model\",\"n_layers\",\"n_heads\",\"ffn_mult\",\"dropout\",\"attn_dropout\",\"batch_size\",\"epochs\",\"lr\",\"weight_decay\",\"warmup_frac\",\"patience\"]:\n",
    "    print(f\"  {k}: {CFG[k]}\")\n",
    "\n",
    "# effective batch via grad accumulation (biar tetap stabil walau batch diturunin)\n",
    "TARGET_EFF_BATCH = 1024 if device.type==\"cuda\" else 256\n",
    "GRAD_ACCUM = max(1, int(math.ceil(TARGET_EFF_BATCH / int(CFG[\"batch_size\"]))))\n",
    "\n",
    "print(f\"\\nGradAccum: {GRAD_ACCUM} (target_eff_batch={TARGET_EFF_BATCH})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset + Standardizer\n",
    "# ----------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = None if y is None else torch.from_numpy(y.astype(np.int64))\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def fit_standardizer(X_tr: np.ndarray):\n",
    "    mu = X_tr.mean(axis=0, dtype=np.float64)\n",
    "    sig = X_tr.std(axis=0, dtype=np.float64)\n",
    "    sig = np.where(sig < 1e-8, 1.0, sig)\n",
    "    return mu.astype(np.float32), sig.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n",
    "    return ((X_in - mu) / sig).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Model (FT-Transformer numeric) + DeepNorm-like scaling (mHC spirit)\n",
    "# ----------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "    def forward(self, x):\n",
    "        # x: (..., d)\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
    "        return (x / rms) * self.scale\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ffn_mult=4, dropout=0.2, attn_dropout=0.1, deepnorm_alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = float(deepnorm_alpha)\n",
    "\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=attn_dropout, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        hidden = int(ffn_mult * d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm + DeepNorm residual scaling\n",
    "        h = self.norm1(x)\n",
    "        attn_out, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = (x * self.alpha) + self.drop1(attn_out)\n",
    "\n",
    "        h = self.norm2(x)\n",
    "        ffn_out = self.ffn(h)\n",
    "        x = (x * self.alpha) + self.drop2(ffn_out)\n",
    "        return x\n",
    "\n",
    "class FTTransformerBig(nn.Module):\n",
    "    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4, dropout=0.2, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # per-feature tokenization (numeric-only)\n",
    "        self.w = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n",
    "        self.b = nn.Parameter(torch.zeros(n_features, d_model))\n",
    "        self.feat_emb = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n",
    "\n",
    "        # DeepNorm alpha (paper spirit): alpha ~ (2L)^(1/4)\n",
    "        alpha = (2.0 * n_layers) ** 0.25\n",
    "\n",
    "        self.token_dropout = nn.Dropout(attn_dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, ffn_mult=ffn_mult, dropout=dropout, attn_dropout=attn_dropout, deepnorm_alpha=alpha)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.final_norm = RMSNorm(d_model)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,F)\n",
    "        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)  # (B,F,D)\n",
    "        tok = tok + self.feat_emb.unsqueeze(0)\n",
    "        tok = self.token_dropout(tok)\n",
    "\n",
    "        B = tok.size(0)\n",
    "        cls = self.cls.expand(B, -1, -1)          # (B,1,D)\n",
    "        seq = torch.cat([cls, tok], dim=1)        # (B,1+F,D)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            seq = blk(seq)\n",
    "\n",
    "        z = self.final_norm(seq[:, 0])            # CLS\n",
    "        logit = self.head(z).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Schedulers (Warmup + Step decay @ 0.8/0.9 epochs)\n",
    "# ----------------------------\n",
    "def make_warmup_then_step_scheduler(optimizer, total_steps, warmup_steps, step_milestones, gammas):\n",
    "    \"\"\"\n",
    "    warmup linear -> base lr\n",
    "    then multiply lr by gammas at given milestone steps (cumulative)\n",
    "    \"\"\"\n",
    "    step_milestones = list(step_milestones)\n",
    "    gammas = list(gammas)\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "\n",
    "        mult = 1.0\n",
    "        for ms, g in zip(step_milestones, gammas):\n",
    "            if step >= ms:\n",
    "                mult *= g\n",
    "        return mult\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Predict helper (anti tuple-batch error)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, loader):\n",
    "    model.eval()\n",
    "    ps = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(xb)\n",
    "            p = torch.sigmoid(logits)\n",
    "        ps.append(p.detach().cpu().numpy())\n",
    "    return np.concatenate(ps, axis=0).astype(np.float32)\n",
    "\n",
    "def safe_logloss(y_true, p):\n",
    "    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1-1e-8)\n",
    "    return float(log_loss(y_true, p, labels=[0,1]))\n",
    "\n",
    "def safe_auc(y_true, p):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return None\n",
    "    return float(roc_auc_score(y_true, p))\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Internal val split (group-safe by case_id)\n",
    "# ----------------------------\n",
    "def make_case_split(df: pd.DataFrame, val_frac=0.08, seed=2025):\n",
    "    g = df.groupby(\"case_id\")[\"y\"].max().reset_index().rename(columns={\"y\":\"case_y\"})\n",
    "    pos_cases = g.loc[g[\"case_y\"] == 1, \"case_id\"].to_numpy()\n",
    "    neg_cases = g.loc[g[\"case_y\"] == 0, \"case_id\"].to_numpy()\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(pos_cases)\n",
    "    rng.shuffle(neg_cases)\n",
    "\n",
    "    n_val_pos = max(1, int(len(pos_cases) * val_frac)) if len(pos_cases) else 0\n",
    "    n_val_neg = max(1, int(len(neg_cases) * val_frac)) if len(neg_cases) else 0\n",
    "\n",
    "    val_cases = np.concatenate([pos_cases[:n_val_pos], neg_cases[:n_val_neg]])\n",
    "    val_set = set(map(int, val_cases.tolist()))\n",
    "    is_val = df[\"case_id\"].astype(int).map(lambda x: int(x) in val_set).to_numpy(dtype=bool)\n",
    "    return is_val\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Train once (full-data with optional internal val)\n",
    "# ----------------------------\n",
    "def train_full_once(seed_offset=0):\n",
    "    seed_everything(FINAL_SEED + seed_offset)\n",
    "\n",
    "    if USE_INTERNAL_VAL:\n",
    "        is_val = make_case_split(df_train_tabular, val_frac=float(VAL_FRAC_CASE), seed=FINAL_SEED + seed_offset)\n",
    "        tr_idx = np.where(~is_val)[0]\n",
    "        va_idx = np.where(is_val)[0]\n",
    "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
    "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
    "        print(f\"  internal split: train={len(tr_idx)} | val={len(va_idx)} | val_pos%={float(y_va.mean())*100:.2f}\")\n",
    "    else:\n",
    "        X_tr, y_tr = X_all, y_all\n",
    "        X_va = y_va = None\n",
    "\n",
    "    mu, sig = fit_standardizer(X_tr)\n",
    "    X_trn = apply_standardizer(X_tr, mu, sig)\n",
    "\n",
    "    ds_tr = TabDataset(X_trn, y_tr)\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr,\n",
    "        batch_size=int(CFG[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=(device.type==\"cuda\"),\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    if USE_INTERNAL_VAL:\n",
    "        X_van = apply_standardizer(X_va, mu, sig)\n",
    "        ds_va = TabDataset(X_van, y_va)\n",
    "        dl_va = DataLoader(\n",
    "            ds_va,\n",
    "            batch_size=int(CFG[\"batch_size\"]),\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=(device.type==\"cuda\"),\n",
    "            drop_last=False\n",
    "        )\n",
    "    else:\n",
    "        dl_va = None\n",
    "\n",
    "    model = FTTransformerBig(\n",
    "        n_features=X_all.shape[1],\n",
    "        d_model=int(CFG[\"d_model\"]),\n",
    "        n_heads=int(CFG[\"n_heads\"]),\n",
    "        n_layers=int(CFG[\"n_layers\"]),\n",
    "        ffn_mult=int(CFG[\"ffn_mult\"]),\n",
    "        dropout=float(CFG[\"dropout\"]),\n",
    "        attn_dropout=float(CFG[\"attn_dropout\"]),\n",
    "    ).to(device)\n",
    "\n",
    "    # imbalance pos_weight (from TRAIN split)\n",
    "    pos = int(y_tr.sum())\n",
    "    neg = int(len(y_tr) - pos)\n",
    "    pos_weight = float(neg / max(1, pos))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(CFG[\"lr\"]),\n",
    "        weight_decay=float(CFG[\"weight_decay\"]),\n",
    "        betas=ADAM_BETAS,\n",
    "        eps=ADAM_EPS,\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = max(1, len(dl_tr))\n",
    "    total_steps = int(CFG[\"epochs\"]) * steps_per_epoch\n",
    "    warmup_steps = int(float(CFG[\"warmup_frac\"]) * total_steps)\n",
    "\n",
    "    ms1 = int(STEP_RATIOS[0] * total_steps)\n",
    "    ms2 = int(STEP_RATIOS[1] * total_steps)\n",
    "    sch = make_warmup_then_step_scheduler(opt, total_steps, warmup_steps, [ms1, ms2], list(STEP_GAMMAS))\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    best_val = 1e18\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    bad = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in range(int(CFG[\"epochs\"])):\n",
    "        model.train()\n",
    "        loss_sum = 0.0\n",
    "        n_sum = 0\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        for it, (xb, yb) in enumerate(dl_tr, 1):\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss = loss / float(GRAD_ACCUM)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (it % GRAD_ACCUM) == 0 or it == len(dl_tr):\n",
    "                if float(CFG.get(\"grad_clip\", 1.0)) > 0:\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG.get(\"grad_clip\", 1.0)))\n",
    "\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "\n",
    "            loss_sum += float(loss.item()) * xb.size(0) * float(GRAD_ACCUM)\n",
    "            n_sum += xb.size(0)\n",
    "\n",
    "        tr_loss = loss_sum / max(1, n_sum)\n",
    "\n",
    "        if dl_va is not None:\n",
    "            p_va = predict_proba(model, dl_va)\n",
    "            vll = safe_logloss(y_va, p_va)\n",
    "            vauc = safe_auc(y_va, p_va)\n",
    "\n",
    "            improved = (best_val - vll) > float(CFG[\"min_delta\"])\n",
    "            if improved:\n",
    "                best_val = float(vll)\n",
    "                best_epoch = int(epoch)\n",
    "                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "\n",
    "            print(f\"  epoch {epoch+1:03d}/{int(CFG['epochs'])} | tr_loss={tr_loss:.5f} | val_ll={vll:.5f} | val_auc={(vauc if vauc is not None else float('nan')):.5f} | bad={bad}\")\n",
    "            if EARLY_STOP and bad >= int(CFG[\"patience\"]):\n",
    "                print(f\"  early stop at epoch {epoch+1}, best_epoch={best_epoch+1}, best_val_ll={best_val:.5f}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"  epoch {epoch+1:03d}/{int(CFG['epochs'])} | tr_loss={tr_loss:.5f}\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # restore best weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "\n",
    "    pack = {\n",
    "        \"type\": \"ft_transformer_big_full\",\n",
    "        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
    "        \"mu\": mu,\n",
    "        \"sig\": sig,\n",
    "        \"cfg\": dict(CFG),\n",
    "        \"seed\": int(FINAL_SEED + seed_offset),\n",
    "        \"pos_weight\": float(pos_weight),\n",
    "        \"train_rows\": int(len(y_tr)),\n",
    "        \"val_rows\": int(len(y_va)) if USE_INTERNAL_VAL else 0,\n",
    "        \"best_epoch\": int(best_epoch + 1) if best_epoch >= 0 else None,\n",
    "        \"best_val_logloss\": float(best_val) if best_state is not None else None,\n",
    "        \"train_time_s\": float(time.time() - t0),\n",
    "        \"grad_accum\": int(GRAD_ACCUM),\n",
    "        \"adam_betas\": list(ADAM_BETAS),\n",
    "        \"step_ratios\": list(STEP_RATIOS),\n",
    "        \"step_gammas\": list(STEP_GAMMAS),\n",
    "    }\n",
    "    return pack\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Train final model(s) with OOM-safe fallback\n",
    "# ----------------------------\n",
    "final_packs = []\n",
    "for s in range(int(N_SEEDS)):\n",
    "    print(f\"\\n[Final Train] seed_offset={s}\")\n",
    "    try:\n",
    "        pack = train_full_once(seed_offset=s)\n",
    "    except RuntimeError as e:\n",
    "        # OOM fallback: reduce batch_size then d_model/layers\n",
    "        msg = str(e).lower()\n",
    "        if (\"out of memory\" in msg) and device.type == \"cuda\":\n",
    "            print(\"  OOM detected. Applying fallback: batch_size -> half, d_model/layers -> downshift\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            CFG[\"batch_size\"] = max(128, int(CFG[\"batch_size\"]) // 2)\n",
    "            CFG[\"d_model\"] = max(256, int(CFG[\"d_model\"]) - 64)\n",
    "            CFG[\"n_layers\"] = max(6, int(CFG[\"n_layers\"]) - 2)\n",
    "            TARGET_EFF_BATCH = 768\n",
    "            GRAD_ACCUM = max(1, int(math.ceil(TARGET_EFF_BATCH / int(CFG[\"batch_size\"]))))\n",
    "\n",
    "            print(\"  New CFG:\")\n",
    "            for k in [\"d_model\",\"n_layers\",\"n_heads\",\"batch_size\",\"epochs\"]:\n",
    "                print(f\"    {k}: {CFG[k]}\")\n",
    "            print(f\"  New GradAccum: {GRAD_ACCUM}\")\n",
    "\n",
    "            pack = train_full_once(seed_offset=s)\n",
    "        else:\n",
    "            raise\n",
    "    final_packs.append(pack)\n",
    "    gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Save artifacts\n",
    "# ----------------------------\n",
    "final_model_path = OUT_DIR / \"final_gate_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"packs\": final_packs,  # list, even if 1 seed\n",
    "        \"bundle_source\": source,\n",
    "    },\n",
    "    final_model_path\n",
    ")\n",
    "\n",
    "final_bundle = {\n",
    "    \"type\": \"ft_transformer_big_full\",\n",
    "    \"n_seeds\": int(N_SEEDS),\n",
    "    \"seeds\": [int(p[\"seed\"]) for p in final_packs],\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"cfg\": dict(CFG),\n",
    "    \"use_internal_val\": bool(USE_INTERNAL_VAL),\n",
    "    \"val_frac_case\": float(VAL_FRAC_CASE) if USE_INTERNAL_VAL else 0.0,\n",
    "    \"early_stop\": bool(EARLY_STOP) if USE_INTERNAL_VAL else False,\n",
    "    \"train_rows\": int(len(y_all)),\n",
    "    \"pos_rate\": float(y_all.mean()),\n",
    "    \"notes\": \"Final model = BIG tabular transformer head over DINOv2-Large-derived features. Save this for inference loading.\",\n",
    "    \"ref_best_bundle\": (best_bundle.get(\"selection\", {}) if isinstance(best_bundle, dict) else {}),\n",
    "}\n",
    "\n",
    "final_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\n",
    "final_bundle_path.write_text(json.dumps(final_bundle, indent=2))\n",
    "\n",
    "print(\"\\nSaved final training artifacts:\")\n",
    "print(\"  model  ->\", final_model_path)\n",
    "print(\"  bundle ->\", final_bundle_path)\n",
    "\n",
    "# Export globals\n",
    "FINAL_GATE_MODEL_PT = str(final_model_path)\n",
    "FINAL_GATE_BUNDLE = final_bundle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c707ce",
   "metadata": {
    "papermill": {
     "duration": 0.012724,
     "end_time": "2026-01-04T15:45:42.585719",
     "exception": false,
     "start_time": "2026-01-04T15:45:42.572995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Finalize & Save Model Bundle (Reproducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6190d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:45:42.613699Z",
     "iopub.status.busy": "2026-01-04T15:45:42.613429Z",
     "iopub.status.idle": "2026-01-04T15:45:46.216816Z",
     "shell.execute_reply": "2026-01-04T15:45:46.215976Z"
    },
    "papermill": {
     "duration": 3.619616,
     "end_time": "2026-01-04T15:45:46.218539",
     "exception": false,
     "start_time": "2026-01-04T15:45:42.598923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found artifacts:\n",
      "  final_model  : /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt (format=torch_pt)\n",
      "  final_bundle : /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n",
      "  feature_cols : /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      "  baseline_report : (missing/skip)\n",
      "  best_gate_config : /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n",
      "\n",
      "OK — Model bundle finalized\n",
      "  manifest     -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_manifest.json\n",
      "  pack (json)  -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_pack.json\n",
      "  pack (joblib)-> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_pack.joblib\n",
      "  thresholds   -> /kaggle/working/recodai_luc_gate_artifacts/thresholds.json\n",
      "  zip          -> /kaggle/working/recodai_luc_gate_artifacts/model_bundle_v3.zip\n",
      "\n",
      "Bundle summary:\n",
      "  model_format : torch_pt\n",
      "  feature_cnt  : 62\n",
      "  T_gate       : 0.5\n",
      "  task         : Recod.ai/LUC — Gate Model (authentic vs forged) — DINOv2 features + Transformer gate (.pt)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 6 — Finalize & Save Model Bundle (Reproducible) — REVISI FULL v3 (TRANSFORMER COMPAT)\n",
    "# - Fokus: bundle artefak penting (Transformer .pt) + threshold placeholder + manifest + ZIP portable\n",
    "# - Tidak ada submission di sini\n",
    "#\n",
    "# REQUIRE:\n",
    "# - Step 2: feature_cols.json\n",
    "# - Step 5: final_gate_model.pt + final_gate_bundle.json\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time, platform, warnings, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "OUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def read_json_safe(p: Path, default=None):\n",
    "    try:\n",
    "        return json.loads(p.read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def pick_first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p is None:\n",
    "            continue\n",
    "        p = Path(p)\n",
    "        if p.exists() and p.is_file():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def safe_add(zf: zipfile.ZipFile, p: Path, arcname: str):\n",
    "    if p is None:\n",
    "        return\n",
    "    p = Path(p)\n",
    "    if p.exists() and p.is_file():\n",
    "        zf.write(p, arcname=arcname)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Locate required artifacts\n",
    "# ----------------------------\n",
    "final_model_pt = OUT_DIR / \"final_gate_model.pt\"\n",
    "final_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\n",
    "feature_cols_path = OUT_DIR / \"feature_cols.json\"\n",
    "\n",
    "if not feature_cols_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing feature_cols: {feature_cols_path} (jalankan Step 2 dulu)\")\n",
    "if not final_model_pt.exists():\n",
    "    raise FileNotFoundError(f\"Missing final model: {final_model_pt} (jalankan Step 5 dulu)\")\n",
    "\n",
    "# Optional / legacy (hanya untuk kompat)\n",
    "final_model_joblib = OUT_DIR / \"final_gate_model.joblib\"  # legacy fallback jika suatu saat ada\n",
    "model_format = \"torch_pt\"\n",
    "final_model_path = final_model_pt\n",
    "\n",
    "# Optional artifacts (gunakan nama yang benar sesuai Step 3/4 kamu)\n",
    "baseline_report_path = pick_first_existing([\n",
    "    OUT_DIR / \"baseline_transformer_cv_report.json\",   # paling mungkin (dari Step 3 kamu)\n",
    "    OUT_DIR / \"baseline_transformer_cv_report_v2.json\",\n",
    "    OUT_DIR / \"baseline_cv_report.json\",               # legacy\n",
    "])\n",
    "\n",
    "opt_config_path = pick_first_existing([\n",
    "    OUT_DIR / \"best_gate_config.json\",                 # dari Step 4\n",
    "])\n",
    "\n",
    "opt_results_csv = pick_first_existing([\n",
    "    OUT_DIR / \"opt_search\" / \"opt_results.csv\",\n",
    "])\n",
    "\n",
    "opt_fold_csv = pick_first_existing([\n",
    "    OUT_DIR / \"opt_search\" / \"opt_fold_details.csv\",\n",
    "])\n",
    "\n",
    "oof_tf_baseline_csv = pick_first_existing([\n",
    "    OUT_DIR / \"oof_baseline_transformer.csv\",\n",
    "])\n",
    "\n",
    "# ini optional dan sering tidak ada (jangan bikin error)\n",
    "oof_baseline_csv = pick_first_existing([\n",
    "    OUT_DIR / \"oof_baseline.csv\",\n",
    "])\n",
    "\n",
    "print(\"Found artifacts:\")\n",
    "print(\"  final_model  :\", final_model_path, f\"(format={model_format})\")\n",
    "print(\"  final_bundle :\", final_bundle_path if final_bundle_path.exists() else \"(missing/skip)\")\n",
    "print(\"  feature_cols :\", feature_cols_path)\n",
    "print(\"  baseline_report :\", baseline_report_path if baseline_report_path else \"(missing/skip)\")\n",
    "print(\"  best_gate_config :\", opt_config_path if opt_config_path else \"(missing/skip)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load metadata\n",
    "# ----------------------------\n",
    "feature_cols = read_json_safe(feature_cols_path, default=[])\n",
    "if not isinstance(feature_cols, list) or len(feature_cols) == 0:\n",
    "    raise ValueError(\"feature_cols.json invalid / empty\")\n",
    "\n",
    "final_bundle = read_json_safe(final_bundle_path, default={}) if final_bundle_path.exists() else {}\n",
    "baseline_report = read_json_safe(baseline_report_path, default=None) if baseline_report_path else None\n",
    "opt_config = read_json_safe(opt_config_path, default=None) if opt_config_path else None\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold placeholders (Transformer)\n",
    "# - PRIORITY:\n",
    "#   (a) thresholds.json (kalau sudah ada)\n",
    "#   (b) best_gate_config.json -> selection.oof_best_thr (Step 4)\n",
    "#   (c) fallback 0.5\n",
    "# ----------------------------\n",
    "thresholds_path = OUT_DIR / \"thresholds.json\"\n",
    "\n",
    "if thresholds_path.exists():\n",
    "    thresholds = read_json_safe(thresholds_path, default={})\n",
    "else:\n",
    "    T_gate = None\n",
    "\n",
    "    # (b) Step 4 selection\n",
    "    if isinstance(opt_config, dict):\n",
    "        sel = opt_config.get(\"selection\", {})\n",
    "        if isinstance(sel, dict):\n",
    "            T_gate = sel.get(\"oof_best_thr\", None)\n",
    "\n",
    "    # fallback\n",
    "    if T_gate is None:\n",
    "        T_gate = 0.5\n",
    "\n",
    "    thresholds = {\n",
    "        \"T_gate\": float(T_gate),\n",
    "        \"beta_for_tuning\": 0.5,\n",
    "        \"guards\": {\n",
    "            \"min_area_frac\": None,\n",
    "            \"max_area_frac\": None,\n",
    "            \"max_components\": None\n",
    "        },\n",
    "        \"notes\": \"Placeholder. Update after calibration/threshold tuning on OOF or validation set.\"\n",
    "    }\n",
    "    thresholds_path.write_text(json.dumps(thresholds, indent=2))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Capture dataset/cfg metadata (if available)\n",
    "# ----------------------------\n",
    "cfg_meta = {}\n",
    "if \"PATHS\" in globals() and isinstance(PATHS, dict):\n",
    "    cfg_meta = {\n",
    "        \"COMP_ROOT\": PATHS.get(\"COMP_ROOT\", None),\n",
    "        \"OUT_DS_ROOT\": PATHS.get(\"OUT_DS_ROOT\", None),\n",
    "        \"OUT_ROOT\": PATHS.get(\"OUT_ROOT\", None),\n",
    "        \"MATCH_CFG_DIR\": PATHS.get(\"MATCH_CFG_DIR\", None),\n",
    "        \"PRED_CFG_DIR\": PATHS.get(\"PRED_CFG_DIR\", None),\n",
    "        \"DINO_CFG_DIR\": PATHS.get(\"DINO_CFG_DIR\", None),\n",
    "        \"DINO_LARGE_DIR\": PATHS.get(\"DINO_LARGE_DIR\", None),\n",
    "        \"PRED_FEAT_TRAIN\": PATHS.get(\"PRED_FEAT_TRAIN\", None),\n",
    "        \"MATCH_FEAT_TRAIN\": PATHS.get(\"MATCH_FEAT_TRAIN\", None),\n",
    "        \"DF_TRAIN_ALL\": PATHS.get(\"DF_TRAIN_ALL\", None),\n",
    "        \"CV_CASE_FOLDS\": PATHS.get(\"CV_CASE_FOLDS\", None),\n",
    "        \"IMG_PROFILE_TRAIN\": PATHS.get(\"IMG_PROFILE_TRAIN\", None),\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Manifest (reproducible)\n",
    "# ----------------------------\n",
    "task_str = \"Recod.ai/LUC — Gate Model (authentic vs forged) — DINOv2 features + Transformer gate (.pt)\"\n",
    "\n",
    "manifest = {\n",
    "    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"bundle_version\": \"v3\",\n",
    "    \"task\": task_str,\n",
    "    \"model_format\": model_format,\n",
    "    \"artifacts\": {\n",
    "        \"final_model\": str(final_model_path),\n",
    "        \"final_bundle\": str(final_bundle_path) if final_bundle_path.exists() else None,\n",
    "        \"feature_cols\": str(feature_cols_path),\n",
    "        \"thresholds\": str(thresholds_path),\n",
    "        \"baseline_report\": str(baseline_report_path) if baseline_report_path else None,\n",
    "        \"best_gate_config\": str(opt_config_path) if opt_config_path else None,\n",
    "        \"opt_results_csv\": str(opt_results_csv) if opt_results_csv else None,\n",
    "        \"opt_fold_details_csv\": str(opt_fold_csv) if opt_fold_csv else None,\n",
    "        \"oof_baseline_csv\": str(oof_baseline_csv) if oof_baseline_csv else None,\n",
    "        \"oof_baseline_transformer_csv\": str(oof_tf_baseline_csv) if oof_tf_baseline_csv else None,\n",
    "    },\n",
    "    \"cfg_meta\": cfg_meta,\n",
    "    \"model_summary\": {\n",
    "        \"type\": (final_bundle.get(\"type\") if isinstance(final_bundle, dict) else None),\n",
    "        \"n_seeds\": (final_bundle.get(\"n_seeds\") if isinstance(final_bundle, dict) else None),\n",
    "        \"seeds\": (final_bundle.get(\"seeds\") if isinstance(final_bundle, dict) else None),\n",
    "        \"train_rows\": (final_bundle.get(\"train_rows\") if isinstance(final_bundle, dict) else None),\n",
    "        \"pos_rate\": (final_bundle.get(\"pos_rate\") if isinstance(final_bundle, dict) else None),\n",
    "        \"feature_count\": int(len(feature_cols)),\n",
    "        \"T_gate\": float(thresholds.get(\"T_gate\", 0.5)),\n",
    "    },\n",
    "    \"baseline_summary\": (baseline_report.get(\"overall\") if isinstance(baseline_report, dict) else None),\n",
    "    \"opt_summary\": (opt_config.get(\"selection\") if isinstance(opt_config, dict) else None),\n",
    "}\n",
    "\n",
    "manifest_path = OUT_DIR / \"model_bundle_manifest.json\"\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Bundle pack (JSON portable) + optional joblib\n",
    "# ----------------------------\n",
    "bundle_pack = {\n",
    "    \"model_format\": model_format,\n",
    "    \"final_model_path\": str(final_model_path),\n",
    "    \"final_bundle\": final_bundle,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"thresholds\": thresholds,\n",
    "    \"cfg_meta\": cfg_meta,\n",
    "    \"manifest\": manifest,\n",
    "}\n",
    "\n",
    "bundle_pack_json = OUT_DIR / \"model_bundle_pack.json\"\n",
    "bundle_pack_json.write_text(json.dumps(bundle_pack, indent=2))\n",
    "\n",
    "bundle_pack_joblib = OUT_DIR / \"model_bundle_pack.joblib\"\n",
    "joblib_ok = False\n",
    "try:\n",
    "    import joblib\n",
    "    joblib.dump(bundle_pack, bundle_pack_joblib)\n",
    "    joblib_ok = True\n",
    "except Exception:\n",
    "    joblib_ok = False\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Create portable ZIP\n",
    "# ----------------------------\n",
    "zip_path = OUT_DIR / \"model_bundle_v3.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    safe_add(zf, final_model_path, final_model_path.name)\n",
    "    safe_add(zf, final_bundle_path, final_bundle_path.name)\n",
    "    safe_add(zf, feature_cols_path, feature_cols_path.name)\n",
    "    safe_add(zf, thresholds_path, thresholds_path.name)\n",
    "    safe_add(zf, manifest_path, manifest_path.name)\n",
    "    safe_add(zf, bundle_pack_json, bundle_pack_json.name)\n",
    "    if joblib_ok:\n",
    "        safe_add(zf, bundle_pack_joblib, bundle_pack_joblib.name)\n",
    "\n",
    "    # optional extras\n",
    "    safe_add(zf, baseline_report_path, baseline_report_path.name if baseline_report_path else \"baseline_report.json\")\n",
    "    safe_add(zf, opt_config_path, opt_config_path.name if opt_config_path else \"best_gate_config.json\")\n",
    "\n",
    "    if opt_results_csv:\n",
    "        safe_add(zf, opt_results_csv, f\"opt_search/{opt_results_csv.name}\")\n",
    "    if opt_fold_csv:\n",
    "        safe_add(zf, opt_fold_csv, f\"opt_search/{opt_fold_csv.name}\")\n",
    "\n",
    "    if oof_baseline_csv:\n",
    "        safe_add(zf, oof_baseline_csv, oof_baseline_csv.name)\n",
    "    if oof_tf_baseline_csv:\n",
    "        safe_add(zf, oof_tf_baseline_csv, oof_tf_baseline_csv.name)\n",
    "\n",
    "print(\"\\nOK — Model bundle finalized\")\n",
    "print(\"  manifest     ->\", manifest_path)\n",
    "print(\"  pack (json)  ->\", bundle_pack_json)\n",
    "print(\"  pack (joblib)->\", (bundle_pack_joblib if joblib_ok else \"(skip; joblib not available)\"))\n",
    "print(\"  thresholds   ->\", thresholds_path)\n",
    "print(\"  zip          ->\", zip_path)\n",
    "\n",
    "print(\"\\nBundle summary:\")\n",
    "print(\"  model_format :\", model_format)\n",
    "print(\"  feature_cnt  :\", len(feature_cols))\n",
    "print(\"  T_gate       :\", thresholds.get(\"T_gate\"))\n",
    "print(\"  task         :\", task_str)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14878066,
     "sourceId": 113558,
     "sourceType": "competition"
    },
    {
     "datasetId": 9171323,
     "sourceId": 14387840,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6435.625087,
   "end_time": "2026-01-04T15:45:48.978643",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-04T13:58:33.353556",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
