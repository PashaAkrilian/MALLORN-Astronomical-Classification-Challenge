{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113558,"databundleVersionId":14878066,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14387840,"sourceType":"datasetVersion","datasetId":9171323},{"sourceId":4537,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3329,"modelId":986}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Paths & Select Config (CFG)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Set Paths & Select Config (CFG) (Kaggle-ready, offline)\n# - Root kompetisi: /kaggle/input/recodai-luc-scientific-image-forgery-detection\n# - Root dataset output: /kaggle/input/recod-ailuc-dinov2-base  (atau variasi nama lain)\n# - Auto-detect jika nama folder beda\n# - Auto-pilih CFG terbaik untuk MATCH + PRED berdasarkan coverage fitur train\n#\n# Output globals:\n# - COMP_ROOT, OUT_DS_ROOT, OUT_ROOT\n# - PATHS (dict jalur penting)\n# - MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR (opsional)\n# ============================================================\n\nimport os, re, json\nfrom pathlib import Path\nimport pandas as pd\n\n# ----------------------------\n# Helper: find competition root\n# ----------------------------\ndef find_comp_root(preferred: str = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\") -> Path:\n    p = Path(preferred)\n    if p.exists():\n        return p\n    # fallback: scan /kaggle/input for a dir that looks like the competition\n    base = Path(\"/kaggle/input\")\n    if not base.exists():\n        raise FileNotFoundError(\"/kaggle/input not found (are you in Kaggle notebook?)\")\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        # heuristic: must have sample_submission.csv and test_images/train_images\n        if (d / \"sample_submission.csv\").exists() and ((d / \"train_images\").exists() or (d / \"test_images\").exists()):\n            cands.append(d)\n    if not cands:\n        raise FileNotFoundError(\n            \"Competition root not found. Expected folder with sample_submission.csv and train_images/test_images.\"\n        )\n    # prefer one containing 'recodai' and 'forgery'\n    cands.sort(key=lambda x: ((\"recodai\" not in x.name.lower()), (\"forgery\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: find output dataset root\n# ----------------------------\ndef find_output_dataset_root(preferred_names=(\n    \"recod-ailuc-dinov2-base\",\n    \"recod-ai-luc-dinov2-base\",\n    \"recodai-luc-dinov2-base\",\n    \"recodai-luc-dinov2\",\n)) -> Path:\n    base = Path(\"/kaggle/input\")\n    # try preferred direct hits first\n    for nm in preferred_names:\n        p = base / nm\n        if p.exists():\n            return p\n\n    # fallback: scan for a dataset that contains recodai_luc/artifacts\n    cands = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        # either directly has recodai_luc, or nested one-level\n        if (d / \"recodai_luc\" / \"artifacts\").exists():\n            cands.append(d)\n            continue\n        # some datasets wrap inside one folder\n        inner = list(d.glob(\"*/recodai_luc/artifacts\"))\n        if inner:\n            cands.append(d)\n    if not cands:\n        raise FileNotFoundError(\n            \"Output dataset root not found. Expected something like /kaggle/input/<...>/recodai_luc/artifacts/\"\n        )\n    # prefer those containing 'dinov2' in name\n    cands.sort(key=lambda x: ((\"dinov2\" not in x.name.lower()), x.name))\n    return cands[0]\n\n# ----------------------------\n# Helper: resolve OUT_ROOT = <dataset>/recodai_luc\n# ----------------------------\ndef resolve_out_root(out_ds_root: Path) -> Path:\n    direct = out_ds_root / \"recodai_luc\"\n    if direct.exists():\n        return direct\n    # else find nested\n    hits = list(out_ds_root.glob(\"*/recodai_luc\"))\n    if hits:\n        return hits[0]\n    raise FileNotFoundError(f\"Could not locate recodai_luc folder under {out_ds_root}\")\n\n# ----------------------------\n# Helper: pick best cfg directory by coverage of train feature csv\n# ----------------------------\ndef pick_best_cfg(cache_root: Path, prefix: str, feat_train_pattern: str) -> Path:\n    \"\"\"\n    prefix: e.g. 'match_base_cfg_' or 'pred_base'\n    feat_train_pattern: glob pattern for feature csv inside cfg dir\n    \"\"\"\n    cands = []\n    for d in cache_root.iterdir():\n        if not d.is_dir():\n            continue\n        if not d.name.startswith(prefix):\n            continue\n        # find feature file\n        feat_files = list(d.glob(feat_train_pattern))\n        if not feat_files:\n            continue\n        feat_path = feat_files[0]\n        # score by row count (coverage)\n        try:\n            n = sum(1 for _ in open(feat_path, \"r\", encoding=\"utf-8\", errors=\"ignore\")) - 1\n        except Exception:\n            n = -1\n        cands.append((n, d, feat_path))\n\n    if not cands:\n        raise FileNotFoundError(f\"No cfg folders found under {cache_root} with prefix={prefix} and {feat_train_pattern}\")\n\n    # choose max rows, tie-break by name\n    cands.sort(key=lambda x: (-x[0], x[1].name))\n    best_n, best_dir, best_feat = cands[0]\n    return best_dir\n\n# ----------------------------\n# 0) Locate roots\n# ----------------------------\nCOMP_ROOT = find_comp_root(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\nOUT_DS_ROOT = find_output_dataset_root()\n\nOUT_ROOT = resolve_out_root(OUT_DS_ROOT)  # .../recodai_luc\nART_DIR = OUT_ROOT / \"artifacts\"\nCACHE_DIR = OUT_ROOT / \"cache\"\n\n# ----------------------------\n# 1) Competition paths (raw images/masks)\n# ----------------------------\nPATHS = {}\nPATHS[\"COMP_ROOT\"] = str(COMP_ROOT)\nPATHS[\"SAMPLE_SUB\"] = str(COMP_ROOT / \"sample_submission.csv\")\n\n# common competition layout (handle if nested)\nPATHS[\"TRAIN_IMAGES\"] = str(COMP_ROOT / \"train_images\")\nPATHS[\"TEST_IMAGES\"]  = str(COMP_ROOT / \"test_images\")\nPATHS[\"TRAIN_MASKS\"]  = str(COMP_ROOT / \"train_masks\")\nPATHS[\"SUPP_IMAGES\"]  = str(COMP_ROOT / \"supplemental_images\")\nPATHS[\"SUPP_MASKS\"]   = str(COMP_ROOT / \"supplemental_masks\")\n\n# optional subfolders inside train_images\nPATHS[\"TRAIN_AUTH_DIR\"] = str(COMP_ROOT / \"train_images\" / \"authentic\")\nPATHS[\"TRAIN_FORG_DIR\"] = str(COMP_ROOT / \"train_images\" / \"forged\")\n\n# ----------------------------\n# 2) Output dataset paths (clean artifacts + cache)\n# ----------------------------\nPATHS[\"OUT_DS_ROOT\"] = str(OUT_DS_ROOT)\nPATHS[\"OUT_ROOT\"] = str(OUT_ROOT)\nPATHS[\"ART_DIR\"] = str(ART_DIR)\nPATHS[\"CACHE_DIR\"] = str(CACHE_DIR)\n\n# artifacts (train tables / folds / profiles)\nPATHS[\"DF_TRAIN_ALL\"] = str(ART_DIR / \"df_train_all.parquet\")\nPATHS[\"DF_TRAIN_CLS\"] = str(ART_DIR / \"df_train_cls.parquet\")\nPATHS[\"DF_TRAIN_SEG\"] = str(ART_DIR / \"df_train_seg.parquet\")\nPATHS[\"DF_TEST\"]      = str(ART_DIR / \"df_test.parquet\")\nPATHS[\"CV_CASE_FOLDS\"]   = str(ART_DIR / \"cv_case_folds.csv\")\nPATHS[\"CV_SAMPLE_FOLDS\"] = str(ART_DIR / \"cv_sample_folds.csv\")\nPATHS[\"IMG_PROFILE_TRAIN\"] = str(ART_DIR / \"image_profile_train.parquet\")\nPATHS[\"IMG_PROFILE_TEST\"]  = str(ART_DIR / \"image_profile_test.parquet\")\nPATHS[\"MASK_PROFILE\"]      = str(ART_DIR / \"mask_profile.parquet\")\nPATHS[\"CASE_SUMMARY\"]      = str(ART_DIR / \"case_summary.parquet\")\n\n# ----------------------------\n# 3) Select best MATCH/PRED CFG dirs automatically\n# ----------------------------\nif not CACHE_DIR.exists():\n    raise FileNotFoundError(f\"CACHE_DIR not found: {CACHE_DIR}\")\n\n# Match cfg dirs look like: match_base_cfg_<hash>\nMATCH_CFG_DIR = pick_best_cfg(\n    CACHE_DIR,\n    prefix=\"match_base_cfg_\",\n    feat_train_pattern=\"match_features_train_all.csv\"\n)\n# Pred cfg dirs look like: pred_base_v3_v7_cfg_<hash> (name may vary; use startswith 'pred_base')\n# We'll scan by startswith 'pred_base' and require pred_features_train_all.csv\nPRED_CFG_DIR = pick_best_cfg(\n    CACHE_DIR,\n    prefix=\"pred_base\",\n    feat_train_pattern=\"pred_features_train_all.csv\"\n)\n\n# DINO cache cfg (optional)\nDINO_CFG_DIR = None\ndino_root = CACHE_DIR / \"dino_v2_large\"\nif dino_root.exists():\n    # choose any cfg_* that has manifest_train_all.csv\n    dino_cands = []\n    for d in dino_root.iterdir():\n        if d.is_dir() and d.name.startswith(\"cfg_\") and (d / \"manifest_train_all.csv\").exists():\n            dino_cands.append(d)\n    if dino_cands:\n        dino_cands.sort(key=lambda x: x.name)\n        DINO_CFG_DIR = dino_cands[0]\n\n# attach feature file paths\nPATHS[\"MATCH_CFG_DIR\"] = str(MATCH_CFG_DIR)\nPATHS[\"PRED_CFG_DIR\"]  = str(PRED_CFG_DIR)\nPATHS[\"DINO_CFG_DIR\"]  = str(DINO_CFG_DIR) if DINO_CFG_DIR else \"\"\n\nPATHS[\"MATCH_FEAT_TRAIN\"] = str(MATCH_CFG_DIR / \"match_features_train_all.csv\")\nPATHS[\"MATCH_FEAT_TEST\"]  = str(MATCH_CFG_DIR / \"match_features_test.csv\")\nPATHS[\"PRED_FEAT_TRAIN\"]  = str(PRED_CFG_DIR / \"pred_features_train_all.csv\")\nPATHS[\"PRED_FEAT_TEST\"]   = str(PRED_CFG_DIR / \"pred_features_test.csv\")\n\n# ----------------------------\n# 4) Sanity checks (no hard fail for optional files)\n# ----------------------------\nmust_exist = [\n    (\"sample_submission.csv\", PATHS[\"SAMPLE_SUB\"]),\n    (\"df_train_all.parquet\",  PATHS[\"DF_TRAIN_ALL\"]),\n    (\"cv_case_folds.csv\",     PATHS[\"CV_CASE_FOLDS\"]),\n    (\"match_features_train_all.csv\", PATHS[\"MATCH_FEAT_TRAIN\"]),\n    (\"pred_features_train_all.csv\",  PATHS[\"PRED_FEAT_TRAIN\"]),\n]\nmissing = [name for name, p in must_exist if not Path(p).exists()]\nif missing:\n    raise FileNotFoundError(\"Missing required files: \" + \", \".join(missing))\n\nprint(\"OK — Roots\")\nprint(\"  COMP_ROOT   :\", COMP_ROOT)\nprint(\"  OUT_DS_ROOT :\", OUT_DS_ROOT)\nprint(\"  OUT_ROOT    :\", OUT_ROOT)\nprint(\"\\nOK — Selected CFG\")\nprint(\"  MATCH_CFG_DIR:\", MATCH_CFG_DIR.name)\nprint(\"  PRED_CFG_DIR :\", PRED_CFG_DIR.name)\nprint(\"  DINO_CFG_DIR :\", (DINO_CFG_DIR.name if DINO_CFG_DIR else \"(not found / optional)\"))\n\nprint(\"\\nOK — Key files\")\nfor k in [\"DF_TRAIN_ALL\",\"CV_CASE_FOLDS\",\"MATCH_FEAT_TRAIN\",\"PRED_FEAT_TRAIN\",\"IMG_PROFILE_TRAIN\"]:\n    p = Path(PATHS[k])\n    print(f\"  {k:16s}: {p}  {'(exists)' if p.exists() else '(missing/optional)'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T09:12:51.781026Z","iopub.execute_input":"2026-01-04T09:12:51.781988Z","iopub.status.idle":"2026-01-04T09:12:51.850505Z","shell.execute_reply.started":"2026-01-04T09:12:51.781939Z","shell.execute_reply":"2026-01-04T09:12:51.848811Z"}},"outputs":[{"name":"stdout","text":"OK — Roots\n  COMP_ROOT   : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n  OUT_DS_ROOT : /kaggle/input/recod-ailuc-dinov2-base\n  OUT_ROOT    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc\n\nOK — Selected CFG\n  MATCH_CFG_DIR: match_base_cfg_f9f7ea3a65c5\n  PRED_CFG_DIR : pred_base_v3_v7_cfg_5dbf0aa165\n  DINO_CFG_DIR : cfg_3246fd54aab0\n\nOK — Key files\n  DF_TRAIN_ALL    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet  (exists)\n  CV_CASE_FOLDS   : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv  (exists)\n  MATCH_FEAT_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv  (exists)\n  PRED_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv  (exists)\n  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet  (exists)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Build Training Table (X, y, folds)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 2 — Build Training Table (X, y, folds) — REVISI FULL (lebih “powerful” untuk Transformer)\n# - Fokus: siapkan df_train_tabular + (X_train, y_train, folds, FEATURE_COLS)\n# - Sumber utama: pred_features + (opsional) match_features + (opsional) image_profile\n# - Split: gunakan cv_case_folds.csv (anti leakage, by case_id)\n# - Tidak ada submission di sini\n#\n# Upgrade utama (dibanding versi sebelumnya):\n# - Feature engineering lebih kaya (log/clip untuk banyak kolom heavy-tail)\n# - Interaction features (mengandung informasi komposisi: sim*count, area*sim, density, dll)\n# - Quantile winsorization opsional (mengurangi outlier ekstrem, stabil untuk Transformer)\n# - Simpan schema fitur + clip caps agar reproducible\n#\n# Catatan:\n# - DINOv2 Large model path (offline): /kaggle/input/dinov2/pytorch/large/1\n#   (Di step ini hanya dicek exist; ekstraksi DINO tidak dilakukan di step ini.)\n# ============================================================\n\nimport os, json, math, gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# 0) Require PATHS\n# ----------------------------\nif \"PATHS\" not in globals() or not isinstance(PATHS, dict):\n    raise RuntimeError(\"Missing PATHS. Jalankan dulu STAGE 0 — Set Paths & Select Config (CFG).\")\n\n# ----------------------------\n# 1) Check DINOv2 Large local path (offline)\n# ----------------------------\nDINO_LARGE_DIR = Path(\"/kaggle/input/dinov2/pytorch/large/1\")\nif not DINO_LARGE_DIR.exists():\n    raise FileNotFoundError(f\"DINOv2-Large path not found: {DINO_LARGE_DIR}\")\nPATHS[\"DINO_LARGE_DIR\"] = str(DINO_LARGE_DIR)\n\n# ----------------------------\n# 2) Feature Engineering Config (fleksibel)\n# ----------------------------\nFE_CFG = {\n    # sumber fitur\n    \"use_match_features\": True,\n    \"use_image_profile\": True,\n\n    # engineering\n    \"add_log_features\": True,\n    \"add_interactions\": True,\n\n    # outlier control\n    \"clip_by_quantile\": True,   # winsorization berbasis quantile train-all (tanpa label)\n    \"clip_q\": 0.999,            # 0.999 ~ cap p99.9\n    \"clip_max_fallback\": 1e9,   # fallback jika quantile tidak valid\n\n    # fill\n    \"fillna_value\": 0.0,\n\n    # dtype\n    \"cast_float32\": True,\n}\n\n# ----------------------------\n# 3) Prefer WORKING features if exist (because you may have re-generated there)\n# ----------------------------\ndef prefer_working(input_path: str, working_candidate: str | None = None) -> Path:\n    p_in = Path(input_path)\n    if working_candidate is not None:\n        p_w = Path(working_candidate)\n        if p_w.exists():\n            return p_w\n    return p_in\n\nmatch_cfg_name = Path(PATHS[\"MATCH_CFG_DIR\"]).name\npred_cfg_name  = Path(PATHS[\"PRED_CFG_DIR\"]).name\n\nWORK_ROOT = Path(\"/kaggle/working/recodai_luc/cache\")\nmatch_feat_work = WORK_ROOT / match_cfg_name / \"match_features_train_all.csv\"\npred_feat_work  = WORK_ROOT / pred_cfg_name  / \"pred_features_train_all.csv\"\n\nPRED_FEAT_TRAIN  = prefer_working(PATHS[\"PRED_FEAT_TRAIN\"],  str(pred_feat_work))\nMATCH_FEAT_TRAIN = prefer_working(PATHS[\"MATCH_FEAT_TRAIN\"], str(match_feat_work))\n\nDF_TRAIN_ALL     = Path(PATHS[\"DF_TRAIN_ALL\"])\nCV_CASE_FOLDS    = Path(PATHS[\"CV_CASE_FOLDS\"])\nIMG_PROFILE_TRAIN = Path(PATHS.get(\"IMG_PROFILE_TRAIN\", \"\"))\n\nfor need_name, need_path in [\n    (\"df_train_all.parquet\", DF_TRAIN_ALL),\n    (\"cv_case_folds.csv\", CV_CASE_FOLDS),\n    (\"pred_features_train_all.csv\", PRED_FEAT_TRAIN),\n]:\n    if not need_path.exists():\n        raise FileNotFoundError(f\"Missing required file: {need_name} -> {need_path}\")\n\nprint(\"Using:\")\nprint(\"  DF_TRAIN_ALL     :\", DF_TRAIN_ALL)\nprint(\"  CV_CASE_FOLDS    :\", CV_CASE_FOLDS)\nprint(\"  PRED_FEAT_TRAIN  :\", PRED_FEAT_TRAIN)\nprint(\"  MATCH_FEAT_TRAIN :\", MATCH_FEAT_TRAIN, \"(optional)\" if MATCH_FEAT_TRAIN.exists() else \"(missing/skip)\")\nprint(\"  IMG_PROFILE_TRAIN:\", IMG_PROFILE_TRAIN, \"(optional)\" if IMG_PROFILE_TRAIN.exists() else \"(missing/skip)\")\nprint(\"  DINO_LARGE_DIR   :\", DINO_LARGE_DIR)\n\n# ----------------------------\n# 4) Load minimal inputs\n# ----------------------------\ndf_base = pd.read_parquet(DF_TRAIN_ALL)\ndf_cv   = pd.read_csv(CV_CASE_FOLDS)\ndf_pred = pd.read_csv(PRED_FEAT_TRAIN)\n\ndf_match = None\nif FE_CFG[\"use_match_features\"] and MATCH_FEAT_TRAIN.exists():\n    try:\n        df_match = pd.read_csv(MATCH_FEAT_TRAIN)\n    except Exception:\n        df_match = None\n\ndf_prof = None\nif FE_CFG[\"use_image_profile\"] and IMG_PROFILE_TRAIN.exists():\n    try:\n        df_prof = pd.read_parquet(IMG_PROFILE_TRAIN)\n    except Exception:\n        df_prof = None\n\n# ----------------------------\n# 5) Normalize keys: uid/sample_id, case_id, variant\n# ----------------------------\ndef ensure_uid_case_variant(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"uid\" not in df.columns:\n        for alt in [\"sample_id\", \"id\", \"key\"]:\n            if alt in df.columns:\n                df = df.rename(columns={alt: \"uid\"})\n                break\n    if \"uid\" not in df.columns:\n        raise ValueError(\"Cannot find uid/sample_id column. Expected 'uid' or 'sample_id'.\")\n\n    if \"case_id\" not in df.columns or \"variant\" not in df.columns:\n        uid = df[\"uid\"].astype(str)\n        if \"case_id\" not in df.columns:\n            df[\"case_id\"] = uid.str.extract(r\"^(\\d+)\")[0].astype(\"Int64\")\n        if \"variant\" not in df.columns:\n            v = uid.str.extract(r\"__(\\w+)$\")[0]\n            v2 = uid.str.extract(r\"_(\\w+)$\")[0]\n            df[\"variant\"] = v.fillna(v2).fillna(\"unk\")\n\n    df[\"case_id\"] = df[\"case_id\"].astype(int)\n    df[\"variant\"] = df[\"variant\"].astype(str)\n    df[\"uid\"] = df[\"uid\"].astype(str)\n    return df\n\ndf_pred = ensure_uid_case_variant(df_pred)\n\ndf_base2 = df_base.copy()\nif \"uid\" not in df_base2.columns:\n    if \"sample_id\" in df_base2.columns:\n        df_base2 = df_base2.rename(columns={\"sample_id\": \"uid\"})\n    elif (\"case_id\" in df_base2.columns and \"variant\" in df_base2.columns):\n        df_base2[\"uid\"] = df_base2[\"case_id\"].astype(str) + \"__\" + df_base2[\"variant\"].astype(str)\n\n# label\nlabel_col = None\nfor cand in [\"y_forged\", \"has_mask\", \"is_forged\", \"forged\"]:\n    if cand in df_base2.columns:\n        label_col = cand\n        break\nif label_col is None and \"y_forged\" in df_pred.columns:\n    label_col = \"y_forged\"\nif label_col is None:\n    raise ValueError(\"Cannot find label column in df_train_all/pred_features (y_forged/has_mask/is_forged/forged).\")\n\n# folds\nif \"case_id\" not in df_cv.columns or \"fold\" not in df_cv.columns:\n    raise ValueError(\"cv_case_folds.csv must contain columns: case_id, fold\")\ndf_cv[\"case_id\"] = df_cv[\"case_id\"].astype(int)\ndf_cv[\"fold\"] = df_cv[\"fold\"].astype(int)\n\n# ----------------------------\n# 6) Merge: start from df_pred (1 row per uid)\n# ----------------------------\ndf_train = df_pred.copy()\n\nif \"y_forged\" in df_train.columns:\n    df_train[\"y\"] = df_train[\"y_forged\"].astype(int)\nelse:\n    if \"uid\" in df_base2.columns:\n        df_train = df_train.merge(df_base2[[\"uid\", label_col]].rename(columns={label_col: \"y\"}), on=\"uid\", how=\"left\")\n    else:\n        if {\"case_id\",\"variant\",label_col}.issubset(df_base2.columns):\n            df_train = df_train.merge(\n                df_base2[[\"case_id\",\"variant\",label_col]].rename(columns={label_col:\"y\"}),\n                on=[\"case_id\",\"variant\"], how=\"left\"\n            )\n        else:\n            raise ValueError(\"Could not merge label from df_train_all (missing uid or case_id+variant).\")\n    df_train[\"y\"] = df_train[\"y\"].astype(int)\n\ndf_train = df_train.drop(columns=[\"fold\"], errors=\"ignore\").merge(df_cv[[\"case_id\",\"fold\"]], on=\"case_id\", how=\"left\")\nif df_train[\"fold\"].isna().any():\n    raise ValueError(f\"Missing fold after merging cv_case_folds.csv: {int(df_train['fold'].isna().sum())} rows\")\ndf_train[\"fold\"] = df_train[\"fold\"].astype(int)\n\n# optional: merge match_features (bring only new cols)\nif df_match is not None:\n    df_match = ensure_uid_case_variant(df_match)\n    base_cols = set(df_train.columns)\n    new_cols = [c for c in df_match.columns if c not in base_cols]\n    keep_cols = [\"uid\"] + [c for c in new_cols if c not in [\"case_id\",\"variant\"]]\n    if len(keep_cols) > 1:\n        df_train = df_train.merge(df_match[keep_cols], on=\"uid\", how=\"left\")\n\n# optional: merge image profile by case_id\nif df_prof is not None and \"case_id\" in df_prof.columns:\n    df_prof2 = df_prof.copy()\n    df_prof2[\"case_id\"] = df_prof2[\"case_id\"].astype(int)\n    df_prof2 = df_prof2.drop_duplicates(\"case_id\")\n    clash = set(df_prof2.columns).intersection(df_train.columns)\n    clash -= {\"case_id\"}\n    if clash:\n        df_prof2 = df_prof2.rename(columns={c: f\"profile_{c}\" for c in clash})\n    df_train = df_train.merge(df_prof2, on=\"case_id\", how=\"left\")\n\n# ----------------------------\n# 7) Feature engineering (lebih kaya + stabil untuk Transformer)\n# ----------------------------\ndef safe_log1p(arr):\n    arr = np.asarray(arr, dtype=np.float64)\n    arr = np.where(np.isfinite(arr), arr, 0.0)\n    arr = np.clip(arr, 0.0, None)\n    return np.log1p(arr)\n\ndef get_clip_cap(series: pd.Series, q: float, fallback: float):\n    s = pd.to_numeric(series, errors=\"coerce\").astype(float)\n    s = s.replace([np.inf, -np.inf], np.nan).dropna()\n    if len(s) == 0:\n        return fallback\n    s_pos = s[s >= 0]\n    if len(s_pos) == 0:\n        return fallback\n    cap = float(s_pos.quantile(q))\n    if not np.isfinite(cap) or cap <= 0:\n        return fallback\n    return cap\n\n# daftar kolom heavy-tail yang biasanya ada di pred/match\nHEAVY_COLS = [\n    \"peak_ratio\",\n    \"best_weight\",\n    \"best_count\",\n    \"n_pairs_thr\",\n    \"n_pairs_mnn\",\n    \"n_pairs\",            # jika ada\n    \"n_comp\",\n    \"largest_comp\",\n    \"grid_area_frac\",\n    \"mask_area_frac\",     # jika ada\n    \"pred_area_frac\",     # jika ada\n]\n\nclip_caps = {}\nif FE_CFG[\"clip_by_quantile\"]:\n    for c in HEAVY_COLS:\n        if c in df_train.columns:\n            clip_caps[c] = get_clip_cap(df_train[c], FE_CFG[\"clip_q\"], FE_CFG[\"clip_max_fallback\"])\n\n# log + cap features\nif FE_CFG[\"add_log_features\"]:\n    for c in HEAVY_COLS:\n        if c in df_train.columns:\n            cap = clip_caps.get(c, FE_CFG[\"clip_max_fallback\"])\n            x = pd.to_numeric(df_train[c], errors=\"coerce\").fillna(0.0).astype(float)\n            x = np.clip(x, 0.0, cap)\n            df_train[f\"{c}_cap\"] = x.astype(np.float32)\n            df_train[f\"log_{c}\"] = safe_log1p(x).astype(np.float32)\n\n# fokus khusus: peak_ratio & best_weight (sentinel sangat besar)\nif \"peak_ratio\" in df_train.columns and \"log_peak_ratio\" not in df_train.columns:\n    x = pd.to_numeric(df_train[\"peak_ratio\"], errors=\"coerce\").fillna(0.0).astype(float)\n    x = np.clip(x, 0.0, 1e6)\n    df_train[\"peak_ratio_cap\"] = x.astype(np.float32)\n    df_train[\"log_peak_ratio\"] = safe_log1p(x).astype(np.float32)\n\nif \"best_weight\" in df_train.columns and \"log_best_weight\" not in df_train.columns:\n    x = pd.to_numeric(df_train[\"best_weight\"], errors=\"coerce\").fillna(0.0).astype(float)\n    x = np.clip(x, 0.0, 1e9)\n    df_train[\"best_weight_cap\"] = x.astype(np.float32)\n    df_train[\"log_best_weight\"] = safe_log1p(x).astype(np.float32)\n\n# interaction features (menambah “informasi” untuk model)\nif FE_CFG[\"add_interactions\"]:\n    # guard: ambil nilai aman\n    def getf(col, default=0.0):\n        if col in df_train.columns:\n            return pd.to_numeric(df_train[col], errors=\"coerce\").fillna(default).astype(float).values\n        return np.full(len(df_train), default, dtype=np.float64)\n\n    best_mean_sim = getf(\"best_mean_sim\", 0.0)\n    best_count    = getf(\"best_count\", 0.0)\n    grid_area     = getf(\"grid_area_frac\", 0.0)\n    has_peak      = getf(\"has_peak\", 0.0)\n    n_comp        = getf(\"n_comp\", 0.0)\n    largest_comp  = getf(\"largest_comp\", 0.0)\n\n    # interaksi populer\n    df_train[\"sim_x_count\"] = (best_mean_sim * best_count).astype(np.float32)\n    df_train[\"area_x_sim\"]  = (grid_area * best_mean_sim).astype(np.float32)\n    df_train[\"area_x_count\"] = (grid_area * best_count).astype(np.float32)\n\n    # density/fragmentation\n    df_train[\"comp_density\"] = (largest_comp / (1.0 + n_comp)).astype(np.float32)   # makin besar -> komponen dominan\n    df_train[\"comp_inv\"]     = (1.0 / (1.0 + n_comp)).astype(np.float32)            # penalti fragmentasi\n\n    # peak gating\n    if \"log_peak_ratio\" in df_train.columns:\n        df_train[\"has_peak_x_logpeak\"] = (has_peak * getf(\"log_peak_ratio\", 0.0)).astype(np.float32)\n    else:\n        df_train[\"has_peak_x_logpeak\"] = (has_peak * 0.0).astype(np.float32)\n\n    # jika ada n_pairs_thr/mnn\n    n_pairs_thr = getf(\"n_pairs_thr\", 0.0)\n    n_pairs_mnn = getf(\"n_pairs_mnn\", 0.0)\n    df_train[\"mnn_ratio\"] = (n_pairs_mnn / (1.0 + n_pairs_thr)).astype(np.float32)\n\n# Replace inf with NaN for numeric\nfor c in df_train.columns:\n    if pd.api.types.is_numeric_dtype(df_train[c]):\n        df_train[c] = df_train[c].replace([np.inf, -np.inf], np.nan)\n\n# ----------------------------\n# 8) Select feature columns (numeric only, exclude identifiers/labels)\n# ----------------------------\nTARGET_COLS = {\"y\", \"y_forged\", \"has_mask\", \"is_forged\", \"forged\"}\nSPLIT_COLS  = {\"fold\"}\n\nnum_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\nfeature_cols = [c for c in num_cols if c not in TARGET_COLS and c not in SPLIT_COLS and c not in [\"case_id\"]]\n\n# fill NaN\ndf_train[feature_cols] = df_train[feature_cols].fillna(FE_CFG[\"fillna_value\"])\n\n# cast float32\nif FE_CFG[\"cast_float32\"]:\n    df_train[feature_cols] = df_train[feature_cols].astype(np.float32)\n\n# ----------------------------\n# 9) Final outputs\n# ----------------------------\ndf_train_tabular = df_train[[\"uid\",\"case_id\",\"variant\",\"fold\",\"y\"] + feature_cols].copy()\nX_train = df_train_tabular[feature_cols]\ny_train = df_train_tabular[\"y\"].astype(int)\nfolds   = df_train_tabular[\"fold\"].astype(int)\n\nprint(\"\\nOK — Training table built\")\nprint(\"  df_train_tabular:\", df_train_tabular.shape)\nprint(\"  X_train:\", X_train.shape, \"| y pos%:\", float(y_train.mean())*100.0)\nprint(\"  folds:\", folds.nunique(), \"unique folds\")\nprint(\"  feature_cols:\", len(feature_cols))\n\n# quick sanity\nif X_train.shape[0] != y_train.shape[0]:\n    raise RuntimeError(\"X_train and y_train row mismatch\")\nif y_train.isna().any():\n    raise RuntimeError(\"y_train contains NaN\")\nif folds.isna().any():\n    raise RuntimeError(\"folds contains NaN\")\n\nFEATURE_COLS = feature_cols\nprint(\"\\nFeature head:\", FEATURE_COLS[:20])\nprint(\"Feature tail:\", FEATURE_COLS[-10:])\n\n# ----------------------------\n# 10) Save reproducible schema (feature list + FE config + clip caps)\n# ----------------------------\nOUT_ART = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_ART.mkdir(parents=True, exist_ok=True)\n\nwith open(OUT_ART / \"feature_cols.json\", \"w\") as f:\n    json.dump(FEATURE_COLS, f, indent=2)\n\nschema = {\n    \"fe_cfg\": FE_CFG,\n    \"clip_caps\": clip_caps,\n    \"n_features\": int(len(FEATURE_COLS)),\n    \"example_feature_head\": FEATURE_COLS[:25],\n}\nwith open(OUT_ART / \"feature_schema.json\", \"w\") as f:\n    json.dump(schema, f, indent=2)\n\nprint(f\"\\nSaved -> {OUT_ART/'feature_cols.json'}\")\nprint(f\"Saved -> {OUT_ART/'feature_schema.json'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T09:12:51.971777Z","iopub.execute_input":"2026-01-04T09:12:51.972213Z","iopub.status.idle":"2026-01-04T09:12:52.256848Z","shell.execute_reply.started":"2026-01-04T09:12:51.972183Z","shell.execute_reply":"2026-01-04T09:12:52.255619Z"}},"outputs":[{"name":"stdout","text":"Using:\n  DF_TRAIN_ALL     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/df_train_all.parquet\n  CV_CASE_FOLDS    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/cv_case_folds.csv\n  PRED_FEAT_TRAIN  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_base_v3_v7_cfg_5dbf0aa165/pred_features_train_all.csv\n  MATCH_FEAT_TRAIN : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_base_cfg_f9f7ea3a65c5/match_features_train_all.csv (optional)\n  IMG_PROFILE_TRAIN: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/artifacts/image_profile_train.parquet (optional)\n  DINO_LARGE_DIR   : /kaggle/input/dinov2/pytorch/large/1\n\nOK — Training table built\n  df_train_tabular: (5176, 71)\n  X_train: (5176, 66) | y pos%: 54.07650695517774\n  folds: 5 unique folds\n  feature_cols: 66\n\nFeature head: ['feat_exists', 'match_exists', 'has_peak', 'peak_ratio', 'best_weight', 'best_count', 'best_mean_sim', 'n_pairs_thr', 'n_pairs_mnn', 'best_inlier_ratio', 'best_weight_frac', 'inlier_ratio', 'pair_count', 'uniq_src', 'uniq_dst', 'mean_sim', 'thr_used', 'cnt_thr_used', 'relaxed_used', 'min_pairs_used']\nFeature tail: ['log_largest_comp', 'grid_area_frac_cap', 'log_grid_area_frac', 'sim_x_count', 'area_x_sim', 'area_x_count', 'comp_density', 'comp_inv', 'has_peak_x_logpeak', 'mnn_ratio']\n\nSaved -> /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\nSaved -> /kaggle/working/recodai_luc_gate_artifacts/feature_schema.json\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Train Baseline Model (Leakage-Safe CV)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 3 — Train Baseline Model (Leakage-Safe CV) — TRANSFORMER BASELINE (REVISI FULL v2)\n# - Baseline: Tabular Transformer (FT-Transformer style for numeric features)\n# - CV: pakai kolom `fold` (by case_id)\n# - Output:\n#   * OOF probabilities\n#   * CV report (AUC, F1, Precision, Recall, LogLoss)\n#   * Simpan model per fold + model_full (torch .pt bundle: state_dict + scaler)\n#\n# FIX UTAMA:\n# - predict_proba() handle batch berupa (xb, yb) / list / tuple  -> TIDAK ERROR lagi\n#\n# Upgrade:\n# - AMP (autocast + GradScaler) jika CUDA\n# - Early stopping pakai val_logloss\n# ============================================================\n\nimport json, gc, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, log_loss\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# 0) Require outputs from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nrequired_cols = {\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"}\nmissing_cols = [c for c in required_cols if c not in df_train_tabular.columns]\nif missing_cols:\n    raise ValueError(f\"df_train_tabular missing columns: {missing_cols}.\")\n\n# ----------------------------\n# 1) Config (kapasitas bisa kamu naikkan di sini)\n# ----------------------------\nCFG = {\n    \"seed\": 2025,\n\n    # model capacity\n    \"d_model\": 384,\n    \"n_layers\": 8,\n    \"n_heads\": 8,\n    \"ffn_mult\": 4,\n    \"dropout\": 0.20,\n    \"attn_dropout\": 0.10,\n\n    # training\n    \"batch_size\": 512,\n    \"epochs\": 80,\n    \"lr\": 2e-4,\n    \"weight_decay\": 5e-3,\n    \"warmup_frac\": 0.10,\n    \"grad_clip\": 1.0,\n\n    # early stopping\n    \"early_stop_patience\": 12,\n    \"early_stop_min_delta\": 1e-4,\n\n    # report threshold (cuma untuk report baseline; tuning nanti beda step)\n    \"report_thr\": 0.5,\n}\n\ndef seed_everything(seed: int = 2025):\n    import random, os\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG[\"seed\"])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\nprint(\"Device:\", device, \"| AMP:\", use_amp)\n\n# opsional (pytorch 2.x): bantu matmul lebih kenceng\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\n# ----------------------------\n# 2) Build arrays + guard\n# ----------------------------\nX = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\nfolds = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\n\nif not np.isfinite(X).all():\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\nn = len(df_train_tabular)\nunique_folds = sorted(pd.Series(folds).unique().tolist())\nn_folds = len(unique_folds)\nn_features = X.shape[1]\n\nprint(\"Transformer baseline setup:\")\nprint(\"  rows      :\", n)\nprint(\"  folds     :\", n_folds, \"|\", unique_folds)\nprint(\"  pos%      :\", float(y.mean()) * 100.0)\nprint(\"  n_features:\", n_features)\n\n# ----------------------------\n# 3) Dataset\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.int64))\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\n# ----------------------------\n# 4) FT-Transformer style model for numeric features\n# ----------------------------\nclass FTTransformer(nn.Module):\n    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n                 dropout=0.2, attn_dropout=0.1):\n        super().__init__()\n        self.n_features = n_features\n        self.d_model = d_model\n\n        self.w = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(n_features, d_model))\n        self.feat_emb = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=ffn_mult * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        self.token_dropout = nn.Dropout(attn_dropout)\n        self.norm = nn.LayerNorm(d_model)\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def forward(self, x):\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n        tok = tok + self.feat_emb.unsqueeze(0)\n        tok = self.token_dropout(tok)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)\n        seq = torch.cat([cls, tok], dim=1)\n\n        z = self.encoder(seq)\n        z = self.norm(z[:, 0])\n        logit = self.head(z).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 5) Scaler helpers (fit only on train fold)\n# ----------------------------\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 6) Scheduler: warmup + cosine\n# ----------------------------\ndef make_warmup_cosine_scheduler(optimizer, total_steps: int, warmup_steps: int):\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        return 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# ----------------------------\n# 7) Predict helper (FIX: handle batch=(xb,yb))\n# ----------------------------\n@torch.no_grad()\ndef predict_proba(model, loader):\n    model.eval()\n    probs = []\n    for batch in loader:\n        # batch bisa: xb saja, atau (xb, yb), atau list/tuple\n        if isinstance(batch, (list, tuple)):\n            xb = batch[0]\n        else:\n            xb = batch\n        xb = xb.to(device, non_blocking=True)\n\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            logits = model(xb)\n            p = torch.sigmoid(logits)\n\n        probs.append(p.detach().cpu().numpy())\n    return np.concatenate(probs, axis=0).astype(np.float32)\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-6, 1-1e-6)\n    return float(log_loss(y_true, p, labels=[0,1]))\n\n# ----------------------------\n# 8) Train one fold (AMP + early stopping)\n# ----------------------------\ndef train_one_fold(X_tr, y_tr, X_va, y_va, cfg):\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    dl_tr = DataLoader(\n        ds_tr, batch_size=cfg[\"batch_size\"], shuffle=True,\n        num_workers=2, pin_memory=(device.type==\"cuda\"),\n        drop_last=False\n    )\n    dl_va = DataLoader(\n        ds_va, batch_size=cfg[\"batch_size\"], shuffle=False,\n        num_workers=2, pin_memory=(device.type==\"cuda\"),\n        drop_last=False\n    )\n\n    model = FTTransformer(\n        n_features=n_features,\n        d_model=cfg[\"d_model\"],\n        n_heads=cfg[\"n_heads\"],\n        n_layers=cfg[\"n_layers\"],\n        ffn_mult=cfg[\"ffn_mult\"],\n        dropout=cfg[\"dropout\"],\n        attn_dropout=cfg[\"attn_dropout\"],\n    ).to(device)\n\n    pos = int(y_tr.sum())\n    neg = int(len(y_tr) - pos)\n    pos_weight = float(neg / max(1, pos))\n    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n    total_steps = int(cfg[\"epochs\"]) * max(1, len(dl_tr))\n    warmup_steps = int(cfg[\"warmup_frac\"] * total_steps)\n    sch = make_warmup_cosine_scheduler(opt, total_steps=total_steps, warmup_steps=warmup_steps)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\n    best = {\"val_logloss\": 1e9, \"epoch\": -1}\n    best_state = None\n    bad = 0\n\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        t0 = time.time()\n        loss_sum = 0.0\n        n_sum = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            opt.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n\n            scaler.scale(loss).backward()\n            if cfg[\"grad_clip\"] and cfg[\"grad_clip\"] > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n\n            scaler.step(opt)\n            scaler.update()\n            sch.step()\n\n            loss_sum += float(loss.item()) * xb.size(0)\n            n_sum += xb.size(0)\n\n        # validate (fix: predict_proba can read (xb,yb) batches)\n        p_va = predict_proba(model, dl_va)\n        vll = safe_logloss(y_va, p_va)\n\n        tr_loss = loss_sum / max(1, n_sum)\n        dt = time.time() - t0\n        print(f\"  epoch {epoch+1:03d}/{cfg['epochs']} | train_loss={tr_loss:.5f} | val_logloss={vll:.5f} | dt={dt:.1f}s\")\n\n        improved = (best[\"val_logloss\"] - vll) > float(cfg[\"early_stop_min_delta\"])\n        if improved:\n            best[\"val_logloss\"] = float(vll)\n            best[\"epoch\"] = int(epoch)\n            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= int(cfg[\"early_stop_patience\"]):\n                print(f\"  early stop at epoch {epoch+1}, best_epoch={best['epoch']+1}, best_val_logloss={best['val_logloss']:.5f}\")\n                break\n\n        gc.collect()\n\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    p_va = predict_proba(model, dl_va)\n\n    pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n    }\n    return pack, p_va, best\n\n# ----------------------------\n# 9) CV loop\n# ----------------------------\noof_pred = np.zeros(n, dtype=np.float32)\nfold_reports = []\n\nmodels_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts/baseline_transformer_folds\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\nfor f in unique_folds:\n    print(f\"\\n[Fold {f}]\")\n    tr_idx = np.where(folds != f)[0]\n    va_idx = np.where(folds == f)[0]\n\n    X_tr, y_tr = X[tr_idx], y[tr_idx]\n    X_va, y_va = X[va_idx], y[va_idx]\n\n    pack, p_va, best = train_one_fold(X_tr, y_tr, X_va, y_va, CFG)\n    oof_pred[va_idx] = p_va\n\n    auc = safe_auc(y_va, p_va)\n\n    thr = float(CFG[\"report_thr\"])\n    yhat = (p_va >= thr).astype(np.int32)\n\n    rep = {\n        \"fold\": int(f),\n        \"n_val\": int(len(va_idx)),\n        \"pos_val\": int(y_va.sum()),\n        \"auc\": auc,\n        f\"f1@{thr}\": float(f1_score(y_va, yhat, zero_division=0)),\n        f\"precision@{thr}\": float(precision_score(y_va, yhat, zero_division=0)),\n        f\"recall@{thr}\": float(recall_score(y_va, yhat, zero_division=0)),\n        \"logloss\": safe_logloss(y_va, p_va),\n        \"best_val_logloss\": float(best[\"val_logloss\"]),\n        \"best_epoch\": int(best[\"epoch\"] + 1),\n    }\n    fold_reports.append(rep)\n\n    torch.save(\n        {\"pack\": pack, \"feature_cols\": FEATURE_COLS},\n        models_dir / f\"baseline_transformer_fold_{f}.pt\"\n    )\n\n    if device.type == \"cuda\":\n        torch.cuda.empty_cache()\n    gc.collect()\n\n# ----------------------------\n# 10) Overall OOF metrics\n# ----------------------------\noof_auc = safe_auc(y, oof_pred)\nthr = float(CFG[\"report_thr\"])\noof_yhat = (oof_pred >= thr).astype(np.int32)\n\noverall = {\n    \"rows\": int(n),\n    \"folds\": int(n_folds),\n    \"pos_total\": int(y.sum()),\n    \"pos_rate\": float(y.mean()),\n    \"oof_auc\": oof_auc,\n    f\"oof_f1@{thr}\": float(f1_score(y, oof_yhat, zero_division=0)),\n    f\"oof_precision@{thr}\": float(precision_score(y, oof_yhat, zero_division=0)),\n    f\"oof_recall@{thr}\": float(recall_score(y, oof_yhat, zero_division=0)),\n    \"oof_logloss\": safe_logloss(y, oof_pred),\n}\n\ndf_rep = pd.DataFrame(fold_reports).sort_values(\"fold\").reset_index(drop=True)\nprint(\"\\nPer-fold report:\")\ndisplay(df_rep)\n\nprint(\"\\nOOF overall:\")\nprint(overall)\n\n# ----------------------------\n# 11) Train \"full baseline model\" (fixed, lebih singkat)\n# ----------------------------\ndef train_full_fixed(X_full_raw, y_full, cfg):\n    mu, sig = fit_standardizer(X_full_raw)\n    X_full = apply_standardizer(X_full_raw, mu, sig)\n\n    ds_full = TabDataset(X_full, y_full)\n    dl_full = DataLoader(\n        ds_full, batch_size=cfg[\"batch_size\"], shuffle=True,\n        num_workers=2, pin_memory=(device.type==\"cuda\"),\n        drop_last=False\n    )\n\n    model = FTTransformer(\n        n_features=n_features,\n        d_model=cfg[\"d_model\"],\n        n_heads=cfg[\"n_heads\"],\n        n_layers=cfg[\"n_layers\"],\n        ffn_mult=cfg[\"ffn_mult\"],\n        dropout=cfg[\"dropout\"],\n        attn_dropout=cfg[\"attn_dropout\"],\n    ).to(device)\n\n    pos = int(y_full.sum())\n    neg = int(len(y_full) - pos)\n    pos_weight = float(neg / max(1, pos))\n    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n    E_FULL = max(10, int(cfg[\"epochs\"] * 0.6))\n    total_steps = E_FULL * max(1, len(dl_full))\n    warmup_steps = int(cfg[\"warmup_frac\"] * total_steps)\n    sch = make_warmup_cosine_scheduler(opt, total_steps=total_steps, warmup_steps=warmup_steps)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\n    print(f\"\\nTraining full baseline transformer for {E_FULL} epochs (fixed)...\")\n    for epoch in range(E_FULL):\n        model.train()\n        loss_sum = 0.0\n        n_sum = 0\n        for xb, yb in dl_full:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True).float()\n\n            opt.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n\n            scaler.scale(loss).backward()\n            if cfg[\"grad_clip\"] and cfg[\"grad_clip\"] > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg[\"grad_clip\"]))\n            scaler.step(opt)\n            scaler.update()\n            sch.step()\n\n            loss_sum += float(loss.item()) * xb.size(0)\n            n_sum += xb.size(0)\n\n        print(f\"  full epoch {epoch+1:03d}/{E_FULL} | loss={loss_sum/max(1,n_sum):.5f}\")\n\n    full_pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n    }\n    return full_pack\n\nout_dir = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\nfull_pack = train_full_fixed(X, y, CFG)\ntorch.save({\"pack\": full_pack, \"feature_cols\": FEATURE_COLS}, out_dir / \"baseline_transformer_model_full.pt\")\n\n# ----------------------------\n# 12) Save OOF + report\n# ----------------------------\ndf_oof = df_train_tabular[[\"uid\", \"case_id\", \"variant\", \"fold\", \"y\"]].copy()\ndf_oof[\"oof_pred_baseline_tf\"] = oof_pred\ndf_oof.to_csv(out_dir / \"oof_baseline_transformer.csv\", index=False)\n\nreport = {\n    \"model\": \"FT-Transformer (numeric tabular) — baseline\",\n    \"cfg\": CFG,\n    \"feature_count\": int(len(FEATURE_COLS)),\n    \"fold_reports\": fold_reports,\n    \"overall\": overall,\n}\nwith open(out_dir / \"baseline_transformer_cv_report.json\", \"w\") as f:\n    json.dump(report, f, indent=2)\n\nprint(\"\\nSaved artifacts:\")\nprint(\"  fold models  ->\", models_dir)\nprint(\"  full model   ->\", out_dir / \"baseline_transformer_model_full.pt\")\nprint(\"  oof preds    ->\", out_dir / \"oof_baseline_transformer.csv\")\nprint(\"  cv report    ->\", out_dir / \"baseline_transformer_cv_report.json\")\n\n# Export globals\nOOF_PRED_BASELINE_TF = oof_pred\nBASELINE_TF_OVERALL = overall\nBASELINE_TF_FOLD_REPORTS = fold_reports\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T09:25:54.298080Z","iopub.execute_input":"2026-01-04T09:25:54.299829Z"}},"outputs":[{"name":"stdout","text":"Device: cpu | AMP: False\nTransformer baseline setup:\n  rows      : 5176\n  folds     : 5 | [0, 1, 2, 3, 4]\n  pos%      : 54.07650695517774\n  n_features: 66\n\n[Fold 0]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Optimize Model & Hyperparameters (Iterative)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 4 — Optimize Model & Hyperparameters (Iterative) — TRANSFORMER ONLY (REVISI FULL)\n# - Fokus: cari konfigurasi Tabular Transformer paling kuat (tanpa LightGBM/CatBoost/sklearn trees)\n# - Validasi: Leakage-safe CV pakai kolom `fold` (by case_id)\n# - Skor utama: Best F-beta (beta=0.5) dari OOF (lebih anti-FP)\n#   + log AUC & LogLoss sebagai sanity metric.\n#\n# Output:\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_results.json\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/opt_fold_details.csv\n# - /kaggle/working/recodai_luc_gate_artifacts/opt_search/oof_preds_<cfg_name>.csv (top configs)\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n# - /kaggle/working/recodai_luc_gate_artifacts/best_gate_model.pt  (fold packs untuk config terbaik)\n#\n# REQUIRE:\n# - Step 2 sudah jalan: df_train_tabular, FEATURE_COLS\n# ============================================================\n\nimport os, json, gc, math, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\nfrom sklearn.metrics import roc_auc_score, log_loss, precision_score, recall_score, fbeta_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------------\n# 0) Require data from Step 2\n# ----------------------------\nneed_vars = [\"df_train_tabular\", \"FEATURE_COLS\"]\nfor v in need_vars:\n    if v not in globals():\n        raise RuntimeError(f\"Missing `{v}`. Jalankan dulu Step 2 — Build Training Table (X, y, folds).\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\nX_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\nfolds_all = df_train_tabular[\"fold\"].to_numpy(dtype=np.int64, copy=True)\nuids_all = df_train_tabular[\"uid\"].astype(str).to_numpy()\n\n# guard\nif not np.isfinite(X_all).all():\n    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n\nunique_folds = sorted(pd.Series(folds_all).unique().tolist())\nn = len(y_all)\npos_rate = float(y_all.mean())\nn_features = X_all.shape[1]\n\nprint(\"Optimize setup (Transformer only):\")\nprint(f\"  rows={n} | folds={len(unique_folds)} | pos%={pos_rate*100:.2f} | n_features={n_features}\")\n\n# ----------------------------\n# 1) Global settings\n# ----------------------------\nSEED = 2025\nBETA = 0.5            # beta < 1 => fokus precision (anti false-positive)\nTHR_GRID = 201        # grid thresholds untuk best Fbeta\nREPORT_TOPK_OOF = 3   # simpan OOF untuk topK config\n\ndef seed_everything(seed=2025):\n    import random, os\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\nprint(\"Device:\", device, \"| AMP:\", use_amp)\n\n# ----------------------------\n# 2) Helpers: fast threshold search + safe metrics\n# ----------------------------\ndef best_fbeta_fast(y_true, p, beta=0.5, grid=201):\n    \"\"\"\n    Cari threshold terbaik untuk F-beta secara vectorized.\n    Output: best dict {fbeta, thr, precision, recall}\n    \"\"\"\n    y = (np.asarray(y_true).astype(np.int32) == 1)\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1.0 - 1e-8)\n\n    thrs = np.linspace(0.01, 0.99, grid, dtype=np.float64)\n    pred = (p[:, None] >= thrs[None, :])\n\n    y1 = y[:, None]\n    tp = (pred & y1).sum(axis=0).astype(np.float64)\n    fp = (pred & (~y1)).sum(axis=0).astype(np.float64)\n    fn = (y.sum().astype(np.float64) - tp)\n\n    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n    recall    = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n\n    b2 = beta * beta\n    denom = (b2 * precision + recall)\n    fbeta = np.divide((1.0 + b2) * precision * recall, denom, out=np.zeros_like(precision), where=denom > 0)\n\n    j = int(np.argmax(fbeta))\n    return {\n        \"fbeta\": float(fbeta[j]),\n        \"thr\": float(thrs[j]),\n        \"precision\": float(precision[j]),\n        \"recall\": float(recall[j]),\n    }\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1 - 1e-8)\n    return float(log_loss(y_true, p, labels=[0, 1]))\n\n# ----------------------------\n# 3) Dataset + Standardizer (fit only on train fold => no leakage)\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.int64))\n\n    def __len__(self): return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 4) Transformer model (FT-Transformer style, numeric-only)\n# ----------------------------\nclass FTTransformer(nn.Module):\n    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n                 dropout=0.2, attn_dropout=0.1):\n        super().__init__()\n        self.n_features = n_features\n        self.d_model = d_model\n\n        # per-feature linear tokenization\n        self.w = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(n_features, d_model))\n        self.feat_emb = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=ffn_mult * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        self.token_dropout = nn.Dropout(attn_dropout)\n        self.norm = nn.LayerNorm(d_model)\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def forward(self, x):\n        # x: (B, F)\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n        tok = tok + self.feat_emb.unsqueeze(0)\n        tok = self.token_dropout(tok)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)\n        seq = torch.cat([cls, tok], dim=1)  # (B, 1+F, D)\n\n        z = self.encoder(seq)\n        z = self.norm(z[:, 0])  # CLS\n        logit = self.head(z).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 5) Scheduler: warmup + cosine\n# ----------------------------\ndef make_warmup_cosine_scheduler(optimizer, total_steps: int, warmup_steps: int):\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        return 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n@torch.no_grad()\ndef predict_proba(model, loader):\n    model.eval()\n    probs = []\n    for xb in loader:\n        xb = xb.to(device)\n        logits = model(xb)\n        p = torch.sigmoid(logits).detach().cpu().numpy()\n        probs.append(p)\n    return np.concatenate(probs, axis=0).astype(np.float32)\n\ndef train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg):\n    # standardize per fold\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n    X_van = apply_standardizer(X_va, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    ds_va = TabDataset(X_van, y_va)\n\n    dl_tr = DataLoader(ds_tr, batch_size=cfg[\"batch_size\"], shuffle=True, num_workers=2,\n                       pin_memory=(device.type==\"cuda\"), drop_last=False)\n    dl_va = DataLoader(ds_va, batch_size=cfg[\"batch_size\"], shuffle=False, num_workers=2,\n                       pin_memory=(device.type==\"cuda\"), drop_last=False)\n\n    model = FTTransformer(\n        n_features=n_features,\n        d_model=cfg[\"d_model\"],\n        n_heads=cfg[\"n_heads\"],\n        n_layers=cfg[\"n_layers\"],\n        ffn_mult=cfg[\"ffn_mult\"],\n        dropout=cfg[\"dropout\"],\n        attn_dropout=cfg[\"attn_dropout\"],\n    ).to(device)\n\n    # imbalance\n    pos = int(y_tr.sum())\n    neg = int(len(y_tr) - pos)\n    pos_weight = float(neg / max(1, pos))\n    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n\n    total_steps = cfg[\"epochs\"] * max(1, len(dl_tr))\n    warmup_steps = int(cfg[\"warmup_frac\"] * total_steps)\n    sch = make_warmup_cosine_scheduler(opt, total_steps=total_steps, warmup_steps=warmup_steps)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\n    best_val = 1e9\n    best_state = None\n    best_epoch = -1\n    bad = 0\n\n    for epoch in range(cfg[\"epochs\"]):\n        model.train()\n        loss_sum = 0.0\n        n_sum = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device)\n            yb = yb.to(device).float()\n\n            opt.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n\n            scaler.scale(loss).backward()\n            if cfg[\"grad_clip\"] and cfg[\"grad_clip\"] > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n            scaler.step(opt)\n            scaler.update()\n            sch.step()\n\n            loss_sum += float(loss.item()) * xb.size(0)\n            n_sum += xb.size(0)\n\n        # val\n        p_va = predict_proba(model, dl_va)\n        vll = safe_logloss(y_va, p_va)\n\n        improved = (best_val - vll) > cfg[\"min_delta\"]\n        if improved:\n            best_val = vll\n            best_epoch = epoch\n            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= cfg[\"patience\"]:\n                break\n\n        gc.collect()\n\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    # final val preds (best)\n    p_va = predict_proba(model, dl_va)\n\n    pack = {\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n        \"best_epoch\": int(best_epoch + 1),\n        \"best_val_logloss\": float(best_val),\n    }\n    return pack, p_va\n\n# ----------------------------\n# 6) CV evaluator for a config\n# ----------------------------\ndef run_cv_config(cfg, cfg_name, beta=0.5, thr_grid=201, verbose=False):\n    oof = np.zeros(n, dtype=np.float32)\n    fold_rows = []\n\n    for f in unique_folds:\n        tr = np.where(folds_all != f)[0]\n        va = np.where(folds_all == f)[0]\n\n        X_tr, y_tr = X_all[tr], y_all[tr]\n        X_va, y_va = X_all[va], y_all[va]\n\n        pack, p_va = train_one_fold_transformer(X_tr, y_tr, X_va, y_va, cfg)\n        oof[va] = p_va\n\n        fold_auc = safe_auc(y_va, p_va)\n        fold_ll  = safe_logloss(y_va, p_va)\n        best_fold = best_fbeta_fast(y_va, p_va, beta=beta, grid=max(51, thr_grid//2))\n\n        fold_rows.append({\n            \"cfg\": cfg_name,\n            \"fold\": int(f),\n            \"n_val\": int(len(va)),\n            \"pos_val\": int(y_va.sum()),\n            \"auc\": fold_auc,\n            \"logloss\": fold_ll,\n            \"best_fbeta\": best_fold[\"fbeta\"],\n            \"best_thr\": best_fold[\"thr\"],\n            \"best_prec\": best_fold[\"precision\"],\n            \"best_rec\": best_fold[\"recall\"],\n            \"best_val_logloss\": pack[\"best_val_logloss\"],\n            \"best_epoch\": pack[\"best_epoch\"],\n        })\n\n        if verbose:\n            print(f\"    fold {f}: best_fbeta={best_fold['fbeta']:.5f} thr={best_fold['thr']:.3f} ll={fold_ll:.5f}\")\n\n        del pack\n        gc.collect()\n\n    # overall\n    oof_auc = safe_auc(y_all, oof)\n    oof_ll  = safe_logloss(y_all, oof)\n    best_oof = best_fbeta_fast(y_all, oof, beta=beta, grid=thr_grid)\n\n    summary = {\n        \"cfg\": cfg_name,\n        \"oof_auc\": oof_auc,\n        \"oof_logloss\": oof_ll,\n        \"oof_best_fbeta\": best_oof[\"fbeta\"],\n        \"oof_best_thr\": best_oof[\"thr\"],\n        \"oof_best_prec\": best_oof[\"precision\"],\n        \"oof_best_rec\": best_oof[\"recall\"],\n        \"d_model\": cfg[\"d_model\"],\n        \"n_layers\": cfg[\"n_layers\"],\n        \"n_heads\": cfg[\"n_heads\"],\n        \"ffn_mult\": cfg[\"ffn_mult\"],\n        \"dropout\": cfg[\"dropout\"],\n        \"attn_dropout\": cfg[\"attn_dropout\"],\n        \"batch_size\": cfg[\"batch_size\"],\n        \"epochs\": cfg[\"epochs\"],\n        \"lr\": cfg[\"lr\"],\n        \"weight_decay\": cfg[\"weight_decay\"],\n        \"warmup_frac\": cfg[\"warmup_frac\"],\n        \"patience\": cfg[\"patience\"],\n    }\n    return summary, fold_rows, oof\n\n# ----------------------------\n# 7) Define candidate configs (Transformer MAX OK)\n#    Kamu bebas menambah/ubah list ini kalau mau lebih banyak trial.\n# ----------------------------\nBASE = dict(\n    batch_size=512,\n    epochs=70,\n    lr=2e-4,\n    weight_decay=5e-3,\n    warmup_frac=0.10,\n    grad_clip=1.0,\n    patience=10,\n    min_delta=1e-4,\n)\n\ncandidates = []\n\n# (A) Strong MAX (recommended)\ncandidates.append((\"tf_max_384x8\", dict(BASE, d_model=384, n_layers=8, n_heads=8, ffn_mult=4, dropout=0.20, attn_dropout=0.10)))\n\n# (B) Bigger (more powerful, risk overfit -> more dropout/wd)\ncandidates.append((\"tf_extreme_512x12\", dict(BASE, d_model=512, n_layers=12, n_heads=16, ffn_mult=4, dropout=0.28, attn_dropout=0.15, lr=1.2e-4, weight_decay=1e-2, epochs=90, patience=12)))\n\n# (C) Faster but still strong\ncandidates.append((\"tf_256x6_fast\", dict(BASE, d_model=256, n_layers=6, n_heads=8, ffn_mult=4, dropout=0.18, attn_dropout=0.08, lr=3e-4, weight_decay=3e-3, epochs=60, patience=9)))\n\n# (D) Alternative regularization (lebih tahan noise)\ncandidates.append((\"tf_384x10_reg\", dict(BASE, d_model=384, n_layers=10, n_heads=8, ffn_mult=4, dropout=0.25, attn_dropout=0.12, lr=1.6e-4, weight_decay=8e-3, epochs=85, patience=12)))\n\n# (E) Slightly different ffn width\ncandidates.append((\"tf_384x8_ffn2\", dict(BASE, d_model=384, n_layers=8, n_heads=8, ffn_mult=2, dropout=0.18, attn_dropout=0.10, lr=2.2e-4, weight_decay=4e-3, epochs=70, patience=10)))\n\nprint(f\"\\nTotal Transformer candidates: {len(candidates)}\")\nprint(\"Primary score: OOF best F-beta (beta=0.5)\")\n\n# ----------------------------\n# 8) Run iterative search\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOPT_DIR = OUT_DIR / \"opt_search\"\nOPT_DIR.mkdir(parents=True, exist_ok=True)\n\nall_summaries = []\nall_fold_rows = []\noof_store = {}\n\nt_start = time.time()\nfor i, (name, cfg) in enumerate(candidates, 1):\n    print(f\"\\n[{i:02d}/{len(candidates)}] CV -> {name}\")\n    summ, fold_rows, oof = run_cv_config(cfg, name, beta=BETA, thr_grid=THR_GRID, verbose=False)\n    all_summaries.append(summ)\n    all_fold_rows.extend(fold_rows)\n    oof_store[name] = oof\n\n    print(f\"  oof_best_fbeta: {summ['oof_best_fbeta']:.6f} | thr: {summ['oof_best_thr']:.3f}\"\n          f\" | auc: {(summ['oof_auc'] if summ['oof_auc'] is not None else float('nan')):.6f}\"\n          f\" | logloss: {summ['oof_logloss']:.6f}\")\n\nprint(f\"\\nSearch done in {(time.time()-t_start)/60:.1f} min\")\n\ndf_sum = pd.DataFrame(all_summaries)\ndf_fold = pd.DataFrame(all_fold_rows)\n\n# rank: primary -fbeta, tie-break logloss\ndf_sum = df_sum.sort_values([\"oof_best_fbeta\", \"oof_logloss\"], ascending=[False, True]).reset_index(drop=True)\n\nprint(\"\\nTop candidates:\")\ndisplay(df_sum.head(10))\n\n# save search results\ndf_sum.to_csv(OPT_DIR / \"opt_results.csv\", index=False)\nwith open(OPT_DIR / \"opt_results.json\", \"w\") as f:\n    json.dump(df_sum.to_dict(orient=\"records\"), f, indent=2)\ndf_fold.to_csv(OPT_DIR / \"opt_fold_details.csv\", index=False)\n\n# ----------------------------\n# 9) Optional: simple OOF ensemble (avg top-K)\n# ----------------------------\nTOPK = min(3, len(df_sum))\ntop_names = df_sum[\"cfg\"].head(TOPK).tolist()\n\nens_summary = None\nif TOPK >= 2:\n    oof_ens = np.mean([oof_store[nm] for nm in top_names], axis=0).astype(np.float32)\n    ens_best = best_fbeta_fast(y_all, oof_ens, beta=BETA, grid=THR_GRID)\n    ens_auc  = safe_auc(y_all, oof_ens)\n    ens_ll   = safe_logloss(y_all, oof_ens)\n\n    ens_summary = {\n        \"cfg\": f\"ensemble_avg_top{TOPK}\",\n        \"members\": top_names,\n        \"oof_auc\": ens_auc,\n        \"oof_logloss\": ens_ll,\n        \"oof_best_fbeta\": ens_best[\"fbeta\"],\n        \"oof_best_thr\": ens_best[\"thr\"],\n        \"oof_best_prec\": ens_best[\"precision\"],\n        \"oof_best_rec\": ens_best[\"recall\"],\n    }\n    print(\"\\nEnsemble OOF (avg) result:\")\n    print(ens_summary)\n\n# save OOF preds for top configs (debugging)\nfor nm in top_names[:REPORT_TOPK_OOF]:\n    df_o = pd.DataFrame({\n        \"uid\": uids_all,\n        \"y\": y_all,\n        \"fold\": folds_all,\n        f\"oof_pred_{nm}\": oof_store[nm]\n    })\n    df_o.to_csv(OPT_DIR / f\"oof_preds_{nm}.csv\", index=False)\n\n# ----------------------------\n# 10) Choose best (single vs ensemble) — default pilih yang fbeta paling tinggi\n# ----------------------------\nbest_single = df_sum.iloc[0].to_dict()\nbest_choice = {\"type\": \"single\", \"name\": best_single[\"cfg\"], \"summary\": best_single}\n\nif ens_summary is not None and ens_summary[\"oof_best_fbeta\"] >= float(best_single[\"oof_best_fbeta\"]):\n    best_choice = {\"type\": \"ensemble_oof_only\", \"name\": ens_summary[\"cfg\"], \"summary\": ens_summary}\n\nprint(\"\\nBest choice:\")\nprint(best_choice)\n\n# ----------------------------\n# 11) Re-train & SAVE fold packs for BEST SINGLE config\n#     (Untuk dipakai di Step 5 Final Training / inference, kita simpan model per fold + scaler)\n#     Catatan: jika best_choice = ensemble_oof_only, kita tetap simpan model untuk best single sebagai baseline kuat.\n# ----------------------------\nbest_cfg_name = best_single[\"cfg\"]\nbest_cfg = None\nfor nm, cfg in candidates:\n    if nm == best_cfg_name:\n        best_cfg = cfg\n        break\nif best_cfg is None:\n    raise RuntimeError(\"Best cfg not found in candidates list (unexpected).\")\n\nprint(f\"\\nRe-train folds for best config -> {best_cfg_name}\")\nbest_fold_packs = []\nbest_oof = np.zeros(n, dtype=np.float32)\n\nfor f in unique_folds:\n    tr = np.where(folds_all != f)[0]\n    va = np.where(folds_all == f)[0]\n\n    X_tr, y_tr = X_all[tr], y_all[tr]\n    X_va, y_va = X_all[va], y_all[va]\n\n    pack, p_va = train_one_fold_transformer(X_tr, y_tr, X_va, y_va, best_cfg)\n    pack[\"fold\"] = int(f)\n    best_fold_packs.append(pack)\n    best_oof[va] = p_va\n\n    gc.collect()\n\n# save best fold model pack\nbest_model_path = OUT_DIR / \"best_gate_model.pt\"\ntorch.save(\n    {\n        \"type\": \"transformer_ft\",\n        \"feature_cols\": FEATURE_COLS,\n        \"fold_packs\": best_fold_packs,\n        \"cfg_name\": best_cfg_name,\n        \"cfg\": best_cfg,\n        \"seed\": SEED,\n    },\n    best_model_path\n)\n\n# config bundle for next steps\nbest_oof_best = best_fbeta_fast(y_all, best_oof, beta=BETA, grid=THR_GRID)\nbest_bundle = {\n    \"type\": \"transformer_ft\",\n    \"selection\": {\n        \"primary_metric\": f\"oof_best_fbeta(beta={BETA})\",\n        \"beta\": BETA,\n        \"chosen_cfg\": best_cfg_name,\n        \"chosen_type\": \"single\",\n        \"oof_best_thr\": best_oof_best[\"thr\"],\n        \"oof_best_fbeta\": best_oof_best[\"fbeta\"],\n        \"oof_best_prec\": best_oof_best[\"precision\"],\n        \"oof_best_rec\": best_oof_best[\"recall\"],\n        \"oof_auc\": safe_auc(y_all, best_oof),\n        \"oof_logloss\": safe_logloss(y_all, best_oof),\n    },\n    \"cfg\": best_cfg,\n    \"feature_cols\": FEATURE_COLS,\n    \"notes\": \"Ini masih Step 4 (opt). Final Training (Step 5) bisa train full / ensemble lebih matang.\",\n}\n\nwith open(OUT_DIR / \"best_gate_config.json\", \"w\") as f:\n    json.dump(best_bundle, f, indent=2)\n\nprint(\"\\nSaved best artifacts:\")\nprint(\"  best model (fold packs) ->\", best_model_path)\nprint(\"  best config             ->\", OUT_DIR / \"best_gate_config.json\")\nprint(\"  opt results             ->\", OPT_DIR / \"opt_results.csv\")\nprint(\"  fold detail             ->\", OPT_DIR / \"opt_fold_details.csv\")\n\n# Export globals for Step 5\nBEST_GATE_BUNDLE = best_bundle\nBEST_TF_CFG_NAME = best_cfg_name\nBEST_TF_CFG = best_cfg\nOPT_RESULTS_DF = df_sum\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Training (Train on Full Data)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 5 — Final Training (Train on Full Data) — TRANSFORMER ONLY (REVISI FULL + “DITINGKATKAN”)\n# - Tujuan: latih ulang model Transformer terbaik pada seluruh data train (full data)\n# - Input: hasil Step 2 (df_train_tabular + FEATURE_COLS)\n# - Ambil konfigurasi terbaik dari:\n#     /kaggle/working/recodai_luc_gate_artifacts/best_gate_config.json\n#   atau dari variabel BEST_GATE_BUNDLE (jika masih ada di memory)\n#\n# Output:\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n#   /kaggle/working/recodai_luc_gate_artifacts/final_gate_bundle.json\n#\n# Upgrade utama:\n# - Full-data training pakai Transformer (FT-Transformer numeric)\n# - Standardizer fit di full train (tanpa leakage issue karena final)\n# - Internal val split by case_id (opsional) untuk early stopping (lebih stabil, tidak “buang” data CV)\n# - AMP + Warmup Cosine + AdamW + pos_weight (imbalance)\n# - Opsi MAX_MODE untuk “naikkan kapasitas” (hati-hati OOM)\n# ============================================================\n\nimport os, json, gc, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import log_loss, roc_auc_score\n\n# ----------------------------\n# 0) Require outputs from Step 2\n# ----------------------------\nif \"df_train_tabular\" not in globals():\n    raise RuntimeError(\"Missing `df_train_tabular`. Jalankan Step 2 dulu.\")\nif \"FEATURE_COLS\" not in globals():\n    raise RuntimeError(\"Missing `FEATURE_COLS`. Jalankan Step 2 dulu.\")\n\ndf_train_tabular = df_train_tabular.copy()\nFEATURE_COLS = list(FEATURE_COLS)\n\n# basic arrays\nX_all = df_train_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\ny_all = df_train_tabular[\"y\"].to_numpy(dtype=np.int64, copy=True)\n\nif not np.isfinite(X_all).all():\n    X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n\nprint(\"Final training data:\")\nprint(\"  rows:\", len(y_all), \"| pos%:\", float(y_all.mean()) * 100.0, \"| n_features:\", X_all.shape[1])\n\n# ----------------------------\n# 1) Load best config (from disk or memory)\n# ----------------------------\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\ncfg_path = OUT_DIR / \"best_gate_config.json\"\n\nif \"BEST_GATE_BUNDLE\" in globals() and isinstance(BEST_GATE_BUNDLE, dict):\n    best_bundle = BEST_GATE_BUNDLE\n    source = \"memory(BEST_GATE_BUNDLE)\"\nelif cfg_path.exists():\n    best_bundle = json.loads(cfg_path.read_text())\n    source = str(cfg_path)\nelse:\n    raise FileNotFoundError(\"Best config not found. Jalankan Step 4 dulu (Transformer opt).\")\n\nprint(\"\\nLoaded best config from:\", source)\nprint(\"  type:\", best_bundle.get(\"type\"))\nprint(\"  chosen_cfg:\", best_bundle.get(\"selection\", {}).get(\"chosen_cfg\"))\n\n# Expect transformer bundle format from Step 4 (transformer_ft)\nif best_bundle.get(\"type\") != \"transformer_ft\":\n    raise ValueError(\"best_gate_config.json bukan transformer_ft. Pastikan Step 4 yang dipakai versi TRANSFORMER ONLY.\")\n\ncfg = dict(best_bundle.get(\"cfg\", {}))\nif not cfg:\n    raise ValueError(\"Config `cfg` kosong di best_gate_config.json (unexpected).\")\n\n# ----------------------------\n# 2) Training knobs (ditingkatkan)\n# ----------------------------\nFINAL_SEED = int(best_bundle.get(\"seed\", 2025))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nuse_amp = (device.type == \"cuda\")\n\n# Internal val split by case_id (recommended untuk early stopping)\nUSE_INTERNAL_VAL = True\nVAL_FRAC_CASE = 0.10     # 10% case_id untuk validasi (group-safe)\nEARLY_STOP = True\nPATIENCE = int(cfg.get(\"patience\", 12))\nMIN_DELTA = float(cfg.get(\"min_delta\", 1e-4))\n\n# “Max mode” untuk menaikkan kapasitas (opsional)\n# - Kalau OOM, matikan MAX_MODE atau turunkan batch_size/d_model/layers.\nMAX_MODE = False\n\n# Multi-seed ensemble opsional (lebih kuat tapi lebih lama)\nN_SEEDS = 1  # set 3 kalau mau lebih powerful (inference nanti rata-rata)\n\ndef seed_everything(seed: int):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(FINAL_SEED)\n\nprint(\"\\nDevice:\", device, \"| AMP:\", use_amp)\nprint(\"USE_INTERNAL_VAL:\", USE_INTERNAL_VAL, \"| VAL_FRAC_CASE:\", VAL_FRAC_CASE)\nprint(\"EARLY_STOP:\", EARLY_STOP, \"| PATIENCE:\", PATIENCE)\nprint(\"MAX_MODE:\", MAX_MODE, \"| N_SEEDS:\", N_SEEDS)\n\n# Apply MAX_MODE override (lebih “powerful”)\nif MAX_MODE:\n    # upgrade kapasitas + regularisasi lebih kuat\n    cfg = dict(cfg)  # copy\n    cfg[\"d_model\"] = int(max(cfg.get(\"d_model\", 384), 512))\n    cfg[\"n_layers\"] = int(max(cfg.get(\"n_layers\", 8), 12))\n    cfg[\"n_heads\"] = int(max(cfg.get(\"n_heads\", 8), 16))\n    cfg[\"ffn_mult\"] = int(max(cfg.get(\"ffn_mult\", 4), 4))\n    cfg[\"dropout\"] = float(max(cfg.get(\"dropout\", 0.20), 0.28))\n    cfg[\"attn_dropout\"] = float(max(cfg.get(\"attn_dropout\", 0.10), 0.15))\n    cfg[\"lr\"] = float(min(cfg.get(\"lr\", 2e-4), 1.2e-4))\n    cfg[\"weight_decay\"] = float(max(cfg.get(\"weight_decay\", 5e-3), 1e-2))\n    cfg[\"epochs\"] = int(max(cfg.get(\"epochs\", 70), 90))\n    cfg[\"batch_size\"] = int(min(cfg.get(\"batch_size\", 512), 512))  # jaga OOM\n    cfg[\"warmup_frac\"] = float(max(cfg.get(\"warmup_frac\", 0.10), 0.10))\n    cfg[\"grad_clip\"] = float(cfg.get(\"grad_clip\", 1.0))\n    print(\"\\n[MAX_MODE] Applied override cfg:\")\n    for k in [\"d_model\",\"n_layers\",\"n_heads\",\"ffn_mult\",\"dropout\",\"attn_dropout\",\"lr\",\"weight_decay\",\"epochs\",\"batch_size\"]:\n        print(f\"  {k}: {cfg[k]}\")\n\n# ----------------------------\n# 3) Dataset + standardizer\n# ----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = None if y is None else torch.from_numpy(y.astype(np.int64))\n\n    def __len__(self): return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        return self.X[idx], self.y[idx]\n\ndef fit_standardizer(X_tr: np.ndarray):\n    mu = X_tr.mean(axis=0, dtype=np.float64)\n    sig = X_tr.std(axis=0, dtype=np.float64)\n    sig = np.where(sig < 1e-8, 1.0, sig)\n    return mu.astype(np.float32), sig.astype(np.float32)\n\ndef apply_standardizer(X_in: np.ndarray, mu: np.ndarray, sig: np.ndarray):\n    return ((X_in - mu) / sig).astype(np.float32)\n\n# ----------------------------\n# 4) Model: FT-Transformer numeric\n# ----------------------------\nclass FTTransformer(nn.Module):\n    def __init__(self, n_features, d_model=384, n_heads=8, n_layers=8, ffn_mult=4,\n                 dropout=0.2, attn_dropout=0.1):\n        super().__init__()\n        self.n_features = n_features\n        self.d_model = d_model\n\n        self.w = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n        self.b = nn.Parameter(torch.zeros(n_features, d_model))\n        self.feat_emb = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=ffn_mult * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        self.token_dropout = nn.Dropout(attn_dropout)\n        self.norm = nn.LayerNorm(d_model)\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def forward(self, x):\n        tok = x.unsqueeze(-1) * self.w.unsqueeze(0) + self.b.unsqueeze(0)\n        tok = tok + self.feat_emb.unsqueeze(0)\n        tok = self.token_dropout(tok)\n\n        B = tok.size(0)\n        cls = self.cls.expand(B, -1, -1)\n        seq = torch.cat([cls, tok], dim=1)\n\n        z = self.encoder(seq)\n        z = self.norm(z[:, 0])\n        logit = self.head(z).squeeze(-1)\n        return logit\n\n# ----------------------------\n# 5) Scheduler: warmup + cosine\n# ----------------------------\ndef make_warmup_cosine_scheduler(optimizer, total_steps: int, warmup_steps: int):\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return float(step + 1) / float(max(1, warmup_steps))\n        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        return 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n@torch.no_grad()\ndef predict_proba(model, loader):\n    model.eval()\n    ps = []\n    for xb in loader:\n        xb = xb.to(device)\n        logits = model(xb)\n        p = torch.sigmoid(logits).detach().cpu().numpy()\n        ps.append(p)\n    return np.concatenate(ps, axis=0).astype(np.float32)\n\ndef safe_logloss(y_true, p):\n    p = np.clip(np.asarray(p, dtype=np.float64), 1e-8, 1-1e-8)\n    return float(log_loss(y_true, p, labels=[0,1]))\n\ndef safe_auc(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return None\n    return float(roc_auc_score(y_true, p))\n\n# ----------------------------\n# 6) Internal validation split by case_id (group-safe)\n# ----------------------------\ndef make_case_split(df: pd.DataFrame, val_frac=0.10, seed=2025):\n    if \"case_id\" not in df.columns:\n        raise ValueError(\"df_train_tabular must contain case_id for group-safe split.\")\n    # case-level label: any forged in that case\n    g = df.groupby(\"case_id\")[\"y\"].max().reset_index().rename(columns={\"y\":\"case_y\"})\n    pos_cases = g.loc[g[\"case_y\"] == 1, \"case_id\"].to_numpy()\n    neg_cases = g.loc[g[\"case_y\"] == 0, \"case_id\"].to_numpy()\n\n    rng = np.random.RandomState(seed)\n    rng.shuffle(pos_cases)\n    rng.shuffle(neg_cases)\n\n    n_val_pos = max(1, int(len(pos_cases) * val_frac)) if len(pos_cases) > 0 else 0\n    n_val_neg = max(1, int(len(neg_cases) * val_frac)) if len(neg_cases) > 0 else 0\n\n    val_cases = np.concatenate([pos_cases[:n_val_pos], neg_cases[:n_val_neg]])\n    val_set = set(map(int, val_cases.tolist()))\n\n    is_val = df[\"case_id\"].astype(int).map(lambda x: int(x) in val_set).to_numpy(dtype=bool)\n    return is_val\n\n# ----------------------------\n# 7) Train full (optionally with internal val early stopping)\n# ----------------------------\ndef train_full_once(seed_offset=0):\n    seed_everything(FINAL_SEED + seed_offset)\n\n    # choose train/val\n    if USE_INTERNAL_VAL:\n        is_val = make_case_split(df_train_tabular, val_frac=VAL_FRAC_CASE, seed=FINAL_SEED + seed_offset)\n        tr_idx = np.where(~is_val)[0]\n        va_idx = np.where(is_val)[0]\n        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n        X_va, y_va = X_all[va_idx], y_all[va_idx]\n        print(f\"  internal split: train={len(tr_idx)} | val={len(va_idx)} | val_pos%={float(y_va.mean())*100.0:.2f}\")\n    else:\n        tr_idx = np.arange(len(y_all))\n        va_idx = None\n        X_tr, y_tr = X_all, y_all\n        X_va = y_va = None\n\n    # standardize from TRAIN split (kalau ada val) / full (kalau tidak)\n    mu, sig = fit_standardizer(X_tr)\n    X_trn = apply_standardizer(X_tr, mu, sig)\n\n    ds_tr = TabDataset(X_trn, y_tr)\n    dl_tr = DataLoader(\n        ds_tr,\n        batch_size=int(cfg[\"batch_size\"]),\n        shuffle=True,\n        num_workers=2,\n        pin_memory=(device.type==\"cuda\"),\n        drop_last=False\n    )\n\n    if USE_INTERNAL_VAL:\n        X_van = apply_standardizer(X_va, mu, sig)\n        ds_va = TabDataset(X_van, y_va)\n        dl_va = DataLoader(\n            ds_va,\n            batch_size=int(cfg[\"batch_size\"]),\n            shuffle=False,\n            num_workers=2,\n            pin_memory=(device.type==\"cuda\"),\n            drop_last=False\n        )\n    else:\n        dl_va = None\n\n    model = FTTransformer(\n        n_features=X_all.shape[1],\n        d_model=int(cfg[\"d_model\"]),\n        n_heads=int(cfg[\"n_heads\"]),\n        n_layers=int(cfg[\"n_layers\"]),\n        ffn_mult=int(cfg[\"ffn_mult\"]),\n        dropout=float(cfg[\"dropout\"]),\n        attn_dropout=float(cfg[\"attn_dropout\"]),\n    ).to(device)\n\n    # pos_weight from TRAIN split\n    pos = int(y_tr.sum())\n    neg = int(len(y_tr) - pos)\n    pos_weight = float(neg / max(1, pos))\n    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=float(cfg[\"lr\"]),\n        weight_decay=float(cfg[\"weight_decay\"]),\n    )\n\n    total_steps = int(cfg[\"epochs\"]) * max(1, len(dl_tr))\n    warmup_steps = int(float(cfg[\"warmup_frac\"]) * total_steps)\n    sch = make_warmup_cosine_scheduler(opt, total_steps=total_steps, warmup_steps=warmup_steps)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\n    best_val = 1e9\n    best_state = None\n    best_epoch = -1\n    bad = 0\n\n    t0 = time.time()\n    for epoch in range(int(cfg[\"epochs\"])):\n        model.train()\n        loss_sum = 0.0\n        n_sum = 0\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device)\n            yb = yb.to(device).float()\n\n            opt.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n\n            scaler.scale(loss).backward()\n            if float(cfg.get(\"grad_clip\", 1.0)) > 0:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.get(\"grad_clip\", 1.0)))\n            scaler.step(opt)\n            scaler.update()\n            sch.step()\n\n            loss_sum += float(loss.item()) * xb.size(0)\n            n_sum += xb.size(0)\n\n        tr_loss = loss_sum / max(1, n_sum)\n\n        if dl_va is not None:\n            p_va = predict_proba(model, dl_va)\n            vll = safe_logloss(y_va, p_va)\n            vauc = safe_auc(y_va, p_va)\n\n            improved = (best_val - vll) > MIN_DELTA\n            if improved:\n                best_val = vll\n                best_epoch = epoch\n                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n                bad = 0\n            else:\n                bad += 1\n\n            print(f\"  epoch {epoch+1:03d}/{int(cfg['epochs'])} | tr_loss={tr_loss:.5f} | val_ll={vll:.5f} | val_auc={(vauc if vauc is not None else float('nan')):.5f} | bad={bad}\")\n            if EARLY_STOP and bad >= PATIENCE:\n                print(f\"  early stop at epoch {epoch+1}, best_epoch={best_epoch+1}, best_val_ll={best_val:.5f}\")\n                break\n        else:\n            # no val\n            print(f\"  epoch {epoch+1:03d}/{int(cfg['epochs'])} | tr_loss={tr_loss:.5f}\")\n\n        gc.collect()\n\n    # restore best (if val used)\n    if best_state is not None:\n        model.load_state_dict(best_state, strict=True)\n\n    # pack\n    pack = {\n        \"type\": \"transformer_ft_full\",\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"mu\": mu,\n        \"sig\": sig,\n        \"cfg\": cfg,\n        \"seed\": int(FINAL_SEED + seed_offset),\n        \"pos_weight\": float(pos_weight),\n        \"train_rows\": int(len(y_tr)),\n        \"val_rows\": int(len(y_va)) if USE_INTERNAL_VAL else 0,\n        \"best_epoch\": int(best_epoch + 1) if best_epoch >= 0 else None,\n        \"best_val_logloss\": float(best_val) if best_state is not None else None,\n        \"train_time_s\": float(time.time() - t0),\n    }\n    return pack\n\n# ----------------------------\n# 8) Train final model(s)\n# ----------------------------\nfinal_packs = []\nfor s in range(N_SEEDS):\n    print(f\"\\n[Final Train] seed_offset={s}\")\n    pack = train_full_once(seed_offset=s)\n    final_packs.append(pack)\n    gc.collect()\n\n# ----------------------------\n# 9) Save final artifacts\n# ----------------------------\nfinal_model_path = OUT_DIR / \"final_gate_model.pt\"\ntorch.save(\n    {\n        \"feature_cols\": FEATURE_COLS,\n        \"packs\": final_packs,     # list (even if N_SEEDS=1)\n        \"bundle_source\": source,\n    },\n    final_model_path\n)\n\nfinal_bundle = {\n    \"type\": \"transformer_ft_full\",\n    \"n_seeds\": int(N_SEEDS),\n    \"seeds\": [int(p[\"seed\"]) for p in final_packs],\n    \"feature_cols\": FEATURE_COLS,\n    \"cfg\": cfg,\n    \"use_internal_val\": bool(USE_INTERNAL_VAL),\n    \"val_frac_case\": float(VAL_FRAC_CASE) if USE_INTERNAL_VAL else 0.0,\n    \"early_stop\": bool(EARLY_STOP) if USE_INTERNAL_VAL else False,\n    \"patience\": int(PATIENCE),\n    \"min_delta\": float(MIN_DELTA),\n    \"train_rows\": int(len(y_all)),\n    \"pos_rate\": float(y_all.mean()),\n    \"max_mode\": bool(MAX_MODE),\n    \"notes\": \"Final full-data training (Transformer only). Inference nanti bisa avg antar-seed (dan/atau fold-pack dari Step 4).\",\n    \"ref_selection\": best_bundle.get(\"selection\", {}),\n}\n\nfinal_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\nfinal_bundle_path.write_text(json.dumps(final_bundle, indent=2))\n\nprint(\"\\nSaved final training artifacts:\")\nprint(\"  model  ->\", final_model_path)\nprint(\"  bundle ->\", final_bundle_path)\n\n# Export globals\nFINAL_GATE_MODEL_PT = str(final_model_path)\nFINAL_GATE_BUNDLE = final_bundle\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finalize & Save Model Bundle (Reproducible)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Step 6 — Finalize & Save Model Bundle (Reproducible) — REVISI FULL (TRANSFORMER COMPAT)\n# - Tujuan: satukan artefak penting jadi bundle yang mudah di-load ulang\n# - Tidak ada submission di sini\n#\n# Bundle berisi (disesuaikan dengan pipeline Transformer .pt):\n#  1) final model: final_gate_model.pt  (fallback: .joblib jika ada)\n#  2) feature_cols.json\n#  3) thresholds.json (placeholder / bisa diisi dari step tuning berikutnya)\n#  4) training reports (baseline/opt/final) jika ada\n#  5) metadata cfg (MATCH_CFG_DIR, PRED_CFG_DIR, DINO_CFG_DIR, roots) dari PATHS (jika ada)\n#  6) manifest + bundle pack + satu file ZIP (portable)\n#\n# REQUIRE:\n# - Step 2 (feature_cols.json sudah dibuat)\n# - Step 5 (final_gate_model.pt + final_gate_bundle.json sudah ada)\n# ============================================================\n\nimport os, json, time, platform, warnings, zipfile\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport joblib\n\nwarnings.filterwarnings(\"ignore\")\n\nOUT_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 0) Locate required artifacts\n# ----------------------------\nfinal_model_pt = OUT_DIR / \"final_gate_model.pt\"\nfinal_model_joblib = OUT_DIR / \"final_gate_model.joblib\"  # legacy fallback\nfinal_bundle_path = OUT_DIR / \"final_gate_bundle.json\"\nfeature_cols_path = OUT_DIR / \"feature_cols.json\"\n\nif not feature_cols_path.exists():\n    raise FileNotFoundError(f\"Missing feature_cols: {feature_cols_path} (jalankan Step 2 dulu)\")\n\n# prefer .pt (Transformer), fallback .joblib\nif final_model_pt.exists():\n    final_model_path = final_model_pt\n    model_format = \"torch_pt\"\nelif final_model_joblib.exists():\n    final_model_path = final_model_joblib\n    model_format = \"joblib\"\nelse:\n    raise FileNotFoundError(\n        f\"Missing final model: {final_model_pt} (or legacy {final_model_joblib}). \"\n        \"Jalankan Step 5 dulu.\"\n    )\n\n# Optional artifacts\nbaseline_report_path = OUT_DIR / \"baseline_cv_report.json\"\nopt_config_path = OUT_DIR / \"best_gate_config.json\"\nopt_results_csv = OUT_DIR / \"opt_search\" / \"opt_results.csv\"\nopt_fold_csv = OUT_DIR / \"opt_search\" / \"opt_fold_details.csv\"\n\noof_baseline_csv = OUT_DIR / \"oof_baseline.csv\"\noof_tf_baseline_csv = OUT_DIR / \"oof_baseline_transformer.csv\"\n\nprint(\"Found artifacts:\")\nprint(\"  final_model  :\", final_model_path, f\"(format={model_format})\")\nprint(\"  final_bundle :\", final_bundle_path if final_bundle_path.exists() else \"(missing/skip)\")\nprint(\"  feature_cols :\", feature_cols_path)\n\n# ----------------------------\n# 1) Load minimal metadata\n# ----------------------------\nfeature_cols = json.loads(feature_cols_path.read_text())\n\nfinal_bundle = {}\nif final_bundle_path.exists():\n    try:\n        final_bundle = json.loads(final_bundle_path.read_text())\n    except Exception:\n        final_bundle = {}\n\nbaseline_report = None\nif baseline_report_path.exists():\n    try:\n        baseline_report = json.loads(baseline_report_path.read_text())\n    except Exception:\n        baseline_report = None\n\nopt_config = None\nif opt_config_path.exists():\n    try:\n        opt_config = json.loads(opt_config_path.read_text())\n    except Exception:\n        opt_config = None\n\n# ----------------------------\n# 2) Threshold placeholders (can be updated later)\n# ----------------------------\nthresholds_path = OUT_DIR / \"thresholds.json\"\nif thresholds_path.exists():\n    thresholds = json.loads(thresholds_path.read_text())\nelse:\n    # pilih T_gate dari:\n    # - final_bundle (kalau ada)\n    # - opt_config.selection.oof_best_thr (kalau ada)\n    # - default 0.5\n    T_gate = None\n    if isinstance(final_bundle, dict):\n        T_gate = final_bundle.get(\"oof_best_thr\", None)\n\n    if T_gate is None and isinstance(opt_config, dict):\n        sel = opt_config.get(\"selection\", {}) if isinstance(opt_config.get(\"selection\", {}), dict) else {}\n        T_gate = sel.get(\"oof_best_thr\", None)\n\n    if T_gate is None:\n        T_gate = 0.5\n\n    thresholds = {\n        \"T_gate\": float(T_gate),\n        \"beta_for_tuning\": 0.5,\n        \"guards\": {\n            \"min_area_frac\": None,\n            \"max_area_frac\": None,\n            \"max_components\": None\n        },\n        \"notes\": \"Placeholder. Update after calibration/threshold tuning.\"\n    }\n    thresholds_path.write_text(json.dumps(thresholds, indent=2))\n\n# ----------------------------\n# 3) Capture dataset/cfg metadata (if available)\n# ----------------------------\ncfg_meta = {}\nif \"PATHS\" in globals() and isinstance(PATHS, dict):\n    cfg_meta = {\n        \"COMP_ROOT\": PATHS.get(\"COMP_ROOT\", None),\n        \"OUT_DS_ROOT\": PATHS.get(\"OUT_DS_ROOT\", None),\n        \"OUT_ROOT\": PATHS.get(\"OUT_ROOT\", None),\n        \"MATCH_CFG_DIR\": PATHS.get(\"MATCH_CFG_DIR\", None),\n        \"PRED_CFG_DIR\": PATHS.get(\"PRED_CFG_DIR\", None),\n        \"DINO_CFG_DIR\": PATHS.get(\"DINO_CFG_DIR\", None),\n        \"DINO_LARGE_DIR\": PATHS.get(\"DINO_LARGE_DIR\", None),\n        \"PRED_FEAT_TRAIN\": PATHS.get(\"PRED_FEAT_TRAIN\", None),\n        \"MATCH_FEAT_TRAIN\": PATHS.get(\"MATCH_FEAT_TRAIN\", None),\n        \"DF_TRAIN_ALL\": PATHS.get(\"DF_TRAIN_ALL\", None),\n        \"CV_CASE_FOLDS\": PATHS.get(\"CV_CASE_FOLDS\", None),\n        \"IMG_PROFILE_TRAIN\": PATHS.get(\"IMG_PROFILE_TRAIN\", None),\n    }\n\n# ----------------------------\n# 4) Build reproducible manifest\n# ----------------------------\ntask_str = \"Recod.ai/LUC — Gate Model (authentic vs forged) — DINOv2 tabular features + Transformer gate\"\n\nmanifest = {\n    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"python\": platform.python_version(),\n    \"platform\": platform.platform(),\n    \"bundle_version\": \"v2\",\n    \"task\": task_str,\n    \"model_format\": model_format,\n    \"artifacts\": {\n        \"final_model\": str(final_model_path),\n        \"final_bundle\": str(final_bundle_path) if final_bundle_path.exists() else None,\n        \"feature_cols\": str(feature_cols_path),\n        \"thresholds\": str(thresholds_path),\n        \"baseline_report\": str(baseline_report_path) if baseline_report_path.exists() else None,\n        \"opt_config\": str(opt_config_path) if opt_config_path.exists() else None,\n        \"opt_results_csv\": str(opt_results_csv) if opt_results_csv.exists() else None,\n        \"opt_fold_details_csv\": str(opt_fold_csv) if opt_fold_csv.exists() else None,\n        \"oof_baseline_csv\": str(oof_baseline_csv) if oof_baseline_csv.exists() else None,\n        \"oof_baseline_transformer_csv\": str(oof_tf_baseline_csv) if oof_tf_baseline_csv.exists() else None,\n    },\n    \"cfg_meta\": cfg_meta,\n    \"model_summary\": {\n        \"type\": (final_bundle.get(\"type\", None) if isinstance(final_bundle, dict) else None),\n        \"n_seeds\": (final_bundle.get(\"n_seeds\", None) if isinstance(final_bundle, dict) else None),\n        \"seeds\": (final_bundle.get(\"seeds\", None) if isinstance(final_bundle, dict) else None),\n        \"train_rows\": (final_bundle.get(\"train_rows\", None) if isinstance(final_bundle, dict) else None),\n        \"pos_rate\": (final_bundle.get(\"pos_rate\", None) if isinstance(final_bundle, dict) else None),\n        \"feature_count\": int(len(feature_cols)),\n        \"T_gate\": float(thresholds.get(\"T_gate\", 0.5)),\n    },\n    \"baseline_summary\": (baseline_report.get(\"overall\") if isinstance(baseline_report, dict) else None),\n    \"opt_summary\": (opt_config.get(\"selection\", None) if isinstance(opt_config, dict) else None),\n}\n\nmanifest_path = OUT_DIR / \"model_bundle_manifest.json\"\nmanifest_path.write_text(json.dumps(manifest, indent=2))\n\n# ----------------------------\n# 5) Create a single \"bundle pack\" (joblib) for easy reload\n#    (pack hanya metadata + pointer path; model tetap file terpisah)\n# ----------------------------\nbundle_pack = {\n    \"model_format\": model_format,\n    \"final_model_path\": str(final_model_path),\n    \"final_bundle\": final_bundle,\n    \"feature_cols\": feature_cols,\n    \"thresholds\": thresholds,\n    \"cfg_meta\": cfg_meta,\n    \"manifest\": manifest,\n}\n\nbundle_pack_path = OUT_DIR / \"model_bundle_pack.joblib\"\njoblib.dump(bundle_pack, bundle_pack_path)\n\n# ----------------------------\n# 6) Create portable ZIP (1 file berisi semua artefak yang ada)\n# ----------------------------\nzip_path = OUT_DIR / \"model_bundle_v2.zip\"\n\ndef _safe_add(zf: zipfile.ZipFile, p: Path, arcname: str):\n    if p is None:\n        return\n    p = Path(p)\n    if p.exists() and p.is_file():\n        zf.write(p, arcname=arcname)\n\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n    _safe_add(zf, final_model_path, final_model_path.name)\n    _safe_add(zf, final_bundle_path, final_bundle_path.name)\n    _safe_add(zf, feature_cols_path, feature_cols_path.name)\n    _safe_add(zf, thresholds_path, thresholds_path.name)\n    _safe_add(zf, manifest_path, manifest_path.name)\n    _safe_add(zf, bundle_pack_path, bundle_pack_path.name)\n\n    # optional extras\n    _safe_add(zf, baseline_report_path, baseline_report_path.name)\n    _safe_add(zf, opt_config_path, opt_config_path.name)\n    if opt_results_csv.exists(): _safe_add(zf, opt_results_csv, f\"opt_search/{opt_results_csv.name}\")\n    if opt_fold_csv.exists(): _safe_add(zf, opt_fold_csv, f\"opt_search/{opt_fold_csv.name}\")\n    if oof_baseline_csv.exists(): _safe_add(zf, oof_baseline_csv, oof_baseline_csv.name)\n    if oof_tf_baseline_csv.exists(): _safe_add(zf, oof_tf_baseline_csv, oof_tf_baseline_csv.name)\n\nprint(\"\\nOK — Model bundle finalized\")\nprint(\"  manifest   ->\", manifest_path)\nprint(\"  pack       ->\", bundle_pack_path)\nprint(\"  thresholds ->\", thresholds_path)\nprint(\"  zip        ->\", zip_path)\n\nprint(\"\\nBundle summary:\")\nprint(\"  model_format :\", model_format)\nprint(\"  feature_cnt  :\", len(feature_cols))\nprint(\"  T_gate       :\", thresholds.get(\"T_gate\"))\nprint(\"  task         :\", task_str)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}