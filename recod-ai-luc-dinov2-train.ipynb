{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c5ca79",
   "metadata": {
    "papermill": {
     "duration": 0.010815,
     "end_time": "2026-01-13T05:56:59.060896",
     "exception": false,
     "start_time": "2026-01-13T05:56:59.050081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set Paths & Select Config (CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa3029e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T05:56:59.078531Z",
     "iopub.status.busy": "2026-01-13T05:56:59.078117Z",
     "iopub.status.idle": "2026-01-13T05:57:08.588944Z",
     "shell.execute_reply": "2026-01-13T05:57:08.587792Z"
    },
    "papermill": {
     "duration": 9.522498,
     "end_time": "2026-01-13T05:57:08.590879",
     "exception": false,
     "start_time": "2026-01-13T05:56:59.068381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP_ROOT      : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n",
      "PROF_DIR       : /kaggle/working/recodai_luc_prof\n",
      "TOKEN_ROOT     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_bind_9894bfdb484a (variant_used=base)\n",
      "MATCH_ROOT     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_cfg_2ed747746f9c\n",
      "RUN_DIR        : /kaggle/working/recodai_luc_runs/run_4747afaf927b\n",
      "CFG_ID         : 4747afaf927b\n",
      "Train manifest : /kaggle/working/recodai_luc_prof/train_manifest.parquet | exists: True\n",
      "Test manifest  : /kaggle/working/recodai_luc_prof/test_manifest.parquet | exists: True\n",
      "Folds          : /kaggle/working/recodai_luc_prof/folds.parquet | exists: True\n",
      "Saved cfg      : /kaggle/working/recodai_luc_runs/run_4747afaf927b/run_cfg.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Set Paths & Select Config (FULL PIPELINE) (ONE CELL)\n",
    "# Target pipeline:\n",
    "#   DINOv2-(Giant/Large/Base) dense descriptors\n",
    "# + Copy-Move Matching (self-similarity / PatchMatch-style features)\n",
    "# + Segmentation decoder (UNet++ or DeepLabV3+ASPP) on token-grid\n",
    "# + Gate model (image-level) + Fold Ensemble\n",
    "#\n",
    "# REVISI FULL (SUPER ROBUST + FUTURE-PROOF):\n",
    "# - Resolve COMP_ROOT automatically\n",
    "# - Ensure PROF artifacts in /kaggle/working/recodai_luc_prof:\n",
    "#     * copy from /kaggle/input if present, else rebuild minimal manifests\n",
    "# - Auto-pick TOKEN_ROOT (prefer giant -> large -> base) from /working or /input\n",
    "# - Auto-pick MATCH_ROOT (support match_cfg_* / patchmatch_cfg_* / ssim_cfg_*) from /working or /input\n",
    "# - Create RUN_DIR with standard subfolders:\n",
    "#     seg/, gate/, oof/, preds/, features/, bundle/, logs/\n",
    "# - Save config: run_cfg.json\n",
    "# Exports globals:\n",
    "#   COMP_ROOT, PROF_DIR, TOKEN_ROOT, MATCH_ROOT, RUN_DIR, CFG, PATHS\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time, re, shutil, hashlib, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "WORK = Path(\"/kaggle/working\")\n",
    "INP  = Path(\"/kaggle/input\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def read_json_safe(p: Path, default=None):\n",
    "    try:\n",
    "        return json.loads(Path(p).read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def write_json(p: Path, obj):\n",
    "    p.write_text(json.dumps(obj, indent=2, ensure_ascii=False))\n",
    "\n",
    "def safe_hw(p: Path):\n",
    "    try:\n",
    "        im = Image.open(p)\n",
    "        w, h = im.size\n",
    "        return int(h), int(w)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def parse_case_id(stem: str):\n",
    "    \"\"\"\n",
    "    Robust case_id parsing:\n",
    "    - handle: '12345', '12345__forg', '12345__auth', 'case_12345_x', etc.\n",
    "    Preference: first long-ish digit group.\n",
    "    \"\"\"\n",
    "    s = str(stem)\n",
    "    # common split tokens\n",
    "    for tok in [\"__\", \"_\", \"-\", \" \"]:\n",
    "        s = s.replace(tok, \" \")\n",
    "    m = re.search(r\"\\b(\\d{3,})\\b\", s)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    m = re.search(r\"(\\d+)\", str(stem))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def find_comp_root():\n",
    "    # Prefer known folder name\n",
    "    cand = INP / \"recodai-luc-scientific-image-forgery-detection\"\n",
    "    if cand.exists() and (cand / \"sample_submission.csv\").exists():\n",
    "        return cand\n",
    "\n",
    "    # Fallback: any dataset containing sample_submission.csv + train_images + test_images\n",
    "    for d in sorted(INP.glob(\"*\")):\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if (d / \"sample_submission.csv\").exists() and (d / \"train_images\").exists() and (d / \"test_images\").exists():\n",
    "            return d\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find competition dataset under /kaggle/input \"\n",
    "        \"(need sample_submission.csv + train_images + test_images).\"\n",
    "    )\n",
    "\n",
    "def _pick_train_subdir(root: Path, keywords):\n",
    "    best, best_n = None, -1\n",
    "    if not root.exists():\n",
    "        return None\n",
    "    for d in [p for p in root.rglob(\"*\") if p.is_dir()]:\n",
    "        name = d.name.lower()\n",
    "        if any(k in name for k in keywords):\n",
    "            n = sum(1 for _ in d.glob(\"*.png\"))\n",
    "            if n > best_n:\n",
    "                best, best_n = d, n\n",
    "    return best\n",
    "\n",
    "def find_mask_path(case_id: int, img_stem: str, mask_dirs):\n",
    "    \"\"\"\n",
    "    Try locate mask file for a given case_id / stem.\n",
    "    Supports patterns: {case_id}.png, {img_stem}.png, {case_id}__*.png\n",
    "    \"\"\"\n",
    "    if case_id is None:\n",
    "        return None\n",
    "    stems = [str(case_id), str(img_stem)]\n",
    "    pats = [\n",
    "        f\"{case_id}.png\",\n",
    "        f\"{img_stem}.png\",\n",
    "        f\"{case_id}__*.png\",\n",
    "        f\"*{case_id}*.png\",\n",
    "    ]\n",
    "    for md in mask_dirs:\n",
    "        if md is None or (not md.exists()):\n",
    "            continue\n",
    "        # exact first\n",
    "        for s in stems:\n",
    "            p = md / f\"{s}.png\"\n",
    "            if p.exists():\n",
    "                return str(p)\n",
    "        # patterns\n",
    "        for pat in pats:\n",
    "            hits = list(md.glob(pat))\n",
    "            if hits:\n",
    "                hits = sorted(hits, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "                return str(hits[0])\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Resolve competition root\n",
    "# ----------------------------\n",
    "COMP_ROOT = find_comp_root()\n",
    "SAMPLE_SUB     = COMP_ROOT / \"sample_submission.csv\"\n",
    "TRAIN_IMG_DIR  = COMP_ROOT / \"train_images\"\n",
    "TEST_IMG_DIR   = COMP_ROOT / \"test_images\"\n",
    "TRAIN_MASK_DIR = COMP_ROOT / \"train_masks\"\n",
    "SUP_MASK_DIR   = COMP_ROOT / \"supplemental_masks\"\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Ensure PROF_DIR in /kaggle/working\n",
    "# ----------------------------\n",
    "PROF_DIR = WORK / \"recodai_luc_prof\"\n",
    "PROF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy as much as possible if exists in input (fast path)\n",
    "PROF_WANT = [\n",
    "    \"paths.json\",\n",
    "    \"train_manifest.parquet\",\n",
    "    \"test_manifest.parquet\",\n",
    "    \"folds.parquet\",\n",
    "    # optional but nice to have if you already built them before\n",
    "    \"image_profile.parquet\",\n",
    "    \"mask_index.parquet\",\n",
    "    \"mask_profile.parquet\",\n",
    "    \"dup_case_images.csv\",\n",
    "    \"sanity_report.json\",\n",
    "]\n",
    "\n",
    "def find_input_prof_dir():\n",
    "    cands = []\n",
    "    for p in INP.glob(\"*/recodai_luc_prof\"):\n",
    "        if p.is_dir():\n",
    "            cands.append(p)\n",
    "    for p in INP.glob(\"*/*/recodai_luc_prof\"):\n",
    "        if p.is_dir():\n",
    "            cands.append(p)\n",
    "\n",
    "    good = []\n",
    "    for p in cands:\n",
    "        if (p/\"train_manifest.parquet\").exists() and (p/\"test_manifest.parquet\").exists():\n",
    "            good.append(p)\n",
    "\n",
    "    if good:\n",
    "        good = sorted(good, key=lambda x: (x/\"train_manifest.parquet\").stat().st_mtime, reverse=True)\n",
    "        return good[0]\n",
    "\n",
    "    for pj in INP.rglob(\"recodai_luc_prof/paths.json\"):\n",
    "        p = pj.parent\n",
    "        if (p/\"train_manifest.parquet\").exists() and (p/\"test_manifest.parquet\").exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def copy_prof_if_missing(src_dir: Path, dst_dir: Path):\n",
    "    if src_dir is None:\n",
    "        return\n",
    "    for fn in PROF_WANT:\n",
    "        sp = src_dir / fn\n",
    "        dp = dst_dir / fn\n",
    "        if sp.exists() and (not dp.exists()):\n",
    "            try:\n",
    "                shutil.copy2(sp, dp)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "copy_prof_if_missing(find_input_prof_dir(), PROF_DIR)\n",
    "\n",
    "# If still missing core manifests -> rebuild minimal\n",
    "paths_json = PROF_DIR / \"paths.json\"\n",
    "train_pq   = PROF_DIR / \"train_manifest.parquet\"\n",
    "test_pq    = PROF_DIR / \"test_manifest.parquet\"\n",
    "folds_pq   = PROF_DIR / \"folds.parquet\"\n",
    "\n",
    "def build_train_manifest():\n",
    "    # expected structure: train_images/authentic and train_images/forged\n",
    "    auth_dir = TRAIN_IMG_DIR / \"authentic\"\n",
    "    forg_dir = TRAIN_IMG_DIR / \"forged\"\n",
    "\n",
    "    if not auth_dir.exists():\n",
    "        auth_dir = _pick_train_subdir(TRAIN_IMG_DIR, [\"auth\", \"real\", \"clean\"]) or auth_dir\n",
    "    if not forg_dir.exists():\n",
    "        forg_dir = _pick_train_subdir(TRAIN_IMG_DIR, [\"forg\", \"fake\", \"tamper\", \"manip\"]) or forg_dir\n",
    "\n",
    "    mask_dirs = [TRAIN_MASK_DIR, SUP_MASK_DIR]\n",
    "\n",
    "    rows = []\n",
    "    if auth_dir.exists():\n",
    "        for p in sorted(auth_dir.glob(\"*.png\")):\n",
    "            cid = parse_case_id(p.stem)\n",
    "            if cid is None:\n",
    "                continue\n",
    "            H, W = safe_hw(p)\n",
    "            rows.append({\n",
    "                \"uid\": p.stem,\n",
    "                \"case_id\": int(cid),\n",
    "                \"y\": 0,\n",
    "                \"img_path\": str(p),\n",
    "                \"mask_path\": None,\n",
    "                \"H\": H, \"W\": W\n",
    "            })\n",
    "\n",
    "    if forg_dir.exists():\n",
    "        for p in sorted(forg_dir.glob(\"*.png\")):\n",
    "            cid = parse_case_id(p.stem)\n",
    "            if cid is None:\n",
    "                continue\n",
    "            H, W = safe_hw(p)\n",
    "            mpath = find_mask_path(cid, p.stem, mask_dirs)\n",
    "            rows.append({\n",
    "                \"uid\": p.stem,\n",
    "                \"case_id\": int(cid),\n",
    "                \"y\": 1,\n",
    "                \"img_path\": str(p),\n",
    "                \"mask_path\": mpath,\n",
    "                \"H\": H, \"W\": W\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"uid\",\"case_id\",\"y\",\"img_path\",\"mask_path\",\"H\",\"W\"])\n",
    "    return df, str(auth_dir), str(forg_dir)\n",
    "\n",
    "def build_test_manifest():\n",
    "    rows = []\n",
    "    for p in sorted(TEST_IMG_DIR.glob(\"*.png\")):\n",
    "        cid = parse_case_id(p.stem)\n",
    "        if cid is None:\n",
    "            continue\n",
    "        H, W = safe_hw(p)\n",
    "        rows.append({\"uid\": p.stem, \"case_id\": int(cid), \"img_path\": str(p), \"H\": H, \"W\": W})\n",
    "    df = pd.DataFrame(rows, columns=[\"uid\",\"case_id\",\"img_path\",\"H\",\"W\"])\n",
    "    return df\n",
    "\n",
    "def build_folds(df_train, n_folds=5, seed=42):\n",
    "    \"\"\"\n",
    "    Leakage-safe by case_id.\n",
    "    If multiple rows per case_id exist, we group by case_id.\n",
    "    Otherwise, plain stratified works.\n",
    "    \"\"\"\n",
    "    df = df_train[[\"uid\",\"case_id\",\"y\"]].copy()\n",
    "    df[\"fold\"] = -1\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame([], columns=[\"uid\",\"case_id\",\"fold\"])\n",
    "\n",
    "    # detect grouping need\n",
    "    multi = (df.groupby(\"case_id\").size().max() > 1)\n",
    "\n",
    "    try:\n",
    "        if multi:\n",
    "            from sklearn.model_selection import StratifiedGroupKFold\n",
    "            sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "            X = np.zeros(len(df))\n",
    "            for f, (_, va) in enumerate(sgkf.split(X, df[\"y\"].values, groups=df[\"case_id\"].values)):\n",
    "                df.loc[df.index[va], \"fold\"] = f\n",
    "        else:\n",
    "            from sklearn.model_selection import StratifiedKFold\n",
    "            skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "            X = np.zeros(len(df))\n",
    "            for f, (_, va) in enumerate(skf.split(X, df[\"y\"].values)):\n",
    "                df.loc[df.index[va], \"fold\"] = f\n",
    "    except Exception:\n",
    "        df[\"fold\"] = (df[\"case_id\"].astype(int) % int(n_folds)).astype(int)\n",
    "\n",
    "    return df[[\"uid\",\"case_id\",\"fold\"]].sort_values([\"fold\",\"case_id\"]).reset_index(drop=True)\n",
    "\n",
    "if (not paths_json.exists()) or (not train_pq.exists()) or (not test_pq.exists()) or (not folds_pq.exists()):\n",
    "    df_tr, auth_dir_used, forg_dir_used = build_train_manifest()\n",
    "    df_te = build_test_manifest()\n",
    "\n",
    "    if len(df_tr) == 0:\n",
    "        raise RuntimeError(\"train_manifest gagal dibuat (train_images kosong / struktur tidak ketemu). Cek folder train_images.\")\n",
    "    df_fd = build_folds(df_tr, n_folds=5, seed=42)\n",
    "\n",
    "    PATHS = {\n",
    "        \"COMP_ROOT\": str(COMP_ROOT),\n",
    "        \"SAMPLE_SUB\": str(SAMPLE_SUB),\n",
    "        \"TRAIN_IMG_DIR\": str(TRAIN_IMG_DIR),\n",
    "        \"TEST_IMG_DIR\": str(TEST_IMG_DIR),\n",
    "        \"TRAIN_AUTH_DIR\": str(auth_dir_used),\n",
    "        \"TRAIN_FORG_DIR\": str(forg_dir_used),\n",
    "        \"TRAIN_MASK_DIR\": str(TRAIN_MASK_DIR),\n",
    "        \"SUP_MASK_DIR\": str(SUP_MASK_DIR),\n",
    "    }\n",
    "    write_json(paths_json, PATHS)\n",
    "    df_tr.to_parquet(train_pq, index=False)\n",
    "    df_te.to_parquet(test_pq, index=False)\n",
    "    df_fd.to_parquet(folds_pq, index=False)\n",
    "\n",
    "PATHS = read_json_safe(paths_json, default={})\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Auto-pick TOKEN_ROOT and MATCH_ROOT (working OR input)\n",
    "# ----------------------------\n",
    "WORK_CACHE = WORK / \"recodai_luc\" / \"cache\"\n",
    "WORK_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _pick_latest_dir(cands, must_have):\n",
    "    cands = [Path(c) for c in cands if Path(c).is_dir() and (Path(c)/must_have).exists()]\n",
    "    if not cands:\n",
    "        return None\n",
    "    cands = sorted(cands, key=lambda p: (p/must_have).stat().st_mtime, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def _collect_token_candidates(base_dir: Path):\n",
    "    # support many names:\n",
    "    # dinov2_base_518_cfg_xxx / dinov2_large_518_cfg_xxx / dinov2_giant_518_cfg_xxx\n",
    "    # also allow dinov2_*cfg_* variants.\n",
    "    if base_dir is None or (not base_dir.exists()):\n",
    "        return []\n",
    "    cands = []\n",
    "    cands += list(base_dir.glob(\"dinov2_*cfg_*\"))\n",
    "    return cands\n",
    "\n",
    "def pick_token_root(prefer_variants=(\"giant\",\"large\",\"base\")):\n",
    "    \"\"\"\n",
    "    Prefer DINOv2 giant -> large -> base.\n",
    "    If not found, fallback to any token cache found.\n",
    "    \"\"\"\n",
    "    # gather candidates from working + input\n",
    "    w_cands = _collect_token_candidates(WORK_CACHE)\n",
    "\n",
    "    i_cands = []\n",
    "    for p in INP.glob(\"*/recodai_luc/cache\"):\n",
    "        i_cands += _collect_token_candidates(p)\n",
    "    for p in INP.glob(\"*/*/recodai_luc/cache\"):\n",
    "        i_cands += _collect_token_candidates(p)\n",
    "\n",
    "    all_cands = w_cands + i_cands\n",
    "\n",
    "    def variant_of(p: Path):\n",
    "        n = p.name.lower()\n",
    "        if \"giant\" in n: return \"giant\"\n",
    "        if \"large\" in n: return \"large\"\n",
    "        if \"base\"  in n: return \"base\"\n",
    "        return \"unknown\"\n",
    "\n",
    "    # 1) preferred order\n",
    "    for v in prefer_variants:\n",
    "        subset = [p for p in all_cands if variant_of(p) == v]\n",
    "        r = _pick_latest_dir(subset, \"tokens_manifest_train.parquet\")\n",
    "        if r is not None:\n",
    "            return r, v\n",
    "\n",
    "    # 2) any\n",
    "    r = _pick_latest_dir(all_cands, \"tokens_manifest_train.parquet\")\n",
    "    if r is not None:\n",
    "        return r, variant_of(r)\n",
    "\n",
    "    # 3) last resort: locate tokens_manifest_train.parquet anywhere\n",
    "    for tm in INP.rglob(\"tokens_manifest_train.parquet\"):\n",
    "        root = tm.parent\n",
    "        return root, variant_of(root)\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def _collect_match_candidates(base_dir: Path):\n",
    "    if base_dir is None or (not base_dir.exists()):\n",
    "        return []\n",
    "    cands = []\n",
    "    # support multiple naming conventions\n",
    "    cands += list(base_dir.glob(\"match_cfg_*\"))\n",
    "    cands += list(base_dir.glob(\"patchmatch_cfg_*\"))\n",
    "    cands += list(base_dir.glob(\"ssim_cfg_*\"))\n",
    "    return cands\n",
    "\n",
    "def pick_match_root():\n",
    "    w_cands = _collect_match_candidates(WORK_CACHE)\n",
    "\n",
    "    i_cands = []\n",
    "    for p in INP.glob(\"*/recodai_luc/cache\"):\n",
    "        i_cands += _collect_match_candidates(p)\n",
    "    for p in INP.glob(\"*/*/recodai_luc/cache\"):\n",
    "        i_cands += _collect_match_candidates(p)\n",
    "\n",
    "    all_cands = w_cands + i_cands\n",
    "\n",
    "    r = _pick_latest_dir(all_cands, \"match_manifest_train.parquet\")\n",
    "    if r is not None:\n",
    "        return r\n",
    "\n",
    "    # fallback find any manifest\n",
    "    for mm in INP.rglob(\"match_manifest_train.parquet\"):\n",
    "        return mm.parent\n",
    "\n",
    "    return None\n",
    "\n",
    "TOKEN_ROOT, DINO_VARIANT_USED = pick_token_root(prefer_variants=(\"giant\",\"large\",\"base\"))\n",
    "MATCH_ROOT = pick_match_root()\n",
    "\n",
    "if TOKEN_ROOT is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"TOKEN_ROOT tidak ditemukan (butuh tokens_manifest_train.parquet). \"\n",
    "        \"Pasang dataset token-cache atau jalankan stage token-cache.\"\n",
    "    )\n",
    "if MATCH_ROOT is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"MATCH_ROOT tidak ditemukan (butuh match_manifest_train.parquet). \"\n",
    "        \"Pasang dataset match-cache atau jalankan stage matching-cache.\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build RUN_DIR + CFG (full pipeline)\n",
    "# ----------------------------\n",
    "RUNS_DIR = WORK / \"recodai_luc_runs\"\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Defaults: target LB1-style architecture (you can tweak later)\n",
    "CFG = {\n",
    "    \"version\": \"luc_fullpipe_v1\",\n",
    "    \"created_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n",
    "    \"paths\": {\n",
    "        \"COMP_ROOT\": str(COMP_ROOT),\n",
    "        \"PROF_DIR\": str(PROF_DIR),\n",
    "        \"SAMPLE_SUB\": str(SAMPLE_SUB),\n",
    "        \"TRAIN_IMG_DIR\": str(TRAIN_IMG_DIR),\n",
    "        \"TEST_IMG_DIR\": str(TEST_IMG_DIR),\n",
    "        \"TRAIN_MASK_DIR\": str(TRAIN_MASK_DIR),\n",
    "        \"SUP_MASK_DIR\": str(SUP_MASK_DIR),\n",
    "        \"TOKEN_ROOT\": str(TOKEN_ROOT),\n",
    "        \"MATCH_ROOT\": str(MATCH_ROOT),\n",
    "    },\n",
    "\n",
    "    # ---------- Token encoder (DINOv2) ----------\n",
    "    \"token\": {\n",
    "        \"dino_variant_prefer\": [\"giant\", \"large\", \"base\"],\n",
    "        \"dino_variant_used\": DINO_VARIANT_USED,  # auto selected by cache\n",
    "        \"img_size\": 518,               # must match token cache build\n",
    "        \"patch\": 14,\n",
    "        \"tok_norm\": \"l2\",\n",
    "        \"reduce_dim\": {\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"linear\",        # \"linear\" (1x1 conv) / \"pca\" (offline) - set later\n",
    "            \"out_dim\": 256\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # ---------- Copy-Move Matching (self-similarity / PatchMatch-style) ----------\n",
    "    \"matching\": {\n",
    "        \"enabled\": True,\n",
    "        \"method\": \"selfsim_topk\",      # computed as cache in MATCH_ROOT\n",
    "        \"seed_channels\": [\"src\", \"tgt\", \"union\", \"conf\"],  # compatible with your current seeds\n",
    "        \"use_best_peak_only\": True,\n",
    "        # if you rebuild match-cache later, these become the canonical params:\n",
    "        \"params\": {\n",
    "            \"topk\": 8,\n",
    "            \"exclude_radius\": 2,       # token units\n",
    "            \"bidir\": True,\n",
    "            \"use_vote_consistency\": True,\n",
    "            \"vote_bins\": 9,            # small hough bins for displacement consistency\n",
    "        },\n",
    "        # what feature maps are expected for seg fusion (later stage will enforce)\n",
    "        \"feat_maps\": [\n",
    "            \"max_sim\", \"sim_margin\", \"dx\", \"dy\", \"vote_strength\", \"bidir_score\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # ---------- Segmentation model (token-grid -> mask) ----------\n",
    "    \"seg\": {\n",
    "        \"enabled\": True,\n",
    "        \"decoder\": \"unetpp_aspp\",      # \"unetpp_aspp\" or \"deeplabv3p_aspp\"\n",
    "        \"base_ch\": 128,\n",
    "        \"dropout\": 0.1,\n",
    "        \"use_matching_seeds\": True,\n",
    "        \"use_matching_feats\": True,\n",
    "        \"loss\": {\n",
    "            \"bce\": 1.0,\n",
    "            \"dice\": 1.0,               # or \"tversky\" in later stage\n",
    "            \"pos_weight\": 2.0\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"seed\": 42,\n",
    "            \"n_folds\": 5,\n",
    "            \"epochs\": 25,\n",
    "            \"batch_size\": 16,\n",
    "            \"lr\": 3e-4,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"amp\": True,\n",
    "            \"ema\": True,\n",
    "            \"accum_steps\": 1,\n",
    "            \"num_workers\": 2,\n",
    "            \"early_stop_patience\": 6,\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # ---------- Gate model (image-level forged/auth to suppress FP) ----------\n",
    "    \"gate\": {\n",
    "        \"enabled\": True,\n",
    "        \"model\": \"lgbm\",               # could be \"logreg\"/\"mlp\" later\n",
    "        \"train_from_oof\": True,\n",
    "        \"features_policy\": \"auto\",     # stage later will define exact feature_cols\n",
    "        \"thr\": None                    # tuned via OOF sweep later\n",
    "    },\n",
    "\n",
    "    # ---------- Ensemble + inference policy ----------\n",
    "    \"ensemble\": {\n",
    "        \"enabled\": True,\n",
    "        \"use_folds\": True,             # 5-fold ensemble\n",
    "        \"tta\": [\"none\", \"hflip\", \"vflip\"],  # minimal; extend later\n",
    "        \"calibration\": {\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"thr_sweep\",      # keep simple & robust\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # ---------- Postprocess defaults (tuned later on OOF) ----------\n",
    "    \"post\": {\n",
    "        \"prob_thr\": 0.5,\n",
    "        \"min_cc_full_pix\": 64,\n",
    "        \"max_area_frac\": 0.90,\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg_id = hashlib.sha1(json.dumps(CFG, sort_keys=True).encode()).hexdigest()[:12]\n",
    "CFG[\"cfg_id\"] = cfg_id\n",
    "\n",
    "RUN_DIR = RUNS_DIR / f\"run_{cfg_id}\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# standard subdirs\n",
    "for sd in [\"seg\", \"gate\", \"oof\", \"preds\", \"features\", \"bundle\", \"logs\"]:\n",
    "    (RUN_DIR / sd).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save config\n",
    "write_json(RUN_DIR / \"run_cfg.json\", CFG)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Summary\n",
    "# ----------------------------\n",
    "print(\"COMP_ROOT      :\", COMP_ROOT)\n",
    "print(\"PROF_DIR       :\", PROF_DIR)\n",
    "print(\"TOKEN_ROOT     :\", TOKEN_ROOT, f\"(variant_used={DINO_VARIANT_USED})\")\n",
    "print(\"MATCH_ROOT     :\", MATCH_ROOT)\n",
    "print(\"RUN_DIR        :\", RUN_DIR)\n",
    "print(\"CFG_ID         :\", cfg_id)\n",
    "print(\"Train manifest :\", train_pq, \"| exists:\", train_pq.exists())\n",
    "print(\"Test manifest  :\", test_pq,  \"| exists:\", test_pq.exists())\n",
    "print(\"Folds          :\", folds_pq, \"| exists:\", folds_pq.exists())\n",
    "print(\"Saved cfg      :\", RUN_DIR / \"run_cfg.json\")\n",
    "\n",
    "# Export globals (explicit)\n",
    "PATHS = PATHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b3284",
   "metadata": {
    "papermill": {
     "duration": 0.007114,
     "end_time": "2026-01-13T05:57:08.605566",
     "exception": false,
     "start_time": "2026-01-13T05:57:08.598452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Training Table (X, y, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40db9241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T05:57:08.622714Z",
     "iopub.status.busy": "2026-01-13T05:57:08.622200Z",
     "iopub.status.idle": "2026-01-13T05:59:07.285618Z",
     "shell.execute_reply": "2026-01-13T05:59:07.284500Z"
    },
    "papermill": {
     "duration": 118.681774,
     "end_time": "2026-01-13T05:59:07.294579",
     "exception": false,
     "start_time": "2026-01-13T05:57:08.612805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_DIR     : /kaggle/working/recodai_luc_runs/run_4747afaf927b\n",
      "CFG_ID      : 4747afaf927b\n",
      "TOKEN_ROOT  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_bind_9894bfdb484a\n",
      "MATCH_ROOT  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_cfg_2ed747746f9c\n",
      "------------------------------------------------------------\n",
      "Train rows  : 2795 | tok_exists: 0 | match_exists: 2795 | forged%: 100.0\n",
      "Test rows   : 1 | tok_exists: 0 | match_exists: 1\n",
      "Token grid  : (37, 37)\n",
      "[gt_tok] 500/2795 | rebuilt=500 | 12.5s\n",
      "[gt_tok] 1000/2795 | rebuilt=1000 | 23.1s\n",
      "[gt_tok] 1500/2795 | rebuilt=1500 | 35.0s\n",
      "[gt_tok] 2000/2795 | rebuilt=2000 | 47.6s\n",
      "[gt_tok] 2500/2795 | rebuilt=2500 | 59.8s\n",
      "------------------------------------------------------------\n",
      "Saved: /kaggle/working/recodai_luc_runs/run_4747afaf927b/features/train_table.parquet\n",
      "Saved: /kaggle/working/recodai_luc_runs/run_4747afaf927b/features/test_table.parquet\n",
      "GT_TOK_DIR: /kaggle/working/recodai_luc_runs/run_4747afaf927b/seg/gt_tok_masks_4747afaf927b\n",
      "GT rebuilt: 2795 / 2795 | time: 66.2s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Build Train/Test Tables + Precompute GT Token Masks (NO FOLD) (ONE CELL)\n",
    "# REVISI FULL:\n",
    "# - Reads latest RUN_DIR/run_cfg.json (or uses global RUN_DIR if exists)\n",
    "# - Loads train/test manifests (no fold concept at all)\n",
    "# - Robust join with TOKEN_ROOT + MATCH_ROOT manifests (prefer uid, fallback case_id)\n",
    "# - KEEPS ALL ROWS (adds tok_exists/match_exists flags; does NOT drop)\n",
    "# - Infers HTOK/WTOK robustly (cfg.json -> manifest cols -> read one npz)\n",
    "# - Precompute GT token masks (union) into RUN_DIR/seg/gt_tok_masks_<cfg_id>/\n",
    "# - Saves tables to RUN_DIR/features/train_table.parquet & test_table.parquet\n",
    "#\n",
    "# Exports globals:\n",
    "#   DF_TRAIN_ALL, DF_TEST_ALL, HTOK, WTOK, GT_TOK_DIR\n",
    "# ============================================================\n",
    "\n",
    "import os, json, re, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "WORK = Path(\"/kaggle/working\")\n",
    "INP  = Path(\"/kaggle/input\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Locate RUN_DIR + load CFG\n",
    "# ----------------------------\n",
    "def _latest_run_dir():\n",
    "    runs = sorted((WORK / \"recodai_luc_runs\").glob(\"run_*/run_cfg.json\"),\n",
    "                  key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return runs[0].parent if runs else None\n",
    "\n",
    "if \"RUN_DIR\" in globals():\n",
    "    _rd = Path(RUN_DIR)\n",
    "    cfg_path = _rd / \"run_cfg.json\"\n",
    "    if not cfg_path.exists():\n",
    "        _rd = _latest_run_dir()\n",
    "        if _rd is None:\n",
    "            raise FileNotFoundError(\"Cannot find RUN_DIR/run_cfg.json. Run STAGE 1 first.\")\n",
    "        cfg_path = _rd / \"run_cfg.json\"\n",
    "    RUN_DIR = _rd\n",
    "else:\n",
    "    RUN_DIR = _latest_run_dir()\n",
    "    if RUN_DIR is None:\n",
    "        raise FileNotFoundError(\"Cannot find any run_*/run_cfg.json under /kaggle/working/recodai_luc_runs. Run STAGE 1 first.\")\n",
    "    cfg_path = RUN_DIR / \"run_cfg.json\"\n",
    "\n",
    "CFG = json.loads(cfg_path.read_text())\n",
    "CFG_ID = CFG.get(\"cfg_id\", \"no_cfgid\")\n",
    "\n",
    "for sd in [\"seg\", \"gate\", \"oof\", \"preds\", \"features\", \"bundle\", \"logs\"]:\n",
    "    (RUN_DIR / sd).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROF_DIR       = Path(CFG[\"paths\"][\"PROF_DIR\"])\n",
    "TOKEN_ROOT     = Path(CFG[\"paths\"][\"TOKEN_ROOT\"])\n",
    "MATCH_ROOT     = Path(CFG[\"paths\"][\"MATCH_ROOT\"])\n",
    "TRAIN_MASK_DIR = Path(CFG[\"paths\"][\"TRAIN_MASK_DIR\"]) if CFG[\"paths\"].get(\"TRAIN_MASK_DIR\") else None\n",
    "SUP_MASK_DIR   = Path(CFG[\"paths\"][\"SUP_MASK_DIR\"]) if CFG[\"paths\"].get(\"SUP_MASK_DIR\") else None\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _infer_case_id_from_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str)\n",
    "    out = s.str.extract(r\"(\\d+)\")[0]\n",
    "    return pd.to_numeric(out, errors=\"coerce\")\n",
    "\n",
    "def ensure_uid_case(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"uid\" not in df.columns:\n",
    "        if \"img_path\" in df.columns:\n",
    "            df[\"uid\"] = df[\"img_path\"].astype(str).apply(lambda x: Path(x).stem)\n",
    "        elif \"npz_path\" in df.columns:\n",
    "            df[\"uid\"] = df[\"npz_path\"].astype(str).apply(lambda x: Path(x).stem)\n",
    "        elif \"tok_npz\" in df.columns:\n",
    "            df[\"uid\"] = df[\"tok_npz\"].astype(str).apply(lambda x: Path(x).stem)\n",
    "        elif \"match_npz\" in df.columns:\n",
    "            df[\"uid\"] = df[\"match_npz\"].astype(str).apply(lambda x: Path(x).stem)\n",
    "        else:\n",
    "            df[\"uid\"] = None\n",
    "\n",
    "    if \"case_id\" not in df.columns:\n",
    "        for c in [\"uid\", \"uid_safe\", \"id\", \"img_path\", \"npz_path\", \"tok_npz\", \"match_npz\"]:\n",
    "            if c in df.columns:\n",
    "                df[\"case_id\"] = _infer_case_id_from_series(df[c])\n",
    "                break\n",
    "    df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n",
    "    df = df[df[\"case_id\"].notna()].copy()\n",
    "    df[\"case_id\"] = df[\"case_id\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def resolve_any_path(p, root: Path, split_hint: str):\n",
    "    if p is None or (isinstance(p, float) and np.isnan(p)):\n",
    "        return None\n",
    "    s = str(p)\n",
    "    if not s:\n",
    "        return None\n",
    "    pp = Path(s)\n",
    "    if pp.exists():\n",
    "        return str(pp)\n",
    "\n",
    "    if not pp.is_absolute():\n",
    "        cand = root / pp\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    name = pp.name\n",
    "    cand1 = root / split_hint / name\n",
    "    if cand1.exists():\n",
    "        return str(cand1)\n",
    "    cand2 = root / name\n",
    "    if cand2.exists():\n",
    "        return str(cand2)\n",
    "\n",
    "    s2 = s.replace(\"\\\\\", \"/\")\n",
    "    for seg in [\"train\", \"test\"]:\n",
    "        if f\"/{seg}/\" in s2:\n",
    "            tail = s2.split(f\"/{seg}/\", 1)[1]\n",
    "            cand3 = root / seg / tail\n",
    "            if cand3.exists():\n",
    "                return str(cand3)\n",
    "    return None\n",
    "\n",
    "def dedup_best(df, key_cols, path_col):\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"_ok\"] = df[path_col].apply(lambda x: isinstance(x, str) and Path(x).exists())\n",
    "    df = df.sort_values([\"_ok\"], ascending=False).drop_duplicates(key_cols, keep=\"first\").drop(columns=[\"_ok\"])\n",
    "    return df\n",
    "\n",
    "def _pick_path_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def find_mask_path(case_id: int, uid: str):\n",
    "    mask_dirs = [TRAIN_MASK_DIR, SUP_MASK_DIR]\n",
    "    if case_id is None:\n",
    "        return None\n",
    "    stems = [str(case_id)]\n",
    "    if uid:\n",
    "        stems.append(str(uid))\n",
    "    pats = [f\"{case_id}.png\", f\"{case_id}__*.png\", f\"*{case_id}*.png\"]\n",
    "    for md in mask_dirs:\n",
    "        if md is None or (not md.exists()):\n",
    "            continue\n",
    "        for st in stems:\n",
    "            p = md / f\"{st}.png\"\n",
    "            if p.exists():\n",
    "                return str(p)\n",
    "        for pat in pats:\n",
    "            hits = list(md.glob(pat))\n",
    "            if hits:\n",
    "                hits = sorted(hits, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "                return str(hits[0])\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load manifests (NO FOLD)\n",
    "# ----------------------------\n",
    "train_pq = PROF_DIR / \"train_manifest.parquet\"\n",
    "test_pq  = PROF_DIR / \"test_manifest.parquet\"\n",
    "\n",
    "if not train_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing {train_pq}. Run STAGE 1 first.\")\n",
    "if not test_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing {test_pq}. Run STAGE 1 first.\")\n",
    "\n",
    "df_train = ensure_uid_case(pd.read_parquet(train_pq).copy())\n",
    "df_test  = ensure_uid_case(pd.read_parquet(test_pq).copy())\n",
    "\n",
    "if \"y\" not in df_train.columns:\n",
    "    raise ValueError(\"train_manifest must contain 'y'.\")\n",
    "if \"img_path\" not in df_train.columns or \"img_path\" not in df_test.columns:\n",
    "    raise ValueError(\"manifests must contain 'img_path'.\")\n",
    "\n",
    "df_train[\"y\"] = pd.to_numeric(df_train[\"y\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df_train[\"uid\"] = df_train[\"uid\"].astype(str)\n",
    "df_test[\"uid\"]  = df_test[\"uid\"].astype(str)\n",
    "\n",
    "# ensure mask_path exists\n",
    "if \"mask_path\" not in df_train.columns:\n",
    "    df_train[\"mask_path\"] = None\n",
    "need_mask = (df_train[\"y\"] == 1) & (df_train[\"mask_path\"].isna() | (df_train[\"mask_path\"].astype(str) == \"None\"))\n",
    "if need_mask.any():\n",
    "    df_train.loc[need_mask, \"mask_path\"] = df_train.loc[need_mask].apply(\n",
    "        lambda r: find_mask_path(int(r[\"case_id\"]), r[\"uid\"]), axis=1\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Load token + match manifests\n",
    "# ----------------------------\n",
    "tok_train_pq = TOKEN_ROOT / \"tokens_manifest_train.parquet\"\n",
    "tok_test_pq  = TOKEN_ROOT / \"tokens_manifest_test.parquet\"\n",
    "mat_train_pq = MATCH_ROOT / \"match_manifest_train.parquet\"\n",
    "mat_test_pq  = MATCH_ROOT / \"match_manifest_test.parquet\"\n",
    "\n",
    "if not tok_train_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing {tok_train_pq} (TOKEN_ROOT wrong?).\")\n",
    "if not mat_train_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing {mat_train_pq} (MATCH_ROOT wrong?).\")\n",
    "\n",
    "df_tok_tr = ensure_uid_case(pd.read_parquet(tok_train_pq).copy())\n",
    "df_tok_te = ensure_uid_case(pd.read_parquet(tok_test_pq).copy()) if tok_test_pq.exists() else pd.DataFrame(columns=[\"uid\",\"case_id\"])\n",
    "df_mat_tr = ensure_uid_case(pd.read_parquet(mat_train_pq).copy())\n",
    "df_mat_te = ensure_uid_case(pd.read_parquet(mat_test_pq).copy()) if mat_test_pq.exists() else pd.DataFrame(columns=[\"uid\",\"case_id\"])\n",
    "\n",
    "tok_path_col = _pick_path_col(df_tok_tr, [\"npz_path\",\"tok_npz\",\"path\",\"file\",\"npz\",\"token_npz\"])\n",
    "if tok_path_col is None:\n",
    "    raise ValueError(\"tokens_manifest_train.parquet has no recognizable npz path column.\")\n",
    "df_tok_tr[\"tok_npz\"] = df_tok_tr[tok_path_col]\n",
    "if len(df_tok_te):\n",
    "    tok_path_col_te = _pick_path_col(df_tok_te, [\"npz_path\",\"tok_npz\",\"path\",\"file\",\"npz\",\"token_npz\"])\n",
    "    df_tok_te[\"tok_npz\"] = df_tok_te[tok_path_col_te] if tok_path_col_te else None\n",
    "\n",
    "mat_path_col = _pick_path_col(df_mat_tr, [\"match_npz\",\"npz_path\",\"path\",\"file\",\"npz\"])\n",
    "if mat_path_col is None:\n",
    "    raise ValueError(\"match_manifest_train.parquet has no recognizable npz path column.\")\n",
    "df_mat_tr[\"match_npz\"] = df_mat_tr[mat_path_col]\n",
    "if len(df_mat_te):\n",
    "    mat_path_col_te = _pick_path_col(df_mat_te, [\"match_npz\",\"npz_path\",\"path\",\"file\",\"npz\"])\n",
    "    df_mat_te[\"match_npz\"] = df_mat_te[mat_path_col_te] if mat_path_col_te else None\n",
    "\n",
    "df_tok_tr[\"tok_npz\"] = df_tok_tr[\"tok_npz\"].apply(lambda x: resolve_any_path(x, TOKEN_ROOT, \"train\"))\n",
    "if len(df_tok_te):\n",
    "    df_tok_te[\"tok_npz\"] = df_tok_te[\"tok_npz\"].apply(lambda x: resolve_any_path(x, TOKEN_ROOT, \"test\"))\n",
    "\n",
    "df_mat_tr[\"match_npz\"] = df_mat_tr[\"match_npz\"].apply(lambda x: resolve_any_path(x, MATCH_ROOT, \"train\"))\n",
    "if len(df_mat_te):\n",
    "    df_mat_te[\"match_npz\"] = df_mat_te[\"match_npz\"].apply(lambda x: resolve_any_path(x, MATCH_ROOT, \"test\"))\n",
    "\n",
    "key_tok = [\"uid\"] if df_tok_tr[\"uid\"].notna().any() else [\"case_id\"]\n",
    "key_mat = [\"uid\"] if df_mat_tr[\"uid\"].notna().any() else [\"case_id\"]\n",
    "df_tok_tr = dedup_best(df_tok_tr, key_tok, \"tok_npz\")\n",
    "df_mat_tr = dedup_best(df_mat_tr, key_mat, \"match_npz\")\n",
    "if len(df_tok_te): df_tok_te = dedup_best(df_tok_te, key_tok, \"tok_npz\")\n",
    "if len(df_mat_te): df_mat_te = dedup_best(df_mat_te, key_mat, \"match_npz\")\n",
    "\n",
    "tok_tr_small = df_tok_tr[[\"uid\",\"case_id\",\"tok_npz\"]].copy()\n",
    "mat_tr_small = df_mat_tr[[\"uid\",\"case_id\",\"match_npz\"]].copy()\n",
    "tok_te_small = df_tok_te[[\"uid\",\"case_id\",\"tok_npz\"]].copy() if len(df_tok_te) else pd.DataFrame(columns=[\"uid\",\"case_id\",\"tok_npz\"])\n",
    "mat_te_small = df_mat_te[[\"uid\",\"case_id\",\"match_npz\"]].copy() if len(df_mat_te) else pd.DataFrame(columns=[\"uid\",\"case_id\",\"match_npz\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build joined train/test tables (KEEP ALL)\n",
    "# ----------------------------\n",
    "use_uid_join = (df_train[\"uid\"].notna().all() and tok_tr_small[\"uid\"].notna().all() and mat_tr_small[\"uid\"].notna().all())\n",
    "\n",
    "if use_uid_join:\n",
    "    DF_TRAIN_ALL = df_train.merge(tok_tr_small[[\"uid\",\"tok_npz\"]], on=\"uid\", how=\"left\") \\\n",
    "                           .merge(mat_tr_small[[\"uid\",\"match_npz\"]], on=\"uid\", how=\"left\")\n",
    "    DF_TEST_ALL  = df_test.merge(tok_te_small[[\"uid\",\"tok_npz\"]], on=\"uid\", how=\"left\") \\\n",
    "                          .merge(mat_te_small[[\"uid\",\"match_npz\"]], on=\"uid\", how=\"left\")\n",
    "else:\n",
    "    DF_TRAIN_ALL = df_train.merge(tok_tr_small[[\"case_id\",\"tok_npz\"]], on=\"case_id\", how=\"left\") \\\n",
    "                           .merge(mat_tr_small[[\"case_id\",\"match_npz\"]], on=\"case_id\", how=\"left\")\n",
    "    DF_TEST_ALL  = df_test.merge(tok_te_small[[\"case_id\",\"tok_npz\"]], on=\"case_id\", how=\"left\") \\\n",
    "                          .merge(mat_te_small[[\"case_id\",\"match_npz\"]], on=\"case_id\", how=\"left\")\n",
    "\n",
    "DF_TRAIN_ALL[\"tok_exists\"]   = DF_TRAIN_ALL[\"tok_npz\"].apply(lambda x: isinstance(x, str) and Path(x).exists())\n",
    "DF_TRAIN_ALL[\"match_exists\"] = DF_TRAIN_ALL[\"match_npz\"].apply(lambda x: isinstance(x, str) and Path(x).exists())\n",
    "DF_TEST_ALL[\"tok_exists\"]    = DF_TEST_ALL[\"tok_npz\"].apply(lambda x: isinstance(x, str) and Path(x).exists())\n",
    "DF_TEST_ALL[\"match_exists\"]  = DF_TEST_ALL[\"match_npz\"].apply(lambda x: isinstance(x, str) and Path(x).exists())\n",
    "\n",
    "print(\"RUN_DIR     :\", RUN_DIR)\n",
    "print(\"CFG_ID      :\", CFG_ID)\n",
    "print(\"TOKEN_ROOT  :\", TOKEN_ROOT)\n",
    "print(\"MATCH_ROOT  :\", MATCH_ROOT)\n",
    "print(\"-\"*60)\n",
    "print(\"Train rows  :\", len(DF_TRAIN_ALL),\n",
    "      \"| tok_exists:\", int(DF_TRAIN_ALL[\"tok_exists\"].sum()),\n",
    "      \"| match_exists:\", int(DF_TRAIN_ALL[\"match_exists\"].sum()),\n",
    "      \"| forged%:\", float(DF_TRAIN_ALL[\"y\"].mean())*100)\n",
    "print(\"Test rows   :\", len(DF_TEST_ALL),\n",
    "      \"| tok_exists:\", int(DF_TEST_ALL[\"tok_exists\"].sum()),\n",
    "      \"| match_exists:\", int(DF_TEST_ALL[\"match_exists\"].sum()))\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Infer token grid size (HTOK, WTOK)\n",
    "# ----------------------------\n",
    "HTOK = WTOK = None\n",
    "cfg_tok_json = TOKEN_ROOT / \"cfg.json\"\n",
    "\n",
    "def _infer_hw_from_npz(npz_path: Path):\n",
    "    z = np.load(npz_path)\n",
    "    keys = list(z.keys())\n",
    "    pref = [\"tok\",\"tokens\",\"x\",\"feat\",\"grid\",\"emb\",\"embedding\"]\n",
    "    for k in pref:\n",
    "        if k in keys:\n",
    "            a = z[k]\n",
    "            if a.ndim == 3:\n",
    "                return int(a.shape[0]), int(a.shape[1])\n",
    "            if a.ndim == 2:\n",
    "                n = int(a.shape[0])\n",
    "                s = int(np.sqrt(n))\n",
    "                if s*s == n:\n",
    "                    return s, s\n",
    "    for k in keys:\n",
    "        a = z[k]\n",
    "        if getattr(a, \"ndim\", 0) == 3:\n",
    "            return int(a.shape[0]), int(a.shape[1])\n",
    "    for k in keys:\n",
    "        a = z[k]\n",
    "        if getattr(a, \"ndim\", 0) == 2:\n",
    "            n = int(a.shape[0])\n",
    "            s = int(np.sqrt(n))\n",
    "            if s*s == n:\n",
    "                return s, s\n",
    "    return None, None\n",
    "\n",
    "if cfg_tok_json.exists():\n",
    "    try:\n",
    "        tok_cfg = json.loads(cfg_tok_json.read_text())\n",
    "        for kH, kW in [(\"HTOK\",\"WTOK\"), (\"htok\",\"wtok\"), (\"Htok\",\"Wtok\")]:\n",
    "            if kH in tok_cfg and kW in tok_cfg:\n",
    "                HTOK = int(tok_cfg[kH]); WTOK = int(tok_cfg[kW])\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if HTOK is None or WTOK is None:\n",
    "    for a,b in [(\"htok\",\"wtok\"),(\"Htok\",\"Wtok\"),(\"HTOK\",\"WTOK\")]:\n",
    "        if a in df_tok_tr.columns and b in df_tok_tr.columns and df_tok_tr[a].notna().any():\n",
    "            HTOK = int(df_tok_tr[a].dropna().iloc[0])\n",
    "            WTOK = int(df_tok_tr[b].dropna().iloc[0])\n",
    "            break\n",
    "\n",
    "if HTOK is None or WTOK is None:\n",
    "    sample = DF_TRAIN_ALL.loc[DF_TRAIN_ALL[\"tok_exists\"], \"tok_npz\"]\n",
    "    if len(sample) == 0:\n",
    "        files = sorted((TOKEN_ROOT / \"train\").glob(\"*.npz\"))\n",
    "        if not files:\n",
    "            raise RuntimeError(\"Cannot infer HTOK/WTOK: no token npz found.\")\n",
    "        sp = files[0]\n",
    "    else:\n",
    "        sp = Path(sample.iloc[0])\n",
    "    HTOK, WTOK = _infer_hw_from_npz(sp)\n",
    "\n",
    "if HTOK is None or WTOK is None:\n",
    "    raise RuntimeError(\"Failed to infer HTOK/WTOK from cfg/manifest/npz.\")\n",
    "\n",
    "print(\"Token grid  :\", (HTOK, WTOK))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Precompute GT token masks (TRAIN only)\n",
    "# NOTE: file name uses uid to avoid collision if case_id duplicates exist\n",
    "# ----------------------------\n",
    "GT_TOK_DIR = RUN_DIR / \"seg\" / f\"gt_tok_masks_{CFG_ID}\"\n",
    "GT_TOK_TRAIN = GT_TOK_DIR / \"train\"\n",
    "GT_TOK_TRAIN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_gt_union_fullres(case_id: int, mask_path: str = None):\n",
    "    if mask_path and isinstance(mask_path, str):\n",
    "        p = Path(mask_path)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                im = Image.open(p).convert(\"L\")\n",
    "                return (np.asarray(im) > 0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n",
    "        if d is None or (not d.exists()):\n",
    "            continue\n",
    "        npy = d / f\"{int(case_id)}.npy\"\n",
    "        if npy.exists():\n",
    "            a = np.asarray(np.load(npy, mmap_mode=\"r\"))\n",
    "            if a.ndim == 2:\n",
    "                return (a > 0)\n",
    "            if a.ndim == 3:\n",
    "                return (a > 0).any(axis=0)\n",
    "\n",
    "    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n",
    "    pats = [f\"{int(case_id)}*.png\", f\"{int(case_id)}*.jpg\", f\"{int(case_id)}*.jpeg\",\n",
    "            f\"{int(case_id)}*.tif\", f\"{int(case_id)}*.tiff\", f\"{int(case_id)}*.bmp\"]\n",
    "    files = []\n",
    "    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n",
    "        if d is None or (not d.exists()):\n",
    "            continue\n",
    "        for pat in pats:\n",
    "            files += list(d.glob(pat))\n",
    "    files = [p for p in files if p.suffix.lower() in exts]\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    m = None\n",
    "    for p in files:\n",
    "        try:\n",
    "            im = Image.open(p).convert(\"L\")\n",
    "            a = (np.asarray(im) > 0)\n",
    "            m = a if m is None else (m | a)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return m\n",
    "\n",
    "def downsample_to_tok(mask_bool, htok, wtok):\n",
    "    if mask_bool is None:\n",
    "        return np.zeros((htok, wtok), dtype=np.uint8)\n",
    "    im = Image.fromarray((mask_bool.astype(np.uint8) * 255))\n",
    "    im = im.resize((wtok, htok), resample=Image.NEAREST)\n",
    "    return (np.asarray(im) > 127).astype(np.uint8)\n",
    "\n",
    "gt_tok_paths = []\n",
    "gt_area_fracs = []\n",
    "rebuilt = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for i, r in DF_TRAIN_ALL.iterrows():\n",
    "    uid = str(r[\"uid\"])\n",
    "    cid = int(r[\"case_id\"])\n",
    "    y   = int(r[\"y\"])\n",
    "    outp = GT_TOK_TRAIN / f\"{uid}.npz\"\n",
    "\n",
    "    if outp.exists():\n",
    "        gt_tok_paths.append(str(outp))\n",
    "        try:\n",
    "            a = np.load(outp)[\"m\"]\n",
    "            gt_area_fracs.append(float(a.mean()))\n",
    "        except Exception:\n",
    "            gt_area_fracs.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    if y == 0:\n",
    "        m_tok = np.zeros((HTOK, WTOK), dtype=np.uint8)\n",
    "    else:\n",
    "        gt_full = load_gt_union_fullres(cid, r.get(\"mask_path\", None))\n",
    "        m_tok = downsample_to_tok(gt_full, HTOK, WTOK)\n",
    "\n",
    "    np.savez_compressed(outp, m=m_tok)\n",
    "    gt_tok_paths.append(str(outp))\n",
    "    gt_area_fracs.append(float(m_tok.mean()))\n",
    "    rebuilt += 1\n",
    "\n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"[gt_tok] {i+1}/{len(DF_TRAIN_ALL)} | rebuilt={rebuilt} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "DF_TRAIN_ALL[\"gt_tok_npz\"] = gt_tok_paths\n",
    "DF_TRAIN_ALL[\"gt_area_frac_tok\"] = gt_area_fracs\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save tables\n",
    "# ----------------------------\n",
    "train_out = RUN_DIR / \"features\" / \"train_table.parquet\"\n",
    "test_out  = RUN_DIR / \"features\" / \"test_table.parquet\"\n",
    "DF_TRAIN_ALL.to_parquet(train_out, index=False)\n",
    "DF_TEST_ALL.to_parquet(test_out, index=False)\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(\"Saved:\", train_out)\n",
    "print(\"Saved:\", test_out)\n",
    "print(\"GT_TOK_DIR:\", GT_TOK_DIR)\n",
    "print(\"GT rebuilt:\", rebuilt, \"/\", len(DF_TRAIN_ALL), \"| time:\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "# Export globals\n",
    "DF_TRAIN_ALL = DF_TRAIN_ALL\n",
    "DF_TEST_ALL  = DF_TEST_ALL\n",
    "GT_TOK_DIR   = GT_TOK_DIR\n",
    "HTOK, WTOK   = HTOK, WTOK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c63261",
   "metadata": {
    "papermill": {
     "duration": 0.007659,
     "end_time": "2026-01-13T05:59:07.309839",
     "exception": false,
     "start_time": "2026-01-13T05:59:07.302180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build & Export Test Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ac5074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T05:59:07.327808Z",
     "iopub.status.busy": "2026-01-13T05:59:07.327427Z",
     "iopub.status.idle": "2026-01-13T05:59:31.294231Z",
     "shell.execute_reply": "2026-01-13T05:59:31.293081Z"
    },
    "papermill": {
     "duration": 23.978817,
     "end_time": "2026-01-13T05:59:31.296596",
     "exception": false,
     "start_time": "2026-01-13T05:59:07.317779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROF_DIR        : /kaggle/working/recodai_luc_prof\n",
      "pred_feat_test  : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_ens/pred_features_test.csv\n",
      "pred_feat_train : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_ens/pred_features_train.csv\n",
      "Created FEATURE_COLS: 8 -> /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      "Inferred from: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_ens/pred_features_train.csv\n",
      "Saved:\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/test_table.parquet\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/test_X.npy\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/test_case_id.npy\n",
      "------------------------------------------------------------\n",
      "Test rows: 1 | missing_before: 0 | missing_after: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE — Build & Export Test Feature Table (ONE CELL) — FIX: AUTO feature_cols.json\n",
    "# - If feature_cols.json missing, infer from pred_features_{train,test}.csv numeric columns\n",
    "# - Auto-pick PROF_DIR + pred_features_test.csv from /kaggle/working or /kaggle/input\n",
    "# - Align to sample_submission order\n",
    "# Outputs:\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/test_table.parquet\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/test_X.npy\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/test_case_id.npy\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json   (auto if missing)\n",
    "# ============================================================\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "WORK = Path(\"/kaggle/working\")\n",
    "INP  = Path(\"/kaggle/input\")\n",
    "\n",
    "ART_DIR = WORK / \"recodai_luc_gate_artifacts\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Find competition root + sample_submission\n",
    "# ----------------------------\n",
    "def find_comp_root():\n",
    "    cand = INP / \"recodai-luc-scientific-image-forgery-detection\"\n",
    "    if cand.exists() and (cand / \"sample_submission.csv\").exists():\n",
    "        return cand\n",
    "    for d in sorted(INP.glob(\"*\")):\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if (d / \"sample_submission.csv\").exists() and (d / \"train_images\").exists() and (d / \"test_images\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Cannot find competition dataset under /kaggle/input (need sample_submission.csv + train_images + test_images).\")\n",
    "\n",
    "COMP_ROOT  = find_comp_root()\n",
    "SAMPLE_SUB = COMP_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Find PROF_DIR (working first, else input)\n",
    "# ----------------------------\n",
    "def pick_prof_dir():\n",
    "    w = WORK / \"recodai_luc_prof\"\n",
    "    if (w / \"test_manifest.parquet\").exists():\n",
    "        return w\n",
    "    # try common input layouts\n",
    "    cands = []\n",
    "    for p in INP.glob(\"*/recodai_luc_prof\"):\n",
    "        if (p / \"test_manifest.parquet\").exists():\n",
    "            cands.append(p)\n",
    "    for p in INP.glob(\"*/*/recodai_luc_prof\"):\n",
    "        if (p / \"test_manifest.parquet\").exists():\n",
    "            cands.append(p)\n",
    "    if cands:\n",
    "        cands = sorted(cands, key=lambda x: (x / \"test_manifest.parquet\").stat().st_mtime, reverse=True)\n",
    "        return cands[0]\n",
    "    # last resort\n",
    "    for pj in INP.rglob(\"recodai_luc_prof/test_manifest.parquet\"):\n",
    "        return pj.parent\n",
    "    raise FileNotFoundError(\"Cannot find recodai_luc_prof/test_manifest.parquet in working or input.\")\n",
    "\n",
    "PROF_DIR = pick_prof_dir()\n",
    "test_manifest_pq = PROF_DIR / \"test_manifest.parquet\"\n",
    "if not test_manifest_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing {test_manifest_pq}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Find pred_features_{test,train}.csv (auto)\n",
    "# ----------------------------\n",
    "def newest_glob(base: Path, pattern: str):\n",
    "    hits = list(base.glob(pattern))\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits = sorted(hits, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return hits[0]\n",
    "\n",
    "def find_pred_features(name=\"test\"):\n",
    "    # prefer working cache first\n",
    "    w_cands = [\n",
    "        newest_glob(WORK, f\"recodai_luc/cache/pred_ens/pred_features_{name}.csv\"),\n",
    "        newest_glob(WORK, f\"recodai_luc/cache/**/pred_features_{name}.csv\"),\n",
    "        newest_glob(WORK, f\"**/pred_features_{name}.csv\"),\n",
    "    ]\n",
    "    for c in w_cands:\n",
    "        if c is not None and c.exists():\n",
    "            return c\n",
    "\n",
    "    # input datasets (your screenshot: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_ens/...)\n",
    "    i_hits = list(INP.glob(f\"**/pred_features_{name}.csv\"))\n",
    "    if i_hits:\n",
    "        i_hits = sorted(i_hits, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        return i_hits[0]\n",
    "    return None\n",
    "\n",
    "pred_feat_test = find_pred_features(\"test\")\n",
    "pred_feat_train = find_pred_features(\"train\")\n",
    "\n",
    "if pred_feat_test is None or (not pred_feat_test.exists()):\n",
    "    raise FileNotFoundError(\"pred_features_test.csv not found in working or input.\")\n",
    "\n",
    "print(\"PROF_DIR        :\", PROF_DIR)\n",
    "print(\"pred_feat_test  :\", pred_feat_test)\n",
    "print(\"pred_feat_train :\", pred_feat_train if pred_feat_train else \"None (ok)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load/Build FEATURE_COLS\n",
    "# ----------------------------\n",
    "cols_path = ART_DIR / \"feature_cols.json\"\n",
    "\n",
    "def ensure_case_id_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"case_id\" in df.columns:\n",
    "        df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n",
    "    else:\n",
    "        for c in [\"uid\", \"id\", \"uid_safe\", \"npz_path\"]:\n",
    "            if c in df.columns:\n",
    "                src = df[c].astype(str)\n",
    "                df[\"case_id\"] = pd.to_numeric(src.str.extract(r\"(\\d+)\")[0], errors=\"coerce\")\n",
    "                break\n",
    "    if \"case_id\" not in df.columns:\n",
    "        raise ValueError(\"Cannot infer case_id from pred_features csv.\")\n",
    "    df = df[df[\"case_id\"].notna()].copy()\n",
    "    df[\"case_id\"] = df[\"case_id\"].astype(int)\n",
    "    return df\n",
    "\n",
    "DROP_KEYS = {\n",
    "    \"case_id\",\"uid\",\"uid_safe\",\"id\",\"y\",\"fold\",\"split\",\"img_path\",\"mask_path\",\n",
    "    \"tok_npz\",\"match_npz\",\"npz_path\",\"path\",\"file\"\n",
    "}\n",
    "\n",
    "def infer_feature_cols(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df = ensure_case_id_col(df)\n",
    "    # candidate cols excluding ids/meta\n",
    "    cand = [c for c in df.columns if c not in DROP_KEYS]\n",
    "    if not cand:\n",
    "        return []\n",
    "    # coerce numeric & keep numeric-ish\n",
    "    num_cols = []\n",
    "    for c in cand:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.notna().any():\n",
    "            num_cols.append(c)\n",
    "    return sorted(num_cols)\n",
    "\n",
    "if cols_path.exists():\n",
    "    FEATURE_COLS = json.loads(cols_path.read_text())\n",
    "    print(\"Loaded FEATURE_COLS:\", len(FEATURE_COLS), \"from\", cols_path)\n",
    "else:\n",
    "    # infer from train if available (preferred), else from test\n",
    "    if pred_feat_train and pred_feat_train.exists():\n",
    "        df_tmp = pd.read_csv(pred_feat_train)\n",
    "        FEATURE_COLS = infer_feature_cols(df_tmp)\n",
    "        src_used = str(pred_feat_train)\n",
    "    else:\n",
    "        df_tmp = pd.read_csv(pred_feat_test)\n",
    "        FEATURE_COLS = infer_feature_cols(df_tmp)\n",
    "        src_used = str(pred_feat_test)\n",
    "\n",
    "    if len(FEATURE_COLS) == 0:\n",
    "        raise RuntimeError(\"Failed to infer FEATURE_COLS (no usable numeric columns).\")\n",
    "\n",
    "    cols_path.write_text(json.dumps(FEATURE_COLS, indent=2))\n",
    "    print(\"Created FEATURE_COLS:\", len(FEATURE_COLS), \"->\", cols_path)\n",
    "    print(\"Inferred from:\", src_used)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Build ordered test_meta (sample_submission order)\n",
    "# ----------------------------\n",
    "df_sub = pd.read_csv(SAMPLE_SUB)\n",
    "id_candidates = [c for c in [\"case_id\", \"id\", \"uid\"] if c in df_sub.columns]\n",
    "id_col = id_candidates[0] if id_candidates else df_sub.columns[0]\n",
    "sub_ids = pd.to_numeric(df_sub[id_col], errors=\"coerce\").dropna().astype(int).tolist()\n",
    "if len(sub_ids) == 0:\n",
    "    raise RuntimeError(\"No numeric ids found in sample_submission.\")\n",
    "\n",
    "df_test_meta = pd.read_parquet(test_manifest_pq).copy()\n",
    "if \"case_id\" not in df_test_meta.columns:\n",
    "    raise ValueError(\"test_manifest.parquet must have case_id column.\")\n",
    "df_test_meta[\"case_id\"] = pd.to_numeric(df_test_meta[\"case_id\"], errors=\"coerce\")\n",
    "df_test_meta = df_test_meta[df_test_meta[\"case_id\"].notna()].copy()\n",
    "df_test_meta[\"case_id\"] = df_test_meta[\"case_id\"].astype(int)\n",
    "\n",
    "base = pd.DataFrame({\"case_id\": sub_ids})\n",
    "df_test_meta = base.merge(df_test_meta, on=\"case_id\", how=\"left\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Load pred features + dedup\n",
    "# ----------------------------\n",
    "df_feat = pd.read_csv(pred_feat_test)\n",
    "df_feat = ensure_case_id_col(df_feat)\n",
    "\n",
    "keep_cols = [\"case_id\"] + [c for c in FEATURE_COLS if c in df_feat.columns]\n",
    "df_feat = df_feat[keep_cols].copy()\n",
    "\n",
    "sort_keys = []\n",
    "for k in [\"best_peak_score\", \"area_frac\", \"area_frac_tok\"]:\n",
    "    if k in df_feat.columns:\n",
    "        sort_keys.append(k)\n",
    "\n",
    "if sort_keys:\n",
    "    df_feat = df_feat.sort_values(sort_keys, ascending=False).drop_duplicates(\"case_id\", keep=\"first\")\n",
    "else:\n",
    "    df_feat = df_feat.drop_duplicates(\"case_id\", keep=\"first\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Join + fill\n",
    "# ----------------------------\n",
    "df_test_tabular = df_test_meta.merge(df_feat, on=\"case_id\", how=\"left\")\n",
    "\n",
    "for c in FEATURE_COLS:\n",
    "    if c not in df_test_tabular.columns:\n",
    "        df_test_tabular[c] = np.nan\n",
    "    df_test_tabular[c] = pd.to_numeric(df_test_tabular[c], errors=\"coerce\")\n",
    "\n",
    "fill_value = 0.0\n",
    "n_missing_before = int(df_test_tabular[FEATURE_COLS].isna().sum().sum())\n",
    "df_test_tabular[FEATURE_COLS] = df_test_tabular[FEATURE_COLS].fillna(fill_value)\n",
    "n_missing_after = int(df_test_tabular[FEATURE_COLS].isna().sum().sum())\n",
    "\n",
    "# arrays\n",
    "X_test = df_test_tabular[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "case_id_test = df_test_tabular[\"case_id\"].to_numpy(dtype=np.int64, copy=True)\n",
    "\n",
    "# save\n",
    "df_test_tabular.to_parquet(ART_DIR / \"test_table.parquet\", index=False)\n",
    "np.save(ART_DIR / \"test_X.npy\", X_test)\n",
    "np.save(ART_DIR / \"test_case_id.npy\", case_id_test)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", ART_DIR / \"feature_cols.json\")\n",
    "print(\" -\", ART_DIR / \"test_table.parquet\")\n",
    "print(\" -\", ART_DIR / \"test_X.npy\")\n",
    "print(\" -\", ART_DIR / \"test_case_id.npy\")\n",
    "print(\"-\"*60)\n",
    "print(\"Test rows:\", len(df_test_tabular), \"| missing_before:\", n_missing_before, \"| missing_after:\", n_missing_after)\n",
    "\n",
    "# Export globals\n",
    "FEATURE_COLS = FEATURE_COLS\n",
    "X_test = X_test\n",
    "case_id_test = case_id_test\n",
    "df_test_tabular = df_test_tabular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85105b8a",
   "metadata": {
    "papermill": {
     "duration": 0.007743,
     "end_time": "2026-01-13T05:59:31.312397",
     "exception": false,
     "start_time": "2026-01-13T05:59:31.304654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Baseline Model (Leakage-Safe CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cdbd17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T05:59:31.330898Z",
     "iopub.status.busy": "2026-01-13T05:59:31.330532Z",
     "iopub.status.idle": "2026-01-13T06:04:04.686436Z",
     "shell.execute_reply": "2026-01-13T06:04:04.685385Z"
    },
    "papermill": {
     "duration": 273.37581,
     "end_time": "2026-01-13T06:04:04.696177",
     "exception": false,
     "start_time": "2026-01-13T05:59:31.320367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP_ROOT     : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n",
      "PROF_DIR      : /kaggle/working/recodai_luc_prof\n",
      "pred_features : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_ens/pred_features_train.csv\n",
      "PRED_TRAIN_DIR: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/pred_ens/train\n",
      "GT mask dirs  : /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks | /kaggle/input/recodai-luc-scientific-image-forgery-detection/supplemental_masks\n",
      "--------------------------------------------------------------------------------\n",
      "FEATURE_COLS: 8 | saved: /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      "Train rows: 2795 | pos_rate: 1.0\n",
      "--------------------------------------------------------------------------------\n",
      "Model: LightGBM\n",
      "Seeds: [42, 202, 777] | n_splits: 5\n",
      "--------------------------------------------------------------------------------\n",
      "[seed 42 split 0] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 42 split 1] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 42 split 2] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 42 split 3] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 42 split 4] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 202 split 0] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 202 split 1] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 202 split 2] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 202 split 3] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 202 split 4] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 777 split 0] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 777 split 1] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 777 split 2] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 777 split 3] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "[seed 777 split 4] auc=nan logloss=0.0000 f1@0.5=1.0000 iter=1\n",
      "--------------------------------------------------------------------------------\n",
      "OOF raw (ensemble): auc=nan logloss=0.0000 f1@0.5=1.0000 prec=1.0000 rec=1.0000\n",
      "best_iter_mean: 1 | best_iter_median: 1\n",
      "--------------------------------------------------------------------------------\n",
      "OOF calibrated: auc=nan logloss=0.0000 f1@0.5=1.0000 | calibration={'enabled': True, 'method': 'isotonic', 'path': '/kaggle/working/recodai_luc_gate_artifacts/models/calibration_isotonic.joblib'}\n",
      "--------------------------------------------------------------------------------\n",
      "[dice-proxy] 500/2795 | miss_gt=0 miss_pr=0 bad_shape=0 | 46.9s\n",
      "[dice-proxy] 1000/2795 | miss_gt=0 miss_pr=0 bad_shape=0 | 86.9s\n",
      "[dice-proxy] 1500/2795 | miss_gt=0 miss_pr=0 bad_shape=0 | 129.8s\n",
      "[dice-proxy] 2000/2795 | miss_gt=0 miss_pr=0 bad_shape=0 | 178.8s\n",
      "[dice-proxy] 2500/2795 | miss_gt=0 miss_pr=0 bad_shape=0 | 223.4s\n",
      "--------------------------------------------------------------------------------\n",
      "Dice-proxy ready | miss_gt=0 miss_pr=0 bad_shape=0 | 247.6s\n",
      "--------------------------------------------------------------------------------\n",
      "BEST (dice-proxy): thr = 0.0 | score = 0.029517006129026413\n",
      "--------------------------------------------------------------------------------\n",
      "Saved:\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/train_table.parquet\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/oof/oof_prob.csv\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/eval/threshold_table.csv\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/eval/best_threshold.json\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/eval/cv_metrics.json\n",
      " - /kaggle/working/recodai_luc_gate_artifacts/final_gate_model.pt\n",
      "Final models: 3 | dir: /kaggle/working/recodai_luc_gate_artifacts/models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE — Train Gate Model (NO-FOLD, Ensemble Multi-Seed, CV OOF, Calibrate, Dice-Proxy Thr)\n",
    "# REVISI FULL (sesuai pipeline: DINO dense + matching + seg decoder + gate + ensemble)\n",
    "#\n",
    "# Input minimal:\n",
    "# - /kaggle/input/.../recodai_luc/cache/pred_ens/pred_features_train.csv  (atau di /kaggle/working)\n",
    "# - pred masks: pred_ens/train/{case_id}.npz  (key: mask/m/pred/...)\n",
    "# - GT masks: train_masks + supplemental_masks\n",
    "# - train_manifest.parquet (recodai_luc_prof) untuk y & case_id\n",
    "#\n",
    "# Output:\n",
    "# - /kaggle/working/recodai_luc_gate_artifacts/feature_cols.json\n",
    "# - models/ (final per-seed models + calibrator)\n",
    "# - oof/oof_prob*.npy + oof_prob.csv\n",
    "# - eval/threshold_table.csv + best_threshold.json + cv_metrics.json\n",
    "# - final_gate_model.pt  (portable)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "WORK = Path(\"/kaggle/working\")\n",
    "INP  = Path(\"/kaggle/input\")\n",
    "\n",
    "ART_DIR   = WORK / \"recodai_luc_gate_artifacts\"\n",
    "MODEL_DIR = ART_DIR / \"models\"\n",
    "OOF_DIR   = ART_DIR / \"oof\"\n",
    "EVAL_DIR  = ART_DIR / \"eval\"\n",
    "for d in [ART_DIR, MODEL_DIR, OOF_DIR, EVAL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cols_path = ART_DIR / \"feature_cols.json\"\n",
    "cfg_path  = ART_DIR / \"train_cfg.json\"  # optional (kalau ada dipakai), kalau tidak -> auto default\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: find competition root + prof dir + pred_features\n",
    "# ----------------------------\n",
    "def find_comp_root():\n",
    "    cand = INP / \"recodai-luc-scientific-image-forgery-detection\"\n",
    "    if cand.exists() and (cand / \"sample_submission.csv\").exists():\n",
    "        return cand\n",
    "    for d in sorted(INP.glob(\"*\")):\n",
    "        if d.is_dir() and (d / \"sample_submission.csv\").exists() and (d / \"train_images\").exists() and (d / \"test_images\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Cannot find competition dataset under /kaggle/input.\")\n",
    "\n",
    "def pick_prof_dir():\n",
    "    w = WORK / \"recodai_luc_prof\"\n",
    "    if (w / \"train_manifest.parquet\").exists():\n",
    "        return w\n",
    "    cands = []\n",
    "    for p in INP.glob(\"*/recodai_luc_prof\"):\n",
    "        if (p / \"train_manifest.parquet\").exists():\n",
    "            cands.append(p)\n",
    "    for p in INP.glob(\"*/*/recodai_luc_prof\"):\n",
    "        if (p / \"train_manifest.parquet\").exists():\n",
    "            cands.append(p)\n",
    "    if cands:\n",
    "        cands = sorted(cands, key=lambda x: (x / \"train_manifest.parquet\").stat().st_mtime, reverse=True)\n",
    "        return cands[0]\n",
    "    for pj in INP.rglob(\"recodai_luc_prof/train_manifest.parquet\"):\n",
    "        return pj.parent\n",
    "    raise FileNotFoundError(\"Cannot find recodai_luc_prof/train_manifest.parquet in working or input.\")\n",
    "\n",
    "def newest_glob(base: Path, pattern: str):\n",
    "    hits = list(base.glob(pattern))\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits = sorted(hits, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return hits[0]\n",
    "\n",
    "def find_pred_features_train():\n",
    "    # working first\n",
    "    cands = [\n",
    "        newest_glob(WORK, \"recodai_luc/cache/pred_ens/pred_features_train.csv\"),\n",
    "        newest_glob(WORK, \"recodai_luc/cache/**/pred_features_train.csv\"),\n",
    "        newest_glob(WORK, \"**/pred_features_train.csv\"),\n",
    "    ]\n",
    "    for c in cands:\n",
    "        if c is not None and c.exists():\n",
    "            return c\n",
    "\n",
    "    hits = list(INP.glob(\"**/pred_features_train.csv\"))\n",
    "    if hits:\n",
    "        hits = sorted(hits, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        return hits[0]\n",
    "    return None\n",
    "\n",
    "def find_pred_mask_train_dir():\n",
    "    # try common\n",
    "    cands = [\n",
    "        WORK / \"recodai_luc/cache/pred_ens/train\",\n",
    "    ]\n",
    "    for c in cands:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    # input\n",
    "    hits = list(INP.glob(\"**/pred_ens/train\"))\n",
    "    hits = [h for h in hits if h.is_dir()]\n",
    "    if hits:\n",
    "        hits = sorted(hits, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        return hits[0]\n",
    "    return None\n",
    "\n",
    "def ensure_case_id_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"case_id\" in df.columns:\n",
    "        df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n",
    "    else:\n",
    "        for c in [\"uid\", \"id\", \"uid_safe\", \"npz_path\", \"path\", \"file\"]:\n",
    "            if c in df.columns:\n",
    "                src = df[c].astype(str)\n",
    "                df[\"case_id\"] = pd.to_numeric(src.str.extract(r\"(\\d+)\")[0], errors=\"coerce\")\n",
    "                break\n",
    "    if \"case_id\" not in df.columns:\n",
    "        raise ValueError(\"Cannot infer case_id.\")\n",
    "    df = df[df[\"case_id\"].notna()].copy()\n",
    "    df[\"case_id\"] = df[\"case_id\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Load CFG (optional)\n",
    "# ----------------------------\n",
    "CFG = {}\n",
    "if cfg_path.exists():\n",
    "    try:\n",
    "        CFG = json.loads(cfg_path.read_text())\n",
    "    except Exception:\n",
    "        CFG = {}\n",
    "\n",
    "COMP_ROOT = Path(CFG.get(\"paths\", {}).get(\"COMP_ROOT\", str(find_comp_root())))\n",
    "PROF_DIR  = Path(CFG.get(\"paths\", {}).get(\"PROF_DIR\", str(pick_prof_dir())))\n",
    "\n",
    "TRAIN_MASK_DIR = Path(CFG.get(\"paths\", {}).get(\"TRAIN_MASK_DIR\", str(COMP_ROOT / \"train_masks\")))\n",
    "SUP_MASK_DIR   = Path(CFG.get(\"paths\", {}).get(\"SUP_MASK_DIR\",   str(COMP_ROOT / \"supplemental_masks\")))\n",
    "\n",
    "pred_feat_train = Path(CFG.get(\"paths\", {}).get(\"PRED_FEATURES_TRAIN\", \"\")) if CFG.get(\"paths\", {}).get(\"PRED_FEATURES_TRAIN\") else None\n",
    "if pred_feat_train is None or (not pred_feat_train.exists()):\n",
    "    pred_feat_train = find_pred_features_train()\n",
    "if pred_feat_train is None or (not Path(pred_feat_train).exists()):\n",
    "    raise FileNotFoundError(\"pred_features_train.csv not found (working/input).\")\n",
    "\n",
    "PRED_TRAIN_DIR = Path(CFG.get(\"paths\", {}).get(\"PRED_TRAIN_DIR\", \"\")) if CFG.get(\"paths\", {}).get(\"PRED_TRAIN_DIR\") else None\n",
    "if PRED_TRAIN_DIR is None or (not PRED_TRAIN_DIR.exists()):\n",
    "    PRED_TRAIN_DIR = find_pred_mask_train_dir()\n",
    "if PRED_TRAIN_DIR is None or (not PRED_TRAIN_DIR.exists()):\n",
    "    raise FileNotFoundError(\"pred_ens/train directory not found (needs {case_id}.npz masks).\")\n",
    "\n",
    "print(\"COMP_ROOT     :\", COMP_ROOT)\n",
    "print(\"PROF_DIR      :\", PROF_DIR)\n",
    "print(\"pred_features :\", pred_feat_train)\n",
    "print(\"PRED_TRAIN_DIR:\", PRED_TRAIN_DIR)\n",
    "print(\"GT mask dirs  :\", TRAIN_MASK_DIR, \"|\", SUP_MASK_DIR)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ----------------------------\n",
    "# Build training table for gate: train_manifest + pred_features_train\n",
    "# ----------------------------\n",
    "train_manifest_pq = PROF_DIR / \"train_manifest.parquet\"\n",
    "if not train_manifest_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing {train_manifest_pq}\")\n",
    "\n",
    "df_man = pd.read_parquet(train_manifest_pq).copy()\n",
    "df_man = ensure_case_id_col(df_man)\n",
    "if \"y\" not in df_man.columns:\n",
    "    raise ValueError(\"train_manifest.parquet must contain y.\")\n",
    "df_man[\"y\"] = pd.to_numeric(df_man[\"y\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df_man = df_man.drop_duplicates(\"case_id\", keep=\"first\")[[\"case_id\", \"y\"]].copy()\n",
    "\n",
    "df_feat = pd.read_csv(pred_feat_train)\n",
    "df_feat = ensure_case_id_col(df_feat)\n",
    "\n",
    "# ----------------------------\n",
    "# FEATURE_COLS: load or infer from df_feat numeric columns\n",
    "# ----------------------------\n",
    "DROP_KEYS = {\n",
    "    \"case_id\",\"uid\",\"uid_safe\",\"id\",\"y\",\"fold\",\"split\",\"img_path\",\"mask_path\",\n",
    "    \"tok_npz\",\"match_npz\",\"npz_path\",\"path\",\"file\"\n",
    "}\n",
    "\n",
    "def infer_feature_cols_from_feat(df: pd.DataFrame):\n",
    "    cand = [c for c in df.columns if c not in DROP_KEYS]\n",
    "    num_cols = []\n",
    "    for c in cand:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.notna().any():\n",
    "            num_cols.append(c)\n",
    "    return sorted(num_cols)\n",
    "\n",
    "if cols_path.exists():\n",
    "    FEATURE_COLS = json.loads(cols_path.read_text())\n",
    "else:\n",
    "    FEATURE_COLS = infer_feature_cols_from_feat(df_feat)\n",
    "    if len(FEATURE_COLS) == 0:\n",
    "        raise RuntimeError(\"Failed to infer FEATURE_COLS from pred_features_train.csv.\")\n",
    "    cols_path.write_text(json.dumps(FEATURE_COLS, indent=2))\n",
    "\n",
    "print(\"FEATURE_COLS:\", len(FEATURE_COLS), \"| saved:\", cols_path)\n",
    "\n",
    "# keep only needed cols\n",
    "keep_cols = [\"case_id\"] + [c for c in FEATURE_COLS if c in df_feat.columns]\n",
    "df_feat = df_feat[keep_cols].copy()\n",
    "\n",
    "# dedup per case_id (prefer largest signal)\n",
    "sort_keys = [k for k in [\"best_peak_score\",\"area_frac\",\"area_frac_tok\"] if k in df_feat.columns]\n",
    "if sort_keys:\n",
    "    df_feat = df_feat.sort_values(sort_keys, ascending=False).drop_duplicates(\"case_id\", keep=\"first\")\n",
    "else:\n",
    "    df_feat = df_feat.drop_duplicates(\"case_id\", keep=\"first\")\n",
    "\n",
    "df_train_tab = df_man.merge(df_feat, on=\"case_id\", how=\"left\")\n",
    "\n",
    "# ensure all feature cols exist\n",
    "for c in FEATURE_COLS:\n",
    "    if c not in df_train_tab.columns:\n",
    "        df_train_tab[c] = np.nan\n",
    "    df_train_tab[c] = pd.to_numeric(df_train_tab[c], errors=\"coerce\")\n",
    "\n",
    "fill_value = float(CFG.get(\"features\", {}).get(\"missing_numeric_fill\", 0.0))\n",
    "df_train_tab[FEATURE_COLS] = df_train_tab[FEATURE_COLS].fillna(fill_value)\n",
    "\n",
    "# save table for debugging/consistency\n",
    "train_table_pq = ART_DIR / \"train_table.parquet\"\n",
    "df_train_tab.to_parquet(train_table_pq, index=False)\n",
    "\n",
    "X = df_train_tab[FEATURE_COLS].to_numpy(dtype=np.float32, copy=True)\n",
    "y = df_train_tab[\"y\"].to_numpy(dtype=np.int64, copy=True)\n",
    "case_ids = df_train_tab[\"case_id\"].to_numpy(dtype=np.int64, copy=True)\n",
    "\n",
    "N = len(y)\n",
    "print(\"Train rows:\", N, \"| pos_rate:\", float(y.mean()))\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ----------------------------\n",
    "# Model: LightGBM preferred, fallback sklearn\n",
    "# ----------------------------\n",
    "use_lgbm = True\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    use_lgbm = False\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# SciPy optional (CC filtering)\n",
    "try:\n",
    "    import scipy.ndimage as ndi\n",
    "    _HAS_SCIPY = True\n",
    "except Exception:\n",
    "    _HAS_SCIPY = False\n",
    "\n",
    "# ----------------------------\n",
    "# Gate ensemble config (multi-seed + CV OOF)\n",
    "# ----------------------------\n",
    "gate_cfg = CFG.get(\"gate_model\", {})\n",
    "seeds = gate_cfg.get(\"seeds\", [42, 202, 777])\n",
    "n_splits = int(gate_cfg.get(\"n_splits\", 5))\n",
    "early_rounds = int(gate_cfg.get(\"early_stopping_rounds\", 200))\n",
    "\n",
    "# sensible default LGB params (kalau cfg tidak isi)\n",
    "default_lgb_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 63,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "params = gate_cfg.get(\"params\", default_lgb_params)\n",
    "# keep n_estimators as upper bound\n",
    "num_boost_round = int(params.get(\"n_estimators\", 4000))\n",
    "\n",
    "print(\"Model:\", \"LightGBM\" if use_lgbm else \"HistGradientBoosting (fallback)\")\n",
    "print(\"Seeds:\", seeds, \"| n_splits:\", n_splits)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CV OOF across seeds (accumulate sum/count -> averaged oof)\n",
    "# ----------------------------\n",
    "oof_sum = np.zeros(N, dtype=np.float32)\n",
    "oof_cnt = np.zeros(N, dtype=np.int32)\n",
    "cv_logs = []\n",
    "best_iters = []\n",
    "\n",
    "if use_lgbm:\n",
    "    for s in seeds:\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=int(s))\n",
    "        for k, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(N), y)):\n",
    "            dtr = lgb.Dataset(X[tr_idx], label=y[tr_idx], feature_name=FEATURE_COLS, free_raw_data=True)\n",
    "            dva = lgb.Dataset(X[va_idx], label=y[va_idx], feature_name=FEATURE_COLS, free_raw_data=True)\n",
    "\n",
    "            # seed per run\n",
    "            run_params = dict(params)\n",
    "            run_params[\"seed\"] = int(s)\n",
    "            run_params[\"feature_fraction_seed\"] = int(s) + 1\n",
    "            run_params[\"bagging_seed\"] = int(s) + 2\n",
    "\n",
    "            booster = lgb.train(\n",
    "                params=run_params,\n",
    "                train_set=dtr,\n",
    "                valid_sets=[dva],\n",
    "                valid_names=[\"val\"],\n",
    "                num_boost_round=num_boost_round,\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=early_rounds, verbose=False)]\n",
    "            )\n",
    "\n",
    "            p = booster.predict(X[va_idx], num_iteration=booster.best_iteration)\n",
    "            p = np.clip(p, 1e-6, 1-1e-6).astype(np.float32)\n",
    "            oof_sum[va_idx] += p\n",
    "            oof_cnt[va_idx] += 1\n",
    "            best_iters.append(int(booster.best_iteration))\n",
    "\n",
    "            auc = float(roc_auc_score(y[va_idx], p)) if len(np.unique(y[va_idx])) > 1 else float(\"nan\")\n",
    "            ll  = float(log_loss(y[va_idx], p, labels=[0,1]))\n",
    "            pred05 = (p >= 0.5).astype(int)\n",
    "            f1v = float(f1_score(y[va_idx], pred05))\n",
    "\n",
    "            cv_logs.append({\n",
    "                \"seed\": int(s), \"split\": int(k),\n",
    "                \"auc\": auc, \"logloss\": ll, \"f1@0.5\": f1v,\n",
    "                \"best_iter\": int(booster.best_iteration),\n",
    "            })\n",
    "            print(f\"[seed {s} split {k}] auc={auc:.4f} logloss={ll:.4f} f1@0.5={f1v:.4f} iter={int(booster.best_iteration)}\")\n",
    "else:\n",
    "    # fallback (no LGBM): use HistGradientBoosting\n",
    "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "    for s in seeds:\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=int(s))\n",
    "        for k, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(N), y)):\n",
    "            clf = HistGradientBoostingClassifier(\n",
    "                learning_rate=0.05,\n",
    "                max_leaf_nodes=63,\n",
    "                min_samples_leaf=40,\n",
    "                max_iter=600,\n",
    "                random_state=int(s) + 1000 + int(k),\n",
    "            )\n",
    "            clf.fit(X[tr_idx], y[tr_idx])\n",
    "            p = clf.predict_proba(X[va_idx])[:, 1]\n",
    "            p = np.clip(p, 1e-6, 1-1e-6).astype(np.float32)\n",
    "            oof_sum[va_idx] += p\n",
    "            oof_cnt[va_idx] += 1\n",
    "\n",
    "            auc = float(roc_auc_score(y[va_idx], p)) if len(np.unique(y[va_idx])) > 1 else float(\"nan\")\n",
    "            ll  = float(log_loss(y[va_idx], p, labels=[0,1]))\n",
    "            pred05 = (p >= 0.5).astype(int)\n",
    "            f1v = float(f1_score(y[va_idx], pred05))\n",
    "\n",
    "            cv_logs.append({\"seed\": int(s), \"split\": int(k), \"auc\": auc, \"logloss\": ll, \"f1@0.5\": f1v})\n",
    "            print(f\"[seed {s} split {k}] auc={auc:.4f} logloss={ll:.4f} f1@0.5={f1v:.4f}\")\n",
    "\n",
    "oof_prob_raw = oof_sum / np.maximum(oof_cnt, 1).astype(np.float32)\n",
    "\n",
    "auc_all = float(roc_auc_score(y, oof_prob_raw)) if len(np.unique(y)) > 1 else float(\"nan\")\n",
    "ll_all  = float(log_loss(y, oof_prob_raw, labels=[0,1]))\n",
    "pred05_all = (oof_prob_raw >= 0.5).astype(int)\n",
    "f1_all = float(f1_score(y, pred05_all))\n",
    "prec_all = float(precision_score(y, pred05_all, zero_division=0))\n",
    "rec_all  = float(recall_score(y, pred05_all, zero_division=0))\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"OOF raw (ensemble): auc={auc_all:.4f} logloss={ll_all:.4f} f1@0.5={f1_all:.4f} prec={prec_all:.4f} rec={rec_all:.4f}\")\n",
    "\n",
    "best_iter_mean = int(np.mean(best_iters)) if best_iters else None\n",
    "best_iter_med  = int(np.median(best_iters)) if best_iters else None\n",
    "print(\"best_iter_mean:\", best_iter_mean, \"| best_iter_median:\", best_iter_med)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Calibration on OOF\n",
    "# ----------------------------\n",
    "cal_cfg = CFG.get(\"calibration\", {\"enabled\": True, \"method\": \"isotonic\"})\n",
    "cal_enabled = bool(cal_cfg.get(\"enabled\", True))\n",
    "cal_method = str(cal_cfg.get(\"method\", \"isotonic\")).lower()\n",
    "\n",
    "oof_prob = oof_prob_raw.copy()\n",
    "cal_pack = {\"enabled\": False}\n",
    "\n",
    "if cal_enabled:\n",
    "    if cal_method == \"isotonic\":\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        iso.fit(oof_prob_raw, y)\n",
    "        oof_prob = np.clip(iso.predict(oof_prob_raw).astype(np.float32), 1e-6, 1-1e-6)\n",
    "        cal_pack = {\"enabled\": True, \"method\": \"isotonic\", \"path\": str(MODEL_DIR / \"calibration_isotonic.joblib\")}\n",
    "        joblib.dump(iso, MODEL_DIR / \"calibration_isotonic.joblib\")\n",
    "    elif cal_method in [\"sigmoid\", \"platt\"]:\n",
    "        p = np.clip(oof_prob_raw, 1e-6, 1-1e-6)\n",
    "        logit = np.log(p/(1-p)).reshape(-1,1)\n",
    "        lr = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
    "        lr.fit(logit, y)\n",
    "        oof_prob = np.clip(lr.predict_proba(logit)[:,1].astype(np.float32), 1e-6, 1-1e-6)\n",
    "        cal_pack = {\"enabled\": True, \"method\": \"sigmoid\", \"path\": str(MODEL_DIR / \"calibration_sigmoid.joblib\")}\n",
    "        joblib.dump(lr, MODEL_DIR / \"calibration_sigmoid.joblib\")\n",
    "\n",
    "auc_cal = float(roc_auc_score(y, oof_prob)) if len(np.unique(y)) > 1 else float(\"nan\")\n",
    "ll_cal  = float(log_loss(y, oof_prob, labels=[0,1]))\n",
    "pred05_cal = (oof_prob >= 0.5).astype(int)\n",
    "f1_cal = float(f1_score(y, pred05_cal))\n",
    "\n",
    "(MODEL_DIR / \"calibration.json\").write_text(json.dumps(cal_pack, indent=2))\n",
    "print(f\"OOF calibrated: auc={auc_cal:.4f} logloss={ll_cal:.4f} f1@0.5={f1_cal:.4f} | calibration={cal_pack}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# save OOF\n",
    "np.save(OOF_DIR / \"oof_prob_raw.npy\", oof_prob_raw)\n",
    "np.save(OOF_DIR / \"oof_prob.npy\", oof_prob)\n",
    "pd.DataFrame({\n",
    "    \"case_id\": case_ids.astype(int),\n",
    "    \"y\": y.astype(int),\n",
    "    \"oof_prob_raw\": oof_prob_raw.astype(float),\n",
    "    \"oof_prob\": oof_prob.astype(float),\n",
    "}).to_csv(OOF_DIR / \"oof_prob.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dice-proxy (GT vs predicted masks) for threshold selection\n",
    "# ----------------------------\n",
    "PP = CFG.get(\"postprocess_ref\", {})\n",
    "MIN_INST_PIX  = int(PP.get(\"min_inst_pix\", 32))\n",
    "MAX_AREA_FRAC = float(PP.get(\"max_area_frac\", 0.90))\n",
    "MAX_INST_KEEP = int(PP.get(\"max_inst_keep\", 8))\n",
    "\n",
    "def _find_mask_files(mask_dir: Path, case_id: int):\n",
    "    if mask_dir is None or (not mask_dir.exists()):\n",
    "        return []\n",
    "    cid = str(int(case_id))\n",
    "    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n",
    "    pats = [f\"{cid}*.png\", f\"{cid}*.jpg\", f\"{cid}*.jpeg\", f\"{cid}*.tif\", f\"{cid}*.tiff\", f\"{cid}*.bmp\"]\n",
    "    out, seen = [], set()\n",
    "    for pat in pats:\n",
    "        for p in mask_dir.glob(pat):\n",
    "            if p.suffix.lower() in exts:\n",
    "                s = str(p)\n",
    "                if s not in seen:\n",
    "                    out.append(p); seen.add(s)\n",
    "    return sorted(out)\n",
    "\n",
    "def load_gt_union(case_id: int):\n",
    "    # npy shortcut\n",
    "    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n",
    "        if d is None or (not d.exists()):\n",
    "            continue\n",
    "        npy = d / f\"{int(case_id)}.npy\"\n",
    "        if npy.exists():\n",
    "            a = np.load(npy, mmap_mode=\"r\")\n",
    "            a = np.asarray(a)\n",
    "            if a.ndim == 2:\n",
    "                return (a > 0)\n",
    "            if a.ndim == 3:\n",
    "                return (a > 0).any(axis=0)\n",
    "    # union images\n",
    "    files = []\n",
    "    files += _find_mask_files(TRAIN_MASK_DIR, case_id)\n",
    "    files += _find_mask_files(SUP_MASK_DIR, case_id)\n",
    "    if not files:\n",
    "        return None\n",
    "    m = None\n",
    "    for p in files:\n",
    "        try:\n",
    "            im = Image.open(p).convert(\"L\")\n",
    "            a = (np.asarray(im) > 0)\n",
    "            m = a if m is None else (m | a)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return m\n",
    "\n",
    "def load_pred_union(case_id: int):\n",
    "    p = PRED_TRAIN_DIR / f\"{int(case_id)}.npz\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    z = np.load(p)\n",
    "    # support multiple keys\n",
    "    for k in [\"mask\", \"m\", \"pred\", \"pred_mask\", \"mask_full\", \"mask_up\"]:\n",
    "        if k in z.files:\n",
    "            a = z[k]\n",
    "            return (np.asarray(a) > 0)\n",
    "    # fallback: first array\n",
    "    for k in z.files:\n",
    "        a = z[k]\n",
    "        if getattr(a, \"ndim\", 0) == 2:\n",
    "            return (np.asarray(a) > 0)\n",
    "    return None\n",
    "\n",
    "def cc_union_filtered(mask_bool: np.ndarray):\n",
    "    if mask_bool is None:\n",
    "        return None, {\"n_inst\": 0, \"area\": 0}\n",
    "    m = mask_bool.astype(bool)\n",
    "    H, W = m.shape\n",
    "    area = int(m.sum())\n",
    "    if area == 0:\n",
    "        return m, {\"n_inst\": 0, \"area\": 0}\n",
    "    if (area / float(H*W)) > MAX_AREA_FRAC:\n",
    "        return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n",
    "    if not _HAS_SCIPY:\n",
    "        if area < MIN_INST_PIX:\n",
    "            return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n",
    "        return m, {\"n_inst\": 1, \"area\": area}\n",
    "    lab, n = ndi.label(m, structure=np.ones((3,3), dtype=np.uint8))\n",
    "    if n <= 0:\n",
    "        return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n",
    "    areas = ndi.sum(m.astype(np.uint8), lab, index=np.arange(1, n+1)).astype(np.int64)\n",
    "    keep = np.where(areas >= MIN_INST_PIX)[0]\n",
    "    if keep.size == 0:\n",
    "        return np.zeros_like(m, dtype=bool), {\"n_inst\": 0, \"area\": 0}\n",
    "    keep = keep[np.argsort(areas[keep])[::-1]][:MAX_INST_KEEP]\n",
    "    out = np.zeros_like(m, dtype=bool)\n",
    "    for k in keep:\n",
    "        out |= (lab == (k + 1))\n",
    "    return out, {\"n_inst\": int(len(keep)), \"area\": int(out.sum())}\n",
    "\n",
    "def dice_score(pr: np.ndarray, gt: np.ndarray):\n",
    "    pr = pr.astype(bool); gt = gt.astype(bool)\n",
    "    a = int(pr.sum()); b = int(gt.sum())\n",
    "    if a == 0 and b == 0: return 1.0\n",
    "    if a == 0 or b == 0: return 0.0\n",
    "    inter = int((pr & gt).sum())\n",
    "    return float((2.0 * inter) / (a + b))\n",
    "\n",
    "dice_use  = np.zeros(N, dtype=np.float32)\n",
    "dice_empty= np.zeros(N, dtype=np.float32)\n",
    "\n",
    "miss_gt = miss_pr = bad_shape = 0\n",
    "t1 = time.time()\n",
    "\n",
    "for i, cid in enumerate(case_ids.tolist()):\n",
    "    gt = load_gt_union(cid)\n",
    "    pr = load_pred_union(cid)\n",
    "\n",
    "    if gt is None:\n",
    "        miss_gt += 1\n",
    "        gt_mask = None\n",
    "        gt_empty = True\n",
    "    else:\n",
    "        gt_mask = gt.astype(bool)\n",
    "        gt_empty = (gt_mask.sum() == 0)\n",
    "\n",
    "    dice_empty[i] = 1.0 if gt_empty else 0.0\n",
    "\n",
    "    if pr is None:\n",
    "        miss_pr += 1\n",
    "        dice_use[i] = dice_empty[i]\n",
    "        continue\n",
    "\n",
    "    pr_mask = pr.astype(bool)\n",
    "    if gt_mask is not None and pr_mask.shape != gt_mask.shape:\n",
    "        bad_shape += 1\n",
    "        im = Image.fromarray((pr_mask.astype(np.uint8)*255))\n",
    "        im = im.resize((gt_mask.shape[1], gt_mask.shape[0]), resample=Image.NEAREST)\n",
    "        pr_mask = (np.asarray(im) > 127)\n",
    "\n",
    "    pr_f, _ = cc_union_filtered(pr_mask)\n",
    "    if gt_mask is None:\n",
    "        dice_use[i] = 1.0 if (pr_f.sum() == 0) else 0.0\n",
    "    else:\n",
    "        gt_f, _ = cc_union_filtered(gt_mask)\n",
    "        dice_use[i] = dice_score(pr_f, gt_f)\n",
    "\n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"[dice-proxy] {i+1}/{N} | miss_gt={miss_gt} miss_pr={miss_pr} bad_shape={bad_shape} | {time.time()-t1:.1f}s\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"Dice-proxy ready | miss_gt={miss_gt} miss_pr={miss_pr} bad_shape={bad_shape} | {time.time()-t1:.1f}s\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Threshold sweep (optimize dice-proxy, tie-break by F1, then FP-rate)\n",
    "# ----------------------------\n",
    "thr_grid = np.linspace(0.0, 1.0, 201, dtype=np.float32)\n",
    "rows = []\n",
    "for thr in thr_grid:\n",
    "    use = (oof_prob >= thr)\n",
    "    score = np.where(use, dice_use, dice_empty).mean()\n",
    "\n",
    "    pred = use.astype(int)\n",
    "    f1v = f1_score(y, pred)\n",
    "    prec = precision_score(y, pred, zero_division=0)\n",
    "    rec  = recall_score(y, pred, zero_division=0)\n",
    "    fp_rate = float(((pred==1) & (y==0)).mean())\n",
    "    fn_rate = float(((pred==0) & (y==1)).mean())\n",
    "\n",
    "    rows.append({\n",
    "        \"thr\": float(thr),\n",
    "        \"score_dice_proxy\": float(score),\n",
    "        \"f1_gate\": float(f1v),\n",
    "        \"precision_gate\": float(prec),\n",
    "        \"recall_gate\": float(rec),\n",
    "        \"fp_rate\": fp_rate,\n",
    "        \"fn_rate\": fn_rate,\n",
    "    })\n",
    "\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "# tie-break: maximize dice_proxy, then maximize f1, then minimize fp_rate\n",
    "df_thr[\"_neg_fp\"] = -df_thr[\"fp_rate\"]\n",
    "df_thr = df_thr.sort_values([\"score_dice_proxy\",\"f1_gate\",\"_neg_fp\"], ascending=[False,False,False]).reset_index(drop=True)\n",
    "best_thr = float(df_thr.loc[0, \"thr\"])\n",
    "best_score = float(df_thr.loc[0, \"score_dice_proxy\"])\n",
    "df_thr = df_thr.drop(columns=[\"_neg_fp\"])\n",
    "\n",
    "df_thr.to_csv(EVAL_DIR / \"threshold_table.csv\", index=False)\n",
    "(EVAL_DIR / \"best_threshold.json\").write_text(json.dumps({\n",
    "    \"recommended_thr\": best_thr,\n",
    "    \"best_score_dice_proxy\": best_score,\n",
    "    \"best_row\": df_thr.loc[0].to_dict(),\n",
    "    \"instance_split_fullres\": {\"MIN_INST_PIX\": MIN_INST_PIX, \"MAX_AREA_FRAC\": MAX_AREA_FRAC, \"MAX_INST_KEEP\": MAX_INST_KEEP},\n",
    "}, indent=2))\n",
    "\n",
    "print(\"BEST (dice-proxy): thr =\", best_thr, \"| score =\", best_score)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train FINAL models on full data (per-seed) for inference\n",
    "# ----------------------------\n",
    "final_models = []\n",
    "if use_lgbm:\n",
    "    # choose num_boost_round for final training\n",
    "    final_rounds = best_iter_med if best_iter_med is not None else int(min(2000, num_boost_round))\n",
    "    dfull = lgb.Dataset(X, label=y, feature_name=FEATURE_COLS, free_raw_data=True)\n",
    "    for s in seeds:\n",
    "        run_params = dict(params)\n",
    "        run_params[\"seed\"] = int(s)\n",
    "        run_params[\"feature_fraction_seed\"] = int(s) + 1\n",
    "        run_params[\"bagging_seed\"] = int(s) + 2\n",
    "        booster = lgb.train(run_params, dfull, num_boost_round=int(final_rounds))\n",
    "        mp = MODEL_DIR / f\"final_seed{s}.txt\"\n",
    "        booster.save_model(str(mp))\n",
    "        final_models.append({\"type\": \"lgbm\", \"seed\": int(s), \"path\": str(mp), \"num_boost_round\": int(final_rounds)})\n",
    "else:\n",
    "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "    for s in seeds:\n",
    "        clf = HistGradientBoostingClassifier(\n",
    "            learning_rate=0.05,\n",
    "            max_leaf_nodes=63,\n",
    "            min_samples_leaf=40,\n",
    "            max_iter=600,\n",
    "            random_state=int(s) + 9999,\n",
    "        )\n",
    "        clf.fit(X, y)\n",
    "        mp = MODEL_DIR / f\"final_seed{s}.joblib\"\n",
    "        joblib.dump(clf, mp)\n",
    "        final_models.append({\"type\": \"hgb\", \"seed\": int(s), \"path\": str(mp)})\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save metrics + portable bundle\n",
    "# ----------------------------\n",
    "cv_metrics = {\n",
    "    \"oof_raw\": {\"auc\": auc_all, \"logloss\": ll_all, \"f1@0.5\": f1_all, \"prec\": prec_all, \"rec\": rec_all},\n",
    "    \"oof_cal\": {\"auc\": auc_cal, \"logloss\": ll_cal, \"f1@0.5\": f1_cal},\n",
    "    \"recommended_thr\": best_thr,\n",
    "    \"best_score_dice_proxy\": best_score,\n",
    "    \"cv_logs\": cv_logs[:],  # can be large; ok\n",
    "    \"best_iter_mean\": best_iter_mean,\n",
    "    \"best_iter_median\": best_iter_med,\n",
    "    \"seeds\": seeds,\n",
    "    \"n_splits\": n_splits,\n",
    "    \"calibration\": cal_pack,\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"paths\": {\n",
    "        \"pred_features_train\": str(pred_feat_train),\n",
    "        \"pred_mask_train_dir\": str(PRED_TRAIN_DIR),\n",
    "        \"train_manifest\": str(train_manifest_pq),\n",
    "    }\n",
    "}\n",
    "(EVAL_DIR / \"cv_metrics.json\").write_text(json.dumps(cv_metrics, indent=2))\n",
    "\n",
    "import torch\n",
    "torch.save({\n",
    "    \"cfg\": CFG,\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"final_models\": final_models,\n",
    "    \"calibration\": cal_pack,\n",
    "    \"recommended_thr\": best_thr,\n",
    "    \"dice_proxy_cfg\": {\"MIN_INST_PIX\": MIN_INST_PIX, \"MAX_AREA_FRAC\": MAX_AREA_FRAC, \"MAX_INST_KEEP\": MAX_INST_KEEP},\n",
    "}, ART_DIR / \"final_gate_model.pt\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", train_table_pq)\n",
    "print(\" -\", cols_path)\n",
    "print(\" -\", OOF_DIR / \"oof_prob.csv\")\n",
    "print(\" -\", EVAL_DIR / \"threshold_table.csv\")\n",
    "print(\" -\", EVAL_DIR / \"best_threshold.json\")\n",
    "print(\" -\", EVAL_DIR / \"cv_metrics.json\")\n",
    "print(\" -\", ART_DIR / \"final_gate_model.pt\")\n",
    "print(\"Final models:\", len(final_models), \"| dir:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebba498",
   "metadata": {
    "papermill": {
     "duration": 0.008795,
     "end_time": "2026-01-13T06:04:04.713604",
     "exception": false,
     "start_time": "2026-01-13T06:04:04.704809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimize Model & Hyperparameters (Iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15859f03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T06:04:04.734144Z",
     "iopub.status.busy": "2026-01-13T06:04:04.733446Z",
     "iopub.status.idle": "2026-01-13T06:04:09.540931Z",
     "shell.execute_reply": "2026-01-13T06:04:09.539738Z"
    },
    "papermill": {
     "duration": 4.82086,
     "end_time": "2026-01-13T06:04:09.542942",
     "exception": false,
     "start_time": "2026-01-13T06:04:04.722082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_CACHE: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache\n",
      "TOKEN_MANIFEST_ROOT: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500\n",
      "TOK_MAN_TRAIN      : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500/tokens_manifest_train.parquet\n",
      "TOKEN_DATA_ROOT    : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500\n",
      "TOKEN_DATA_ROOT npz: 2796\n",
      "Indexed token files (unique basenames): 2796\n",
      "----------------------------------------------------------------------\n",
      "train_table rows: 2795\n",
      "tok_npz not-null rate: 1.0\n",
      "tok_npz exists rate   : 1.0\n",
      "----------------------------------------------------------------------\n",
      "Saved patched: /kaggle/working/recodai_luc_gate_artifacts/train_table.patched_tokens.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FIX v2 — PATCH train_table.parquet with valid tok_npz\n",
    "# - Auto-detect TOKEN_MANIFEST_ROOT (where tokens_manifest_train.parquet exists)\n",
    "# - Auto-detect TOKEN_DATA_ROOT (a sibling dir that actually contains token .npz)\n",
    "# - Build fast basename->absolute_path index from TOKEN_DATA_ROOT/{train,test,train_all,test_all}\n",
    "# - Join into train_table by case_id (fallback uid if needed)\n",
    "# Output:\n",
    "#   /kaggle/working/recodai_luc_gate_artifacts/train_table.patched_tokens.parquet\n",
    "# ============================================================\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ART_DIR = Path(\"/kaggle/working/recodai_luc_gate_artifacts\")\n",
    "TRAIN_TABLE_IN = ART_DIR / \"train_table.parquet\"\n",
    "TRAIN_TABLE_OUT = ART_DIR / \"train_table.patched_tokens.parquet\"\n",
    "\n",
    "if not TRAIN_TABLE_IN.exists():\n",
    "    raise FileNotFoundError(f\"Missing {TRAIN_TABLE_IN}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Locate base cache dir (from cfg if possible, else fallback)\n",
    "# ------------------------------------------------------------\n",
    "base_cache = None\n",
    "cfg_try = [ART_DIR / \"train_cfg.json\", Path(\"/kaggle/working/recodai_luc_mask_artifacts/mask_cfg.json\")]\n",
    "for cp in cfg_try:\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            j = json.loads(cp.read_text())\n",
    "            if \"paths\" in j and \"TOKEN_ROOT\" in j[\"paths\"]:\n",
    "                base_cache = Path(j[\"paths\"][\"TOKEN_ROOT\"]).parent  # parent of token root\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if base_cache is None:\n",
    "    # fallback to your dataset structure\n",
    "    base_cache = Path(\"/kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache\")\n",
    "\n",
    "if not base_cache.exists():\n",
    "    raise FileNotFoundError(f\"Base cache dir not found: {base_cache}\")\n",
    "\n",
    "print(\"BASE_CACHE:\", base_cache)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Find TOKEN_MANIFEST_ROOT: where tokens_manifest_train.parquet exists\n",
    "# ------------------------------------------------------------\n",
    "man_candidates = sorted(base_cache.glob(\"**/tokens_manifest_train.parquet\"))\n",
    "if not man_candidates:\n",
    "    raise FileNotFoundError(f\"Cannot find tokens_manifest_train.parquet under {base_cache}\")\n",
    "\n",
    "# prefer the one that sits next to a train/ folder OR most recently modified\n",
    "def man_score(p: Path):\n",
    "    root = p.parent\n",
    "    has_train = int((root/\"train\").exists() or (root/\"train_all\").exists())\n",
    "    return (has_train, p.stat().st_mtime)\n",
    "\n",
    "man_candidates = sorted(man_candidates, key=man_score, reverse=True)\n",
    "TOK_MAN_TRAIN = man_candidates[0]\n",
    "TOKEN_MANIFEST_ROOT = TOK_MAN_TRAIN.parent\n",
    "\n",
    "print(\"TOKEN_MANIFEST_ROOT:\", TOKEN_MANIFEST_ROOT)\n",
    "print(\"TOK_MAN_TRAIN      :\", TOK_MAN_TRAIN)\n",
    "\n",
    "df_tok = pd.read_parquet(TOK_MAN_TRAIN).copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Pick TOKEN_DATA_ROOT: a dir under base_cache with most *.npz in {train,test,train_all,test_all}\n",
    "#    (This fixes your 'bind' root that has no npz)\n",
    "# ------------------------------------------------------------\n",
    "def count_npz_under(root: Path):\n",
    "    total = 0\n",
    "    for sub in [\"train\", \"test\", \"train_all\", \"test_all\"]:\n",
    "        d = root / sub\n",
    "        if d.exists():\n",
    "            total += sum(1 for _ in d.glob(\"*.npz\"))\n",
    "            total += sum(1 for _ in d.glob(\"*.npy\"))\n",
    "    return total\n",
    "\n",
    "# candidate roots = direct children of base_cache that look like dinov2_*\n",
    "root_candidates = [p for p in base_cache.iterdir() if p.is_dir()]\n",
    "root_scored = [(count_npz_under(p), p) for p in root_candidates]\n",
    "root_scored.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "TOKEN_DATA_ROOT = root_scored[0][1]\n",
    "best_cnt = root_scored[0][0]\n",
    "\n",
    "print(\"TOKEN_DATA_ROOT    :\", TOKEN_DATA_ROOT)\n",
    "print(\"TOKEN_DATA_ROOT npz:\", best_cnt)\n",
    "\n",
    "if best_cnt == 0:\n",
    "    # last resort: maybe tokens are deeper; try a broader scan but still limited\n",
    "    deep = []\n",
    "    for p in base_cache.glob(\"**/train\"):\n",
    "        if p.is_dir():\n",
    "            deep.append(p.parent)\n",
    "    deep = list(dict.fromkeys(deep))  # unique preserve order\n",
    "    deep_scored = [(count_npz_under(p), p) for p in deep]\n",
    "    deep_scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    if deep_scored and deep_scored[0][0] > 0:\n",
    "        TOKEN_DATA_ROOT = deep_scored[0][1]\n",
    "        best_cnt = deep_scored[0][0]\n",
    "        print(\"DEEP PICK TOKEN_DATA_ROOT:\", TOKEN_DATA_ROOT, \"| npz:\", best_cnt)\n",
    "\n",
    "if best_cnt == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Still no .npz/.npy found in any candidate TOKEN_DATA_ROOT.\\n\"\n",
    "        f\"Check dataset mount under: {base_cache}\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Build basename -> abs path index (FAST, no rglob)\n",
    "# ------------------------------------------------------------\n",
    "name2path = {}\n",
    "for sub in [\"train\", \"test\", \"train_all\", \"test_all\"]:\n",
    "    d = TOKEN_DATA_ROOT / sub\n",
    "    if not d.exists():\n",
    "        continue\n",
    "    for p in d.glob(\"*.npz\"):\n",
    "        name2path[p.name] = str(p)\n",
    "    for p in d.glob(\"*.npy\"):\n",
    "        name2path[p.name] = str(p)\n",
    "\n",
    "print(\"Indexed token files (unique basenames):\", len(name2path))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Normalize df_tok: ensure case_id & pick path column\n",
    "# ------------------------------------------------------------\n",
    "def infer_case_id_from_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str)\n",
    "    out = s.str.extract(r\"(\\d+)\")[0]\n",
    "    return pd.to_numeric(out, errors=\"coerce\")\n",
    "\n",
    "if \"case_id\" not in df_tok.columns:\n",
    "    for c in [\"uid\", \"uid_safe\", \"id\", \"npz_path\", \"tok_npz\", \"path\", \"file\"]:\n",
    "        if c in df_tok.columns:\n",
    "            df_tok[\"case_id\"] = infer_case_id_from_series(df_tok[c])\n",
    "            break\n",
    "\n",
    "df_tok[\"case_id\"] = pd.to_numeric(df_tok[\"case_id\"], errors=\"coerce\")\n",
    "df_tok = df_tok[df_tok[\"case_id\"].notna()].copy()\n",
    "df_tok[\"case_id\"] = df_tok[\"case_id\"].astype(int)\n",
    "\n",
    "# pick a token path column\n",
    "def pick_token_path_col(df):\n",
    "    priority = [\"npz_path\",\"tok_npz\",\"token_npz\",\"path\",\"file\"]\n",
    "    for c in priority:\n",
    "        if c in df.columns and df[c].astype(str).str.contains(r\"\\.npz$|\\.npy$\", regex=True, na=False).mean() > 0.2:\n",
    "            return c\n",
    "    # fallback: any column with many .npz/.npy strings\n",
    "    best, best_rate = None, 0.0\n",
    "    for c in df.columns:\n",
    "        rate = df[c].astype(str).str.contains(r\"\\.npz$|\\.npy$\", regex=True, na=False).mean()\n",
    "        if rate > best_rate:\n",
    "            best, best_rate = c, rate\n",
    "    return best\n",
    "\n",
    "tok_path_col = pick_token_path_col(df_tok)\n",
    "if tok_path_col is None:\n",
    "    raise ValueError(\"Cannot find token path column in tokens_manifest_train.parquet\")\n",
    "\n",
    "df_tok[\"tok_raw\"] = df_tok[tok_path_col].astype(str)\n",
    "\n",
    "def resolve_tok(p):\n",
    "    if p is None:\n",
    "        return None\n",
    "    s = str(p)\n",
    "    if not s or s.lower() == \"nan\":\n",
    "        return None\n",
    "    pp = Path(s)\n",
    "    if pp.exists():\n",
    "        return str(pp)\n",
    "\n",
    "    bn = pp.name\n",
    "    if bn in name2path:\n",
    "        return name2path[bn]\n",
    "\n",
    "    # try tail after manifest root name (handles absolute from other root)\n",
    "    s2 = s.replace(\"\\\\\", \"/\")\n",
    "    root_name = TOKEN_DATA_ROOT.name\n",
    "    if root_name in s2:\n",
    "        tail = s2.split(root_name, 1)[1].lstrip(\"/\")\n",
    "        cand = TOKEN_DATA_ROOT / tail\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    # try common folders\n",
    "    for sub in [\"train\", \"test\", \"train_all\", \"test_all\"]:\n",
    "        cand = TOKEN_DATA_ROOT / sub / bn\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    return None\n",
    "\n",
    "df_tok[\"tok_npz\"] = df_tok[\"tok_raw\"].map(resolve_tok)\n",
    "\n",
    "# dedup per case_id prefer existing\n",
    "df_tok[\"_ok\"] = df_tok[\"tok_npz\"].map(lambda x: isinstance(x, str) and Path(x).exists())\n",
    "df_tok = df_tok.sort_values([\"case_id\",\"_ok\"], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\").drop(columns=[\"_ok\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Patch train_table.parquet\n",
    "# ------------------------------------------------------------\n",
    "df_table = pd.read_parquet(TRAIN_TABLE_IN).copy()\n",
    "if \"case_id\" not in df_table.columns:\n",
    "    raise ValueError(\"train_table.parquet must have case_id\")\n",
    "df_table[\"case_id\"] = pd.to_numeric(df_table[\"case_id\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "\n",
    "df_table = df_table.merge(df_tok[[\"case_id\",\"tok_npz\"]], on=\"case_id\", how=\"left\")\n",
    "df_table[\"tok_exists\"] = df_table[\"tok_npz\"].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "\n",
    "join_rate = float(df_table[\"tok_npz\"].notna().mean())\n",
    "exists_rate = float(df_table[\"tok_exists\"].mean())\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(\"train_table rows:\", len(df_table))\n",
    "print(\"tok_npz not-null rate:\", join_rate)\n",
    "print(\"tok_npz exists rate   :\", exists_rate)\n",
    "\n",
    "if exists_rate < 0.90:\n",
    "    bad = df_table[~df_table[\"tok_exists\"]]\n",
    "    print(\"Bad examples (case_id, tok_npz):\")\n",
    "    print(bad[[\"case_id\",\"tok_npz\"]].head(12).to_string(index=False))\n",
    "\n",
    "df_table.to_parquet(TRAIN_TABLE_OUT, index=False)\n",
    "print(\"-\"*70)\n",
    "print(\"Saved patched:\", TRAIN_TABLE_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa972a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T06:04:09.564412Z",
     "iopub.status.busy": "2026-01-13T06:04:09.563975Z",
     "iopub.status.idle": "2026-01-13T15:01:45.479099Z",
     "shell.execute_reply": "2026-01-13T15:01:45.475638Z"
    },
    "papermill": {
     "duration": 32255.932869,
     "end_time": "2026-01-13T15:01:45.484676",
     "exception": false,
     "start_time": "2026-01-13T06:04:09.551807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu | AMP: False\n",
      "COMP_ROOT     : /kaggle/input/recodai-luc-scientific-image-forgery-detection\n",
      "TRAIN_MASK_DIR: /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks | exists: True\n",
      "SUP_MASK_DIR  : /kaggle/input/recodai-luc-scientific-image-forgery-detection/supplemental_masks | exists: True\n",
      "TOK_MAN_TRAIN       : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500/tokens_manifest_train.parquet\n",
      "TOKEN_MANIFEST_ROOT : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500 | token_files: 2796\n",
      "BASE_CACHE          : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache\n",
      "TOKEN_DATA_ROOT     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500 | token_files: 2796\n",
      "TOKEN indexed files : 2796\n",
      "INFO: token path column missing; joined from tokens manifest using 'npz_path' (exist-rate≈1.000)\n",
      "Token exists rate: 1.0 | rows: 2795\n",
      "TRAIN_TABLE: /kaggle/working/recodai_luc_gate_artifacts/train_table.parquet | rows_used: 2795 | pos_rate: 1.0\n",
      "TOK_COL: tok_npz\n",
      "MATCH_ROOT: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_cfg_2ed747746f9c\n",
      "MATCH npz coverage: 2795 / 2795\n",
      "PATCH/HTOK_MATCH/WTOK_MATCH: 14 37 37\n",
      "TOKEN GRID: (37, 37) | D: 768 | Input channels: 769\n",
      "Internal splits: [0, 1, 2, 3, 4] | N_SPLITS: 5\n",
      "[trial 01 | split 0] ep 1/6 loss=1.0948 val_dice_proxy=0.26266 time=284.7s\n",
      "[trial 01 | split 0] ep 2/6 loss=0.9014 val_dice_proxy=0.29498 time=276.0s\n",
      "[trial 01 | split 0] ep 3/6 loss=0.8584 val_dice_proxy=0.33838 time=265.7s\n",
      "[trial 01 | split 0] ep 4/6 loss=0.8214 val_dice_proxy=0.36199 time=289.5s\n",
      "[trial 01 | split 0] ep 5/6 loss=0.7880 val_dice_proxy=0.38316 time=276.9s\n",
      "[trial 01 | split 0] ep 6/6 loss=0.7572 val_dice_proxy=0.39135 time=293.4s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.3913487341390366 | trial: 1 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "[trial 02 | split 1] ep 1/6 loss=0.9201 val_dice_proxy=0.30860 time=271.3s\n",
      "[trial 02 | split 1] ep 2/6 loss=0.7211 val_dice_proxy=0.35640 time=285.2s\n",
      "[trial 02 | split 1] ep 3/6 loss=0.6595 val_dice_proxy=0.39323 time=278.8s\n",
      "[trial 02 | split 1] ep 4/6 loss=0.6103 val_dice_proxy=0.40486 time=266.2s\n",
      "[trial 02 | split 1] ep 5/6 loss=0.5577 val_dice_proxy=0.40461 time=300.7s\n",
      "[trial 02 | split 1] ep 6/6 loss=0.5119 val_dice_proxy=0.44307 time=277.7s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.44306866829325414 | trial: 2 | val_split: 1\n",
      "------------------------------------------------------------\n",
      "[trial 03 | split 2] ep 1/6 loss=0.8051 val_dice_proxy=0.28231 time=638.6s\n",
      "[trial 03 | split 2] ep 2/6 loss=0.5969 val_dice_proxy=0.33007 time=641.3s\n",
      "[trial 03 | split 2] ep 3/6 loss=0.5595 val_dice_proxy=0.35562 time=635.1s\n",
      "[trial 03 | split 2] ep 4/6 loss=0.5320 val_dice_proxy=0.36151 time=632.3s\n",
      "[trial 03 | split 2] ep 5/6 loss=0.5056 val_dice_proxy=0.38267 time=622.7s\n",
      "[trial 03 | split 2] ep 6/6 loss=0.4831 val_dice_proxy=0.40291 time=633.8s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.44306866829325414 | trial: 2 | val_split: 1\n",
      "------------------------------------------------------------\n",
      "[trial 04 | split 3] ep 1/6 loss=0.8594 val_dice_proxy=0.30581 time=439.8s\n",
      "[trial 04 | split 3] ep 2/6 loss=0.7298 val_dice_proxy=0.37012 time=446.0s\n",
      "[trial 04 | split 3] ep 3/6 loss=0.6626 val_dice_proxy=0.37802 time=421.8s\n",
      "[trial 04 | split 3] ep 4/6 loss=0.6249 val_dice_proxy=0.41292 time=441.8s\n",
      "[trial 04 | split 3] ep 5/6 loss=0.5964 val_dice_proxy=0.43940 time=433.1s\n",
      "[trial 04 | split 3] ep 6/6 loss=0.5756 val_dice_proxy=0.44979 time=425.9s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4497863860966872 | trial: 4 | val_split: 3\n",
      "------------------------------------------------------------\n",
      "[trial 05 | split 4] ep 1/6 loss=1.0035 val_dice_proxy=0.24163 time=289.1s\n",
      "[trial 05 | split 4] ep 2/6 loss=0.7601 val_dice_proxy=0.27658 time=272.6s\n",
      "[trial 05 | split 4] ep 3/6 loss=0.6989 val_dice_proxy=0.33997 time=290.6s\n",
      "[trial 05 | split 4] ep 4/6 loss=0.6608 val_dice_proxy=0.39230 time=281.6s\n",
      "[trial 05 | split 4] ep 5/6 loss=0.6249 val_dice_proxy=0.37742 time=289.6s\n",
      "[trial 05 | split 4] ep 6/6 loss=0.5930 val_dice_proxy=0.39721 time=269.0s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4497863860966872 | trial: 4 | val_split: 3\n",
      "------------------------------------------------------------\n",
      "[trial 06 | split 0] ep 1/6 loss=1.0967 val_dice_proxy=0.27648 time=630.7s\n",
      "[trial 06 | split 0] ep 2/6 loss=0.9473 val_dice_proxy=0.33523 time=620.9s\n",
      "[trial 06 | split 0] ep 3/6 loss=0.8874 val_dice_proxy=0.36435 time=627.5s\n",
      "[trial 06 | split 0] ep 4/6 loss=0.8211 val_dice_proxy=0.39762 time=623.6s\n",
      "[trial 06 | split 0] ep 5/6 loss=0.7484 val_dice_proxy=0.42023 time=635.1s\n",
      "[trial 06 | split 0] ep 6/6 loss=0.7133 val_dice_proxy=0.45050 time=628.2s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4504968632223682 | trial: 6 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "[trial 07 | split 1] ep 1/6 loss=1.0007 val_dice_proxy=0.18932 time=624.1s\n",
      "[trial 07 | split 1] ep 2/6 loss=0.8676 val_dice_proxy=0.25419 time=628.3s\n",
      "[trial 07 | split 1] ep 3/6 loss=0.8300 val_dice_proxy=0.28764 time=641.9s\n",
      "[trial 07 | split 1] ep 4/6 loss=0.7940 val_dice_proxy=0.32595 time=627.8s\n",
      "[trial 07 | split 1] ep 5/6 loss=0.7576 val_dice_proxy=0.35219 time=631.8s\n",
      "[trial 07 | split 1] ep 6/6 loss=0.7276 val_dice_proxy=0.37895 time=633.5s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4504968632223682 | trial: 6 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "[trial 08 | split 2] ep 1/6 loss=0.8772 val_dice_proxy=0.25929 time=421.0s\n",
      "[trial 08 | split 2] ep 2/6 loss=0.7670 val_dice_proxy=0.33208 time=413.8s\n",
      "[trial 08 | split 2] ep 3/6 loss=0.7242 val_dice_proxy=0.36270 time=428.8s\n",
      "[trial 08 | split 2] ep 4/6 loss=0.6788 val_dice_proxy=0.41174 time=423.0s\n",
      "[trial 08 | split 2] ep 5/6 loss=0.6261 val_dice_proxy=0.41277 time=414.9s\n",
      "[trial 08 | split 2] ep 6/6 loss=0.5899 val_dice_proxy=0.40529 time=432.3s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4504968632223682 | trial: 6 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "[trial 09 | split 3] ep 1/6 loss=1.3473 val_dice_proxy=0.29780 time=438.3s\n",
      "[trial 09 | split 3] ep 2/6 loss=1.1087 val_dice_proxy=0.33972 time=426.0s\n",
      "[trial 09 | split 3] ep 3/6 loss=1.0348 val_dice_proxy=0.36213 time=436.1s\n",
      "[trial 09 | split 3] ep 4/6 loss=0.9742 val_dice_proxy=0.39232 time=440.5s\n",
      "[trial 09 | split 3] ep 5/6 loss=0.9271 val_dice_proxy=0.41678 time=425.5s\n",
      "[trial 09 | split 3] ep 6/6 loss=0.8738 val_dice_proxy=0.44001 time=441.3s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4504968632223682 | trial: 6 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "[trial 10 | split 4] ep 1/6 loss=1.3059 val_dice_proxy=0.09213 time=266.8s\n",
      "[trial 10 | split 4] ep 2/6 loss=1.1218 val_dice_proxy=0.11136 time=297.7s\n",
      "[trial 10 | split 4] ep 3/6 loss=1.0726 val_dice_proxy=0.14760 time=265.6s\n",
      "[trial 10 | split 4] ep 4/6 loss=1.0379 val_dice_proxy=0.19211 time=294.3s\n",
      "[trial 10 | split 4] ep 5/6 loss=1.0110 val_dice_proxy=0.20980 time=275.6s\n",
      "[trial 10 | split 4] ep 6/6 loss=0.9848 val_dice_proxy=0.23252 time=285.2s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4504968632223682 | trial: 6 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "[trial 11 | split 0] ep 1/6 loss=1.2332 val_dice_proxy=0.26599 time=631.2s\n",
      "[trial 11 | split 0] ep 2/6 loss=1.0065 val_dice_proxy=0.31916 time=638.4s\n",
      "[trial 11 | split 0] ep 3/6 loss=0.9164 val_dice_proxy=0.34198 time=624.8s\n",
      "[trial 11 | split 0] ep 4/6 loss=0.8443 val_dice_proxy=0.38934 time=634.3s\n",
      "[trial 11 | split 0] ep 5/6 loss=0.7885 val_dice_proxy=0.40732 time=637.4s\n",
      "[trial 11 | split 0] ep 6/6 loss=0.7293 val_dice_proxy=0.40751 time=636.9s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4504968632223682 | trial: 6 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "[trial 12 | split 1] ep 1/6 loss=0.8499 val_dice_proxy=0.17910 time=424.9s\n",
      "[trial 12 | split 1] ep 2/6 loss=0.6860 val_dice_proxy=0.26387 time=432.5s\n",
      "[trial 12 | split 1] ep 3/6 loss=0.6467 val_dice_proxy=0.31619 time=436.2s\n",
      "[trial 12 | split 1] ep 4/6 loss=0.6239 val_dice_proxy=0.35485 time=417.6s\n",
      "[trial 12 | split 1] ep 5/6 loss=0.5928 val_dice_proxy=0.37760 time=441.4s\n",
      "[trial 12 | split 1] ep 6/6 loss=0.5637 val_dice_proxy=0.38784 time=433.4s\n",
      "------------------------------------------------------------\n",
      "CURRENT BEST: 0.4504968632223682 | trial: 6 | val_split: 0\n",
      "------------------------------------------------------------\n",
      "DONE in 32250.4s\n",
      "Saved: /kaggle/working/recodai_luc_hybrid_opt/trials.csv\n",
      "Saved: /kaggle/working/recodai_luc_hybrid_opt/best_config.json\n",
      "Saved: /kaggle/working/recodai_luc_hybrid_opt/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE — Optimize Hybrid UNet(+ASPP) Token-Decoder + Gate Head (ONE CELL)\n",
    "# REVISI FULL (NO-FOLD, TOKEN/MATCH PATH AUTO-RESOLVE via FILE INDEX, FAIL-SAFE SEED)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time, random, warnings, re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ----------------------------\n",
    "# Config (env override)\n",
    "# ----------------------------\n",
    "OUT_DIR = Path(\"/kaggle/working/recodai_luc_hybrid_opt\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WORK = Path(\"/kaggle/working\")\n",
    "INP  = Path(\"/kaggle/input\")\n",
    "CACHE_DIR = Path(\"/kaggle/working/recodai_luc/cache\")\n",
    "\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "MAX_TRIALS = int(os.environ.get(\"MAX_TRIALS\", \"12\"))\n",
    "TRIAL_EPOCHS = int(os.environ.get(\"TRIAL_EPOCHS\", \"6\"))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"32\"))\n",
    "NUM_WORKERS = int(os.environ.get(\"NUM_WORKERS\", \"2\"))\n",
    "ACCUM_STEPS = int(os.environ.get(\"ACCUM_STEPS\", \"1\"))\n",
    "USE_AMP = bool(int(os.environ.get(\"USE_AMP\", \"0\")))  # CPU -> off\n",
    "EARLYSTOP_PATIENCE = int(os.environ.get(\"EARLYSTOP_PATIENCE\", \"2\"))\n",
    "\n",
    "N_SPLITS = int(os.environ.get(\"N_SPLITS\", \"5\"))\n",
    "VAL_SPLIT_ROTATE = bool(int(os.environ.get(\"VAL_SPLIT_ROTATE\", \"1\")))\n",
    "\n",
    "LR_RANGE = (2e-4, 2e-3)\n",
    "WD_RANGE = (0.0, 0.05)\n",
    "DROPOUT_RANGE = (0.0, 0.25)\n",
    "BASE_CH_CHOICES = [64, 96, 128]\n",
    "LAMBDA_SEG_RANGE = (0.6, 1.2)\n",
    "LAMBDA_CLS_RANGE = (0.3, 0.9)\n",
    "FOCAL_GAMMA_CHOICES = [0.0, 1.0, 2.0]\n",
    "\n",
    "T1_RANGE = (0.50, 0.65)\n",
    "T0_RANGE = (0.25, 0.45)\n",
    "SEED_DILATE_CHOICES = [0, 1, 2]\n",
    "THR_GATE_RANGE = (0.20, 0.80)\n",
    "\n",
    "MIN_TOK_AREA_CHOICES = [1, 2, 3]\n",
    "MAX_TOK_AREA_FRAC_CHOICES = [0.70, 0.80, 0.90]\n",
    "MAX_INST_KEEP_CHOICES = [4, 8, 12]\n",
    "\n",
    "MIN_PEAK_SCORE_KEEP_CHOICES = [5, 6, 7]\n",
    "MIN_AREA_FRAC_KEEP_CHOICES = [0.0003, 0.0005, 0.0010]\n",
    "\n",
    "# ----------------------------\n",
    "# Repro + device\n",
    "# ----------------------------\n",
    "def seed_everything(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "\n",
    "seed_everything(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "amp_ok = (USE_AMP and device.type == \"cuda\")\n",
    "print(\"DEVICE:\", device, \"| AMP:\", amp_ok)\n",
    "\n",
    "# ----------------------------\n",
    "# Find competition root\n",
    "# ----------------------------\n",
    "def find_comp_root():\n",
    "    cand = INP / \"recodai-luc-scientific-image-forgery-detection\"\n",
    "    if cand.exists() and (cand / \"train_images\").exists() and (cand / \"sample_submission.csv\").exists():\n",
    "        return cand\n",
    "    for d in sorted(INP.glob(\"*\")):\n",
    "        if d.is_dir() and (d / \"train_images\").exists() and (d / \"sample_submission.csv\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Cannot find competition dataset under /kaggle/input.\")\n",
    "\n",
    "COMP_ROOT = find_comp_root()\n",
    "TRAIN_MASK_DIR = COMP_ROOT / \"train_masks\"\n",
    "SUP_MASK_DIR   = COMP_ROOT / \"supplemental_masks\"\n",
    "print(\"COMP_ROOT     :\", COMP_ROOT)\n",
    "print(\"TRAIN_MASK_DIR:\", TRAIN_MASK_DIR, \"| exists:\", TRAIN_MASK_DIR.exists())\n",
    "print(\"SUP_MASK_DIR  :\", SUP_MASK_DIR,   \"| exists:\", SUP_MASK_DIR.exists())\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def infer_case_id_any(x):\n",
    "    if x is None:\n",
    "        return np.nan\n",
    "    s = str(x)\n",
    "    if s.lower() in [\"nan\",\"none\",\"\"]:\n",
    "        return np.nan\n",
    "    m = re.search(r\"\\b(\\d{3,})\\b\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "    m = re.search(r\"(\\d+)\", s)\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "def build_npz_index(root: Path):\n",
    "    \"\"\"\n",
    "    Build filename -> fullpath mapping from existing npz files under root.\n",
    "    Prefer shallower paths; keep latest mtime if duplicates.\n",
    "    \"\"\"\n",
    "    if root is None or (not root.exists()):\n",
    "        return {}, []\n",
    "    files = list(root.glob(\"**/*.npz\"))\n",
    "    mp = {}\n",
    "    for p in files:\n",
    "        bn = p.name\n",
    "        if bn not in mp:\n",
    "            mp[bn] = p\n",
    "        else:\n",
    "            try:\n",
    "                if p.stat().st_mtime > mp[bn].stat().st_mtime:\n",
    "                    mp[bn] = p\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {k: str(v) for k,v in mp.items()}, files\n",
    "\n",
    "def resolve_by_index(p, root: Path, idx_map: dict):\n",
    "    if p is None:\n",
    "        return None\n",
    "    s = str(p)\n",
    "    if s.lower() in [\"nan\",\"none\",\"\"]:\n",
    "        return None\n",
    "    pp = Path(s)\n",
    "    if pp.exists():\n",
    "        return str(pp)\n",
    "\n",
    "    # relative under root\n",
    "    if root is not None and root.exists() and (not pp.is_absolute()):\n",
    "        cand = root / pp\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    # fallback basename lookup\n",
    "    bn = pp.name\n",
    "    if bn in idx_map:\n",
    "        return idx_map[bn]\n",
    "\n",
    "    # last: try common subdirs\n",
    "    if root is not None and root.exists():\n",
    "        for seg in [\"train\", \"test\", \"train_all\", \"test_all\", \"\"]:\n",
    "            cand = (root / seg / bn) if seg else (root / bn)\n",
    "            if cand.exists():\n",
    "                return str(cand)\n",
    "\n",
    "    return None\n",
    "\n",
    "def pick_best_path_col(df, candidates, root: Path, idx_map: dict):\n",
    "    best = (None, -1.0)\n",
    "    for c in candidates:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        rr = df[c].map(lambda x: resolve_by_index(x, root, idx_map))\n",
    "        ex = rr.map(lambda x: isinstance(x,str) and Path(x).exists()).mean() if len(rr) else 0.0\n",
    "        if ex > best[1]:\n",
    "            best = (c, float(ex))\n",
    "    return best[0], best[1]\n",
    "\n",
    "# ----------------------------\n",
    "# Locate tokens_manifest_train.parquet (BEST by real token coverage)\n",
    "# + Resolve TOKEN_DATA_ROOT even if manifest is under *_bind_* (no npz)\n",
    "# ----------------------------\n",
    "def _count_tok_files(root: Path):\n",
    "    if root is None or (not root.exists()):\n",
    "        return 0\n",
    "    total = 0\n",
    "    for sub in [\"train\",\"test\",\"train_all\",\"test_all\"]:\n",
    "        d = root / sub\n",
    "        if d.exists():\n",
    "            total += sum(1 for _ in d.glob(\"*.npz\"))\n",
    "            total += sum(1 for _ in d.glob(\"*.npy\"))\n",
    "    return total\n",
    "\n",
    "def pick_best_tokens_manifest_train():\n",
    "    hits = []\n",
    "    hits += list(WORK.glob(\"**/tokens_manifest_train.parquet\"))\n",
    "    hits += list(INP.glob(\"**/tokens_manifest_train.parquet\"))\n",
    "    hits = [p for p in hits if p.exists()]\n",
    "    if not hits:\n",
    "        return None\n",
    "\n",
    "    # score: prefer parent that actually has token files in train/test dirs\n",
    "    def score(p: Path):\n",
    "        r = p.parent\n",
    "        n_tok = _count_tok_files(r)\n",
    "        has_train_dir = int((r/\"train\").exists() or (r/\"train_all\").exists())\n",
    "        # (has_tokens, n_tokens, has_train_dir, mtime)\n",
    "        return (int(n_tok > 0), int(n_tok), has_train_dir, float(p.stat().st_mtime))\n",
    "\n",
    "    hits = sorted(hits, key=score, reverse=True)\n",
    "    return hits[0]\n",
    "\n",
    "TOK_MAN_TRAIN = pick_best_tokens_manifest_train()\n",
    "if TOK_MAN_TRAIN is None:\n",
    "    raise FileNotFoundError(\"tokens_manifest_train.parquet not found. Token cache must exist.\")\n",
    "\n",
    "TOKEN_MANIFEST_ROOT = TOK_MAN_TRAIN.parent\n",
    "\n",
    "# base cache is one level above token cfg dir (the folder that contains dinov2_base_... siblings)\n",
    "BASE_CACHE = TOKEN_MANIFEST_ROOT.parent\n",
    "if not BASE_CACHE.exists():\n",
    "    # fallback for odd layouts\n",
    "    BASE_CACHE = Path(\"/kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache\")\n",
    "\n",
    "# pick best TOKEN_DATA_ROOT among siblings under BASE_CACHE\n",
    "sib_dirs = [p for p in BASE_CACHE.iterdir() if p.is_dir()]\n",
    "sib_scored = sorted([(_count_tok_files(d), d) for d in sib_dirs], key=lambda x: x[0], reverse=True)\n",
    "best_cnt, best_root = sib_scored[0] if sib_scored else (0, None)\n",
    "\n",
    "# If manifest root has 0 tokens, switch data root to sibling with tokens\n",
    "manifest_cnt = _count_tok_files(TOKEN_MANIFEST_ROOT)\n",
    "TOKEN_DATA_ROOT = TOKEN_MANIFEST_ROOT if manifest_cnt > 0 else best_root\n",
    "\n",
    "print(\"TOK_MAN_TRAIN       :\", TOK_MAN_TRAIN)\n",
    "print(\"TOKEN_MANIFEST_ROOT :\", TOKEN_MANIFEST_ROOT, \"| token_files:\", manifest_cnt)\n",
    "print(\"BASE_CACHE          :\", BASE_CACHE)\n",
    "print(\"TOKEN_DATA_ROOT     :\", TOKEN_DATA_ROOT, \"| token_files:\", best_cnt)\n",
    "\n",
    "if TOKEN_DATA_ROOT is None or (not Path(TOKEN_DATA_ROOT).exists()) or _count_tok_files(Path(TOKEN_DATA_ROOT)) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No token .npz/.npy found in any TOKEN_DATA_ROOT candidate.\\n\"\n",
    "        f\"Checked siblings under: {BASE_CACHE}\"\n",
    "    )\n",
    "\n",
    "# Build fast basename->fullpath index (DO NOT use **/*.npz on big trees)\n",
    "def build_tok_index(data_root: Path):\n",
    "    mp = {}\n",
    "    files = []\n",
    "    for sub in [\"train\",\"test\",\"train_all\",\"test_all\"]:\n",
    "        d = data_root / sub\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for p in d.glob(\"*.npz\"):\n",
    "            files.append(p); mp[p.name] = str(p)\n",
    "        for p in d.glob(\"*.npy\"):\n",
    "            files.append(p); mp[p.name] = str(p)\n",
    "    return mp, files\n",
    "\n",
    "tok_idx_map, tok_files = build_tok_index(Path(TOKEN_DATA_ROOT))\n",
    "print(\"TOKEN indexed files :\", len(tok_files))\n",
    "\n",
    "def resolve_tok_path(p):\n",
    "    if p is None:\n",
    "        return None\n",
    "    s = str(p)\n",
    "    if s.lower() in [\"nan\",\"none\",\"\"]:\n",
    "        return None\n",
    "    pp = Path(s)\n",
    "    if pp.exists():\n",
    "        return str(pp)\n",
    "\n",
    "    # try relative under TOKEN_DATA_ROOT\n",
    "    if not pp.is_absolute():\n",
    "        cand = Path(TOKEN_DATA_ROOT) / pp\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    # basename lookup\n",
    "    bn = pp.name\n",
    "    if bn in tok_idx_map:\n",
    "        return tok_idx_map[bn]\n",
    "\n",
    "    # last: try common subdirs\n",
    "    for sub in [\"train\",\"test\",\"train_all\",\"test_all\"]:\n",
    "        cand = Path(TOKEN_DATA_ROOT) / sub / bn\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Load train_table.parquet\n",
    "# ----------------------------\n",
    "table_candidates = [\n",
    "    WORK / \"recodai_luc_gate_artifacts\" / \"train_table.parquet\",\n",
    "    WORK / \"recodai_luc_hybrid_artifacts\" / \"train_table.parquet\",\n",
    "    WORK / \"recodai_luc_prof\" / \"train_table.parquet\",\n",
    "]\n",
    "TRAIN_TABLE = next((p for p in table_candidates if p.exists()), None)\n",
    "if TRAIN_TABLE is None:\n",
    "    raise FileNotFoundError(\"Cannot find train_table.parquet in known locations.\")\n",
    "\n",
    "df = pd.read_parquet(TRAIN_TABLE).copy()\n",
    "for need in [\"case_id\",\"y\"]:\n",
    "    if need not in df.columns:\n",
    "        raise ValueError(f\"train_table missing required col: {need}\")\n",
    "\n",
    "df[\"case_id\"] = pd.to_numeric(df[\"case_id\"], errors=\"coerce\")\n",
    "df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df = df[df[\"case_id\"].notna()].copy()\n",
    "df[\"case_id\"] = df[\"case_id\"].astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# Ensure token path column exists:\n",
    "#  - if missing in train_table -> join from tokens_manifest_train.parquet\n",
    "#  - auto-select correct path column in tokens manifest by existence-rate\n",
    "# ----------------------------\n",
    "tok_col = None\n",
    "for c in [\"tok_path\",\"token_path\",\"dino_path\",\"feat_path\",\"emb_path\",\"token_npz\",\"tok_npz\",\"npz_path\",\"path\",\"file\",\"npz\"]:\n",
    "    if c in df.columns:\n",
    "        tok_col = c\n",
    "        break\n",
    "\n",
    "df_tok = pd.read_parquet(TOK_MAN_TRAIN).copy()\n",
    "\n",
    "# ensure case_id in tokens manifest\n",
    "if \"case_id\" not in df_tok.columns:\n",
    "    if \"uid\" in df_tok.columns:\n",
    "        df_tok[\"case_id\"] = df_tok[\"uid\"].apply(infer_case_id_any)\n",
    "    else:\n",
    "        # try any column with digits\n",
    "        any_col = df_tok.columns[0]\n",
    "        df_tok[\"case_id\"] = df_tok[any_col].apply(infer_case_id_any)\n",
    "\n",
    "df_tok[\"case_id\"] = pd.to_numeric(df_tok[\"case_id\"], errors=\"coerce\")\n",
    "df_tok = df_tok[df_tok[\"case_id\"].notna()].copy()\n",
    "df_tok[\"case_id\"] = df_tok[\"case_id\"].astype(int)\n",
    "\n",
    "# pick best path column in tokens manifest\n",
    "tok_path_candidates = [\"tok_npz\",\"npz_path\",\"path\",\"file\",\"npz\",\"token_npz\"]\n",
    "best_col, best_rate = pick_best_path_col(df_tok, tok_path_candidates, TOKEN_ROOT, tok_idx_map)\n",
    "if best_col is None:\n",
    "    raise ValueError(\"Cannot detect token path column in tokens_manifest_train.parquet.\")\n",
    "\n",
    "df_tok[\"tok_npz\"] = df_tok[best_col].map(lambda x: resolve_by_index(x, TOKEN_ROOT, tok_idx_map))\n",
    "df_tok[\"_ok\"] = df_tok[\"tok_npz\"].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "\n",
    "# keep best per case_id\n",
    "def _mt(p):\n",
    "    try:\n",
    "        return float(Path(p).stat().st_mtime)\n",
    "    except Exception:\n",
    "        return -1.0\n",
    "\n",
    "df_tok[\"_mt\"] = df_tok[\"tok_npz\"].map(_mt)\n",
    "df_tok = df_tok.sort_values([\"case_id\",\"_ok\",\"_mt\"], ascending=[True, False, False]) \\\n",
    "               .drop_duplicates(\"case_id\", keep=\"first\") \\\n",
    "               .drop(columns=[\"_ok\",\"_mt\"])\n",
    "\n",
    "# if train_table doesn't have token col -> merge\n",
    "if tok_col is None:\n",
    "    df = df.merge(df_tok[[\"case_id\",\"tok_npz\"]], on=\"case_id\", how=\"left\")\n",
    "    tok_col = \"tok_npz\"\n",
    "    print(f\"INFO: token path column missing; joined from tokens manifest using '{best_col}' (exist-rate≈{best_rate:.3f})\")\n",
    "else:\n",
    "    # normalize existing tok_col via index too\n",
    "    df[tok_col] = df[tok_col].map(lambda x: resolve_by_index(x, TOKEN_ROOT, tok_idx_map))\n",
    "\n",
    "ok_tok = df[tok_col].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "print(\"Token exists rate:\", float(ok_tok.mean()), \"| rows:\", len(df))\n",
    "if ok_tok.mean() < 0.50:\n",
    "    # Hard fail with diagnostics (lebih jelas daripada rows_used=0)\n",
    "    print(\"DIAG: tokens manifest best_col =\", best_col, \"| best_rate≈\", best_rate)\n",
    "    print(\"DIAG: df_tok columns:\", df_tok.columns.tolist()[:50])\n",
    "    print(\"DIAG: sample df_tok tok_npz:\", df_tok[\"tok_npz\"].dropna().head(5).tolist())\n",
    "    raise RuntimeError(\n",
    "        \"Token exists rate is too low. This indicates path resolution mismatch.\\n\"\n",
    "        \"Check TOKEN_ROOT contents vs manifest paths.\"\n",
    "    )\n",
    "\n",
    "df = df[ok_tok].copy()\n",
    "df = df.drop_duplicates(\"case_id\", keep=\"first\").reset_index(drop=True)\n",
    "print(\"TRAIN_TABLE:\", TRAIN_TABLE, \"| rows_used:\", len(df), \"| pos_rate:\", float(df[\"y\"].mean()))\n",
    "print(\"TOK_COL:\", tok_col)\n",
    "\n",
    "# ----------------------------\n",
    "# MATCH ROOT (optional; fail-safe)\n",
    "# ----------------------------\n",
    "def pick_latest_match_root():\n",
    "    cands = []\n",
    "    cands += list(CACHE_DIR.glob(\"match_cfg_*\"))\n",
    "    cands += list(INP.glob(\"**/recodai_luc/cache/match_cfg_*\"))\n",
    "    cands = [c for c in cands if (c/\"cfg.json\").exists() and (c/\"match_manifest_train.parquet\").exists()]\n",
    "    if not cands:\n",
    "        return None\n",
    "    cands = sorted(cands, key=lambda p: (p/\"cfg.json\").stat().st_mtime, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "MATCH_ROOT = pick_latest_match_root()\n",
    "match_map = {}\n",
    "score_map = None\n",
    "PATCH = 14\n",
    "HTOK_MATCH = WTOK_MATCH = None\n",
    "\n",
    "if MATCH_ROOT is None:\n",
    "    print(\"WARNING: MATCH_ROOT not found. Seed will be zeros (still runs).\")\n",
    "else:\n",
    "    try:\n",
    "        MATCH_CFG = json.loads((MATCH_ROOT/\"cfg.json\").read_text())\n",
    "        PATCH = int(MATCH_CFG.get(\"patch\", MATCH_CFG.get(\"patch_size\", 14)))\n",
    "        HTOK_MATCH = int(MATCH_CFG.get(\"Ht\", MATCH_CFG.get(\"htok\", MATCH_CFG.get(\"HTOK\", 37))))\n",
    "        WTOK_MATCH = int(MATCH_CFG.get(\"Wt\", MATCH_CFG.get(\"wtok\", MATCH_CFG.get(\"WTOK\", 37))))\n",
    "\n",
    "        mtrain_pq = MATCH_ROOT / \"match_manifest_train.parquet\"\n",
    "        df_m = pd.read_parquet(mtrain_pq).copy()\n",
    "\n",
    "        # ensure case_id\n",
    "        if \"case_id\" not in df_m.columns:\n",
    "            if \"uid\" in df_m.columns:\n",
    "                df_m[\"case_id\"] = df_m[\"uid\"].apply(infer_case_id_any)\n",
    "            else:\n",
    "                # try from any path col later\n",
    "                df_m[\"case_id\"] = np.nan\n",
    "        df_m[\"case_id\"] = pd.to_numeric(df_m[\"case_id\"], errors=\"coerce\")\n",
    "        df_m = df_m[df_m[\"case_id\"].notna()].copy()\n",
    "        df_m[\"case_id\"] = df_m[\"case_id\"].astype(int)\n",
    "\n",
    "        # build index for match npz\n",
    "        match_idx_map, match_files = build_npz_index(MATCH_ROOT)\n",
    "        # pick best path column\n",
    "        match_path_candidates = [\"match_npz\",\"npz_path\",\"path\",\"file\",\"npz\"]\n",
    "        best_mcol, best_mrate = pick_best_path_col(df_m, match_path_candidates, MATCH_ROOT, match_idx_map)\n",
    "        if best_mcol is None:\n",
    "            raise ValueError(\"Cannot detect match path column in match_manifest_train.parquet.\")\n",
    "\n",
    "        df_m[\"match_npz\"] = df_m[best_mcol].map(lambda x: resolve_by_index(x, MATCH_ROOT, match_idx_map))\n",
    "        df_m[\"_ok\"] = df_m[\"match_npz\"].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "        df_m = df_m[df_m[\"_ok\"]].copy()\n",
    "\n",
    "        # pick best per case_id (score or mtime)\n",
    "        score_cols = [c for c in [\"best_peak_score\",\"peak_score_max\",\"max_peak_score\",\"score_max\",\"best_score\",\"peak_max\"] if c in df_m.columns]\n",
    "        if score_cols:\n",
    "            sc = score_cols[0]\n",
    "            df_m[sc] = pd.to_numeric(df_m[sc], errors=\"coerce\").fillna(-1)\n",
    "            df_m = df_m.sort_values([\"case_id\", sc], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\n",
    "            score_map = df_m.set_index(\"case_id\")[sc].to_dict()\n",
    "        else:\n",
    "            df_m[\"_mt\"] = df_m[\"match_npz\"].map(lambda p: Path(p).stat().st_mtime if Path(p).exists() else -1.0)\n",
    "            df_m = df_m.sort_values([\"case_id\",\"_mt\"], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\n",
    "\n",
    "        match_map = df_m.set_index(\"case_id\")[\"match_npz\"].to_dict()\n",
    "\n",
    "        print(\"MATCH_ROOT:\", MATCH_ROOT)\n",
    "        print(\"MATCH npz coverage:\", len(match_map), \"/\", df[\"case_id\"].nunique())\n",
    "        print(\"PATCH/HTOK_MATCH/WTOK_MATCH:\", PATCH, HTOK_MATCH, WTOK_MATCH)\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: match manifest failed -> seed zeros. Reason:\", repr(e))\n",
    "        MATCH_ROOT = None\n",
    "        match_map = {}\n",
    "        score_map = None\n",
    "        PATCH = 14\n",
    "        HTOK_MATCH = WTOK_MATCH = None\n",
    "\n",
    "# ----------------------------\n",
    "# Infer token grid dims + embedding dim\n",
    "# ----------------------------\n",
    "def load_tok_npz_any(path_str: str):\n",
    "    z = np.load(path_str)\n",
    "    keys = list(z.files)\n",
    "    if not keys:\n",
    "        raise ValueError(\"empty npz\")\n",
    "    pref = [\"tok\",\"tokens\",\"grid\",\"token_grid\",\"x\",\"feat\",\"emb\",\"f\"]\n",
    "    a = None\n",
    "    for k in pref:\n",
    "        if k in z.files:\n",
    "            a = z[k]; break\n",
    "    if a is None:\n",
    "        a = z[keys[0]]\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim != 3:\n",
    "        raise ValueError(f\"Unknown token array shape: {a.shape}\")\n",
    "    return a.astype(np.float32)\n",
    "\n",
    "tok0 = load_tok_npz_any(df.iloc[0][tok_col])\n",
    "# normalize dim inference\n",
    "if tok0.ndim == 3 and tok0.shape[0] < 200 and tok0.shape[1] < 200 and tok0.shape[2] > 32:\n",
    "    # likely (Ht,Wt,D)\n",
    "    HTOK, WTOK, D_TOK = int(tok0.shape[0]), int(tok0.shape[1]), int(tok0.shape[2])\n",
    "elif tok0.ndim == 3 and tok0.shape[0] > 32 and tok0.shape[1] < 200 and tok0.shape[2] < 200:\n",
    "    # maybe (D,H,W)\n",
    "    D_TOK, HTOK, WTOK = int(tok0.shape[0]), int(tok0.shape[1]), int(tok0.shape[2])\n",
    "else:\n",
    "    # fallback assume (Ht,Wt,D)\n",
    "    HTOK, WTOK, D_TOK = int(tok0.shape[0]), int(tok0.shape[1]), int(tok0.shape[2])\n",
    "\n",
    "D_IN = D_TOK + 1\n",
    "print(\"TOKEN GRID:\", (HTOK, WTOK), \"| D:\", D_TOK, \"| Input channels:\", D_IN)\n",
    "\n",
    "# ----------------------------\n",
    "# SciPy optional\n",
    "# ----------------------------\n",
    "try:\n",
    "    import scipy.ndimage as ndi\n",
    "    _HAS_SCIPY = True\n",
    "except Exception:\n",
    "    _HAS_SCIPY = False\n",
    "\n",
    "# ----------------------------\n",
    "# GT/Seed caches\n",
    "# ----------------------------\n",
    "_gt_tok_cache = {}\n",
    "_seed_tok_cache = {}\n",
    "\n",
    "def _find_mask_files(mask_dir: Path, case_id: int):\n",
    "    if mask_dir is None or (not mask_dir.exists()):\n",
    "        return []\n",
    "    cid = str(int(case_id))\n",
    "    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n",
    "    pats = [f\"{cid}*.png\", f\"{cid}*.jpg\", f\"{cid}*.jpeg\", f\"{cid}*.tif\", f\"{cid}*.tiff\", f\"{cid}*.bmp\",\n",
    "            f\"{cid}__*.png\", f\"{cid}_*.png\"]\n",
    "    out, seen = [], set()\n",
    "    for pat in pats:\n",
    "        for p in mask_dir.glob(pat):\n",
    "            if p.suffix.lower() in exts:\n",
    "                s = str(p)\n",
    "                if s not in seen:\n",
    "                    out.append(p); seen.add(s)\n",
    "    return sorted(out)\n",
    "\n",
    "def load_gt_union_full(case_id: int):\n",
    "    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n",
    "        if d is None or (not d.exists()):\n",
    "            continue\n",
    "        npy = d / f\"{int(case_id)}.npy\"\n",
    "        if npy.exists():\n",
    "            a = np.load(npy, mmap_mode=\"r\")\n",
    "            a = np.asarray(a)\n",
    "            if a.ndim == 2:\n",
    "                return (a > 0)\n",
    "            if a.ndim == 3:\n",
    "                return (a > 0).any(axis=0)\n",
    "\n",
    "    files = []\n",
    "    files += _find_mask_files(TRAIN_MASK_DIR, case_id)\n",
    "    files += _find_mask_files(SUP_MASK_DIR, case_id)\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    m = None\n",
    "    for p in files:\n",
    "        try:\n",
    "            im = Image.open(p).convert(\"L\")\n",
    "            a = (np.asarray(im) > 0)\n",
    "            m = a if m is None else (m | a)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return m\n",
    "\n",
    "def downsample_bool_to_tok(mask_bool: np.ndarray, h=HTOK, w=WTOK):\n",
    "    if mask_bool is None:\n",
    "        return np.zeros((h,w), dtype=np.float32)\n",
    "    im = Image.fromarray((mask_bool.astype(np.uint8)*255))\n",
    "    im = im.resize((w, h), resample=Image.NEAREST)\n",
    "    return (np.asarray(im) > 127).astype(np.float32)\n",
    "\n",
    "def get_gt_tok(case_id: int):\n",
    "    if case_id in _gt_tok_cache:\n",
    "        return _gt_tok_cache[case_id]\n",
    "    gt_full = load_gt_union_full(case_id)\n",
    "    gt_tok = downsample_bool_to_tok(gt_full, HTOK, WTOK)\n",
    "    _gt_tok_cache[case_id] = gt_tok\n",
    "    return gt_tok\n",
    "\n",
    "def resize_bool_grid(x_bool: np.ndarray, h, w):\n",
    "    if x_bool is None:\n",
    "        return np.zeros((h,w), dtype=bool)\n",
    "    if x_bool.shape == (h,w):\n",
    "        return x_bool.astype(bool)\n",
    "    im = Image.fromarray((x_bool.astype(np.uint8)*255))\n",
    "    im = im.resize((w, h), resample=Image.NEAREST)\n",
    "    return (np.asarray(im) > 127)\n",
    "\n",
    "def load_seed_tok(case_id: int, topk=8):\n",
    "    if case_id in _seed_tok_cache:\n",
    "        return _seed_tok_cache[case_id]\n",
    "\n",
    "    p = match_map.get(int(case_id), None)\n",
    "    if p is None or (not Path(p).exists()):\n",
    "        out = (np.zeros((HTOK,WTOK), dtype=np.float32), 0)\n",
    "        _seed_tok_cache[case_id] = out\n",
    "        return out\n",
    "\n",
    "    try:\n",
    "        z = np.load(p)\n",
    "    except Exception:\n",
    "        out = (np.zeros((HTOK,WTOK), dtype=np.float32), 0)\n",
    "        _seed_tok_cache[case_id] = out\n",
    "        return out\n",
    "\n",
    "    scores = None\n",
    "    for k in [\"peak_score\",\"scores\",\"score\",\"peak_scores\"]:\n",
    "        if k in z.files:\n",
    "            scores = np.asarray(z[k]).reshape(-1)\n",
    "            break\n",
    "\n",
    "    src = tgt = None\n",
    "    for a,b in [(\"src_masks\",\"tgt_masks\"), (\"src_m\",\"tgt_m\"), (\"src\",\"tgt\")]:\n",
    "        if a in z.files and b in z.files:\n",
    "            src = np.asarray(z[a]); tgt = np.asarray(z[b])\n",
    "            break\n",
    "\n",
    "    best = int(np.max(scores)) if scores is not None and len(scores) else 0\n",
    "\n",
    "    if src is None or tgt is None or src.ndim != 3 or tgt.ndim != 3 or src.shape[0] == 0:\n",
    "        out = (np.zeros((HTOK,WTOK), dtype=np.float32), best)\n",
    "        _seed_tok_cache[case_id] = out\n",
    "        return out\n",
    "\n",
    "    if scores is not None and len(scores) == src.shape[0]:\n",
    "        idx = np.argsort(scores)[::-1][:min(topk, len(scores))]\n",
    "        src = src[idx]; tgt = tgt[idx]\n",
    "    else:\n",
    "        if src.shape[0] > topk:\n",
    "            src = src[:topk]; tgt = tgt[:topk]\n",
    "\n",
    "    seed_match = ((src>0) | (tgt>0)).any(axis=0).astype(bool)\n",
    "\n",
    "    # if match grid differs, resize to token grid\n",
    "    if seed_match.shape != (HTOK,WTOK):\n",
    "        seed_tok = resize_bool_grid(seed_match, HTOK, WTOK).astype(np.float32)\n",
    "    else:\n",
    "        seed_tok = seed_match.astype(np.float32)\n",
    "\n",
    "    out = (seed_tok, best)\n",
    "    _seed_tok_cache[case_id] = out\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Token loader normalized to (HTOK,WTOK,D)\n",
    "# ----------------------------\n",
    "def load_tok_npz(path_str: str):\n",
    "    a = load_tok_npz_any(path_str)\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim != 3:\n",
    "        raise ValueError(f\"Bad token array ndim: {a.ndim}\")\n",
    "\n",
    "    # (Ht,Wt,D)\n",
    "    if a.shape[0] == HTOK and a.shape[1] == WTOK:\n",
    "        return a.astype(np.float32)\n",
    "\n",
    "    # (D,Ht,Wt)\n",
    "    if a.shape[1] == HTOK and a.shape[2] == WTOK:\n",
    "        return np.transpose(a, (1,2,0)).astype(np.float32)\n",
    "\n",
    "    # resize each channel if (H,W,D) mismatched\n",
    "    H0,W0,D0 = a.shape\n",
    "    out = np.zeros((HTOK,WTOK,D0), np.float32)\n",
    "    for d in range(D0):\n",
    "        im = Image.fromarray(a[:,:,d].astype(np.float32))\n",
    "        im = im.resize((WTOK,HTOK), resample=Image.BILINEAR)\n",
    "        out[:,:,d] = np.asarray(im).astype(np.float32)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "class HybridTokDS(Dataset):\n",
    "    def __init__(self, df_in: pd.DataFrame, tok_col: str):\n",
    "        self.df = df_in.reset_index(drop=True)\n",
    "        self.tok_col = tok_col\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        cid = int(r[\"case_id\"])\n",
    "        y = float(r[\"y\"])\n",
    "        tok = load_tok_npz(r[self.tok_col])  # (Ht,Wt,D)\n",
    "        seed, best_score = load_seed_tok(cid, topk=8)\n",
    "        gt_tok = get_gt_tok(cid)\n",
    "\n",
    "        tok_ch = np.transpose(tok, (2,0,1))  # (D,H,W)\n",
    "        x = np.concatenate([tok_ch, seed[None,:,:]], axis=0).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"x\": torch.from_numpy(x),\n",
    "            \"gt\": torch.from_numpy(gt_tok[None,:,:].astype(np.float32)),\n",
    "            \"y\": torch.tensor([y], dtype=torch.float32),\n",
    "            \"best_score\": torch.tensor([float(best_score)], dtype=torch.float32),\n",
    "            \"case_id\": torch.tensor([cid], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "# ----------------------------\n",
    "# Internal splits (NO-FOLD)\n",
    "# ----------------------------\n",
    "base = df[[\"case_id\",\"y\"]].copy().reset_index(drop=True)\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "split_id = np.full(len(base), -1, dtype=np.int32)\n",
    "X_dummy = np.zeros(len(base), dtype=np.uint8)\n",
    "for k, (_, va) in enumerate(skf.split(X_dummy, base[\"y\"].values)):\n",
    "    split_id[va] = k\n",
    "df = df.merge(pd.DataFrame({\"case_id\": base[\"case_id\"].values, \"split\": split_id}), on=\"case_id\", how=\"left\")\n",
    "df[\"split\"] = df[\"split\"].astype(int)\n",
    "split_ids = sorted(df[\"split\"].unique().tolist())\n",
    "print(\"Internal splits:\", split_ids, \"| N_SPLITS:\", N_SPLITS)\n",
    "\n",
    "# ----------------------------\n",
    "# Morphology/CC + Dice\n",
    "# ----------------------------\n",
    "def dilate_tok(x_bool, it=1):\n",
    "    if it <= 0: return x_bool.astype(bool)\n",
    "    x = x_bool.astype(bool)\n",
    "    if _HAS_SCIPY:\n",
    "        return ndi.binary_dilation(x, iterations=it)\n",
    "    for _ in range(it):\n",
    "        xp = np.pad(x, 1, mode=\"constant\", constant_values=False)\n",
    "        y = np.zeros_like(x, dtype=bool)\n",
    "        for dy in (-1,0,1):\n",
    "            for dx in (-1,0,1):\n",
    "                y |= xp[1+dy:1+dy+x.shape[0], 1+dx:1+dx+x.shape[1]]\n",
    "        x = y\n",
    "    return x\n",
    "\n",
    "def label_cc(x_bool):\n",
    "    x = x_bool.astype(bool)\n",
    "    if _HAS_SCIPY:\n",
    "        lab, n = ndi.label(x, structure=np.ones((3,3), dtype=np.uint8))\n",
    "        return lab, int(n)\n",
    "    H,W = x.shape\n",
    "    lab = np.zeros((H,W), dtype=np.int32); cur=0\n",
    "    for y0 in range(H):\n",
    "        for x0 in range(W):\n",
    "            if (not x[y0,x0]) or lab[y0,x0]!=0: continue\n",
    "            cur += 1\n",
    "            st=[(y0,x0)]; lab[y0,x0]=cur\n",
    "            while st:\n",
    "                yy,xx=st.pop()\n",
    "                for dy in (-1,0,1):\n",
    "                    for dx in (-1,0,1):\n",
    "                        if dy==0 and dx==0: continue\n",
    "                        ny,nx=yy+dy,xx+dx\n",
    "                        if 0<=ny<H and 0<=nx<W and x[ny,nx] and lab[ny,nx]==0:\n",
    "                            lab[ny,nx]=cur; st.append((ny,nx))\n",
    "    return lab, int(cur)\n",
    "\n",
    "def inst_split_union_tok(mask_bool, min_area=2, max_area_frac=0.8, max_keep=8):\n",
    "    H,W = mask_bool.shape\n",
    "    lab,n = label_cc(mask_bool)\n",
    "    if n<=0:\n",
    "        return np.zeros((H,W), dtype=bool), 0\n",
    "    insts=[]; areas=[]\n",
    "    for k in range(1,n+1):\n",
    "        m = (lab==k)\n",
    "        a = int(m.sum())\n",
    "        if a < min_area: continue\n",
    "        if a / float(H*W) > max_area_frac: continue\n",
    "        insts.append(m); areas.append(a)\n",
    "    if not insts:\n",
    "        return np.zeros((H,W), dtype=bool), 0\n",
    "    order = np.argsort(np.asarray(areas))[::-1][:max_keep]\n",
    "    uni = np.zeros((H,W), dtype=bool)\n",
    "    for i in order:\n",
    "        uni |= insts[i]\n",
    "    return uni, int(len(order))\n",
    "\n",
    "def dice(pr_bool, gt_bool):\n",
    "    a=int(pr_bool.sum()); b=int(gt_bool.sum())\n",
    "    if a==0 and b==0: return 1.0\n",
    "    if a==0 or b==0: return 0.0\n",
    "    inter=int((pr_bool & gt_bool).sum())\n",
    "    return float((2.0*inter)/(a+b))\n",
    "\n",
    "# ----------------------------\n",
    "# Model (odd-grid safe)\n",
    "# ----------------------------\n",
    "def _best_gn_groups(c, max_groups=32):\n",
    "    for g in [32,16,8,4,2,1]:\n",
    "        if g <= max_groups and c % g == 0:\n",
    "            return g\n",
    "    return 1\n",
    "\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, s=1, p=1, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c_in, c_out, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.drop = nn.Dropout2d(drop) if drop > 0 else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.act(self.bn(self.conv(x))))\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, c_in, c_out, rates=(1,2,4)):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, 3, padding=r, dilation=r, bias=False),\n",
    "                nn.BatchNorm2d(c_out),\n",
    "                nn.SiLU(inplace=True),\n",
    "            ) for r in rates\n",
    "        ])\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(len(rates)*c_out, c_out, 1, bias=False),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        ys = [b(x) for b in self.blocks]\n",
    "        y = torch.cat(ys, dim=1)\n",
    "        return self.proj(y)\n",
    "\n",
    "class HybridUNet(nn.Module):\n",
    "    def __init__(self, c_in, base_ch=96, drop=0.1):\n",
    "        super().__init__()\n",
    "        c1, c2, c3 = base_ch, base_ch*2, base_ch*3\n",
    "        self.in_norm = nn.GroupNorm(_best_gn_groups(c_in), c_in)\n",
    "\n",
    "        self.e1 = nn.Sequential(ConvBNAct(c_in, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n",
    "        self.p1 = nn.MaxPool2d(2)\n",
    "        self.e2 = nn.Sequential(ConvBNAct(c1, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n",
    "        self.p2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.e3 = nn.Sequential(ConvBNAct(c2, c3, drop=drop), ConvBNAct(c3, c3, drop=drop))\n",
    "        self.aspp = ASPP(c3, c3, rates=(1,2,4))\n",
    "\n",
    "        self.u2 = nn.ConvTranspose2d(c3, c2, 2, stride=2)\n",
    "        self.d2 = nn.Sequential(ConvBNAct(c2+c2, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n",
    "\n",
    "        self.u1 = nn.ConvTranspose2d(c2, c1, 2, stride=2)\n",
    "        self.d1 = nn.Sequential(ConvBNAct(c1+c1, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n",
    "\n",
    "        self.seg_head = nn.Conv2d(c1, 1, 1)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c3, c3//2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(drop if drop>0 else 0.0),\n",
    "            nn.Linear(c3//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_norm(x)\n",
    "        e1 = self.e1(x)\n",
    "        e2 = self.e2(self.p1(e1))\n",
    "        e3 = self.e3(self.p2(e2))\n",
    "        b  = self.aspp(e3)\n",
    "\n",
    "        cls_logit = self.cls_head(b)\n",
    "\n",
    "        d2 = self.u2(b)\n",
    "        if d2.shape[-2:] != e2.shape[-2:]:\n",
    "            d2 = F.interpolate(d2, size=e2.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        d2 = self.d2(torch.cat([d2, e2], dim=1))\n",
    "\n",
    "        d1 = self.u1(d2)\n",
    "        if d1.shape[-2:] != e1.shape[-2:]:\n",
    "            d1 = F.interpolate(d1, size=e1.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        d1 = self.d1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        seg_logit = self.seg_head(d1)\n",
    "        return seg_logit, cls_logit\n",
    "\n",
    "# ----------------------------\n",
    "# Loss\n",
    "# ----------------------------\n",
    "def dice_loss_from_logits(logits, target, eps=1e-6):\n",
    "    p = torch.sigmoid(logits)\n",
    "    inter = (p * target).sum(dim=(2,3))\n",
    "    den = (p + target).sum(dim=(2,3)) + eps\n",
    "    d = (2.0 * inter) / den\n",
    "    return 1.0 - d.mean()\n",
    "\n",
    "def bce_focal_from_logits(logits, target, gamma=2.0):\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, target, reduction=\"none\")\n",
    "    if gamma <= 0:\n",
    "        return bce.mean()\n",
    "    p = torch.sigmoid(logits)\n",
    "    pt = target * p + (1-target) * (1-p)\n",
    "    w = (1-pt).pow(gamma)\n",
    "    return (w * bce).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, dl, cfg_pp):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    T1 = cfg_pp[\"T1\"]; T0 = cfg_pp[\"T0\"]; dil_it = cfg_pp[\"seed_dilate_it\"]\n",
    "    thr_gate = cfg_pp[\"thr_gate\"]\n",
    "    min_tok_area = cfg_pp[\"min_tok_area\"]\n",
    "    max_tok_area_frac = cfg_pp[\"max_tok_area_frac\"]\n",
    "    max_inst_keep = cfg_pp[\"max_inst_keep\"]\n",
    "    min_peak_keep = cfg_pp[\"min_peak_score_keep\"]\n",
    "    min_area_frac_keep = cfg_pp[\"min_area_frac_keep\"]\n",
    "\n",
    "    for batch in dl:\n",
    "        x = batch[\"x\"].to(device, non_blocking=True)\n",
    "        gt = batch[\"gt\"].to(device, non_blocking=True)\n",
    "        best_score = batch[\"best_score\"].cpu().numpy().reshape(-1)\n",
    "\n",
    "        seg_logit, cls_logit = model(x)\n",
    "        p_gate = torch.sigmoid(cls_logit).detach().cpu().numpy().reshape(-1)\n",
    "        p_tok = torch.sigmoid(seg_logit).detach().cpu().numpy()[:,0]\n",
    "        seed = x.detach().cpu().numpy()[:,-1]\n",
    "        gt_np = gt.detach().cpu().numpy()[:,0] > 0.5\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            if p_gate[i] < thr_gate:\n",
    "                pr = np.zeros((HTOK,WTOK), dtype=bool)\n",
    "            else:\n",
    "                prob = p_tok[i]\n",
    "                hard = prob >= T1\n",
    "                soft = prob >= T0\n",
    "                sd = dilate_tok(seed[i] > 0.5, dil_it)\n",
    "                fused = hard | (sd & soft)\n",
    "\n",
    "                uni, _ = inst_split_union_tok(\n",
    "                    fused, min_area=min_tok_area, max_area_frac=max_tok_area_frac, max_keep=max_inst_keep\n",
    "                )\n",
    "                area_frac = float(uni.mean())\n",
    "                if (best_score[i] < min_peak_keep) and (area_frac < min_area_frac_keep):\n",
    "                    uni = np.zeros((HTOK,WTOK), dtype=bool)\n",
    "                pr = uni\n",
    "\n",
    "            scores.append(dice(pr, gt_np[i]))\n",
    "\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "# ----------------------------\n",
    "# Trial sampling + train\n",
    "# ----------------------------\n",
    "def sample_trial_cfg(trial_id: int, val_split: int):\n",
    "    def log_uniform(a,b):\n",
    "        return float(np.exp(np.random.uniform(np.log(a), np.log(b))))\n",
    "    lr = log_uniform(*LR_RANGE)\n",
    "    wd = float(np.random.uniform(*WD_RANGE))\n",
    "    drop = float(np.random.uniform(*DROPOUT_RANGE))\n",
    "    base_ch = int(np.random.choice(BASE_CH_CHOICES))\n",
    "    lam_seg = float(np.random.uniform(*LAMBDA_SEG_RANGE))\n",
    "    lam_cls = float(np.random.uniform(*LAMBDA_CLS_RANGE))\n",
    "    gamma = float(np.random.choice(FOCAL_GAMMA_CHOICES))\n",
    "\n",
    "    T1 = float(np.random.uniform(*T1_RANGE))\n",
    "    T0 = float(np.random.uniform(*T0_RANGE))\n",
    "    if T0 > T1:\n",
    "        T0, T1 = max(0.05, T1-0.05), T1\n",
    "\n",
    "    dil_it = int(np.random.choice(SEED_DILATE_CHOICES))\n",
    "    thr_gate = float(np.random.uniform(*THR_GATE_RANGE))\n",
    "\n",
    "    min_tok_area = int(np.random.choice(MIN_TOK_AREA_CHOICES))\n",
    "    max_tok_area_frac = float(np.random.choice(MAX_TOK_AREA_FRAC_CHOICES))\n",
    "    max_inst_keep = int(np.random.choice(MAX_INST_KEEP_CHOICES))\n",
    "\n",
    "    min_peak_keep = int(np.random.choice(MIN_PEAK_SCORE_KEEP_CHOICES))\n",
    "    min_area_frac_keep = float(np.random.choice(MIN_AREA_FRAC_KEEP_CHOICES))\n",
    "\n",
    "    return {\n",
    "        \"trial_id\": int(trial_id),\n",
    "        \"val_split\": int(val_split),\n",
    "        \"lr\": lr, \"weight_decay\": wd, \"dropout\": drop, \"base_ch\": base_ch,\n",
    "        \"lambda_seg\": lam_seg, \"lambda_cls\": lam_cls, \"focal_gamma\": gamma,\n",
    "        \"T1\": T1, \"T0\": T0, \"seed_dilate_it\": dil_it, \"thr_gate\": thr_gate,\n",
    "        \"min_tok_area\": min_tok_area, \"max_tok_area_frac\": max_tok_area_frac, \"max_inst_keep\": max_inst_keep,\n",
    "        \"min_peak_score_keep\": min_peak_keep, \"min_area_frac_keep\": min_area_frac_keep,\n",
    "    }\n",
    "\n",
    "def train_trial(cfg_trial):\n",
    "    val_split = cfg_trial[\"val_split\"]\n",
    "    df_tr = df[df[\"split\"] != val_split].reset_index(drop=True)\n",
    "    df_va = df[df[\"split\"] == val_split].reset_index(drop=True)\n",
    "\n",
    "    # oversample positives\n",
    "    pos = df_tr[df_tr[\"y\"]==1]\n",
    "    neg = df_tr[df_tr[\"y\"]==0]\n",
    "    if len(pos) > 0 and len(neg) > 0:\n",
    "        take = min(len(neg), len(pos)*3)\n",
    "        neg_s = neg.sample(n=take, replace=False, random_state=SEED)\n",
    "        pos_s = pos.sample(n=take, replace=True,  random_state=SEED)\n",
    "        df_tr_use = pd.concat([neg_s, pos_s], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "    else:\n",
    "        df_tr_use = df_tr\n",
    "\n",
    "    ds_tr = HybridTokDS(df_tr_use, tok_col=tok_col)\n",
    "    ds_va = HybridTokDS(df_va,     tok_col=tok_col)\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS,\n",
    "                       pin_memory=(device.type==\"cuda\"), drop_last=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                       pin_memory=(device.type==\"cuda\"), drop_last=False)\n",
    "\n",
    "    model = HybridUNet(c_in=D_IN, base_ch=cfg_trial[\"base_ch\"], drop=cfg_trial[\"dropout\"]).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg_trial[\"lr\"], weight_decay=cfg_trial[\"weight_decay\"])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp_ok)\n",
    "\n",
    "    best_score = -1.0\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, TRIAL_EPOCHS+1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        loss_meter = 0.0\n",
    "        nsteps = 0\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        for step, batch in enumerate(dl_tr, start=1):\n",
    "            x  = batch[\"x\"].to(device, non_blocking=True)\n",
    "            gt = batch[\"gt\"].to(device, non_blocking=True)\n",
    "            yb = batch[\"y\"].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=amp_ok):\n",
    "                seg_logit, cls_logit = model(x)\n",
    "                l_seg = bce_focal_from_logits(seg_logit, gt, gamma=cfg_trial[\"focal_gamma\"]) + dice_loss_from_logits(seg_logit, gt)\n",
    "                l_cls = F.binary_cross_entropy_with_logits(cls_logit, yb)\n",
    "                loss = cfg_trial[\"lambda_seg\"] * l_seg + cfg_trial[\"lambda_cls\"] * l_cls\n",
    "                loss = loss / ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if step % ACCUM_STEPS == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            loss_meter += float(loss.item()) * ACCUM_STEPS\n",
    "            nsteps += 1\n",
    "\n",
    "        score = eval_model(model, dl_va, cfg_trial)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[trial {cfg_trial['trial_id']:02d} | split {val_split}] ep {ep}/{TRIAL_EPOCHS} \"\n",
    "              f\"loss={loss_meter/max(1,nsteps):.4f} val_dice_proxy={score:.5f} time={dt:.1f}s\")\n",
    "\n",
    "        if score > best_score + 1e-5:\n",
    "            best_score = score\n",
    "            best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= EARLYSTOP_PATIENCE:\n",
    "                break\n",
    "\n",
    "    return best_score, best_state\n",
    "\n",
    "# ----------------------------\n",
    "# Run trials\n",
    "# ----------------------------\n",
    "trials = []\n",
    "global_best = {\"score\": -1.0, \"cfg\": None, \"state\": None}\n",
    "\n",
    "t_all = time.time()\n",
    "\n",
    "for t in range(1, MAX_TRIALS+1):\n",
    "    val_split = split_ids[(t-1) % len(split_ids)] if VAL_SPLIT_ROTATE else split_ids[0]\n",
    "    cfg_trial = sample_trial_cfg(t, val_split)\n",
    "\n",
    "    try:\n",
    "        score, state = train_trial(cfg_trial)\n",
    "        cfg_trial[\"score\"] = float(score)\n",
    "        cfg_trial[\"status\"] = \"ok\"\n",
    "    except Exception as e:\n",
    "        print(f\"[trial {t}] FAILED:\", repr(e))\n",
    "        cfg_trial[\"score\"] = float(\"nan\")\n",
    "        cfg_trial[\"status\"] = \"fail\"\n",
    "        trials.append(cfg_trial)\n",
    "        pd.DataFrame(trials).to_csv(OUT_DIR / \"trials.csv\", index=False)\n",
    "        continue\n",
    "\n",
    "    trials.append(cfg_trial)\n",
    "\n",
    "    if score > global_best[\"score\"]:\n",
    "        global_best[\"score\"] = float(score)\n",
    "        global_best[\"cfg\"] = cfg_trial\n",
    "        global_best[\"state\"] = state\n",
    "\n",
    "    pd.DataFrame(trials).to_csv(OUT_DIR / \"trials.csv\", index=False)\n",
    "    (OUT_DIR / \"best_config.json\").write_text(json.dumps(global_best[\"cfg\"], indent=2) if global_best[\"cfg\"] else \"{}\")\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"CURRENT BEST:\", global_best[\"score\"], \"| trial:\", global_best[\"cfg\"][\"trial_id\"], \"| val_split:\", global_best[\"cfg\"][\"val_split\"])\n",
    "    print(\"-\"*60)\n",
    "\n",
    "if global_best[\"state\"] is not None:\n",
    "    pack = {\n",
    "        \"model_type\": \"HybridUNet\",\n",
    "        \"input_channels\": int(D_IN),\n",
    "        \"HTOK\": int(HTOK), \"WTOK\": int(WTOK), \"PATCH\": int(PATCH),\n",
    "        \"tok_col\": str(tok_col),\n",
    "        \"token_manifest_train\": str(TOK_MAN_TRAIN),\n",
    "        \"token_root\": str(TOKEN_ROOT),\n",
    "        \"match_root\": str(MATCH_ROOT) if MATCH_ROOT else None,\n",
    "        \"train_table\": str(TRAIN_TABLE),\n",
    "        \"mask_dirs\": {\"TRAIN_MASK_DIR\": str(TRAIN_MASK_DIR), \"SUP_MASK_DIR\": str(SUP_MASK_DIR)},\n",
    "        \"best_cfg\": global_best[\"cfg\"],\n",
    "        \"state_dict\": global_best[\"state\"],\n",
    "        \"meta\": {\"seed\": int(SEED), \"n_splits\": int(N_SPLITS), \"val_split_rotate\": bool(VAL_SPLIT_ROTATE)},\n",
    "    }\n",
    "    torch.save(pack, OUT_DIR / \"best_model.pt\")\n",
    "\n",
    "print(\"DONE in\", f\"{time.time()-t_all:.1f}s\")\n",
    "print(\"Saved:\", OUT_DIR / \"trials.csv\")\n",
    "print(\"Saved:\", OUT_DIR / \"best_config.json\")\n",
    "print(\"Saved:\", OUT_DIR / \"best_model.pt\" if (OUT_DIR / \"best_model.pt\").exists() else \"(no best_model.pt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0899a63",
   "metadata": {
    "papermill": {
     "duration": 0.02692,
     "end_time": "2026-01-13T15:01:45.524760",
     "exception": false,
     "start_time": "2026-01-13T15:01:45.497840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Training (Train on Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730c6bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T15:01:45.605945Z",
     "iopub.status.busy": "2026-01-13T15:01:45.605288Z",
     "iopub.status.idle": "2026-01-13T16:45:18.045245Z",
     "shell.execute_reply": "2026-01-13T16:45:18.042740Z"
    },
    "papermill": {
     "duration": 6212.5242,
     "end_time": "2026-01-13T16:45:18.061683",
     "exception": false,
     "start_time": "2026-01-13T15:01:45.537483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu | AMP: False\n",
      "TOK_MAN_TRAIN       : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500/tokens_manifest_train.parquet\n",
      "TOKEN_MANIFEST_ROOT : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500 | token_files: 2796\n",
      "BASE_CACHE          : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache\n",
      "TOKEN_DATA_ROOT     : /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/dinov2_base_518_cfg_543289469500 | token_files: 2796\n",
      "TOKEN indexed files : 2796\n",
      "TRAIN token exists rate: 1.0 | rows: 2795\n",
      "TEST token exists rate: 1.0 | rows: 1\n",
      "TRAIN_TABLE: /kaggle/working/recodai_luc_gate_artifacts/train_table.patched_tokens.parquet | rows_used: 2795 | pos_rate: 1.0\n",
      "TEST_TABLE : /kaggle/working/recodai_luc_gate_artifacts/test_table.parquet\n",
      "TOK_COL_TR : tok_npz\n",
      "TOK_COL_TE : tok_npz\n",
      "TOKEN GRID: (37, 37) | DIN: 768 | CIN: 769\n",
      "MATCH_ROOT: /kaggle/input/recod-ailuc-dinov2-base/recodai_luc/cache/match_cfg_2ed747746f9c\n",
      "PATCH: 14 | match_map_tr: 2795 | match_map_te: 1\n",
      "------------------------------------------------------------\n",
      "FINAL TRAIN CFG: {'epochs': 8, 'batch': 16, 'accum': 1, 'lr': 0.0005301899318728767, 'wd': 0.019744075908778486, 'base_ch': 128, 'dropout': 0.07337204367950953, 'pos_weight': 0.0, 'lam_seg': 1.0892768570729006, 'lam_cls': 0.7241144063085703, 'focal_gamma': 2.0}\n",
      "------------------------------------------------------------\n",
      "[ep 01/8] loss=0.9970 lr=0.000510011 | 706.0s\n",
      "[ep 02/8] loss=0.8752 lr=0.000452545 | 1397.3s\n",
      "[ep 04/8] loss=0.7200 lr=0.000265095 | 2808.4s\n",
      "[ep 06/8] loss=0.6336 lr=7.76445e-05 | 4217.4s\n",
      "[ep 08/8] loss=0.5732 lr=0 | 5616.7s\n",
      "TRAIN DONE | 5616.7s\n",
      "[export train] 1600/2795 | 174.8s\n",
      "[export train] done | wrote=2795 | 288.4s\n",
      "[export test] done | wrote=1 | 0.5s\n",
      "[dice-proxy] 800/2795 | 81.9s\n",
      "[dice-proxy] 1600/2795 | 160.1s\n",
      "[dice-proxy] 2400/2795 | 240.4s\n",
      "BEST_THR: 0.0 | best_score_dice_proxy: 0.5970196723937988\n",
      "------------------------------------------------------------\n",
      "SAVED:\n",
      " - /kaggle/working/recodai_luc_hybrid_artifacts/final_hybrid_model.pt\n",
      " - /kaggle/working/recodai_luc_hybrid_artifacts/threshold_table.csv\n",
      " - /kaggle/working/recodai_luc_hybrid_artifacts/best_threshold.json\n",
      "MASKPROB_DIR: /kaggle/working/recodai_luc/cache/mask_prob_hybrid_8d3a1c7f72\n",
      "CFG_ID: 8d3a1c7f72\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE — Final Training (Train on Full Data) (ONE CELL) — HYBRID (OPSI-1) — REVISI FULL\n",
    "# - Token & Match path auto-resolve via FILE INDEX (works with *_bind_* manifests)\n",
    "# - Fail-safe seed (if match missing/broken -> zeros, still runs)\n",
    "# - Export mask-prob cache: /kaggle/working/recodai_luc/cache/mask_prob_hybrid_<cfgid>/{case_id}.npz\n",
    "#   keys: prob_tok (float16), p_gate (float16)\n",
    "# - Sweep best gate threshold on TRAIN using Dice-proxy (token-space)\n",
    "# - Save final model: /kaggle/working/recodai_luc_hybrid_artifacts/final_hybrid_model.pt\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time, math, random, hashlib, warnings, re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "WORK     = Path(\"/kaggle/working\")\n",
    "INP      = Path(\"/kaggle/input\")\n",
    "\n",
    "OPT_DIR  = WORK / \"recodai_luc_hybrid_opt\"\n",
    "OUT_DIR  = WORK / \"recodai_luc_hybrid_artifacts\"\n",
    "PROF_DIR = WORK / \"recodai_luc_prof\"\n",
    "CACHE_W  = WORK / \"recodai_luc/cache\"  # working cache (if exists)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_W.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_cfg_path = OPT_DIR / \"best_config.json\"\n",
    "if not best_cfg_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing {best_cfg_path}. Run Optimize stage first.\")\n",
    "BEST = json.loads(best_cfg_path.read_text())\n",
    "\n",
    "paths_json = PROF_DIR / \"paths.json\"\n",
    "if not paths_json.exists():\n",
    "    raise FileNotFoundError(f\"Missing {paths_json}\")\n",
    "PATHS = json.loads(paths_json.read_text())\n",
    "\n",
    "TRAIN_MASK_DIR = Path(PATHS.get(\"TRAIN_MASK_DIR\",\"\")) if PATHS.get(\"TRAIN_MASK_DIR\") else None\n",
    "SUP_MASK_DIR   = Path(PATHS.get(\"SUP_MASK_DIR\",\"\")) if PATHS.get(\"SUP_MASK_DIR\") else None\n",
    "\n",
    "# ----------------------------\n",
    "# Repro / device\n",
    "# ----------------------------\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "def seed_everything(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = bool(int(os.environ.get(\"USE_AMP\", \"1\"))) and (device.type == \"cuda\")\n",
    "print(\"DEVICE:\", device, \"| AMP:\", USE_AMP)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: ids\n",
    "# ----------------------------\n",
    "def infer_case_id_any(x):\n",
    "    if x is None:\n",
    "        return np.nan\n",
    "    s = str(x)\n",
    "    if s.lower() in [\"nan\",\"none\",\"\"]:\n",
    "        return np.nan\n",
    "    m = re.search(r\"\\b(\\d{3,})\\b\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "    m = re.search(r\"(\\d+)\", s)\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: token manifest + token data root resolver (bind-safe)\n",
    "# ----------------------------\n",
    "def _count_tok_files(root: Path):\n",
    "    if root is None or (not root.exists()):\n",
    "        return 0\n",
    "    total = 0\n",
    "    for sub in [\"train\",\"test\",\"train_all\",\"test_all\"]:\n",
    "        d = root / sub\n",
    "        if d.exists():\n",
    "            total += sum(1 for _ in d.glob(\"*.npz\"))\n",
    "            total += sum(1 for _ in d.glob(\"*.npy\"))\n",
    "    return total\n",
    "\n",
    "def pick_best_tokens_manifest_train():\n",
    "    hits = []\n",
    "    hits += list(WORK.glob(\"**/tokens_manifest_train.parquet\"))\n",
    "    hits += list(INP.glob(\"**/tokens_manifest_train.parquet\"))\n",
    "    hits = [p for p in hits if p.exists()]\n",
    "    if not hits:\n",
    "        return None\n",
    "\n",
    "    def score(p: Path):\n",
    "        r = p.parent\n",
    "        n_tok = _count_tok_files(r)\n",
    "        has_train_dir = int((r/\"train\").exists() or (r/\"train_all\").exists())\n",
    "        return (int(n_tok > 0), int(n_tok), has_train_dir, float(p.stat().st_mtime))\n",
    "\n",
    "    hits = sorted(hits, key=score, reverse=True)\n",
    "    return hits[0]\n",
    "\n",
    "TOK_MAN_TRAIN = pick_best_tokens_manifest_train()\n",
    "if TOK_MAN_TRAIN is None:\n",
    "    raise FileNotFoundError(\"tokens_manifest_train.parquet not found. Token cache must exist.\")\n",
    "\n",
    "TOKEN_MANIFEST_ROOT = TOK_MAN_TRAIN.parent\n",
    "BASE_CACHE = TOKEN_MANIFEST_ROOT.parent if TOKEN_MANIFEST_ROOT.parent.exists() else TOKEN_MANIFEST_ROOT\n",
    "\n",
    "# choose best sibling that has real token files\n",
    "sib_dirs = [p for p in BASE_CACHE.iterdir() if p.is_dir()]\n",
    "sib_scored = sorted([(_count_tok_files(d), d) for d in sib_dirs], key=lambda x: x[0], reverse=True)\n",
    "best_cnt, best_root = sib_scored[0] if sib_scored else (0, None)\n",
    "\n",
    "manifest_cnt = _count_tok_files(TOKEN_MANIFEST_ROOT)\n",
    "TOKEN_DATA_ROOT = TOKEN_MANIFEST_ROOT if manifest_cnt > 0 else best_root\n",
    "\n",
    "print(\"TOK_MAN_TRAIN       :\", TOK_MAN_TRAIN)\n",
    "print(\"TOKEN_MANIFEST_ROOT :\", TOKEN_MANIFEST_ROOT, \"| token_files:\", manifest_cnt)\n",
    "print(\"BASE_CACHE          :\", BASE_CACHE)\n",
    "print(\"TOKEN_DATA_ROOT     :\", TOKEN_DATA_ROOT, \"| token_files:\", best_cnt)\n",
    "\n",
    "if TOKEN_DATA_ROOT is None or (not Path(TOKEN_DATA_ROOT).exists()) or _count_tok_files(Path(TOKEN_DATA_ROOT)) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No token .npz/.npy found for TOKEN_DATA_ROOT.\\n\"\n",
    "        f\"Checked siblings under: {BASE_CACHE}\"\n",
    "    )\n",
    "\n",
    "def build_tok_index(data_root: Path):\n",
    "    mp = {}\n",
    "    files = []\n",
    "    for sub in [\"train\",\"test\",\"train_all\",\"test_all\"]:\n",
    "        d = data_root / sub\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for p in d.glob(\"*.npz\"):\n",
    "            files.append(p); mp[p.name] = str(p)\n",
    "        for p in d.glob(\"*.npy\"):\n",
    "            files.append(p); mp[p.name] = str(p)\n",
    "    return mp, files\n",
    "\n",
    "tok_idx_map, tok_files = build_tok_index(Path(TOKEN_DATA_ROOT))\n",
    "print(\"TOKEN indexed files :\", len(tok_files))\n",
    "\n",
    "def resolve_tok_path(p):\n",
    "    if p is None:\n",
    "        return None\n",
    "    s = str(p)\n",
    "    if s.lower() in [\"nan\",\"none\",\"\"]:\n",
    "        return None\n",
    "    pp = Path(s)\n",
    "    if pp.exists():\n",
    "        return str(pp)\n",
    "\n",
    "    # relative under TOKEN_DATA_ROOT\n",
    "    if not pp.is_absolute():\n",
    "        cand = Path(TOKEN_DATA_ROOT) / pp\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    # basename lookup\n",
    "    bn = pp.name\n",
    "    if bn in tok_idx_map:\n",
    "        return tok_idx_map[bn]\n",
    "\n",
    "    # try add extension\n",
    "    if \".\" not in bn:\n",
    "        if (bn + \".npz\") in tok_idx_map:\n",
    "            return tok_idx_map[bn + \".npz\"]\n",
    "        if (bn + \".npy\") in tok_idx_map:\n",
    "            return tok_idx_map[bn + \".npy\"]\n",
    "\n",
    "    # last: common subdirs\n",
    "    for sub in [\"train\",\"test\",\"train_all\",\"test_all\"]:\n",
    "        cand = Path(TOKEN_DATA_ROOT) / sub / bn\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "        if \".\" not in bn:\n",
    "            cand2 = Path(TOKEN_DATA_ROOT) / sub / (bn + \".npz\")\n",
    "            if cand2.exists():\n",
    "                return str(cand2)\n",
    "            cand3 = Path(TOKEN_DATA_ROOT) / sub / (bn + \".npy\")\n",
    "            if cand3.exists():\n",
    "                return str(cand3)\n",
    "\n",
    "    return None\n",
    "\n",
    "def pick_best_path_col(df, candidates):\n",
    "    best = (None, -1.0)\n",
    "    for c in candidates:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        rr = df[c].map(resolve_tok_path)\n",
    "        ex = rr.map(lambda x: isinstance(x,str) and Path(x).exists()).mean() if len(rr) else 0.0\n",
    "        if ex > best[1]:\n",
    "            best = (c, float(ex))\n",
    "    return best[0], best[1]\n",
    "\n",
    "# ----------------------------\n",
    "# Find train/test tables\n",
    "# ----------------------------\n",
    "train_table_cands = [\n",
    "    WORK / \"recodai_luc_gate_artifacts\" / \"train_table.patched_tokens.parquet\",\n",
    "    WORK / \"recodai_luc_gate_artifacts\" / \"train_table.parquet\",\n",
    "    WORK / \"recodai_luc_hybrid_artifacts\" / \"train_table.parquet\",\n",
    "    PROF_DIR / \"train_table.parquet\",\n",
    "]\n",
    "test_table_cands = [\n",
    "    WORK / \"recodai_luc_gate_artifacts\" / \"test_table.patched_tokens.parquet\",\n",
    "    WORK / \"recodai_luc_gate_artifacts\" / \"test_table.parquet\",\n",
    "    WORK / \"recodai_luc_hybrid_artifacts\" / \"test_table.parquet\",\n",
    "    PROF_DIR / \"test_table.parquet\",\n",
    "]\n",
    "\n",
    "TRAIN_TABLE = next((p for p in train_table_cands if p.exists()), None)\n",
    "TEST_TABLE  = next((p for p in test_table_cands if p.exists()), None)\n",
    "if TRAIN_TABLE is None:\n",
    "    raise FileNotFoundError(\"Cannot find train_table.parquet. Run Build Training Table stage first.\")\n",
    "\n",
    "df_tr = pd.read_parquet(TRAIN_TABLE).copy()\n",
    "for need in [\"case_id\",\"y\"]:\n",
    "    if need not in df_tr.columns:\n",
    "        raise ValueError(f\"train_table missing required col: {need}\")\n",
    "df_tr[\"case_id\"] = pd.to_numeric(df_tr[\"case_id\"], errors=\"coerce\")\n",
    "df_tr = df_tr[df_tr[\"case_id\"].notna()].copy()\n",
    "df_tr[\"case_id\"] = df_tr[\"case_id\"].astype(int)\n",
    "df_tr[\"y\"] = pd.to_numeric(df_tr[\"y\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "df_te = None\n",
    "if TEST_TABLE is not None and Path(TEST_TABLE).exists():\n",
    "    df_te = pd.read_parquet(TEST_TABLE).copy()\n",
    "    if \"case_id\" not in df_te.columns:\n",
    "        raise ValueError(\"test_table missing case_id\")\n",
    "    df_te[\"case_id\"] = pd.to_numeric(df_te[\"case_id\"], errors=\"coerce\")\n",
    "    df_te = df_te[df_te[\"case_id\"].notna()].copy()\n",
    "    df_te[\"case_id\"] = df_te[\"case_id\"].astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# Ensure token path columns exist in train/test (join from tokens manifests if needed)\n",
    "# ----------------------------\n",
    "tok_col_tr = None\n",
    "for c in [\"tok_path\",\"token_path\",\"dino_path\",\"feat_path\",\"emb_path\",\"token_npz\",\"tok_npz\",\"npz_path\",\"path\",\"file\",\"npz\"]:\n",
    "    if c in df_tr.columns:\n",
    "        tok_col_tr = c\n",
    "        break\n",
    "\n",
    "df_tok_tr = pd.read_parquet(TOK_MAN_TRAIN).copy()\n",
    "if \"case_id\" not in df_tok_tr.columns:\n",
    "    if \"uid\" in df_tok_tr.columns:\n",
    "        df_tok_tr[\"case_id\"] = df_tok_tr[\"uid\"].apply(infer_case_id_any)\n",
    "    else:\n",
    "        df_tok_tr[\"case_id\"] = df_tok_tr[df_tok_tr.columns[0]].apply(infer_case_id_any)\n",
    "\n",
    "df_tok_tr[\"case_id\"] = pd.to_numeric(df_tok_tr[\"case_id\"], errors=\"coerce\")\n",
    "df_tok_tr = df_tok_tr[df_tok_tr[\"case_id\"].notna()].copy()\n",
    "df_tok_tr[\"case_id\"] = df_tok_tr[\"case_id\"].astype(int)\n",
    "\n",
    "tok_path_candidates = [\"tok_npz\",\"npz_path\",\"path\",\"file\",\"npz\",\"token_npz\"]\n",
    "best_col_tr, best_rate_tr = pick_best_path_col(df_tok_tr, tok_path_candidates)\n",
    "if best_col_tr is None:\n",
    "    raise RuntimeError(\"Cannot detect token path column in tokens_manifest_train.parquet.\")\n",
    "\n",
    "df_tok_tr[\"tok_npz\"] = df_tok_tr[best_col_tr].map(resolve_tok_path)\n",
    "df_tok_tr[\"_ok\"] = df_tok_tr[\"tok_npz\"].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "\n",
    "def _mt(p):\n",
    "    try:\n",
    "        return float(Path(p).stat().st_mtime)\n",
    "    except Exception:\n",
    "        return -1.0\n",
    "\n",
    "df_tok_tr[\"_mt\"] = df_tok_tr[\"tok_npz\"].map(_mt)\n",
    "df_tok_tr = df_tok_tr.sort_values([\"case_id\",\"_ok\",\"_mt\"], ascending=[True, False, False]) \\\n",
    "                     .drop_duplicates(\"case_id\", keep=\"first\") \\\n",
    "                     .drop(columns=[\"_ok\",\"_mt\"])\n",
    "\n",
    "if tok_col_tr is None:\n",
    "    df_tr = df_tr.merge(df_tok_tr[[\"case_id\",\"tok_npz\"]], on=\"case_id\", how=\"left\")\n",
    "    tok_col_tr = \"tok_npz\"\n",
    "    print(f\"INFO: train token col missing -> joined from TOK_MAN_TRAIN using '{best_col_tr}' (exist-rate≈{best_rate_tr:.3f})\")\n",
    "else:\n",
    "    df_tr[tok_col_tr] = df_tr[tok_col_tr].map(resolve_tok_path)\n",
    "\n",
    "ok_tr = df_tr[tok_col_tr].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "print(\"TRAIN token exists rate:\", float(ok_tr.mean()), \"| rows:\", len(df_tr))\n",
    "if ok_tr.mean() < 0.50:\n",
    "    print(\"DIAG train best_col:\", best_col_tr, \"| best_rate≈\", best_rate_tr)\n",
    "    print(\"DIAG sample tok_npz:\", df_tok_tr[\"tok_npz\"].dropna().head(5).tolist())\n",
    "    raise RuntimeError(\"TRAIN token exists rate too low -> path mismatch. Fix TOKEN_DATA_ROOT / manifest mapping.\")\n",
    "df_tr = df_tr[ok_tr].drop_duplicates(\"case_id\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# test tokens: try best manifest test if needed\n",
    "tok_col_te = tok_col_tr\n",
    "if df_te is not None:\n",
    "    tok_col_te = None\n",
    "    for c in [\"tok_path\",\"token_path\",\"dino_path\",\"feat_path\",\"emb_path\",\"token_npz\",\"tok_npz\",\"npz_path\",\"path\",\"file\",\"npz\"]:\n",
    "        if c in df_te.columns:\n",
    "            tok_col_te = c\n",
    "            break\n",
    "\n",
    "    # locate tokens_manifest_test.parquet (prefer sibling of train manifest)\n",
    "    TOK_MAN_TEST = None\n",
    "    cand = TOKEN_MANIFEST_ROOT / \"tokens_manifest_test.parquet\"\n",
    "    if cand.exists():\n",
    "        TOK_MAN_TEST = cand\n",
    "    else:\n",
    "        hits = list(WORK.glob(\"**/tokens_manifest_test.parquet\")) + list(INP.glob(\"**/tokens_manifest_test.parquet\"))\n",
    "        hits = [p for p in hits if p.exists()]\n",
    "        TOK_MAN_TEST = sorted(hits, key=lambda p: p.stat().st_mtime, reverse=True)[0] if hits else None\n",
    "\n",
    "    if tok_col_te is None:\n",
    "        if TOK_MAN_TEST is None:\n",
    "            print(\"WARNING: test token col missing and tokens_manifest_test.parquet not found -> skip test export.\")\n",
    "            df_te = None\n",
    "            tok_col_te = tok_col_tr\n",
    "        else:\n",
    "            df_tok_te = pd.read_parquet(TOK_MAN_TEST).copy()\n",
    "            if \"case_id\" not in df_tok_te.columns:\n",
    "                if \"uid\" in df_tok_te.columns:\n",
    "                    df_tok_te[\"case_id\"] = df_tok_te[\"uid\"].apply(infer_case_id_any)\n",
    "                else:\n",
    "                    df_tok_te[\"case_id\"] = df_tok_te[df_tok_te.columns[0]].apply(infer_case_id_any)\n",
    "            df_tok_te[\"case_id\"] = pd.to_numeric(df_tok_te[\"case_id\"], errors=\"coerce\")\n",
    "            df_tok_te = df_tok_te[df_tok_te[\"case_id\"].notna()].copy()\n",
    "            df_tok_te[\"case_id\"] = df_tok_te[\"case_id\"].astype(int)\n",
    "\n",
    "            best_col_te, best_rate_te = pick_best_path_col(df_tok_te, tok_path_candidates)\n",
    "            if best_col_te is None:\n",
    "                print(\"WARNING: cannot detect token path column in tokens_manifest_test.parquet -> skip test export.\")\n",
    "                df_te = None\n",
    "                tok_col_te = tok_col_tr\n",
    "            else:\n",
    "                df_tok_te[\"tok_npz\"] = df_tok_te[best_col_te].map(resolve_tok_path)\n",
    "                df_tok_te[\"_ok\"] = df_tok_te[\"tok_npz\"].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "                df_tok_te[\"_mt\"] = df_tok_te[\"tok_npz\"].map(_mt)\n",
    "                df_tok_te = df_tok_te.sort_values([\"case_id\",\"_ok\",\"_mt\"], ascending=[True, False, False]) \\\n",
    "                                     .drop_duplicates(\"case_id\", keep=\"first\") \\\n",
    "                                     .drop(columns=[\"_ok\",\"_mt\"])\n",
    "                df_te = df_te.merge(df_tok_te[[\"case_id\",\"tok_npz\"]], on=\"case_id\", how=\"left\")\n",
    "                tok_col_te = \"tok_npz\"\n",
    "                df_te[tok_col_te] = df_te[tok_col_te].map(resolve_tok_path)\n",
    "                ok_te = df_te[tok_col_te].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "                print(\"TEST token exists rate:\", float(ok_te.mean()), \"| rows:\", len(df_te))\n",
    "                df_te = df_te[ok_te].drop_duplicates(\"case_id\", keep=\"first\").reset_index(drop=True)\n",
    "    else:\n",
    "        df_te[tok_col_te] = df_te[tok_col_te].map(resolve_tok_path)\n",
    "        ok_te = df_te[tok_col_te].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "        print(\"TEST token exists rate:\", float(ok_te.mean()), \"| rows:\", len(df_te))\n",
    "        df_te = df_te[ok_te].drop_duplicates(\"case_id\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(\"TRAIN_TABLE:\", TRAIN_TABLE, \"| rows_used:\", len(df_tr), \"| pos_rate:\", float(df_tr[\"y\"].mean()))\n",
    "print(\"TEST_TABLE :\", TEST_TABLE if df_te is not None else \"(none/skip)\")\n",
    "print(\"TOK_COL_TR :\", tok_col_tr)\n",
    "print(\"TOK_COL_TE :\", tok_col_te)\n",
    "\n",
    "# ----------------------------\n",
    "# Infer token grid dims + token dim from first train token\n",
    "# ----------------------------\n",
    "def load_tok_any(path_str: str):\n",
    "    p = Path(path_str)\n",
    "    if p.suffix.lower() == \".npy\":\n",
    "        a = np.load(p, mmap_mode=\"r\")\n",
    "    else:\n",
    "        z = np.load(p)\n",
    "        for k in [\"tok\",\"tokens\",\"grid\",\"token_grid\",\"x\",\"feat\",\"emb\",\"f\"]:\n",
    "            if k in z.files:\n",
    "                a = z[k]; break\n",
    "        else:\n",
    "            keys = list(z.files)\n",
    "            if not keys:\n",
    "                raise ValueError(\"empty npz\")\n",
    "            a = z[keys[0]]\n",
    "    return np.asarray(a)\n",
    "\n",
    "tok0 = load_tok_any(df_tr.iloc[0][tok_col_tr])\n",
    "if tok0.ndim != 3:\n",
    "    raise RuntimeError(f\"Token array ndim must be 3, got {tok0.shape}\")\n",
    "\n",
    "# decide layout\n",
    "if tok0.shape[0] < 200 and tok0.shape[1] < 200 and tok0.shape[2] > 32:\n",
    "    # (Ht,Wt,D)\n",
    "    HTOK, WTOK, DIN = int(tok0.shape[0]), int(tok0.shape[1]), int(tok0.shape[2])\n",
    "elif tok0.shape[0] > 32 and tok0.shape[1] < 200 and tok0.shape[2] < 200:\n",
    "    # (D,H,W)\n",
    "    DIN, HTOK, WTOK = int(tok0.shape[0]), int(tok0.shape[1]), int(tok0.shape[2])\n",
    "else:\n",
    "    HTOK, WTOK, DIN = int(tok0.shape[0]), int(tok0.shape[1]), int(tok0.shape[2])\n",
    "\n",
    "CIN = DIN + 1\n",
    "print(\"TOKEN GRID:\", (HTOK, WTOK), \"| DIN:\", DIN, \"| CIN:\", CIN)\n",
    "\n",
    "def load_tok_norm(path_str: str):\n",
    "    a = load_tok_any(path_str).astype(np.float32)\n",
    "    if a.ndim != 3:\n",
    "        raise ValueError(f\"Unknown token array shape: {a.shape}\")\n",
    "\n",
    "    # (Ht,Wt,D)\n",
    "    if a.shape[0] == HTOK and a.shape[1] == WTOK:\n",
    "        return a\n",
    "\n",
    "    # (D,Ht,Wt)\n",
    "    if a.shape[1] == HTOK and a.shape[2] == WTOK:\n",
    "        return np.transpose(a, (1,2,0))\n",
    "\n",
    "    # resize fallback (H,W,D)\n",
    "    H0,W0,D0 = a.shape\n",
    "    out = np.zeros((HTOK,WTOK,D0), np.float32)\n",
    "    for d in range(D0):\n",
    "        im = Image.fromarray(a[:,:,d].astype(np.float32))\n",
    "        im = im.resize((WTOK,HTOK), resample=Image.BILINEAR)\n",
    "        out[:,:,d] = np.asarray(im).astype(np.float32)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Match root (robust; optional)\n",
    "# ----------------------------\n",
    "def build_npz_index(root: Path):\n",
    "    if root is None or (not root.exists()):\n",
    "        return {}, []\n",
    "    files = list(root.glob(\"**/*.npz\"))\n",
    "    mp = {}\n",
    "    for p in files:\n",
    "        bn = p.name\n",
    "        if bn not in mp:\n",
    "            mp[bn] = p\n",
    "        else:\n",
    "            try:\n",
    "                if p.stat().st_mtime > mp[bn].stat().st_mtime:\n",
    "                    mp[bn] = p\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {k: str(v) for k,v in mp.items()}, files\n",
    "\n",
    "def resolve_by_index(p, root: Path, idx_map: dict):\n",
    "    if p is None:\n",
    "        return None\n",
    "    s = str(p)\n",
    "    if s.lower() in [\"nan\",\"none\",\"\"]:\n",
    "        return None\n",
    "    pp = Path(s)\n",
    "    if pp.exists():\n",
    "        return str(pp)\n",
    "\n",
    "    # relative under root\n",
    "    if root is not None and root.exists() and (not pp.is_absolute()):\n",
    "        cand = root / pp\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "\n",
    "    bn = pp.name\n",
    "    if bn in idx_map:\n",
    "        return idx_map[bn]\n",
    "\n",
    "    # try add extension\n",
    "    if \".\" not in bn:\n",
    "        if (bn + \".npz\") in idx_map:\n",
    "            return idx_map[bn + \".npz\"]\n",
    "\n",
    "    # common subdirs\n",
    "    if root is not None and root.exists():\n",
    "        for seg in [\"train\",\"test\",\"train_all\",\"test_all\",\"\"]:\n",
    "            cand = (root/seg/bn) if seg else (root/bn)\n",
    "            if cand.exists():\n",
    "                return str(cand)\n",
    "            if \".\" not in bn:\n",
    "                cand2 = (root/seg/(bn+\".npz\")) if seg else (root/(bn+\".npz\"))\n",
    "                if cand2.exists():\n",
    "                    return str(cand2)\n",
    "    return None\n",
    "\n",
    "def pick_latest_match_root():\n",
    "    cands = []\n",
    "    cands += list(CACHE_W.glob(\"match_cfg_*\"))\n",
    "    cands += list(INP.glob(\"**/recodai_luc/cache/match_cfg_*\"))\n",
    "    cands = [c for c in cands if (c/\"cfg.json\").exists()]\n",
    "    if not cands:\n",
    "        return None\n",
    "    cands = sorted(cands, key=lambda p: (p/\"cfg.json\").stat().st_mtime, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "MATCH_ROOT = pick_latest_match_root()\n",
    "PATCH = 14\n",
    "match_map_tr = {}\n",
    "match_map_te = {}\n",
    "\n",
    "if MATCH_ROOT is None:\n",
    "    print(\"WARNING: MATCH_ROOT not found. Seed will be zeros.\")\n",
    "else:\n",
    "    try:\n",
    "        MATCH_CFG = json.loads((MATCH_ROOT/\"cfg.json\").read_text())\n",
    "        PATCH = int(MATCH_CFG.get(\"patch\", MATCH_CFG.get(\"patch_size\", 14)))\n",
    "\n",
    "        def build_match_map(pq: Path):\n",
    "            if pq is None or (not pq.exists()):\n",
    "                return {}\n",
    "            dfm = pd.read_parquet(pq).copy()\n",
    "\n",
    "            # ensure case_id\n",
    "            if \"case_id\" not in dfm.columns:\n",
    "                if \"uid\" in dfm.columns:\n",
    "                    dfm[\"case_id\"] = dfm[\"uid\"].apply(infer_case_id_any)\n",
    "                else:\n",
    "                    dfm[\"case_id\"] = np.nan\n",
    "            dfm[\"case_id\"] = pd.to_numeric(dfm[\"case_id\"], errors=\"coerce\")\n",
    "            dfm = dfm[dfm[\"case_id\"].notna()].copy()\n",
    "            dfm[\"case_id\"] = dfm[\"case_id\"].astype(int)\n",
    "\n",
    "            idx_map, _ = build_npz_index(MATCH_ROOT)\n",
    "\n",
    "            # pick best path col\n",
    "            path_cands = [\"match_npz\",\"npz_path\",\"path\",\"file\",\"npz\"]\n",
    "            best = (None, -1.0)\n",
    "            for c in path_cands:\n",
    "                if c not in dfm.columns:\n",
    "                    continue\n",
    "                rr = dfm[c].map(lambda x: resolve_by_index(x, MATCH_ROOT, idx_map))\n",
    "                ex = rr.map(lambda x: isinstance(x,str) and Path(x).exists()).mean() if len(rr) else 0.0\n",
    "                if ex > best[1]:\n",
    "                    best = (c, float(ex))\n",
    "            best_col = best[0]\n",
    "            if best_col is None:\n",
    "                return {}\n",
    "\n",
    "            dfm[\"match_npz\"] = dfm[best_col].map(lambda x: resolve_by_index(x, MATCH_ROOT, idx_map))\n",
    "            dfm[\"_ok\"] = dfm[\"match_npz\"].map(lambda x: isinstance(x,str) and Path(x).exists())\n",
    "            dfm = dfm[dfm[\"_ok\"]].copy()\n",
    "            if len(dfm) == 0:\n",
    "                return {}\n",
    "\n",
    "            score_cols = [c for c in [\"best_peak_score\",\"peak_score_max\",\"max_peak_score\",\"score_max\",\"best_score\",\"peak_max\"] if c in dfm.columns]\n",
    "            if score_cols:\n",
    "                sc = score_cols[0]\n",
    "                dfm[sc] = pd.to_numeric(dfm[sc], errors=\"coerce\").fillna(-1)\n",
    "                dfm = dfm.sort_values([\"case_id\", sc], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\n",
    "            else:\n",
    "                dfm[\"_mt\"] = dfm[\"match_npz\"].map(lambda p: Path(p).stat().st_mtime if Path(p).exists() else -1.0)\n",
    "                dfm = dfm.sort_values([\"case_id\",\"_mt\"], ascending=[True, False]).drop_duplicates(\"case_id\", keep=\"first\")\n",
    "\n",
    "            return dfm.set_index(\"case_id\")[\"match_npz\"].to_dict()\n",
    "\n",
    "        match_map_tr = build_match_map(MATCH_ROOT / \"match_manifest_train.parquet\")\n",
    "        match_map_te = build_match_map(MATCH_ROOT / \"match_manifest_test.parquet\")\n",
    "\n",
    "        print(\"MATCH_ROOT:\", MATCH_ROOT)\n",
    "        print(\"PATCH:\", PATCH, \"| match_map_tr:\", len(match_map_tr), \"| match_map_te:\", len(match_map_te))\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: match manifest parse failed -> seed zeros. Reason:\", repr(e))\n",
    "        MATCH_ROOT = None\n",
    "        match_map_tr = {}\n",
    "        match_map_te = {}\n",
    "        PATCH = 14\n",
    "\n",
    "# ----------------------------\n",
    "# SciPy optional\n",
    "# ----------------------------\n",
    "try:\n",
    "    import scipy.ndimage as ndi\n",
    "    _HAS_SCIPY = True\n",
    "except Exception:\n",
    "    _HAS_SCIPY = False\n",
    "\n",
    "def dilate_tok(x_bool, it=1):\n",
    "    if it <= 0:\n",
    "        return x_bool.astype(bool)\n",
    "    x = x_bool.astype(bool)\n",
    "    if _HAS_SCIPY:\n",
    "        return ndi.binary_dilation(x, iterations=it)\n",
    "    for _ in range(it):\n",
    "        xp = np.pad(x, 1, mode=\"constant\", constant_values=False)\n",
    "        y = np.zeros_like(x, dtype=bool)\n",
    "        for dy in (-1,0,1):\n",
    "            for dx in (-1,0,1):\n",
    "                y |= xp[1+dy:1+dy+x.shape[0], 1+dx:1+dx+x.shape[1]]\n",
    "        x = y\n",
    "    return x\n",
    "\n",
    "def label_cc(x_bool):\n",
    "    x = x_bool.astype(bool)\n",
    "    if _HAS_SCIPY:\n",
    "        lab, n = ndi.label(x, structure=np.ones((3,3), dtype=np.uint8))\n",
    "        return lab, int(n)\n",
    "    H,W = x.shape\n",
    "    lab = np.zeros((H,W), dtype=np.int32); cur=0\n",
    "    for y in range(H):\n",
    "        for x0 in range(W):\n",
    "            if (not x[y,x0]) or lab[y,x0]!=0:\n",
    "                continue\n",
    "            cur += 1\n",
    "            st=[(y,x0)]; lab[y,x0]=cur\n",
    "            while st:\n",
    "                yy,xx=st.pop()\n",
    "                for dy in (-1,0,1):\n",
    "                    for dx in (-1,0,1):\n",
    "                        if dy==0 and dx==0:\n",
    "                            continue\n",
    "                        ny,nx=yy+dy,xx+dx\n",
    "                        if 0<=ny<H and 0<=nx<W and x[ny,nx] and lab[ny,nx]==0:\n",
    "                            lab[ny,nx]=cur; st.append((ny,nx))\n",
    "    return lab, int(cur)\n",
    "\n",
    "def inst_split_union_tok(mask_bool, min_area=2, max_area_frac=0.8, max_keep=8):\n",
    "    H,W = mask_bool.shape\n",
    "    lab,n = label_cc(mask_bool)\n",
    "    if n<=0:\n",
    "        return np.zeros((H,W), dtype=bool), 0\n",
    "    insts=[]; areas=[]\n",
    "    for k in range(1,n+1):\n",
    "        m = (lab==k)\n",
    "        a = int(m.sum())\n",
    "        if a < min_area:\n",
    "            continue\n",
    "        if a / float(H*W) > max_area_frac:\n",
    "            continue\n",
    "        insts.append(m); areas.append(a)\n",
    "    if not insts:\n",
    "        return np.zeros((H,W), dtype=bool), 0\n",
    "    order = np.argsort(np.asarray(areas))[::-1][:max_keep]\n",
    "    uni = np.zeros((H,W), dtype=bool)\n",
    "    for i in order:\n",
    "        uni |= insts[i]\n",
    "    return uni, int(len(order))\n",
    "\n",
    "def dice(pr_bool, gt_bool):\n",
    "    a=int(pr_bool.sum()); b=int(gt_bool.sum())\n",
    "    if a==0 and b==0:\n",
    "        return 1.0\n",
    "    if a==0 or b==0:\n",
    "        return 0.0\n",
    "    inter=int((pr_bool & gt_bool).sum())\n",
    "    return float((2.0*inter)/(a+b))\n",
    "\n",
    "# ----------------------------\n",
    "# GT union loader -> token GT\n",
    "# ----------------------------\n",
    "def _find_mask_files(mask_dir: Path, case_id: int):\n",
    "    if mask_dir is None or (not mask_dir.exists()):\n",
    "        return []\n",
    "    cid = str(int(case_id))\n",
    "    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n",
    "    pats = [f\"{cid}*.png\", f\"{cid}*.jpg\", f\"{cid}*.jpeg\", f\"{cid}*.tif\", f\"{cid}*.tiff\", f\"{cid}*.bmp\",\n",
    "            f\"{cid}__*.png\", f\"{cid}_*.png\"]\n",
    "    out, seen = [], set()\n",
    "    for pat in pats:\n",
    "        for p in mask_dir.glob(pat):\n",
    "            if p.suffix.lower() in exts:\n",
    "                s = str(p)\n",
    "                if s not in seen:\n",
    "                    out.append(p); seen.add(s)\n",
    "    return sorted(out)\n",
    "\n",
    "def load_gt_union_full(case_id: int):\n",
    "    # fast npy union if exists\n",
    "    for d in [TRAIN_MASK_DIR, SUP_MASK_DIR]:\n",
    "        if d is None or (not d.exists()):\n",
    "            continue\n",
    "        npy = d / f\"{int(case_id)}.npy\"\n",
    "        if npy.exists():\n",
    "            a = np.load(npy, mmap_mode=\"r\")\n",
    "            a = np.asarray(a)\n",
    "            if a.ndim == 2:\n",
    "                return (a > 0)\n",
    "            if a.ndim == 3:\n",
    "                return (a > 0).any(axis=0)\n",
    "\n",
    "    files = []\n",
    "    if TRAIN_MASK_DIR is not None:\n",
    "        files += _find_mask_files(TRAIN_MASK_DIR, case_id)\n",
    "    if SUP_MASK_DIR is not None:\n",
    "        files += _find_mask_files(SUP_MASK_DIR, case_id)\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    m = None\n",
    "    for p in files:\n",
    "        try:\n",
    "            im = Image.open(p).convert(\"L\")\n",
    "            a = (np.asarray(im) > 0)\n",
    "            m = a if m is None else (m | a)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return m\n",
    "\n",
    "def downsample_bool_to_tok(mask_bool: np.ndarray):\n",
    "    if mask_bool is None:\n",
    "        return np.zeros((HTOK,WTOK), dtype=np.float32)\n",
    "    im = Image.fromarray((mask_bool.astype(np.uint8)*255))\n",
    "    im = im.resize((WTOK, HTOK), resample=Image.NEAREST)\n",
    "    return (np.asarray(im) > 127).astype(np.float32)\n",
    "\n",
    "def resize_bool_grid(x_bool: np.ndarray, h, w):\n",
    "    if x_bool is None:\n",
    "        return np.zeros((h,w), dtype=bool)\n",
    "    if x_bool.shape == (h,w):\n",
    "        return x_bool.astype(bool)\n",
    "    im = Image.fromarray((x_bool.astype(np.uint8)*255))\n",
    "    im = im.resize((w, h), resample=Image.NEAREST)\n",
    "    return (np.asarray(im) > 127)\n",
    "\n",
    "# ----------------------------\n",
    "# Seed from match_npz (token union) + best_score (fail-safe)\n",
    "# ----------------------------\n",
    "def load_seed_tok(case_id: int, is_test=False, topk=8):\n",
    "    mm = match_map_te if is_test else match_map_tr\n",
    "    p = mm.get(int(case_id), None)\n",
    "    if p is None or (not Path(p).exists()):\n",
    "        return np.zeros((HTOK,WTOK), dtype=np.float32), 0\n",
    "\n",
    "    try:\n",
    "        z = np.load(p)\n",
    "    except Exception:\n",
    "        return np.zeros((HTOK,WTOK), dtype=np.float32), 0\n",
    "\n",
    "    scores = None\n",
    "    for k in [\"peak_score\",\"scores\",\"score\",\"peak_scores\"]:\n",
    "        if k in z.files:\n",
    "            scores = np.asarray(z[k]).reshape(-1)\n",
    "            break\n",
    "\n",
    "    src = tgt = None\n",
    "    for a,b in [(\"src_masks\",\"tgt_masks\"), (\"src_m\",\"tgt_m\"), (\"src\",\"tgt\")]:\n",
    "        if a in z.files and b in z.files:\n",
    "            src = np.asarray(z[a]); tgt = np.asarray(z[b])\n",
    "            break\n",
    "\n",
    "    best = int(np.max(scores)) if scores is not None and len(scores) else 0\n",
    "\n",
    "    if src is None or tgt is None or src.ndim != 3 or tgt.ndim != 3 or src.shape[0] == 0:\n",
    "        return np.zeros((HTOK,WTOK), dtype=np.float32), best\n",
    "\n",
    "    if scores is not None and len(scores) == src.shape[0]:\n",
    "        idx = np.argsort(scores)[::-1][:min(topk, len(scores))]\n",
    "        src = src[idx]; tgt = tgt[idx]\n",
    "    else:\n",
    "        if src.shape[0] > topk:\n",
    "            src = src[:topk]; tgt = tgt[:topk]\n",
    "\n",
    "    seed_match = ((src>0) | (tgt>0)).any(axis=0).astype(bool)\n",
    "    if seed_match.shape != (HTOK,WTOK):\n",
    "        seed_tok = resize_bool_grid(seed_match, HTOK, WTOK).astype(np.float32)\n",
    "    else:\n",
    "        seed_tok = seed_match.astype(np.float32)\n",
    "\n",
    "    return seed_tok, best\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset / loaders\n",
    "# ----------------------------\n",
    "class TrainDS(Dataset):\n",
    "    def __init__(self, df_in: pd.DataFrame, tok_col: str):\n",
    "        self.df = df_in.reset_index(drop=True)\n",
    "        self.tok_col = tok_col\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        cid = int(r[\"case_id\"])\n",
    "        y = float(r[\"y\"])\n",
    "        tok = load_tok_norm(r[self.tok_col])          # (Ht,Wt,D)\n",
    "        seed, best_score = load_seed_tok(cid, is_test=False, topk=8)\n",
    "        gt_full = load_gt_union_full(cid)\n",
    "        gt_tok = downsample_bool_to_tok(gt_full)      # (Ht,Wt) float\n",
    "        tok_ch = np.transpose(tok, (2,0,1))           # (D,Ht,Wt)\n",
    "        x = np.concatenate([tok_ch, seed[None,:,:]], axis=0).astype(np.float32)\n",
    "        return {\n",
    "            \"x\": torch.from_numpy(x),\n",
    "            \"gt\": torch.from_numpy(gt_tok[None,:,:].astype(np.float32)),\n",
    "            \"y\": torch.tensor([y], dtype=torch.float32),\n",
    "            \"best_score\": torch.tensor([float(best_score)], dtype=torch.float32),\n",
    "            \"case_id\": torch.tensor([cid], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "class InferDS(Dataset):\n",
    "    def __init__(self, df_in: pd.DataFrame, tok_col: str, is_test: bool):\n",
    "        self.df = df_in.reset_index(drop=True)\n",
    "        self.tok_col = tok_col\n",
    "        self.is_test = is_test\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        cid = int(r[\"case_id\"])\n",
    "        tok = load_tok_norm(r[self.tok_col])\n",
    "        seed, best_score = load_seed_tok(cid, is_test=self.is_test, topk=8)\n",
    "        tok_ch = np.transpose(tok, (2,0,1))\n",
    "        x = np.concatenate([tok_ch, seed[None,:,:]], axis=0).astype(np.float32)\n",
    "        return {\n",
    "            \"x\": torch.from_numpy(x),\n",
    "            \"best_score\": torch.tensor([float(best_score)], dtype=torch.float32),\n",
    "            \"case_id\": torch.tensor([cid], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "# ----------------------------\n",
    "# Model (odd-size safe)\n",
    "# ----------------------------\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, s=1, p=1, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c_in, c_out, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.drop = nn.Dropout2d(drop) if drop > 0 else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.act(self.bn(self.conv(x))))\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, c_in, c_out, rates=(1,2,4)):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, 3, padding=r, dilation=r, bias=False),\n",
    "                nn.BatchNorm2d(c_out),\n",
    "                nn.SiLU(inplace=True),\n",
    "            ) for r in rates\n",
    "        ])\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(len(rates)*c_out, c_out, 1, bias=False),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = torch.cat([b(x) for b in self.blocks], dim=1)\n",
    "        return self.proj(y)\n",
    "\n",
    "class HybridUNet(nn.Module):\n",
    "    def __init__(self, c_in, base_ch=96, drop=0.1):\n",
    "        super().__init__()\n",
    "        c1, c2, c3 = base_ch, base_ch*2, base_ch*3\n",
    "        self.e1 = nn.Sequential(ConvBNAct(c_in, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n",
    "        self.pool = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.e2 = nn.Sequential(ConvBNAct(c1, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n",
    "        self.e3 = nn.Sequential(ConvBNAct(c2, c3, drop=drop), ConvBNAct(c3, c3, drop=drop))\n",
    "        self.aspp = ASPP(c3, c3, rates=(1,2,4))\n",
    "\n",
    "        self.d2 = nn.Sequential(ConvBNAct(c3+c2, c2, drop=drop), ConvBNAct(c2, c2, drop=drop))\n",
    "        self.d1 = nn.Sequential(ConvBNAct(c2+c1, c1, drop=drop), ConvBNAct(c1, c1, drop=drop))\n",
    "\n",
    "        self.seg_head = nn.Conv2d(c1, 1, 1)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c3, c3//2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(drop if drop>0 else 0.0),\n",
    "            nn.Linear(c3//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.e1(x)\n",
    "        e2 = self.e2(self.pool(e1))\n",
    "        e3 = self.e3(self.pool(e2))\n",
    "        b  = self.aspp(e3)\n",
    "\n",
    "        cls_logit = self.cls_head(b)\n",
    "\n",
    "        u2 = F.interpolate(b, size=e2.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        d2 = self.d2(torch.cat([u2, e2], dim=1))\n",
    "\n",
    "        u1 = F.interpolate(d2, size=e1.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        d1 = self.d1(torch.cat([u1, e1], dim=1))\n",
    "\n",
    "        seg_logit = self.seg_head(d1)\n",
    "        return seg_logit, cls_logit\n",
    "\n",
    "# ----------------------------\n",
    "# Loss\n",
    "# ----------------------------\n",
    "def dice_loss_from_logits(logits, target, eps=1e-6):\n",
    "    p = torch.sigmoid(logits)\n",
    "    inter = (p * target).sum(dim=(2,3))\n",
    "    den = (p + target).sum(dim=(2,3)) + eps\n",
    "    d = (2.0 * inter) / den\n",
    "    return 1.0 - d.mean()\n",
    "\n",
    "def bce_focal_from_logits(logits, target, gamma=2.0):\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, target, reduction=\"none\")\n",
    "    if gamma <= 0:\n",
    "        return bce.mean()\n",
    "    p = torch.sigmoid(logits)\n",
    "    pt = target * p + (1-target) * (1-p)\n",
    "    w = (1-pt).pow(gamma)\n",
    "    return (w * bce).mean()\n",
    "\n",
    "# ----------------------------\n",
    "# Final training config (from BEST + env overrides)\n",
    "# ----------------------------\n",
    "EPOCHS = int(os.environ.get(\"EPOCHS_FINAL\", \"18\" if device.type==\"cuda\" else \"8\"))\n",
    "BATCH  = int(os.environ.get(\"BATCH_SIZE\", \"32\" if device.type==\"cuda\" else \"16\"))\n",
    "NUM_WORKERS = int(os.environ.get(\"NUM_WORKERS\", \"2\"))\n",
    "ACCUM = int(os.environ.get(\"ACCUM_STEPS\", \"1\"))\n",
    "\n",
    "base_ch = int(BEST[\"base_ch\"])\n",
    "dropout = float(BEST[\"dropout\"])\n",
    "lr = float(BEST[\"lr\"])\n",
    "wd = float(BEST[\"weight_decay\"])\n",
    "lam_seg = float(BEST[\"lambda_seg\"])\n",
    "lam_cls = float(BEST[\"lambda_cls\"])\n",
    "focal_gamma = float(BEST[\"focal_gamma\"])\n",
    "\n",
    "pos = int(df_tr[\"y\"].sum())\n",
    "neg = int(len(df_tr) - pos)\n",
    "pos_weight = float(neg / max(1, pos))\n",
    "bce_cls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], dtype=torch.float32, device=device))\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(\"FINAL TRAIN CFG:\", {\n",
    "    \"epochs\": EPOCHS, \"batch\": BATCH, \"accum\": ACCUM,\n",
    "    \"lr\": lr, \"wd\": wd, \"base_ch\": base_ch, \"dropout\": dropout,\n",
    "    \"pos_weight\": pos_weight, \"lam_seg\": lam_seg, \"lam_cls\": lam_cls, \"focal_gamma\": focal_gamma\n",
    "})\n",
    "print(\"-\"*60)\n",
    "\n",
    "ds = TrainDS(df_tr, tok_col=tok_col_tr)\n",
    "dl = DataLoader(\n",
    "    ds, batch_size=BATCH, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "model = HybridUNet(c_in=CIN, base_ch=base_ch, drop=dropout).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, EPOCHS))\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "t0 = time.time()\n",
    "model.train()\n",
    "opt.zero_grad(set_to_none=True)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    loss_meter = 0.0\n",
    "    nsteps = 0\n",
    "    for step, batch in enumerate(dl, start=1):\n",
    "        x  = batch[\"x\"].to(device, non_blocking=True)\n",
    "        gt = batch[\"gt\"].to(device, non_blocking=True)\n",
    "        yb = batch[\"y\"].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            seg_logit, cls_logit = model(x)\n",
    "            l_seg = bce_focal_from_logits(seg_logit, gt, gamma=focal_gamma) + dice_loss_from_logits(seg_logit, gt)\n",
    "            l_cls = bce_cls(cls_logit, yb)\n",
    "            loss = lam_seg * l_seg + lam_cls * l_cls\n",
    "            loss = loss / ACCUM\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if step % ACCUM == 0:\n",
    "            scaler.step(opt); scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss_meter += float(loss.item()) * ACCUM\n",
    "        nsteps += 1\n",
    "\n",
    "    sched.step()\n",
    "    if ep == 1 or ep % 2 == 0 or ep == EPOCHS:\n",
    "        print(f\"[ep {ep:02d}/{EPOCHS}] loss={loss_meter/max(1,nsteps):.4f} lr={sched.get_last_lr()[0]:.6g} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(\"TRAIN DONE |\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "# ----------------------------\n",
    "# Export mask-prob cache (TRAIN+TEST) + p_gate\n",
    "# ----------------------------\n",
    "def cfg_hash(d):\n",
    "    s = json.dumps(d, sort_keys=True).encode()\n",
    "    return hashlib.md5(s).hexdigest()[:10]\n",
    "\n",
    "CFG_EXPORT = {\n",
    "    \"hybrid\": True,\n",
    "    \"best_config\": BEST,\n",
    "    \"token_manifest_train\": str(TOK_MAN_TRAIN),\n",
    "    \"token_manifest_root\": str(TOKEN_MANIFEST_ROOT),\n",
    "    \"token_data_root\": str(TOKEN_DATA_ROOT),\n",
    "    \"match_root\": str(MATCH_ROOT) if MATCH_ROOT else None,\n",
    "    \"tok_grid\": {\"HTOK\": HTOK, \"WTOK\": WTOK, \"PATCH\": PATCH},\n",
    "    \"train_table\": str(TRAIN_TABLE),\n",
    "    \"test_table\": str(TEST_TABLE) if df_te is not None else None,\n",
    "    \"tok_col_tr\": tok_col_tr,\n",
    "    \"tok_col_te\": tok_col_te,\n",
    "}\n",
    "CFG_ID = cfg_hash(CFG_EXPORT)\n",
    "\n",
    "MASKPROB_DIR = CACHE_W / f\"mask_prob_hybrid_{CFG_ID}\"\n",
    "MASKPROB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_probs(df_in: pd.DataFrame, tok_col_use: str, is_test: bool, tag: str):\n",
    "    ds_inf = InferDS(df_in, tok_col=tok_col_use, is_test=is_test)\n",
    "    dl_inf = DataLoader(\n",
    "        ds_inf, batch_size=BATCH, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"),\n",
    "        drop_last=False\n",
    "    )\n",
    "    model.eval()\n",
    "    t1 = time.time()\n",
    "    wrote = 0\n",
    "    for j, batch in enumerate(dl_inf, start=1):\n",
    "        x = batch[\"x\"].to(device, non_blocking=True)\n",
    "        seg_logit, cls_logit = model(x)\n",
    "        prob_tok = torch.sigmoid(seg_logit).detach().cpu().numpy()[:,0]        # (B,Ht,Wt)\n",
    "        p_gate   = torch.sigmoid(cls_logit).detach().cpu().numpy().reshape(-1) # (B,)\n",
    "        cids = batch[\"case_id\"].cpu().numpy().reshape(-1)\n",
    "\n",
    "        for i in range(len(cids)):\n",
    "            cid = int(cids[i])\n",
    "            np.savez_compressed(\n",
    "                MASKPROB_DIR / f\"{cid}.npz\",\n",
    "                prob_tok=prob_tok[i].astype(np.float16),\n",
    "                p_gate=np.float16(p_gate[i]),\n",
    "            )\n",
    "            wrote += 1\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            print(f\"[export {tag}] {wrote}/{len(ds_inf)} | {time.time()-t1:.1f}s\")\n",
    "\n",
    "    print(f\"[export {tag}] done | wrote={wrote} | {time.time()-t1:.1f}s\")\n",
    "\n",
    "export_probs(df_tr[[\"case_id\", tok_col_tr]].copy(), tok_col_tr, is_test=False, tag=\"train\")\n",
    "if df_te is not None and len(df_te) > 0:\n",
    "    export_probs(df_te[[\"case_id\", tok_col_te]].copy(), tok_col_te, is_test=True, tag=\"test\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold sweep on TRAIN using Dice-proxy (token-space)\n",
    "# score(thr) = mean( p_gate>=thr ? dice(postproc(seg,seed),gt) : dice(empty,gt) )\n",
    "# ----------------------------\n",
    "T1 = float(BEST[\"T1\"]); T0 = float(BEST[\"T0\"]); dil_it = int(BEST[\"seed_dilate_it\"])\n",
    "min_tok_area = int(BEST[\"min_tok_area\"])\n",
    "max_tok_area_frac = float(BEST[\"max_tok_area_frac\"])\n",
    "max_inst_keep = int(BEST[\"max_inst_keep\"])\n",
    "min_peak_keep = int(BEST[\"min_peak_score_keep\"])\n",
    "min_area_frac_keep = float(BEST[\"min_area_frac_keep\"])\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_dice_arrays_train():\n",
    "    ds_inf = TrainDS(df_tr[[\"case_id\",\"y\",tok_col_tr]].copy(), tok_col=tok_col_tr)\n",
    "    dl_inf = DataLoader(ds_inf, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                        pin_memory=(device.type==\"cuda\"), drop_last=False)\n",
    "    model.eval()\n",
    "\n",
    "    p_gate_all = np.zeros(len(ds_inf), np.float32)\n",
    "    dice_use   = np.zeros(len(ds_inf), np.float32)\n",
    "    dice_empty = np.zeros(len(ds_inf), np.float32)\n",
    "\n",
    "    k0 = 0\n",
    "    t2 = time.time()\n",
    "    for batch in dl_inf:\n",
    "        x = batch[\"x\"].to(device, non_blocking=True)\n",
    "        gt = batch[\"gt\"].cpu().numpy()[:,0] > 0.5\n",
    "        best_score = batch[\"best_score\"].cpu().numpy().reshape(-1)\n",
    "\n",
    "        seg_logit, cls_logit = model(x)\n",
    "        prob = torch.sigmoid(seg_logit).cpu().numpy()[:,0]\n",
    "        pg   = torch.sigmoid(cls_logit).cpu().numpy().reshape(-1)\n",
    "        seed = batch[\"x\"].cpu().numpy()[:,-1]  # last channel (B,Ht,Wt)\n",
    "\n",
    "        B = prob.shape[0]\n",
    "        for i in range(B):\n",
    "            idx = k0 + i\n",
    "            p_gate_all[idx] = pg[i]\n",
    "\n",
    "            gt_empty = (gt[i].sum() == 0)\n",
    "            dice_empty[idx] = 1.0 if gt_empty else 0.0\n",
    "\n",
    "            hard = prob[i] >= T1\n",
    "            soft = prob[i] >= T0\n",
    "            sd = dilate_tok(seed[i] > 0.5, dil_it)\n",
    "            fused = hard | (sd & soft)\n",
    "\n",
    "            uni, _ = inst_split_union_tok(fused, min_area=min_tok_area, max_area_frac=max_tok_area_frac, max_keep=max_inst_keep)\n",
    "            area_frac = float(uni.mean())\n",
    "            if (best_score[i] < min_peak_keep) and (area_frac < min_area_frac_keep):\n",
    "                uni = np.zeros((HTOK,WTOK), dtype=bool)\n",
    "\n",
    "            dice_use[idx] = dice(uni, gt[i])\n",
    "\n",
    "        k0 += B\n",
    "        if k0 % 800 == 0:\n",
    "            print(f\"[dice-proxy] {k0}/{len(ds_inf)} | {time.time()-t2:.1f}s\")\n",
    "\n",
    "    return p_gate_all, dice_use, dice_empty\n",
    "\n",
    "p_gate_all, dice_use, dice_empty = build_dice_arrays_train()\n",
    "\n",
    "thr_grid = np.linspace(0.0, 1.0, 201, dtype=np.float32)\n",
    "rows = []\n",
    "best_thr = 0.5\n",
    "best_score = -1.0\n",
    "\n",
    "for thr in thr_grid:\n",
    "    use = (p_gate_all >= thr)\n",
    "    score = float(np.where(use, dice_use, dice_empty).mean())\n",
    "    rows.append({\"thr\": float(thr), \"score_dice_proxy\": score})\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_thr = float(thr)\n",
    "\n",
    "df_thr = pd.DataFrame(rows)\n",
    "df_thr.to_csv(OUT_DIR / \"threshold_table.csv\", index=False)\n",
    "\n",
    "(OUT_DIR / \"best_threshold.json\").write_text(json.dumps({\n",
    "    \"recommended_thr\": float(best_thr),\n",
    "    \"best_score_dice_proxy\": float(best_score),\n",
    "    \"pp\": {\n",
    "        \"T1\": float(T1), \"T0\": float(T0), \"seed_dilate_it\": int(dil_it),\n",
    "        \"min_tok_area\": int(min_tok_area),\n",
    "        \"max_tok_area_frac\": float(max_tok_area_frac),\n",
    "        \"max_inst_keep\": int(max_inst_keep),\n",
    "        \"min_peak_score_keep\": int(min_peak_keep),\n",
    "        \"min_area_frac_keep\": float(min_area_frac_keep),\n",
    "    },\n",
    "    \"cfg_id\": CFG_ID\n",
    "}, indent=2))\n",
    "\n",
    "print(\"BEST_THR:\", best_thr, \"| best_score_dice_proxy:\", best_score)\n",
    "\n",
    "# ----------------------------\n",
    "# Save final model bundle (.pt)\n",
    "# ----------------------------\n",
    "pack = {\n",
    "    \"model_type\": \"HybridUNet\",\n",
    "    \"state_dict\": {k: v.detach().cpu() for k,v in model.state_dict().items()},\n",
    "    \"best_config\": BEST,\n",
    "    \"recommended_thr\": float(best_thr),\n",
    "    \"best_score_dice_proxy\": float(best_score),\n",
    "    \"cfg_export\": CFG_EXPORT,\n",
    "    \"cfg_id\": CFG_ID,\n",
    "    \"maskprob_dir\": str(MASKPROB_DIR),\n",
    "    \"tok_grid\": {\"HTOK\": int(HTOK), \"WTOK\": int(WTOK), \"PATCH\": int(PATCH)},\n",
    "    \"input_channels\": int(CIN),\n",
    "    \"token_dim\": int(DIN),\n",
    "    \"meta\": {\n",
    "        \"seed\": int(SEED),\n",
    "        \"train_rows\": int(len(df_tr)),\n",
    "        \"test_rows\": int(len(df_te)) if df_te is not None else 0,\n",
    "        \"train_table\": str(TRAIN_TABLE),\n",
    "        \"test_table\": str(TEST_TABLE) if df_te is not None else None,\n",
    "        \"tok_col_tr\": str(tok_col_tr),\n",
    "        \"tok_col_te\": str(tok_col_te),\n",
    "    }\n",
    "}\n",
    "torch.save(pack, OUT_DIR / \"final_hybrid_model.pt\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(\"SAVED:\")\n",
    "print(\" -\", OUT_DIR / \"final_hybrid_model.pt\")\n",
    "print(\" -\", OUT_DIR / \"threshold_table.csv\")\n",
    "print(\" -\", OUT_DIR / \"best_threshold.json\")\n",
    "print(\"MASKPROB_DIR:\", MASKPROB_DIR)\n",
    "print(\"CFG_ID:\", CFG_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9deb1",
   "metadata": {
    "papermill": {
     "duration": 0.014468,
     "end_time": "2026-01-13T16:45:18.089539",
     "exception": false,
     "start_time": "2026-01-13T16:45:18.075071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Finalize & Save Model Bundle (Reproducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c15a2ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:45:18.125712Z",
     "iopub.status.busy": "2026-01-13T16:45:18.125103Z",
     "iopub.status.idle": "2026-01-13T16:45:21.887581Z",
     "shell.execute_reply": "2026-01-13T16:45:21.886512Z"
    },
    "papermill": {
     "duration": 3.786828,
     "end_time": "2026-01-13T16:45:21.890330",
     "exception": false,
     "start_time": "2026-01-13T16:45:18.103502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUNDLE_DIR: /kaggle/working/recodai_luc_hybrid_bundle_8d3a1c7f72_20260113_164518\n",
      "ZIP_PATH  : /kaggle/working/hybrid_model_bundle_8d3a1c7f72_20260113_164518.zip\n",
      "Files: ['README.txt', 'best_config.json', 'best_threshold.json', 'final_hybrid_model.pt', 'manifest.json', 'match_cfg.json', 'paths.json', 'threshold_table.csv']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE — Finalize & Save Model Bundle (Reproducible) (ONE CELL) — HYBRID (OPSI-1) — REVISI FULL\n",
    "# Bundle (portable):\n",
    "# - final_hybrid_model.pt\n",
    "# - best_threshold.json + threshold_table.csv\n",
    "# - best_config.json (if exists)\n",
    "# - paths.json + match cfg.json (if resolvable)\n",
    "# - manifest.json (sha256 + env + metadata)\n",
    "# - ZIP: hybrid_model_bundle_<cfg_id>_<stamp>.zip\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time, hashlib, shutil, platform, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "OPT_DIR  = Path(\"/kaggle/working/recodai_luc_hybrid_opt\")\n",
    "OUT_DIR  = Path(\"/kaggle/working/recodai_luc_hybrid_artifacts\")\n",
    "PROF_DIR = Path(\"/kaggle/working/recodai_luc_prof\")\n",
    "\n",
    "FINAL_PT = OUT_DIR / \"final_hybrid_model.pt\"\n",
    "BEST_THR = OUT_DIR / \"best_threshold.json\"\n",
    "THR_TAB  = OUT_DIR / \"threshold_table.csv\"\n",
    "BEST_CFG = OPT_DIR / \"best_config.json\"\n",
    "PATHS_JS = PROF_DIR / \"paths.json\"\n",
    "\n",
    "for p in [FINAL_PT, BEST_THR]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing {p}. Run Final Training stage first.\")\n",
    "\n",
    "pack = torch.load(FINAL_PT, map_location=\"cpu\")\n",
    "cfg_id = str(pack.get(\"cfg_id\", \"unknown\"))\n",
    "\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\", time.gmtime())\n",
    "BUNDLE_DIR = Path(f\"/kaggle/working/recodai_luc_hybrid_bundle_{cfg_id}_{stamp}\")\n",
    "BUNDLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sha256_file(p: Path, chunk=1<<20):\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def safe_copy(src: Path, dst_dir: Path, new_name: str = None):\n",
    "    if src is None:\n",
    "        return None\n",
    "    src = Path(src)\n",
    "    if not src.exists():\n",
    "        return None\n",
    "    dst = dst_dir / (new_name if new_name else src.name)\n",
    "    shutil.copy2(src, dst)\n",
    "    return dst\n",
    "\n",
    "# resolve match cfg.json if possible\n",
    "match_cfg_json = None\n",
    "try:\n",
    "    match_root = pack.get(\"cfg_export\", {}).get(\"match_root\", None)\n",
    "    if match_root and Path(match_root).exists():\n",
    "        p = Path(match_root) / \"cfg.json\"\n",
    "        if p.exists():\n",
    "            match_cfg_json = p\n",
    "except Exception:\n",
    "    match_cfg_json = None\n",
    "\n",
    "copied = {}\n",
    "copied[\"final_hybrid_model.pt\"] = str(safe_copy(FINAL_PT, BUNDLE_DIR, \"final_hybrid_model.pt\"))\n",
    "copied[\"best_threshold.json\"]   = str(safe_copy(BEST_THR, BUNDLE_DIR, \"best_threshold.json\"))\n",
    "\n",
    "if THR_TAB.exists():\n",
    "    copied[\"threshold_table.csv\"] = str(safe_copy(THR_TAB, BUNDLE_DIR, \"threshold_table.csv\"))\n",
    "\n",
    "if BEST_CFG.exists():\n",
    "    copied[\"best_config.json\"] = str(safe_copy(BEST_CFG, BUNDLE_DIR, \"best_config.json\"))\n",
    "\n",
    "if PATHS_JS.exists():\n",
    "    copied[\"paths.json\"] = str(safe_copy(PATHS_JS, BUNDLE_DIR, \"paths.json\"))\n",
    "\n",
    "if match_cfg_json is not None and Path(match_cfg_json).exists():\n",
    "    copied[\"match_cfg.json\"] = str(safe_copy(match_cfg_json, BUNDLE_DIR, \"match_cfg.json\"))\n",
    "\n",
    "# README\n",
    "readme = BUNDLE_DIR / \"README.txt\"\n",
    "readme.write_text(\n",
    "    \"Hybrid (OPSI-1) bundle contents:\\n\"\n",
    "    \"- final_hybrid_model.pt : Torch state_dict + cfg + recommended_thr + maskprob_dir + metadata\\n\"\n",
    "    \"- best_threshold.json   : recommended_thr + postprocess params + cfg_id\\n\"\n",
    "    \"- threshold_table.csv   : sweep table (optional)\\n\"\n",
    "    \"- best_config.json      : HPO best trial (optional)\\n\"\n",
    "    \"- paths.json            : dataset paths used (optional)\\n\"\n",
    "    \"- match_cfg.json        : robust matching cfg used (optional)\\n\\n\"\n",
    "    \"Load example:\\n\"\n",
    "    \"  import torch, json\\n\"\n",
    "    \"  pack = torch.load('final_hybrid_model.pt', map_location='cpu')\\n\"\n",
    "    \"  thr  = json.load(open('best_threshold.json'))['recommended_thr']\\n\"\n",
    ")\n",
    "\n",
    "# Manifest (sha256 + env + metadata)\n",
    "files = sorted([p for p in BUNDLE_DIR.glob(\"*\") if p.is_file() and p.name != \"manifest.json\"])\n",
    "hashes = {p.name: sha256_file(p) for p in files}\n",
    "\n",
    "try:\n",
    "    thr_js = json.loads(BEST_THR.read_text())\n",
    "    rec_thr = float(pack.get(\"recommended_thr\", thr_js.get(\"recommended_thr\", 0.5)))\n",
    "except Exception:\n",
    "    rec_thr = float(pack.get(\"recommended_thr\", 0.5))\n",
    "\n",
    "meta = {\n",
    "    \"bundle_dir\": str(BUNDLE_DIR),\n",
    "    \"created_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n",
    "    \"cfg_id\": cfg_id,\n",
    "    \"recommended_thr\": rec_thr,\n",
    "    \"best_score_dice_proxy\": float(pack.get(\"best_score_dice_proxy\", np.nan)),\n",
    "    \"tok_grid\": pack.get(\"tok_grid\", {}),\n",
    "    \"input_channels\": int(pack.get(\"input_channels\", -1)),\n",
    "    \"token_dim\": int(pack.get(\"token_dim\", -1)),\n",
    "    \"maskprob_dir\": str(pack.get(\"maskprob_dir\", \"\")),\n",
    "    \"cfg_export\": pack.get(\"cfg_export\", {}),\n",
    "    \"env\": {\n",
    "        \"python\": platform.python_version(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"torch\": torch.__version__,\n",
    "        \"cuda_available\": bool(torch.cuda.is_available()),\n",
    "        \"cuda_version\": torch.version.cuda,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "    },\n",
    "    \"files_copied\": copied,\n",
    "    \"sha256\": hashes,\n",
    "}\n",
    "\n",
    "manifest_path = BUNDLE_DIR / \"manifest.json\"\n",
    "manifest_path.write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "# ZIP\n",
    "ZIP_PATH = Path(f\"/kaggle/working/hybrid_model_bundle_{cfg_id}_{stamp}.zip\")\n",
    "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in sorted(BUNDLE_DIR.glob(\"*\")):\n",
    "        if p.is_file():\n",
    "            z.write(p, arcname=p.name)\n",
    "\n",
    "print(\"BUNDLE_DIR:\", BUNDLE_DIR)\n",
    "print(\"ZIP_PATH  :\", ZIP_PATH)\n",
    "print(\"Files:\", [p.name for p in sorted(BUNDLE_DIR.glob('*')) if p.is_file()])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14878066,
     "sourceId": 113558,
     "sourceType": "competition"
    },
    {
     "datasetId": 9171323,
     "sourceId": 14476636,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38909.156713,
   "end_time": "2026-01-13T16:45:24.647021",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-13T05:56:55.490308",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
