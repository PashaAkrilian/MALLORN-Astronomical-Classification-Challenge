{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ccd9f1",
   "metadata": {},
   "source": [
    "# Pahami struktur data & indeks object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd226a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 (REVISI FULL) — Pahami struktur data & indeks object_id -> split\n",
    "# + (OPSIONAL) Sequential scan per split (chunked) untuk ringkasan lightcurve files\n",
    "#\n",
    "# Root dataset:\n",
    "#   D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\n",
    "#\n",
    "# Output (artifacts/):\n",
    "#   - train_log_clean.csv\n",
    "#   - test_log_clean.csv\n",
    "#   - index_object_split.csv\n",
    "#   - splits_summary.csv\n",
    "#   - split_files_summary.csv      (jika SCAN_SPLIT_FILES=True)\n",
    "# ============================================================\n",
    "\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "\n",
    "# Sequential scan settings (opsional)\n",
    "SCAN_SPLIT_FILES = True            # True = scan train/test_full_lightcurves.csv tiap split (chunked)\n",
    "CHUNK_ROWS = 1_000_000             # makin besar makin cepat tapi lebih berat RAM\n",
    "ONLY_SCAN_SPLITS_IN_LOG = True     # True = scan split yang muncul di log saja, bukan selalu 01..20\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS\n",
    "# ----------------------------\n",
    "PATHS = {\n",
    "    \"root\": DATA_ROOT,\n",
    "    \"train_log\": DATA_ROOT / \"train_log.csv\",\n",
    "    \"test_log\":  DATA_ROOT / \"test_log.csv\",\n",
    "    \"sample_submission\": DATA_ROOT / \"sample_submission.csv\",\n",
    "    \"artifacts\": DATA_ROOT / \"artifacts\",\n",
    "}\n",
    "PATHS[\"artifacts\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _fail(msg: str):\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "for k in [\"train_log\", \"test_log\", \"sample_submission\"]:\n",
    "    if not PATHS[k].exists():\n",
    "        _fail(f\"Missing file: {PATHS[k]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "_SPLIT_RE = re.compile(r\"(\\d+)\")\n",
    "def normalize_split(x) -> str:\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    m = _SPLIT_RE.search(s)\n",
    "    if not m:\n",
    "        return s\n",
    "    n = int(m.group(1))\n",
    "    return f\"split_{n:02d}\"\n",
    "\n",
    "def read_csv_safely(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols, name=\"df\"):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        _fail(f\"{name} missing columns: {missing}\\nFound columns: {list(df.columns)}\")\n",
    "\n",
    "def file_bytes(path: Path) -> int:\n",
    "    try:\n",
    "        return path.stat().st_size\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def scan_object_ids_csv(csv_path: Path, chunk_rows: int = 1_000_000) -> dict:\n",
    "    \"\"\"\n",
    "    Scan hanya kolom 'object_id' secara chunked.\n",
    "    Return: total_rows, unique_object_ids (count), ok (bool), err (str)\n",
    "    \"\"\"\n",
    "    res = {\n",
    "        \"path\": str(csv_path),\n",
    "        \"exists\": csv_path.exists(),\n",
    "        \"bytes\": file_bytes(csv_path),\n",
    "        \"total_rows\": 0,\n",
    "        \"unique_object_ids\": 0,\n",
    "        \"ok\": False,\n",
    "        \"err\": \"\",\n",
    "    }\n",
    "    if not csv_path.exists():\n",
    "        res[\"err\"] = \"file_missing\"\n",
    "        return res\n",
    "\n",
    "    try:\n",
    "        uniq = set()\n",
    "        total = 0\n",
    "        # Hanya baca kolom object_id agar ringan\n",
    "        for chunk in pd.read_csv(csv_path, usecols=[\"object_id\"], dtype={\"object_id\": \"string\"},\n",
    "                                 chunksize=chunk_rows, low_memory=False):\n",
    "            # dropna + convert to python str\n",
    "            vals = chunk[\"object_id\"].dropna().astype(str).tolist()\n",
    "            total += len(vals)\n",
    "            uniq.update(vals)\n",
    "\n",
    "        res[\"total_rows\"] = int(total)\n",
    "        res[\"unique_object_ids\"] = int(len(uniq))\n",
    "        res[\"ok\"] = True\n",
    "        return res\n",
    "    except ValueError as e:\n",
    "        # biasanya terjadi jika kolom object_id tidak ditemukan\n",
    "        res[\"err\"] = f\"ValueError: {e}\"\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        res[\"err\"] = f\"{type(e).__name__}: {e}\"\n",
    "        return res\n",
    "\n",
    "# ----------------------------\n",
    "# Load logs\n",
    "# ----------------------------\n",
    "df_train_log = read_csv_safely(PATHS[\"train_log\"])\n",
    "df_test_log  = read_csv_safely(PATHS[\"test_log\"])\n",
    "\n",
    "ensure_cols(df_train_log, [\"object_id\", \"Z\", \"EBV\", \"split\", \"target\"], \"train_log\")\n",
    "ensure_cols(df_test_log,  [\"object_id\", \"Z\", \"EBV\", \"split\"], \"test_log\")\n",
    "\n",
    "df_train_log = df_train_log.copy()\n",
    "df_test_log  = df_test_log.copy()\n",
    "\n",
    "# Normalize split naming\n",
    "df_train_log[\"split\"] = df_train_log[\"split\"].apply(normalize_split)\n",
    "df_test_log[\"split\"]  = df_test_log[\"split\"].apply(normalize_split)\n",
    "\n",
    "# Coerce types\n",
    "df_train_log[\"object_id\"] = df_train_log[\"object_id\"].astype(str)\n",
    "df_test_log[\"object_id\"]  = df_test_log[\"object_id\"].astype(str)\n",
    "\n",
    "for col in [\"Z\", \"EBV\"]:\n",
    "    df_train_log_CONFIRM = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n",
    "    df_test_log_CONFIRM  = pd.to_numeric(df_test_log[col], errors=\"coerce\")\n",
    "    df_train_log[col] = df_train_log_CONFIRM\n",
    "    df_test_log[col]  = df_test_log_CONFIRM\n",
    "\n",
    "if \"Z_err\" in df_train_log.columns:\n",
    "    df_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\")\n",
    "if \"Z_err\" in df_test_log.columns:\n",
    "    df_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "df_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ----------------------------\n",
    "# Basic sanity checks\n",
    "# ----------------------------\n",
    "dup_tr = int(df_train_log[\"object_id\"].duplicated().sum())\n",
    "dup_te = int(df_test_log[\"object_id\"].duplicated().sum())\n",
    "if dup_tr > 0 or dup_te > 0:\n",
    "    print(f\"[WARN] Duplicate object_id found | train={dup_tr}, test={dup_te}. Keeping first occurrence.\")\n",
    "    df_train_log = df_train_log.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "    df_test_log  = df_test_log.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "overlap = set(df_train_log[\"object_id\"]).intersection(set(df_test_log[\"object_id\"]))\n",
    "if len(overlap) > 0:\n",
    "    print(f\"[WARN] Found {len(overlap)} object_id present in BOTH train and test (unexpected). Example: {list(sorted(overlap))[:3]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build split -> ids mapping\n",
    "# ----------------------------\n",
    "split_to_train_ids = df_train_log.groupby(\"split\")[\"object_id\"].apply(list).to_dict()\n",
    "split_to_test_ids  = df_test_log.groupby(\"split\")[\"object_id\"].apply(list).to_dict()\n",
    "\n",
    "all_splits_in_log = sorted(set(df_train_log[\"split\"].dropna()).union(set(df_test_log[\"split\"].dropna())))\n",
    "\n",
    "# Optional: enforce split_01..split_20 existence check\n",
    "missing_split_dirs = [s for s in all_splits_in_log if not (PATHS[\"root\"] / s).exists()]\n",
    "if missing_split_dirs:\n",
    "    print(\"[WARN] Some split folders referenced in logs do not exist on disk:\")\n",
    "    for s in missing_split_dirs[:50]:\n",
    "        print(\"  -\", s)\n",
    "\n",
    "# ----------------------------\n",
    "# Combined index dataframe (1 row per object_id)\n",
    "# ----------------------------\n",
    "df_train_idx = df_train_log[[\"object_id\", \"split\", \"Z\", \"EBV\"]].copy()\n",
    "df_train_idx[\"is_train\"] = 1\n",
    "df_train_idx[\"target\"] = df_train_log[\"target\"]\n",
    "\n",
    "df_test_idx = df_test_log[[\"object_id\", \"split\", \"Z\", \"EBV\"]].copy()\n",
    "df_test_idx[\"is_train\"] = 0\n",
    "df_test_idx[\"target\"] = pd.NA\n",
    "\n",
    "if \"Z_err\" in df_train_log.columns or \"Z_err\" in df_test_log.columns:\n",
    "    df_train_idx[\"Z_err\"] = df_train_log[\"Z_err\"] if \"Z_err\" in df_train_log.columns else pd.NA\n",
    "    df_test_idx[\"Z_err\"]  = df_test_log[\"Z_err\"]  if \"Z_err\" in df_test_log.columns  else pd.NA\n",
    "\n",
    "df_index = pd.concat([df_train_idx, df_test_idx], ignore_index=True)\n",
    "df_index = df_index.sort_values([\"is_train\", \"split\", \"object_id\"], ascending=[False, True, True]).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Summaries\n",
    "# ----------------------------\n",
    "pos = int((df_train_log[\"target\"] == 1).sum())\n",
    "neg = int((df_train_log[\"target\"] == 0).sum())\n",
    "tot = len(df_train_log)\n",
    "pos_rate = pos / max(tot, 1)\n",
    "\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Root             : {PATHS['root']}\")\n",
    "print(f\"Train objects    : {len(df_train_log):,}\")\n",
    "print(f\"Test objects     : {len(df_test_log):,}\")\n",
    "print(f\"Train target     : pos={pos:,} | neg={neg:,} | pos_rate={pos_rate:.4f}\")\n",
    "print(f\"Splits in log    : {len(all_splits_in_log)} | example: {all_splits_in_log[:5]}\")\n",
    "\n",
    "def _split_summary(df_log: pd.DataFrame, is_train: int) -> pd.DataFrame:\n",
    "    g = df_log.groupby(\"split\").agg(\n",
    "        n_objects=(\"object_id\", \"count\"),\n",
    "        z_mean=(\"Z\", \"mean\"),\n",
    "        z_std=(\"Z\", \"std\"),\n",
    "        ebv_mean=(\"EBV\", \"mean\"),\n",
    "        ebv_std=(\"EBV\", \"std\"),\n",
    "    ).reset_index()\n",
    "    g[\"is_train\"] = is_train\n",
    "    if is_train and \"target\" in df_log.columns:\n",
    "        gg = df_log.groupby(\"split\")[\"target\"].agg(\n",
    "            pos=lambda x: int((x == 1).sum()),\n",
    "            neg=lambda x: int((x == 0).sum()),\n",
    "        ).reset_index()\n",
    "        g = g.merge(gg, on=\"split\", how=\"left\")\n",
    "        g[\"pos_rate\"] = g[\"pos\"] / g[\"n_objects\"].clip(lower=1)\n",
    "    return g\n",
    "\n",
    "df_sum_train = _split_summary(df_train_log, 1)\n",
    "df_sum_test  = _split_summary(df_test_log, 0)\n",
    "df_splits_summary = pd.concat([df_sum_train, df_sum_test], ignore_index=True).sort_values(\n",
    "    [\"is_train\",\"split\"], ascending=[False, True]\n",
    ")\n",
    "\n",
    "print(\"\\n=== SPLIT SUMMARY (top 10 rows) ===\")\n",
    "print(df_splits_summary.head(10).to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# OPTIONAL: Sequential scan split files (chunked)\n",
    "# ----------------------------\n",
    "df_split_files_summary = None\n",
    "\n",
    "if SCAN_SPLIT_FILES:\n",
    "    print(\"\\n=== SEQUENTIAL SCAN SPLIT FILES (chunked object_id only) ===\")\n",
    "    if ONLY_SCAN_SPLITS_IN_LOG:\n",
    "        splits_to_scan = all_splits_in_log\n",
    "    else:\n",
    "        splits_to_scan = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "\n",
    "    rows = []\n",
    "    for i, sp in enumerate(splits_to_scan, start=1):\n",
    "        sp_dir = PATHS[\"root\"] / sp\n",
    "        train_lc = sp_dir / \"train_full_lightcurves.csv\"\n",
    "        test_lc  = sp_dir / \"test_full_lightcurves.csv\"\n",
    "\n",
    "        print(f\"[{i}/{len(splits_to_scan)}] {sp} | scanning train/test lightcurves...\")\n",
    "\n",
    "        tr_scan = scan_object_ids_csv(train_lc, chunk_rows=CHUNK_ROWS)\n",
    "        te_scan = scan_object_ids_csv(test_lc,  chunk_rows=CHUNK_ROWS)\n",
    "\n",
    "        # Compare to logs (berapa object log yang seharusnya ada)\n",
    "        tr_log_n = len(split_to_train_ids.get(sp, []))\n",
    "        te_log_n = len(split_to_test_ids.get(sp, []))\n",
    "\n",
    "        rows.append({\n",
    "            \"split\": sp,\n",
    "            \"train_lc_exists\": tr_scan[\"exists\"],\n",
    "            \"train_lc_bytes\": tr_scan[\"bytes\"],\n",
    "            \"train_lc_total_rows\": tr_scan[\"total_rows\"],\n",
    "            \"train_lc_unique_object_ids\": tr_scan[\"unique_object_ids\"],\n",
    "            \"train_lc_ok\": tr_scan[\"ok\"],\n",
    "            \"train_lc_err\": tr_scan[\"err\"],\n",
    "            \"train_log_objects\": tr_log_n,\n",
    "            \"train_coverage_ratio\": (tr_scan[\"unique_object_ids\"] / tr_log_n) if tr_log_n > 0 else pd.NA,\n",
    "\n",
    "            \"test_lc_exists\": te_scan[\"exists\"],\n",
    "            \"test_lc_bytes\": te_scan[\"bytes\"],\n",
    "            \"test_lc_total_rows\": te_scan[\"total_rows\"],\n",
    "            \"test_lc_unique_object_ids\": te_scan[\"unique_object_ids\"],\n",
    "            \"test_lc_ok\": te_scan[\"ok\"],\n",
    "            \"test_lc_err\": te_scan[\"err\"],\n",
    "            \"test_log_objects\": te_log_n,\n",
    "            \"test_coverage_ratio\": (te_scan[\"unique_object_ids\"] / te_log_n) if te_log_n > 0 else pd.NA,\n",
    "        })\n",
    "\n",
    "    df_split_files_summary = pd.DataFrame(rows).sort_values(\"split\").reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== SPLIT FILES SUMMARY (top 10) ===\")\n",
    "    print(df_split_files_summary.head(10).to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# Save artifacts\n",
    "# ----------------------------\n",
    "(df_train_log).to_csv(PATHS[\"artifacts\"] / \"train_log_clean.csv\", index=False)\n",
    "(df_test_log).to_csv(PATHS[\"artifacts\"] / \"test_log_clean.csv\", index=False)\n",
    "(df_index).to_csv(PATHS[\"artifacts\"] / \"index_object_split.csv\", index=False)\n",
    "(df_splits_summary).to_csv(PATHS[\"artifacts\"] / \"splits_summary.csv\", index=False)\n",
    "\n",
    "if df_split_files_summary is not None:\n",
    "    df_split_files_summary.to_csv(PATHS[\"artifacts\"] / \"split_files_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts to:\", PATHS[\"artifacts\"])\n",
    "print(\" - train_log_clean.csv\")\n",
    "print(\" - test_log_clean.csv\")\n",
    "print(\" - index_object_split.csv\")\n",
    "print(\" - splits_summary.csv\")\n",
    "if df_split_files_summary is not None:\n",
    "    print(\" - split_files_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a69fa",
   "metadata": {},
   "source": [
    "# Baseline super cepat (cek pipeline benar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — BASELINE SUPER CEPAT (CEK PIPELINE BENAR) [REVISI]\n",
    "# Model: Logistic Regression (CPU cepat)\n",
    "# Fitur: hanya dari log (Z, EBV, Z_err + missing flags)\n",
    "#\n",
    "# Fix:\n",
    "# - sample_submission.csv di dataset kamu pakai kolom 'prediction' (bukan 'target')\n",
    "# - kode ini auto-detect nama kolom prediksi dari sample_submission\n",
    "#\n",
    "# Input:\n",
    "#   artifacts/train_log_clean.csv\n",
    "#   artifacts/test_log_clean.csv\n",
    "#   sample_submission.csv\n",
    "#\n",
    "# Output:\n",
    "#   artifacts/baseline_oof.csv\n",
    "#   artifacts/baseline_threshold.txt\n",
    "#   submissions/sub_baseline_logreg.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 2025\n",
    "N_FOLDS = 5\n",
    "\n",
    "# ----------------------------\n",
    "# Load artifacts from Stage 1\n",
    "# ----------------------------\n",
    "train_path = ART_DIR / \"train_log_clean.csv\"\n",
    "test_path  = ART_DIR / \"test_log_clean.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {train_path}. Jalankan Stage 1 dulu.\")\n",
    "if not test_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_path}. Jalankan Stage 1 dulu.\")\n",
    "if not sample_sub_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {sample_sub_path}\")\n",
    "\n",
    "df_tr = pd.read_csv(train_path, low_memory=False)\n",
    "df_te = pd.read_csv(test_path, low_memory=False)\n",
    "df_sub = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Detect submission prediction column name\n",
    "# ----------------------------\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "SUB_PRED_COL = pred_cols[0]  # di kasus kamu: 'prediction'\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# Minimal checks\n",
    "# ----------------------------\n",
    "need_tr = [\"object_id\", \"Z\", \"EBV\", \"split\", \"target\"]\n",
    "need_te = [\"object_id\", \"Z\", \"EBV\", \"split\"]\n",
    "\n",
    "for c in need_tr:\n",
    "    if c not in df_tr.columns:\n",
    "        raise ValueError(f\"train missing col: {c} | found={list(df_tr.columns)}\")\n",
    "for c in need_te:\n",
    "    if c not in df_te.columns:\n",
    "        raise ValueError(f\"test missing col: {c} | found={list(df_te.columns)}\")\n",
    "\n",
    "# Ensure numeric\n",
    "for col in [\"Z\", \"EBV\"]:\n",
    "    df_tr[col] = pd.to_numeric(df_tr[col], errors=\"coerce\")\n",
    "    df_te[col] = pd.to_numeric(df_te[col], errors=\"coerce\")\n",
    "\n",
    "# Z_err optional: train biasanya kosong/tidak ada, test ada\n",
    "if \"Z_err\" not in df_tr.columns:\n",
    "    df_tr[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_tr[\"Z_err\"] = pd.to_numeric(df_tr[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "if \"Z_err\" not in df_te.columns:\n",
    "    df_te[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_te[\"Z_err\"] = pd.to_numeric(df_te[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "# Target\n",
    "y = pd.to_numeric(df_tr[\"target\"], errors=\"coerce\").astype(int).values\n",
    "\n",
    "# ----------------------------\n",
    "# Feature set (log-only)\n",
    "# ----------------------------\n",
    "def build_log_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = pd.DataFrame({\n",
    "        \"Z\": df[\"Z\"].astype(float),\n",
    "        \"EBV\": df[\"EBV\"].astype(float),\n",
    "        \"Z_err\": df[\"Z_err\"].astype(float),\n",
    "        \"Z_isna\": df[\"Z\"].isna().astype(int),\n",
    "        \"EBV_isna\": df[\"EBV\"].isna().astype(int),\n",
    "        \"Zerr_isna\": df[\"Z_err\"].isna().astype(int),\n",
    "    })\n",
    "\n",
    "    # stabilisasi sederhana (optional)\n",
    "    def _p99(a):\n",
    "        a = np.asarray(a, dtype=float)\n",
    "        a = a[np.isfinite(a)]\n",
    "        if a.size == 0:\n",
    "            return np.nan\n",
    "        return float(np.nanpercentile(a, 99))\n",
    "\n",
    "    z99 = _p99(X[\"Z\"].values)\n",
    "    e99 = _p99(X[\"EBV\"].values)\n",
    "    zerr99 = _p99(X[\"Z_err\"].values)\n",
    "\n",
    "    X[\"Z_clip\"] = X[\"Z\"].clip(lower=0, upper=z99 if np.isfinite(z99) else 10.0)\n",
    "    X[\"EBV_clip\"] = X[\"EBV\"].clip(lower=0, upper=e99 if np.isfinite(e99) else 1.0)\n",
    "    X[\"Zerr_clip\"] = X[\"Z_err\"].clip(lower=0, upper=zerr99 if np.isfinite(zerr99) else 1.0)\n",
    "    return X\n",
    "\n",
    "X_tr = build_log_features(df_tr)\n",
    "X_te = build_log_features(df_te)\n",
    "\n",
    "# ----------------------------\n",
    "# Model pipeline (fast CPU)\n",
    "# ----------------------------\n",
    "clf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=500,\n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# CV + OOF\n",
    "# ----------------------------\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "oof_proba = np.zeros(len(X_tr), dtype=np.float32)\n",
    "\n",
    "fold_scores = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tr, y), start=1):\n",
    "    X_train, X_val = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
    "    y_train, y_val = y[tr_idx], y[va_idx]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    p_val = clf.predict_proba(X_val)[:, 1]\n",
    "    oof_proba[va_idx] = p_val\n",
    "\n",
    "    f1_05 = f1_score(y_val, (p_val >= 0.5).astype(int))\n",
    "    fold_scores.append(f1_05)\n",
    "    print(f\"[FOLD {fold}/{N_FOLDS}] F1@0.50 = {f1_05:.5f}\")\n",
    "\n",
    "print(\"\\n=== CV SUMMARY (threshold=0.50) ===\")\n",
    "print(f\"Mean F1@0.50: {float(np.mean(fold_scores)):.5f} | Std: {float(np.std(fold_scores)):.5f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold tuning (global OOF)\n",
    "# ----------------------------\n",
    "thr_grid = np.linspace(0.01, 0.99, 99)\n",
    "best_thr, best_f1 = 0.5, -1.0\n",
    "for thr in thr_grid:\n",
    "    f1 = f1_score(y, (oof_proba >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "print(\"\\n=== OOF THRESHOLD TUNING ===\")\n",
    "print(f\"Best threshold = {best_thr:.2f}\")\n",
    "print(f\"OOF F1(best)   = {best_f1:.5f}\")\n",
    "\n",
    "y_hat = (oof_proba >= best_thr).astype(int)\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(confusion_matrix(y, y_hat))\n",
    "\n",
    "print(\"\\nClassification Report (OOF):\")\n",
    "print(classification_report(y, y_hat, digits=4))\n",
    "\n",
    "# Save OOF artifact\n",
    "df_oof = pd.DataFrame({\n",
    "    \"object_id\": df_tr[\"object_id\"].astype(str),\n",
    "    \"oof_proba\": oof_proba,\n",
    "    \"target\": y\n",
    "})\n",
    "df_oof.to_csv(ART_DIR / \"baseline_oof.csv\", index=False)\n",
    "\n",
    "with open(ART_DIR / \"baseline_threshold.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{best_thr}\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train full + predict test + export submission\n",
    "# ----------------------------\n",
    "clf.fit(X_tr, y)\n",
    "test_proba = clf.predict_proba(X_te)[:, 1]\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_te[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int)\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_baseline_logreg.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Saved:\", ART_DIR / \"baseline_oof.csv\")\n",
    "print(\"Saved:\", ART_DIR / \"baseline_threshold.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f488b",
   "metadata": {},
   "source": [
    "# Koreksi extinction (de-extinct flux) + fitur statistik per band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad19f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — DE-EXTINCT FLUX + FITUR STATISTIK PER BAND (CHUNKED)\n",
    "#\n",
    "# Output:\n",
    "#   artifacts/features_lc_train.csv\n",
    "#   artifacts/features_lc_test.csv\n",
    "#   artifacts/features_log.csv              (log features per object)\n",
    "#   artifacts/features_merged_train.csv     (log + lc)\n",
    "#   artifacts/features_merged_test.csv      (log + lc)\n",
    "#\n",
    "# Notes:\n",
    "# - Hemat RAM: baca lightcurve per split, per chunk\n",
    "# - Statistik yang dihitung (per object_id x band):\n",
    "#   n_obs, flux_mean, flux_std, flux_min, flux_max, amp, snr_mean, frac_snr_gt3, frac_snr_gt5, time_span\n",
    "# ============================================================\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_ROWS = 1_000_000\n",
    "BANDS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Load logs (clean)\n",
    "# ----------------------------\n",
    "train_log_path = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path  = ART_DIR / \"test_log_clean.csv\"\n",
    "if not train_log_path.exists() or not test_log_path.exists():\n",
    "    raise FileNotFoundError(\"Missing cleaned logs. Jalankan Stage 1 dulu.\")\n",
    "\n",
    "df_tr = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te = pd.read_csv(test_log_path, low_memory=False)\n",
    "\n",
    "# numeric\n",
    "for col in [\"Z\", \"EBV\"]:\n",
    "    df_tr[col] = pd.to_numeric(df_tr[col], errors=\"coerce\")\n",
    "    df_te[col] = pd.to_numeric(df_te[col], errors=\"coerce\")\n",
    "\n",
    "if \"Z_err\" not in df_tr.columns:\n",
    "    df_tr[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_tr[\"Z_err\"] = pd.to_numeric(df_tr[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "if \"Z_err\" not in df_te.columns:\n",
    "    df_te[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_te[\"Z_err\"] = pd.to_numeric(df_te[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "df_tr[\"object_id\"] = df_tr[\"object_id\"].astype(str)\n",
    "df_te[\"object_id\"] = df_te[\"object_id\"].astype(str)\n",
    "\n",
    "# ----------------------------\n",
    "# EBV map for de-extinction\n",
    "# ----------------------------\n",
    "ebv_map = pd.concat([\n",
    "    df_tr[[\"object_id\", \"EBV\"]],\n",
    "    df_te[[\"object_id\", \"EBV\"]],\n",
    "], ignore_index=True).drop_duplicates(\"object_id\")\n",
    "ebv_dict = dict(zip(ebv_map[\"object_id\"].values, ebv_map[\"EBV\"].values))\n",
    "\n",
    "# ----------------------------\n",
    "# Auto-extract extinction coefficients from Using_the_Data notebook (if exists)\n",
    "# ----------------------------\n",
    "def try_extract_extinction_coeffs(root: Path):\n",
    "    \"\"\"\n",
    "    Cari file *Using_the_Data*.ipynb, lalu coba ekstrak dict yang berisi key u,g,r,i,z,y dan value float.\n",
    "    \"\"\"\n",
    "    ipynbs = list(root.rglob(\"*Using_the_Data*.ipynb\"))\n",
    "    if not ipynbs:\n",
    "        return None, None\n",
    "\n",
    "    for nb_path in ipynbs[:5]:\n",
    "        try:\n",
    "            nb = json.loads(nb_path.read_text(encoding=\"utf-8\"))\n",
    "            cells = nb.get(\"cells\", [])\n",
    "            text = []\n",
    "            for c in cells:\n",
    "                if c.get(\"cell_type\") in (\"code\", \"markdown\"):\n",
    "                    src = c.get(\"source\", [])\n",
    "                    if isinstance(src, list):\n",
    "                        text.append(\"\".join(src))\n",
    "                    elif isinstance(src, str):\n",
    "                        text.append(src)\n",
    "            blob = \"\\n\".join(text)\n",
    "\n",
    "            # cari pattern dict: 'u': 4.0, 'g': 3.0, ...\n",
    "            pairs = re.findall(r\"['\\\"]([ugrizy])['\\\"]\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)\", blob)\n",
    "            if not pairs:\n",
    "                continue\n",
    "\n",
    "            d = {}\n",
    "            for k, v in pairs:\n",
    "                d[k] = float(v)\n",
    "\n",
    "            # harus punya semua bands\n",
    "            if all(b in d for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]):\n",
    "                return d, nb_path\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return None, None\n",
    "\n",
    "EXT_COEFF, SRC_NB = try_extract_extinction_coeffs(DATA_ROOT)\n",
    "\n",
    "# fallback (jika notebook tidak ketemu / gagal parse)\n",
    "# kamu boleh ganti angka ini bila ingin sama persis dengan notebook Using_the_Data.\n",
    "FALLBACK_EXT_COEFF = {\n",
    "    \"u\": 4.2,\n",
    "    \"g\": 3.3,\n",
    "    \"r\": 2.3,\n",
    "    \"i\": 1.7,\n",
    "    \"z\": 1.3,\n",
    "    \"y\": 1.1,\n",
    "}\n",
    "\n",
    "if EXT_COEFF is None:\n",
    "    EXT_COEFF = FALLBACK_EXT_COEFF\n",
    "    print(\"[WARN] Tidak berhasil ekstrak koefisien extinction dari Using_the_Data notebook.\")\n",
    "    print(\"       Pakai FALLBACK_EXT_COEFF. Jika mau 100% sama, ganti nilai dict ini sesuai notebook.\")\n",
    "else:\n",
    "    print(f\"[INFO] Extinction coeffs loaded from: {SRC_NB}\")\n",
    "print(\"[INFO] EXT_COEFF =\", EXT_COEFF)\n",
    "\n",
    "def de_extinct_flux(flux: np.ndarray, ebv: np.ndarray, band: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    flux_corr = flux * 10^(0.4 * A_lambda), dengan A_lambda = EXT_COEFF[band] * EBV\n",
    "    \"\"\"\n",
    "    R = float(EXT_COEFF.get(band, 0.0))\n",
    "    A = R * ebv\n",
    "    factor = np.power(10.0, 0.4 * A)\n",
    "    return flux * factor\n",
    "\n",
    "# ----------------------------\n",
    "# Lightcurve column detection (robust to naming)\n",
    "# ----------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\s\\(\\)\\[\\]\\-]+\", \"\", s)\n",
    "    s = s.replace(\"_\", \"\")\n",
    "    return s\n",
    "\n",
    "def detect_lc_columns(csv_path: Path):\n",
    "    head = pd.read_csv(csv_path, nrows=0)\n",
    "    cols = list(head.columns)\n",
    "    ncols = {_norm(c): c for c in cols}\n",
    "\n",
    "    def pick(cands):\n",
    "        for c in cands:\n",
    "            if c in ncols:\n",
    "                return ncols[c]\n",
    "        return None\n",
    "\n",
    "    col_object = pick([\"objectid\", \"object_id\"])\n",
    "    col_time   = pick([\"timemjd\", \"time\", \"mjd\", \"timemodifiedjuliandate\"])\n",
    "    col_flux   = pick([\"flux\"])\n",
    "    col_ferr   = pick([\"fluxerr\", \"flux_err\", \"fluxerror\", \"fluxunc\", \"fluxuncertainty\"])\n",
    "    col_filt   = pick([\"filter\", \"band\", \"passband\"])\n",
    "\n",
    "    got = {\n",
    "        \"object_id\": col_object,\n",
    "        \"mjd\": col_time,\n",
    "        \"flux\": col_flux,\n",
    "        \"flux_err\": col_ferr,\n",
    "        \"filter\": col_filt,\n",
    "    }\n",
    "    if any(v is None for v in got.values()):\n",
    "        raise ValueError(f\"Kolom lightcurve tidak terdeteksi lengkap di {csv_path}\\n\"\n",
    "                         f\"Detected mapping: {got}\\nAll columns: {cols}\")\n",
    "    return got\n",
    "\n",
    "# ----------------------------\n",
    "# Chunked scan + partial aggregation\n",
    "# ----------------------------\n",
    "def partial_agg_from_file(csv_path: Path, object_ids_set: set, ebv_dict: dict, chunk_rows: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return partial aggregated stats per (object_id, filter) dari 1 file CSV lightcurve.\n",
    "    \"\"\"\n",
    "    if not csv_path.exists():\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    colmap = detect_lc_columns(csv_path)\n",
    "    usecols = list(colmap.values())\n",
    "\n",
    "    parts = []\n",
    "    reader = pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_rows, low_memory=False)\n",
    "\n",
    "    for chunk in reader:\n",
    "        chunk = chunk.rename(columns={v: k for k, v in colmap.items()})\n",
    "\n",
    "        # types\n",
    "        chunk[\"object_id\"] = chunk[\"object_id\"].astype(str)\n",
    "        chunk = chunk[chunk[\"object_id\"].isin(object_ids_set)]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        chunk[\"filter\"] = chunk[\"filter\"].astype(str).str.strip().str.lower()\n",
    "        chunk = chunk[chunk[\"filter\"].isin(BANDS)]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        chunk[\"mjd\"] = pd.to_numeric(chunk[\"mjd\"], errors=\"coerce\")\n",
    "        chunk[\"flux\"] = pd.to_numeric(chunk[\"flux\"], errors=\"coerce\")\n",
    "        chunk[\"flux_err\"] = pd.to_numeric(chunk[\"flux_err\"], errors=\"coerce\")\n",
    "\n",
    "        chunk = chunk.dropna(subset=[\"mjd\", \"flux\", \"flux_err\"])\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # EBV mapping\n",
    "        ebv = chunk[\"object_id\"].map(ebv_dict).astype(float).values\n",
    "        flx = chunk[\"flux\"].astype(float).values\n",
    "\n",
    "        # de-extinct per band (vectorized per group band)\n",
    "        # lebih cepat: apply per band mask\n",
    "        flux_corr = flx.copy()\n",
    "        for b in BANDS:\n",
    "            m = (chunk[\"filter\"].values == b)\n",
    "            if m.any():\n",
    "                flux_corr[m] = de_extinct_flux(flux_corr[m], ebv[m], b)\n",
    "\n",
    "        chunk[\"flux_corr\"] = flux_corr\n",
    "\n",
    "        # SNR\n",
    "        ferr = chunk[\"flux_err\"].astype(float).values\n",
    "        snr = np.zeros_like(ferr, dtype=float)\n",
    "        mpos = ferr > 0\n",
    "        snr[mpos] = np.abs(flux_corr[mpos]) / ferr[mpos]\n",
    "        chunk[\"snr\"] = snr\n",
    "        chunk[\"snr_gt3\"] = (snr > 3.0).astype(int)\n",
    "        chunk[\"snr_gt5\"] = (snr > 5.0).astype(int)\n",
    "\n",
    "        # partial groupby stats\n",
    "        g = chunk.groupby([\"object_id\", \"filter\"]).agg(\n",
    "            n_obs=(\"flux_corr\", \"size\"),\n",
    "            sum_flux=(\"flux_corr\", \"sum\"),\n",
    "            sum_flux2=(\"flux_corr\", lambda x: float(np.sum(np.square(x.values)))),\n",
    "            min_flux=(\"flux_corr\", \"min\"),\n",
    "            max_flux=(\"flux_corr\", \"max\"),\n",
    "            sum_snr=(\"snr\", \"sum\"),\n",
    "            cnt_snr_gt3=(\"snr_gt3\", \"sum\"),\n",
    "            cnt_snr_gt5=(\"snr_gt5\", \"sum\"),\n",
    "            min_time=(\"mjd\", \"min\"),\n",
    "            max_time=(\"mjd\", \"max\"),\n",
    "        ).reset_index()\n",
    "\n",
    "        parts.append(g)\n",
    "\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "def combine_partials(df_partials: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine partial stats menjadi final stats per (object_id, filter).\n",
    "    \"\"\"\n",
    "    if df_partials.empty:\n",
    "        return df_partials\n",
    "\n",
    "    g = df_partials.groupby([\"object_id\", \"filter\"]).agg(\n",
    "        n_obs=(\"n_obs\", \"sum\"),\n",
    "        sum_flux=(\"sum_flux\", \"sum\"),\n",
    "        sum_flux2=(\"sum_flux2\", \"sum\"),\n",
    "        min_flux=(\"min_flux\", \"min\"),\n",
    "        max_flux=(\"max_flux\", \"max\"),\n",
    "        sum_snr=(\"sum_snr\", \"sum\"),\n",
    "        cnt_snr_gt3=(\"cnt_snr_gt3\", \"sum\"),\n",
    "        cnt_snr_gt5=(\"cnt_snr_gt5\", \"sum\"),\n",
    "        min_time=(\"min_time\", \"min\"),\n",
    "        max_time=(\"max_time\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # derive features\n",
    "    n = g[\"n_obs\"].astype(float).values\n",
    "    mean = g[\"sum_flux\"].values / np.clip(n, 1.0, None)\n",
    "    var = (g[\"sum_flux2\"].values / np.clip(n, 1.0, None)) - np.square(mean)\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var)\n",
    "\n",
    "    g[\"flux_mean\"] = mean\n",
    "    g[\"flux_std\"] = std\n",
    "    g[\"amp\"] = g[\"max_flux\"] - g[\"min_flux\"]\n",
    "    g[\"snr_mean\"] = g[\"sum_snr\"].values / np.clip(n, 1.0, None)\n",
    "    g[\"frac_snr_gt3\"] = g[\"cnt_snr_gt3\"].values / np.clip(n, 1.0, None)\n",
    "    g[\"frac_snr_gt5\"] = g[\"cnt_snr_gt5\"].values / np.clip(n, 1.0, None)\n",
    "    g[\"time_span\"] = g[\"max_time\"] - g[\"min_time\"]\n",
    "\n",
    "    # keep only final columns\n",
    "    keep = [\n",
    "        \"object_id\", \"filter\",\n",
    "        \"n_obs\", \"flux_mean\", \"flux_std\", \"min_flux\", \"max_flux\",\n",
    "        \"amp\", \"snr_mean\", \"frac_snr_gt3\", \"frac_snr_gt5\", \"time_span\"\n",
    "    ]\n",
    "    return g[keep]\n",
    "\n",
    "def pivot_band_features(df_band: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pivot per band → 1 row per object_id (wide).\n",
    "    \"\"\"\n",
    "    if df_band.empty:\n",
    "        return pd.DataFrame(columns=[\"object_id\"])\n",
    "\n",
    "    feats = [c for c in df_band.columns if c not in (\"object_id\", \"filter\")]\n",
    "    wide = df_band.pivot(index=\"object_id\", columns=\"filter\", values=feats)\n",
    "\n",
    "    # flatten columns: (feat, band) -> f\"{band}__{feat}\"\n",
    "    wide.columns = [f\"{band}__{feat}\" for (feat, band) in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "    return wide\n",
    "\n",
    "# ----------------------------\n",
    "# Process all splits (train + test)\n",
    "# ----------------------------\n",
    "all_splits = sorted(set(df_tr[\"split\"].dropna().astype(str)).union(set(df_te[\"split\"].dropna().astype(str))))\n",
    "print(f\"[INFO] Total splits to process: {len(all_splits)} | example: {all_splits[:5]}\")\n",
    "\n",
    "train_wides = []\n",
    "test_wides = []\n",
    "\n",
    "for i, sp in enumerate(all_splits, start=1):\n",
    "    sp_dir = DATA_ROOT / sp\n",
    "    tr_file = sp_dir / \"train_full_lightcurves.csv\"\n",
    "    te_file = sp_dir / \"test_full_lightcurves.csv\"\n",
    "\n",
    "    tr_ids = set(df_tr.loc[df_tr[\"split\"].astype(str).eq(sp), \"object_id\"].astype(str).tolist())\n",
    "    te_ids = set(df_te.loc[df_te[\"split\"].astype(str).eq(sp), \"object_id\"].astype(str).tolist())\n",
    "\n",
    "    print(f\"\\n[{i}/{len(all_splits)}] {sp}\")\n",
    "    print(f\"  train_ids={len(tr_ids):,} | test_ids={len(te_ids):,}\")\n",
    "    print(f\"  files: train_exists={tr_file.exists()} | test_exists={te_file.exists()}\")\n",
    "\n",
    "    # TRAIN split\n",
    "    if len(tr_ids) > 0 and tr_file.exists():\n",
    "        part_tr = partial_agg_from_file(tr_file, tr_ids, ebv_dict, CHUNK_ROWS)\n",
    "        band_tr = combine_partials(part_tr)\n",
    "        wide_tr = pivot_band_features(band_tr)\n",
    "        train_wides.append(wide_tr)\n",
    "\n",
    "    # TEST split\n",
    "    if len(te_ids) > 0 and te_file.exists():\n",
    "        part_te = partial_agg_from_file(te_file, te_ids, ebv_dict, CHUNK_ROWS)\n",
    "        band_te = combine_partials(part_te)\n",
    "        wide_te = pivot_band_features(band_te)\n",
    "        test_wides.append(wide_te)\n",
    "\n",
    "# concat all splits\n",
    "df_feat_tr = pd.concat(train_wides, ignore_index=True) if train_wides else pd.DataFrame(columns=[\"object_id\"])\n",
    "df_feat_te = pd.concat(test_wides, ignore_index=True) if test_wides else pd.DataFrame(columns=[\"object_id\"])\n",
    "\n",
    "# de-dup (safety)\n",
    "df_feat_tr = df_feat_tr.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "df_feat_te = df_feat_te.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n[INFO] Lightcurve features built:\")\n",
    "print(\"  train rows:\", len(df_feat_tr), \"| cols:\", df_feat_tr.shape[1])\n",
    "print(\"  test  rows:\", len(df_feat_te), \"| cols:\", df_feat_te.shape[1])\n",
    "\n",
    "# ----------------------------\n",
    "# Log features (per object_id)\n",
    "# ----------------------------\n",
    "def build_log_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = pd.DataFrame({\n",
    "        \"object_id\": df[\"object_id\"].astype(str),\n",
    "        \"Z\": df[\"Z\"].astype(float),\n",
    "        \"EBV\": df[\"EBV\"].astype(float),\n",
    "        \"Z_err\": df[\"Z_err\"].astype(float),\n",
    "        \"Z_isna\": df[\"Z\"].isna().astype(int),\n",
    "        \"EBV_isna\": df[\"EBV\"].isna().astype(int),\n",
    "        \"Zerr_isna\": df[\"Z_err\"].isna().astype(int),\n",
    "    })\n",
    "    return out\n",
    "\n",
    "df_log_tr = build_log_features(df_tr)\n",
    "df_log_te = build_log_features(df_te)\n",
    "\n",
    "df_log_all = pd.concat([df_log_tr.assign(is_train=1), df_log_te.assign(is_train=0)], ignore_index=True)\n",
    "df_log_all.to_csv(ART_DIR / \"features_log.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Merge log + lc features\n",
    "# ----------------------------\n",
    "df_train_merged = df_log_tr.merge(df_feat_tr, on=\"object_id\", how=\"left\")\n",
    "df_test_merged  = df_log_te.merge(df_feat_te, on=\"object_id\", how=\"left\")\n",
    "\n",
    "# save\n",
    "df_feat_tr.to_csv(ART_DIR / \"features_lc_train.csv\", index=False)\n",
    "df_feat_te.to_csv(ART_DIR / \"features_lc_test.csv\", index=False)\n",
    "df_train_merged.to_csv(ART_DIR / \"features_merged_train.csv\", index=False)\n",
    "df_test_merged.to_csv(ART_DIR / \"features_merged_test.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== STAGE 3 DONE ===\")\n",
    "print(\"Saved:\")\n",
    "print(\" -\", ART_DIR / \"features_lc_train.csv\")\n",
    "print(\" -\", ART_DIR / \"features_lc_test.csv\")\n",
    "print(\" -\", ART_DIR / \"features_log.csv\")\n",
    "print(\" -\", ART_DIR / \"features_merged_train.csv\")\n",
    "print(\" -\", ART_DIR / \"features_merged_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae998a3",
   "metadata": {},
   "source": [
    "# Model utama CPU: LightGBM + CV yang benar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — MODEL UTAMA CPU: LightGBM + CV yang benar + Threshold tuning (F1)\n",
    "#\n",
    "# Prasyarat:\n",
    "# - artifacts/train_log_clean.csv\n",
    "# - artifacts/test_log_clean.csv\n",
    "# - artifacts/features_merged_train.csv   (hasil STAGE 3)\n",
    "# - artifacts/features_merged_test.csv    (hasil STAGE 3)\n",
    "# - sample_submission.csv\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/lgbm_oof.csv\n",
    "# - artifacts/lgbm_threshold.txt\n",
    "# - artifacts/lgbm_cv_report.txt\n",
    "# - artifacts/lgbm_feature_importance.csv\n",
    "# - submissions/sub_lgbm_v01.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED    = 2025\n",
    "N_FOLDS = 5\n",
    "\n",
    "# ----------------------------\n",
    "# LightGBM import (CPU)\n",
    "# ----------------------------\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"LightGBM belum terpasang. Install dulu di environment VSCode kamu:\\n\"\n",
    "        \"  pip install lightgbm\\n\"\n",
    "        f\"Original error: {type(e).__name__}: {e}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Load inputs\n",
    "# ----------------------------\n",
    "train_log_path = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path  = ART_DIR / \"test_log_clean.csv\"\n",
    "feat_tr_path   = ART_DIR / \"features_merged_train.csv\"\n",
    "feat_te_path   = ART_DIR / \"features_merged_test.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "for p in [train_log_path, test_log_path, feat_tr_path, feat_te_path, sample_sub_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p}. Pastikan Stage 1 & 3 sudah dijalankan.\")\n",
    "\n",
    "df_tr_log = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te_log = pd.read_csv(test_log_path, low_memory=False)\n",
    "df_tr_feat = pd.read_csv(feat_tr_path, low_memory=False)\n",
    "df_te_feat = pd.read_csv(feat_te_path, low_memory=False)\n",
    "df_sub = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Detect submission prediction column name\n",
    "# ----------------------------\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]  # mis: 'prediction'\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare train/test tables (merge target)\n",
    "# ----------------------------\n",
    "need_log_cols = [\"object_id\", \"target\"]\n",
    "for c in need_log_cols:\n",
    "    if c not in df_tr_log.columns:\n",
    "        raise ValueError(f\"train_log_clean missing '{c}'. Found: {list(df_tr_log.columns)}\")\n",
    "\n",
    "df_tr_log[\"object_id\"] = df_tr_log[\"object_id\"].astype(str)\n",
    "df_te_log[\"object_id\"] = df_te_log[\"object_id\"].astype(str)\n",
    "df_tr_feat[\"object_id\"] = df_tr_feat[\"object_id\"].astype(str)\n",
    "df_te_feat[\"object_id\"] = df_te_feat[\"object_id\"].astype(str)\n",
    "\n",
    "# target\n",
    "df_tr_log[\"target\"] = pd.to_numeric(df_tr_log[\"target\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "df_train = df_tr_feat.merge(df_tr_log[[\"object_id\", \"target\"]], on=\"object_id\", how=\"left\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    bad = df_train[df_train[\"target\"].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Ada object_id di features_merged_train yang tidak ketemu target. Example: {bad}\")\n",
    "\n",
    "df_test = df_te_feat.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Define features\n",
    "# ----------------------------\n",
    "drop_cols = {\"object_id\", \"target\"}\n",
    "feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "\n",
    "# Force numeric (LightGBM expects numeric)\n",
    "for c in feature_cols:\n",
    "    df_train[c] = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
    "    df_test[c]  = pd.to_numeric(df_test[c], errors=\"coerce\")\n",
    "\n",
    "X = df_train[feature_cols]\n",
    "y = df_train[\"target\"].astype(int).values\n",
    "X_test = df_test[feature_cols]\n",
    "\n",
    "print(f\"[INFO] X_train shape: {X.shape} | X_test shape: {X_test.shape}\")\n",
    "print(f\"[INFO] Pos rate: {(y==1).mean():.5f} | pos={int((y==1).sum()):,} neg={int((y==0).sum()):,}\")\n",
    "\n",
    "# class imbalance\n",
    "pos = max(int((y == 1).sum()), 1)\n",
    "neg = max(int((y == 0).sum()), 1)\n",
    "scale_pos_weight = neg / pos\n",
    "print(f\"[INFO] scale_pos_weight = {scale_pos_weight:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# LightGBM params (CPU-friendly)\n",
    "# ----------------------------\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=5000,\n",
    "    num_leaves=64,\n",
    "    min_child_samples=150,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    # imbalance\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# CV + OOF\n",
    "# ----------------------------\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_proba = np.zeros(len(X), dtype=np.float32)\n",
    "best_iters = []\n",
    "fold_f1_05 = []\n",
    "\n",
    "feat_importance_accum = np.zeros(len(feature_cols), dtype=np.float64)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), start=1):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    p_va = model.predict_proba(X_va)[:, 1]\n",
    "    oof_proba[va_idx] = p_va\n",
    "\n",
    "    f1_at_05 = f1_score(y_va, (p_va >= 0.5).astype(int))\n",
    "    fold_f1_05.append(float(f1_at_05))\n",
    "\n",
    "    best_it = int(model.best_iteration_) if getattr(model, \"best_iteration_\", None) else lgb_params[\"n_estimators\"]\n",
    "    best_iters.append(best_it)\n",
    "\n",
    "    # feature importance (gain)\n",
    "    try:\n",
    "        fi = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "        feat_importance_accum += fi\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[FOLD {fold}/{N_FOLDS}] best_iter={best_it} | F1@0.50={f1_at_05:.5f}\")\n",
    "\n",
    "print(\"\\n=== CV SUMMARY (threshold=0.50) ===\")\n",
    "print(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.5f} | Std: {float(np.std(fold_f1_05)):.5f}\")\n",
    "print(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.1f} | max={int(np.max(best_iters))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold tuning (global OOF) for F1\n",
    "# ----------------------------\n",
    "thr_grid = np.linspace(0.01, 0.99, 99)\n",
    "best_thr, best_f1 = 0.5, -1.0\n",
    "for thr in thr_grid:\n",
    "    f1 = f1_score(y, (oof_proba >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "print(\"\\n=== OOF THRESHOLD TUNING ===\")\n",
    "print(f\"Best threshold = {best_thr:.2f}\")\n",
    "print(f\"OOF F1(best)   = {best_f1:.5f}\")\n",
    "\n",
    "y_hat = (oof_proba >= best_thr).astype(int)\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(confusion_matrix(y, y_hat))\n",
    "\n",
    "print(\"\\nClassification Report (OOF):\")\n",
    "print(classification_report(y, y_hat, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# Save OOF + threshold + report\n",
    "# ----------------------------\n",
    "df_oof = pd.DataFrame({\n",
    "    \"object_id\": df_train[\"object_id\"].astype(str),\n",
    "    \"oof_proba\": oof_proba,\n",
    "    \"target\": y\n",
    "})\n",
    "df_oof.to_csv(ART_DIR / \"lgbm_oof.csv\", index=False)\n",
    "\n",
    "with open(ART_DIR / \"lgbm_threshold.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{best_thr}\\n\")\n",
    "\n",
    "with open(ART_DIR / \"lgbm_cv_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== CV SUMMARY (threshold=0.50) ===\\n\")\n",
    "    f.write(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.6f} | Std: {float(np.std(fold_f1_05)):.6f}\\n\")\n",
    "    f.write(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.2f} | max={int(np.max(best_iters))}\\n\\n\")\n",
    "    f.write(\"=== OOF THRESHOLD TUNING ===\\n\")\n",
    "    f.write(f\"Best threshold = {best_thr:.4f}\\n\")\n",
    "    f.write(f\"OOF F1(best)   = {best_f1:.6f}\\n\")\n",
    "\n",
    "# Feature importance\n",
    "fi_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance_gain_sum\": feat_importance_accum\n",
    "}).sort_values(\"importance_gain_sum\", ascending=False).reset_index(drop=True)\n",
    "fi_df.to_csv(ART_DIR / \"lgbm_feature_importance.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\" -\", ART_DIR / \"lgbm_oof.csv\")\n",
    "print(\" -\", ART_DIR / \"lgbm_threshold.txt\")\n",
    "print(\" -\", ART_DIR / \"lgbm_cv_report.txt\")\n",
    "print(\" -\", ART_DIR / \"lgbm_feature_importance.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train final model on full train\n",
    "# (gunakan rata-rata best_iteration dari CV)\n",
    "# ----------------------------\n",
    "final_n_estimators = int(max(200, round(float(np.mean(best_iters)))))\n",
    "final_params = dict(lgb_params)\n",
    "final_params[\"n_estimators\"] = final_n_estimators\n",
    "\n",
    "final_model = lgb.LGBMClassifier(**final_params)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# Build submission\n",
    "# ----------------------------\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_test[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int),\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_lgbm_v01.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Final n_estimators:\", final_n_estimators)\n",
    "print(\"Saved submission:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3042a969",
   "metadata": {},
   "source": [
    "# Threshold tuning khusus F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510947f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — THRESHOLD TUNING KHUSUS F1 (LEBIH HALUS)\n",
    "#\n",
    "# Tujuan:\n",
    "# - Cari threshold terbaik untuk memaksimalkan F1 berdasarkan OOF proba\n",
    "# - Opsional: threshold per-fold (lebih robust), lalu voting / averaging threshold\n",
    "#\n",
    "# Input:\n",
    "# - artifacts/lgbm_oof.csv            (from STAGE 4)\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/lgbm_threshold_grid.csv\n",
    "# - artifacts/lgbm_threshold_best.txt\n",
    "# - artifacts/lgbm_threshold_report.txt\n",
    "# - (opsional) artifacts/lgbm_threshold_per_fold.csv  (jika fold tersedia di oof)\n",
    "#\n",
    "# Catatan:\n",
    "# - STAGE 4 di atas menyimpan oof_proba + target, tapi tidak simpan fold.\n",
    "# - Jadi tuning per-fold hanya bisa jika kamu juga punya kolom fold.\n",
    "#   Kalau belum ada, skrip ini fokus global OOF threshold (yang sudah cukup kuat).\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "\n",
    "OOF_PATH = ART_DIR / \"lgbm_oof.csv\"\n",
    "if not OOF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {OOF_PATH}. Jalankan STAGE 4 dulu.\")\n",
    "\n",
    "df_oof = pd.read_csv(OOF_PATH, low_memory=False)\n",
    "\n",
    "need = [\"oof_proba\", \"target\"]\n",
    "for c in need:\n",
    "    if c not in df_oof.columns:\n",
    "        raise ValueError(f\"OOF missing '{c}'. Found: {list(df_oof.columns)}\")\n",
    "\n",
    "p = pd.to_numeric(df_oof[\"oof_proba\"], errors=\"coerce\").astype(float).values\n",
    "y = pd.to_numeric(df_oof[\"target\"], errors=\"coerce\").astype(int).values\n",
    "\n",
    "# Drop NaN safety\n",
    "mask = np.isfinite(p)\n",
    "if mask.mean() < 0.999:\n",
    "    df_oof = df_oof.loc[mask].reset_index(drop=True)\n",
    "    p = p[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Coarse grid (0.00..1.00 step 0.01)\n",
    "# ----------------------------\n",
    "thr_grid_coarse = np.linspace(0.0, 1.0, 1001)  # step 0.001 (lebih halus dari 0.01)\n",
    "rows = []\n",
    "\n",
    "best_thr = 0.5\n",
    "best_f1 = -1.0\n",
    "\n",
    "for thr in thr_grid_coarse:\n",
    "    pred = (p >= thr).astype(int)\n",
    "    f1 = f1_score(y, pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "    # simpan ringkas (biar file tidak kegedean, simpan tiap 0.005)\n",
    "    # tapi kita tetap cari best pakai full 0.001 grid\n",
    "    if abs((thr * 1000) % 5) < 1e-9:  # every 0.005\n",
    "        prec = precision_score(y, pred, zero_division=0)\n",
    "        rec  = recall_score(y, pred, zero_division=0)\n",
    "        rows.append((thr, f1, prec, rec))\n",
    "\n",
    "df_grid = pd.DataFrame(rows, columns=[\"threshold\", \"f1\", \"precision\", \"recall\"])\n",
    "df_grid.to_csv(ART_DIR / \"lgbm_threshold_grid.csv\", index=False)\n",
    "\n",
    "print(\"=== THRESHOLD TUNING (GLOBAL OOF) ===\")\n",
    "print(f\"Best threshold (grid 0.001) = {best_thr:.3f}\")\n",
    "print(f\"Best OOF F1                 = {best_f1:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Fine local search around best (optional)\n",
    "#    - cari di sekitar best_thr ± 0.02 dengan step 0.0002\n",
    "# ----------------------------\n",
    "lo = max(0.0, best_thr - 0.02)\n",
    "hi = min(1.0, best_thr + 0.02)\n",
    "thr_grid_fine = np.linspace(lo, hi, int(round((hi - lo) / 0.0002)) + 1)\n",
    "\n",
    "best_thr2 = best_thr\n",
    "best_f12 = best_f1\n",
    "\n",
    "for thr in thr_grid_fine:\n",
    "    pred = (p >= thr).astype(int)\n",
    "    f1 = f1_score(y, pred)\n",
    "    if f1 > best_f12:\n",
    "        best_f12 = float(f1)\n",
    "        best_thr2 = float(thr)\n",
    "\n",
    "print(\"\\n=== FINE SEARCH ===\")\n",
    "print(f\"Best threshold (fine) = {best_thr2:.4f}\")\n",
    "print(f\"Best OOF F1 (fine)    = {best_f12:.6f}\")\n",
    "\n",
    "# Save best threshold\n",
    "with open(ART_DIR / \"lgbm_threshold_best.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{best_thr2}\\n\")\n",
    "\n",
    "# Report\n",
    "pred_best = (p >= best_thr2).astype(int)\n",
    "prec_best = precision_score(y, pred_best, zero_division=0)\n",
    "rec_best  = recall_score(y, pred_best, zero_division=0)\n",
    "\n",
    "report = []\n",
    "report.append(\"=== THRESHOLD TUNING REPORT (GLOBAL OOF) ===\")\n",
    "report.append(f\"OOF samples         : {len(y)}\")\n",
    "report.append(f\"Positive rate (y=1) : {float((y==1).mean()):.6f}\")\n",
    "report.append(\"\")\n",
    "report.append(f\"Best threshold      : {best_thr2:.6f}\")\n",
    "report.append(f\"F1                  : {best_f12:.6f}\")\n",
    "report.append(f\"Precision           : {float(prec_best):.6f}\")\n",
    "report.append(f\"Recall              : {float(rec_best):.6f}\")\n",
    "report.append(\"\")\n",
    "report.append(\"Saved files:\")\n",
    "report.append(f\"- {ART_DIR / 'lgbm_threshold_grid.csv'}\")\n",
    "report.append(f\"- {ART_DIR / 'lgbm_threshold_best.txt'}\")\n",
    "\n",
    "(ART_DIR / \"lgbm_threshold_report.txt\").write_text(\"\\n\".join(report), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n\".join(report[-6:]))\n",
    "\n",
    "# ----------------------------\n",
    "# (Optional) If user later wants: apply this threshold to create a new submission\n",
    "# - Submission dibuat di STAGE 4. Kalau kamu ingin regenerate submission\n",
    "#   dengan threshold baru, tinggal rerun STAGE 4 bagian inferensi,\n",
    "#   atau aku buatkan STAGE 5b khusus \"re-export submission from saved proba\".\n",
    "# ----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72ba03",
   "metadata": {},
   "source": [
    "# Tangani “domain shift” redshift (train spec-z vs test photo-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 — TANGANI DOMAIN SHIFT REDSHIFT\n",
    "# (train spec-z vs test photo-z) dengan:\n",
    "# 1) fitur robust Z/Z_err (fill + transform)\n",
    "# 2) NOISE AUGMENTATION pada Z di TRAIN (agar mirip photo-z)\n",
    "# 3) LightGBM + Stratified CV + threshold tuning F1\n",
    "#\n",
    "# Prasyarat:\n",
    "# - artifacts/train_log_clean.csv\n",
    "# - artifacts/test_log_clean.csv\n",
    "# - artifacts/features_merged_train.csv  (STAGE 3)\n",
    "# - artifacts/features_merged_test.csv   (STAGE 3)\n",
    "# - sample_submission.csv\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/lgbm_zaug_oof.csv\n",
    "# - artifacts/lgbm_zaug_threshold.txt\n",
    "# - artifacts/lgbm_zaug_cv_report.txt\n",
    "# - artifacts/lgbm_zaug_feature_importance.csv\n",
    "# - submissions/sub_lgbm_zaug_v02.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED    = 2025\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Noise augmentation strength\n",
    "AUG_COPIES = 1          # 0=off, 1=duplikasi 1x (jadi 2x data), 2=jadi 3x data, dst\n",
    "AUG_FLOOR  = 0.01       # floor noise (absolute), mencegah sigma=0\n",
    "AUG_REL    = 0.02       # noise tambahan proporsional (1+z): sigma += AUG_REL*(1+z)\n",
    "\n",
    "# Threshold tuning grid\n",
    "THR_STEP = 0.001\n",
    "\n",
    "# ----------------------------\n",
    "# LightGBM import\n",
    "# ----------------------------\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"LightGBM belum terpasang. Install dulu:\\n\"\n",
    "        \"  pip install lightgbm\\n\"\n",
    "        f\"Original error: {type(e).__name__}: {e}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Load inputs\n",
    "# ----------------------------\n",
    "train_log_path  = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path   = ART_DIR / \"test_log_clean.csv\"\n",
    "feat_tr_path    = ART_DIR / \"features_merged_train.csv\"\n",
    "feat_te_path    = ART_DIR / \"features_merged_test.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "for p in [train_log_path, test_log_path, feat_tr_path, feat_te_path, sample_sub_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p}. Jalankan STAGE 1 & 3 dulu.\")\n",
    "\n",
    "df_tr_log  = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te_log  = pd.read_csv(test_log_path, low_memory=False)\n",
    "df_tr_feat = pd.read_csv(feat_tr_path, low_memory=False)\n",
    "df_te_feat = pd.read_csv(feat_te_path, low_memory=False)\n",
    "df_sub     = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# Detect submission prediction column name\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]  # contoh: 'prediction'\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare train table with target\n",
    "# ----------------------------\n",
    "df_tr_log[\"object_id\"] = df_tr_log[\"object_id\"].astype(str)\n",
    "df_te_log[\"object_id\"] = df_te_log[\"object_id\"].astype(str)\n",
    "df_tr_feat[\"object_id\"] = df_tr_feat[\"object_id\"].astype(str)\n",
    "df_te_feat[\"object_id\"] = df_te_feat[\"object_id\"].astype(str)\n",
    "\n",
    "if \"target\" not in df_tr_log.columns:\n",
    "    raise ValueError(f\"train_log_clean missing target. Found: {list(df_tr_log.columns)}\")\n",
    "df_tr_log[\"target\"] = pd.to_numeric(df_tr_log[\"target\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "df_train = df_tr_feat.merge(df_tr_log[[\"object_id\", \"target\"]], on=\"object_id\", how=\"left\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    bad = df_train[df_train[\"target\"].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Ada object_id di features_merged_train yang tidak ketemu target. Example: {bad}\")\n",
    "\n",
    "df_test = df_te_feat.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Compute Z_err reference from TEST (photo-z error distribution)\n",
    "# ----------------------------\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "zerr_test = None\n",
    "if \"Z_err\" in df_te_log.columns:\n",
    "    zerr_test = _to_num(df_te_log[\"Z_err\"])\n",
    "elif \"Z_err\" in df_test.columns:\n",
    "    zerr_test = _to_num(df_test[\"Z_err\"])\n",
    "\n",
    "if zerr_test is None:\n",
    "    zerr_fill = 0.05\n",
    "    print(\"[WARN] Tidak menemukan Z_err di test. Pakai zerr_fill=0.05\")\n",
    "else:\n",
    "    zerr_test = zerr_test.astype(float)\n",
    "    zerr_fill = float(np.nanmedian(zerr_test.values))\n",
    "    if not np.isfinite(zerr_fill) or zerr_fill <= 0:\n",
    "        zerr_fill = 0.05\n",
    "    print(f\"[INFO] zerr_fill (median test Z_err) = {zerr_fill:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Feature engineering: robust Z/Z_err + Z augmentation column\n",
    "# ----------------------------\n",
    "def add_redshift_domainshift_features(df: pd.DataFrame, zerr_fill: float, is_train: bool, rng: np.random.Generator):\n",
    "    \"\"\"\n",
    "    Menambahkan fitur yang membuat model lebih robust terhadap perbedaan spec-z vs photo-z.\n",
    "    - Z_filled\n",
    "    - Zerr_filled\n",
    "    - transforms\n",
    "    - Z_aug (train: noisy, test: no-noise)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Ensure Z, Z_err exist as columns\n",
    "    if \"Z\" not in out.columns:\n",
    "        out[\"Z\"] = np.nan\n",
    "    if \"Z_err\" not in out.columns:\n",
    "        out[\"Z_err\"] = np.nan\n",
    "\n",
    "    out[\"Z\"] = pd.to_numeric(out[\"Z\"], errors=\"coerce\")\n",
    "    out[\"Z_err\"] = pd.to_numeric(out[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "    z = out[\"Z\"].astype(float).values\n",
    "    zerr = out[\"Z_err\"].astype(float).values\n",
    "\n",
    "    z_isna = ~np.isfinite(z)\n",
    "    zerr_isna = ~np.isfinite(zerr)\n",
    "\n",
    "    z_f = z.copy()\n",
    "    z_f[z_isna] = 0.0\n",
    "\n",
    "    zerr_f = zerr.copy()\n",
    "    zerr_f[zerr_isna] = zerr_fill\n",
    "    zerr_f = np.clip(zerr_f, 0.0, None)\n",
    "\n",
    "    out[\"Z_filled\"] = z_f\n",
    "    out[\"Zerr_filled\"] = zerr_f\n",
    "    out[\"Z_missing\"] = z_isna.astype(int)\n",
    "    out[\"Zerr_missing\"] = zerr_isna.astype(int)\n",
    "\n",
    "    # Robust transforms\n",
    "    out[\"log1pZ\"] = np.log1p(np.clip(z_f, 0.0, None))\n",
    "    out[\"log1pZerr\"] = np.log1p(np.clip(zerr_f, 0.0, None))\n",
    "    out[\"inv1pZ\"] = 1.0 / (1.0 + np.clip(z_f, 0.0, None))\n",
    "    out[\"Z_div_Zerr\"] = z_f / (zerr_f + 1e-6)\n",
    "\n",
    "    # Noise augmentation on Z (train only)\n",
    "    if is_train:\n",
    "        sigma = np.sqrt(np.square(zerr_f) + np.square(AUG_FLOOR) + np.square(AUG_REL * (1.0 + z_f)))\n",
    "        noise = rng.normal(loc=0.0, scale=sigma, size=z_f.shape[0])\n",
    "        z_aug = np.clip(z_f + noise, 0.0, None)\n",
    "        out[\"Z_aug\"] = z_aug\n",
    "        out[\"Z_aug_absdiff\"] = np.abs(z_aug - z_f)\n",
    "    else:\n",
    "        out[\"Z_aug\"] = z_f\n",
    "        out[\"Z_aug_absdiff\"] = 0.0\n",
    "\n",
    "    return out\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "df_train_fe = add_redshift_domainshift_features(df_train, zerr_fill=zerr_fill, is_train=True, rng=rng)\n",
    "df_test_fe  = add_redshift_domainshift_features(df_test,  zerr_fill=zerr_fill, is_train=False, rng=rng)\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: Data augmentation by duplicating TRAIN with different noise draws\n",
    "# ----------------------------\n",
    "if AUG_COPIES > 0:\n",
    "    aug_list = [df_train_fe]\n",
    "    for k in range(AUG_COPIES):\n",
    "        rng_k = np.random.default_rng(SEED + 1000 * (k + 1))\n",
    "        df_k = add_redshift_domainshift_features(df_train, zerr_fill=zerr_fill, is_train=True, rng=rng_k)\n",
    "        aug_list.append(df_k)\n",
    "    df_train_fe = pd.concat(aug_list, ignore_index=True)\n",
    "    print(f\"[INFO] Augmented train rows: {len(df_train_fe):,} (AUG_COPIES={AUG_COPIES})\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build X/y\n",
    "# ----------------------------\n",
    "y = df_train_fe[\"target\"].astype(int).values\n",
    "\n",
    "drop_cols = {\"object_id\", \"target\"}\n",
    "feature_cols = [c for c in df_train_fe.columns if c not in drop_cols]\n",
    "\n",
    "# Numeric coercion\n",
    "for c in feature_cols:\n",
    "    df_train_fe[c] = pd.to_numeric(df_train_fe[c], errors=\"coerce\")\n",
    "    df_test_fe[c]  = pd.to_numeric(df_test_fe[c], errors=\"coerce\")\n",
    "\n",
    "X = df_train_fe[feature_cols]\n",
    "X_test = df_test_fe[feature_cols]\n",
    "\n",
    "print(f\"[INFO] X_train shape: {X.shape} | X_test shape: {X_test.shape}\")\n",
    "pos = max(int((y == 1).sum()), 1)\n",
    "neg = max(int((y == 0).sum()), 1)\n",
    "scale_pos_weight = neg / pos\n",
    "print(f\"[INFO] Pos rate: {(y==1).mean():.6f} | scale_pos_weight={scale_pos_weight:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# LightGBM params (CPU)\n",
    "# ----------------------------\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=7000,\n",
    "    num_leaves=96,\n",
    "    min_child_samples=120,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# CV + OOF\n",
    "# ----------------------------\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_proba = np.zeros(len(X), dtype=np.float32)\n",
    "best_iters = []\n",
    "fold_f1_05 = []\n",
    "feat_importance_accum = np.zeros(len(feature_cols), dtype=np.float64)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), start=1):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)]\n",
    "    )\n",
    "\n",
    "    p_va = model.predict_proba(X_va)[:, 1]\n",
    "    oof_proba[va_idx] = p_va\n",
    "\n",
    "    f1_at_05 = f1_score(y_va, (p_va >= 0.5).astype(int))\n",
    "    fold_f1_05.append(float(f1_at_05))\n",
    "\n",
    "    best_it = int(model.best_iteration_) if getattr(model, \"best_iteration_\", None) else lgb_params[\"n_estimators\"]\n",
    "    best_iters.append(best_it)\n",
    "\n",
    "    try:\n",
    "        fi = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "        feat_importance_accum += fi\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[FOLD {fold}/{N_FOLDS}] best_iter={best_it} | F1@0.50={f1_at_05:.5f}\")\n",
    "\n",
    "print(\"\\n=== CV SUMMARY (threshold=0.50) ===\")\n",
    "print(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.5f} | Std: {float(np.std(fold_f1_05)):.5f}\")\n",
    "print(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.1f} | max={int(np.max(best_iters))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold tuning (global OOF) for F1\n",
    "# ----------------------------\n",
    "thr_grid = np.arange(0.0, 1.0 + THR_STEP, THR_STEP)\n",
    "best_thr, best_f1 = 0.5, -1.0\n",
    "for thr in thr_grid:\n",
    "    f1 = f1_score(y, (oof_proba >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "print(\"\\n=== OOF THRESHOLD TUNING ===\")\n",
    "print(f\"Best threshold = {best_thr:.4f}\")\n",
    "print(f\"OOF F1(best)   = {best_f1:.6f}\")\n",
    "\n",
    "y_hat = (oof_proba >= best_thr).astype(int)\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(confusion_matrix(y, y_hat))\n",
    "print(\"\\nClassification Report (OOF):\")\n",
    "print(classification_report(y, y_hat, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# Save artifacts\n",
    "# ----------------------------\n",
    "# OOF saved with original (non-augmented) object_id list is tricky because we augmented rows.\n",
    "# Jadi kita simpan OOF untuk seluruh training rows (augmented) untuk debugging,\n",
    "# dan juga simpan OOF untuk baris pertama tiap object_id (original order) jika tersedia.\n",
    "oof_path = ART_DIR / \"lgbm_zaug_oof.csv\"\n",
    "df_oof = pd.DataFrame({\n",
    "    \"object_id\": df_train_fe[\"object_id\"].astype(str),\n",
    "    \"oof_proba\": oof_proba,\n",
    "    \"target\": y\n",
    "})\n",
    "df_oof.to_csv(oof_path, index=False)\n",
    "\n",
    "thr_path = ART_DIR / \"lgbm_zaug_threshold.txt\"\n",
    "with open(thr_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{best_thr}\\n\")\n",
    "\n",
    "rep_path = ART_DIR / \"lgbm_zaug_cv_report.txt\"\n",
    "with open(rep_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== CV SUMMARY (threshold=0.50) ===\\n\")\n",
    "    f.write(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.6f} | Std: {float(np.std(fold_f1_05)):.6f}\\n\")\n",
    "    f.write(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.2f} | max={int(np.max(best_iters))}\\n\\n\")\n",
    "    f.write(\"=== OOF THRESHOLD TUNING ===\\n\")\n",
    "    f.write(f\"Best threshold = {best_thr:.6f}\\n\")\n",
    "    f.write(f\"OOF F1(best)   = {best_f1:.6f}\\n\")\n",
    "\n",
    "fi_path = ART_DIR / \"lgbm_zaug_feature_importance.csv\"\n",
    "fi_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance_gain_sum\": feat_importance_accum\n",
    "}).sort_values(\"importance_gain_sum\", ascending=False).reset_index(drop=True)\n",
    "fi_df.to_csv(fi_path, index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\" -\", oof_path)\n",
    "print(\" -\", thr_path)\n",
    "print(\" -\", rep_path)\n",
    "print(\" -\", fi_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Train final model on full train (augmented)\n",
    "# ----------------------------\n",
    "final_n_estimators = int(max(300, round(float(np.mean(best_iters)))))\n",
    "final_params = dict(lgb_params)\n",
    "final_params[\"n_estimators\"] = final_n_estimators\n",
    "\n",
    "final_model = lgb.LGBMClassifier(**final_params)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# Build submission\n",
    "# ----------------------------\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_test_fe[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int),\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_lgbm_zaug_v02.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Final n_estimators:\", final_n_estimators)\n",
    "print(\"Saved submission:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67815b",
   "metadata": {},
   "source": [
    "# Feature upgrade yang sering menang di lightcurve (masih CPU-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 — FEATURE UPGRADE (CPU-FRIENDLY)\n",
    "# Upgrade fitur lightcurve TANPA baca ulang CSV besar:\n",
    "# - Derive \"shape-ish\" features per band dari fitur STAGE 3 (mean/std/min/max/amp/snr/time_span)\n",
    "# - Tambah cross-band features (ratio/diff antar band)\n",
    "# - Tambah global aggregation across bands (jumlah band hadir, total n_obs, peak band, dll)\n",
    "#\n",
    "# Input (STAGE 3):\n",
    "#   artifacts/features_merged_train.csv\n",
    "#   artifacts/features_merged_test.csv\n",
    "#\n",
    "# Output:\n",
    "#   artifacts/features_upg_train.csv\n",
    "#   artifacts/features_upg_test.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "\n",
    "IN_TR = ART_DIR / \"features_merged_train.csv\"\n",
    "IN_TE = ART_DIR / \"features_merged_test.csv\"\n",
    "OUT_TR = ART_DIR / \"features_upg_train.csv\"\n",
    "OUT_TE = ART_DIR / \"features_upg_test.csv\"\n",
    "\n",
    "BANDS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "BASE_FEATS = [\"n_obs\", \"flux_mean\", \"flux_std\", \"min_flux\", \"max_flux\", \"amp\",\n",
    "              \"snr_mean\", \"frac_snr_gt3\", \"frac_snr_gt5\", \"time_span\"]\n",
    "\n",
    "EPS = 1e-9\n",
    "\n",
    "for p in [IN_TR, IN_TE]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p}. Jalankan STAGE 3 dulu.\")\n",
    "\n",
    "df_tr = pd.read_csv(IN_TR, low_memory=False)\n",
    "df_te = pd.read_csv(IN_TE, low_memory=False)\n",
    "\n",
    "if \"object_id\" not in df_tr.columns or \"object_id\" not in df_te.columns:\n",
    "    raise ValueError(\"features_merged_* wajib punya kolom object_id\")\n",
    "\n",
    "df_tr[\"object_id\"] = df_tr[\"object_id\"].astype(str)\n",
    "df_te[\"object_id\"] = df_te[\"object_id\"].astype(str)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def colname(band: str, feat: str) -> str:\n",
    "    return f\"{band}__{feat}\"\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols: list):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "def safe_series(df: pd.DataFrame, c: str, default=np.nan):\n",
    "    if c in df.columns:\n",
    "        return pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return pd.Series(default, index=df.index, dtype=\"float64\")\n",
    "\n",
    "def add_band_derived(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Pastikan base columns numeric\n",
    "    cols = [colname(b, f) for b in BANDS for f in BASE_FEATS if colname(b, f) in out.columns]\n",
    "    ensure_numeric(out, cols)\n",
    "\n",
    "    for b in BANDS:\n",
    "        n_obs = safe_series(out, colname(b, \"n_obs\"), default=0.0).fillna(0.0)\n",
    "        mu    = safe_series(out, colname(b, \"flux_mean\"))\n",
    "        sd    = safe_series(out, colname(b, \"flux_std\"))\n",
    "        mn    = safe_series(out, colname(b, \"min_flux\"))\n",
    "        mx    = safe_series(out, colname(b, \"max_flux\"))\n",
    "        amp   = safe_series(out, colname(b, \"amp\"))\n",
    "        snr_m = safe_series(out, colname(b, \"snr_mean\"))\n",
    "        tspan = safe_series(out, colname(b, \"time_span\"))\n",
    "\n",
    "        present = (n_obs > 0).astype(int)\n",
    "        out[colname(b, \"present\")] = present\n",
    "\n",
    "        # Coef of variation (robust)\n",
    "        out[colname(b, \"cv_absmean\")] = sd / (np.abs(mu) + EPS)\n",
    "\n",
    "        # Peak positivity (untuk ratio/cross-band yang lebih stabil)\n",
    "        peak_pos = np.clip(mx, 0.0, None)\n",
    "        out[colname(b, \"peak_pos\")] = peak_pos\n",
    "        out[colname(b, \"log1p_peak_pos\")] = np.log1p(peak_pos)\n",
    "\n",
    "        # Peak-to-mean ratio (pakai abs mean biar stabil)\n",
    "        out[colname(b, \"peak_over_absmean\")] = mx / (np.abs(mu) + EPS)\n",
    "\n",
    "        # Amp normalized\n",
    "        out[colname(b, \"amp_over_absmean\")] = amp / (np.abs(mu) + EPS)\n",
    "        out[colname(b, \"amp_over_std\")] = amp / (sd + EPS)\n",
    "\n",
    "        # Negative flux indicator\n",
    "        out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
    "\n",
    "        # Time density proxy (karena gap exact sulit tanpa sort)\n",
    "        out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
    "\n",
    "        # SNR density proxy\n",
    "        out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n",
    "\n",
    "    return out\n",
    "\n",
    "def add_cross_band(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Convenience arrays for cross-band operations\n",
    "    peak_pos_cols = [colname(b, \"peak_pos\") for b in BANDS]\n",
    "    amp_cols      = [colname(b, \"amp\") for b in BANDS]\n",
    "    nobs_cols     = [colname(b, \"n_obs\") for b in BANDS]\n",
    "    pres_cols     = [colname(b, \"present\") for b in BANDS]\n",
    "    tspan_cols    = [colname(b, \"time_span\") for b in BANDS]\n",
    "\n",
    "    ensure_numeric(out, [c for c in peak_pos_cols+amp_cols+nobs_cols+tspan_cols if c in out.columns])\n",
    "\n",
    "    # Global counts\n",
    "    pres_mat = np.column_stack([safe_series(out, c, default=0.0).fillna(0.0).values for c in pres_cols])\n",
    "    out[\"n_bands_present\"] = pres_mat.sum(axis=1).astype(int)\n",
    "\n",
    "    nobs_mat = np.column_stack([safe_series(out, c, default=0.0).fillna(0.0).values for c in nobs_cols])\n",
    "    out[\"total_n_obs\"] = nobs_mat.sum(axis=1).astype(float)\n",
    "    out[\"mean_n_obs_per_band_present\"] = out[\"total_n_obs\"] / (out[\"n_bands_present\"].clip(lower=1).astype(float))\n",
    "\n",
    "    # Peak across bands (positive peak)\n",
    "    peak_mat = np.column_stack([safe_series(out, c, default=np.nan).values for c in peak_pos_cols])\n",
    "    peak_mat = np.where(np.isfinite(peak_mat), peak_mat, -np.inf)\n",
    "\n",
    "    max_peak = np.max(peak_mat, axis=1)\n",
    "    sum_peak = np.sum(np.where(np.isfinite(peak_mat) & (peak_mat > -np.inf), np.clip(peak_mat, 0.0, None), 0.0), axis=1)\n",
    "    argmax_peak = np.argmax(peak_mat, axis=1)\n",
    "\n",
    "    out[\"max_peak_pos\"] = np.where(np.isfinite(max_peak), max_peak, np.nan)\n",
    "    out[\"sum_peak_pos\"] = sum_peak\n",
    "    out[\"peak_concentration\"] = out[\"max_peak_pos\"] / (out[\"sum_peak_pos\"] + EPS)\n",
    "\n",
    "    # Encode peak band as integer 0..5 + one-hot\n",
    "    out[\"peak_band_idx\"] = argmax_peak.astype(int)\n",
    "    for i, b in enumerate(BANDS):\n",
    "        out[f\"peak_band_is_{b}\"] = (out[\"peak_band_idx\"] == i).astype(int)\n",
    "\n",
    "    # Amp across bands\n",
    "    amp_mat = np.column_stack([safe_series(out, c, default=np.nan).values for c in amp_cols])\n",
    "    amp_mat = np.where(np.isfinite(amp_mat), amp_mat, -np.inf)\n",
    "    out[\"max_amp\"] = np.where(np.isfinite(np.max(amp_mat, axis=1)), np.max(amp_mat, axis=1), np.nan)\n",
    "    out[\"mean_amp\"] = np.nanmean(np.where(amp_mat > -np.inf, amp_mat, np.nan), axis=1)\n",
    "    out[\"amp_concentration\"] = out[\"max_amp\"] / (np.nansum(np.where(np.isfinite(amp_mat) & (amp_mat > -np.inf), np.clip(amp_mat, 0.0, None), 0.0), axis=1) + EPS)\n",
    "\n",
    "    # Time span across bands\n",
    "    tspan_mat = np.column_stack([safe_series(out, c, default=np.nan).values for c in tspan_cols])\n",
    "    out[\"max_time_span\"] = np.nanmax(tspan_mat, axis=1)\n",
    "    out[\"mean_time_span\"] = np.nanmean(tspan_mat, axis=1)\n",
    "\n",
    "    # Adjacent band ratios (peak_pos & amp) + log ratios (lebih stabil)\n",
    "    adj_pairs = [(\"u\",\"g\"), (\"g\",\"r\"), (\"r\",\"i\"), (\"i\",\"z\"), (\"z\",\"y\")]\n",
    "    for b1, b2 in adj_pairs:\n",
    "        p1 = safe_series(out, colname(b1, \"peak_pos\"))\n",
    "        p2 = safe_series(out, colname(b2, \"peak_pos\"))\n",
    "        out[f\"peakpos_ratio_{b1}{b2}\"] = (p1 + EPS) / (p2 + EPS)\n",
    "        out[f\"log_peakpos_ratio_{b1}{b2}\"] = np.log1p(np.clip(p1, 0.0, None)) - np.log1p(np.clip(p2, 0.0, None))\n",
    "\n",
    "        a1 = safe_series(out, colname(b1, \"amp\"))\n",
    "        a2 = safe_series(out, colname(b2, \"amp\"))\n",
    "        out[f\"amp_ratio_{b1}{b2}\"] = (a1 + EPS) / (a2 + EPS)\n",
    "        out[f\"log_amp_ratio_{b1}{b2}\"] = np.log1p(np.clip(a1, 0.0, None)) - np.log1p(np.clip(a2, 0.0, None))\n",
    "\n",
    "    # Broad color-like: blue vs red (g+r) vs (i+z+y)\n",
    "    blue = safe_series(out, colname(\"g\",\"peak_pos\")) + safe_series(out, colname(\"r\",\"peak_pos\"))\n",
    "    red  = safe_series(out, colname(\"i\",\"peak_pos\")) + safe_series(out, colname(\"z\",\"peak_pos\")) + safe_series(out, colname(\"y\",\"peak_pos\"))\n",
    "    out[\"peakpos_blue_over_red\"] = (blue + EPS) / (red + EPS)\n",
    "    out[\"log_peakpos_blue_over_red\"] = np.log1p(np.clip(blue, 0.0, None)) - np.log1p(np.clip(red, 0.0, None))\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Build upgraded features\n",
    "# ----------------------------\n",
    "df_tr_upg = add_cross_band(add_band_derived(df_tr))\n",
    "df_te_upg = add_cross_band(add_band_derived(df_te))\n",
    "\n",
    "# Final sanity: keep column order stable-ish (object_id first)\n",
    "def reorder(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = list(df.columns)\n",
    "    if \"object_id\" in cols:\n",
    "        cols = [\"object_id\"] + [c for c in cols if c != \"object_id\"]\n",
    "        return df[cols]\n",
    "    return df\n",
    "\n",
    "df_tr_upg = reorder(df_tr_upg)\n",
    "df_te_upg = reorder(df_te_upg)\n",
    "\n",
    "df_tr_upg.to_csv(OUT_TR, index=False)\n",
    "df_te_upg.to_csv(OUT_TE, index=False)\n",
    "\n",
    "print(\"=== STAGE 7 DONE ===\")\n",
    "print(\"Saved:\", OUT_TR)\n",
    "print(\"Saved:\", OUT_TE)\n",
    "print(\"Train shape:\", df_tr_upg.shape, \"| Test shape:\", df_te_upg.shape)\n",
    "\n",
    "# Quick peek of new columns count\n",
    "new_cols = set(df_tr_upg.columns) - set(df_tr.columns)\n",
    "print(\"Added features:\", len(new_cols))\n",
    "print(\"Example added:\", sorted(list(new_cols))[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40617a3",
   "metadata": {},
   "source": [
    "# Ensemble ringan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 8 — ENSEMBLE RINGAN (CPU-FRIENDLY)\n",
    "# Tujuan:\n",
    "# - Gabungkan prediksi dari beberapa model/varian (mis: LGBM v01, LGBM z-aug v02, LGBM fitur upgrade)\n",
    "# - Ensemble dengan average probability (lebih stabil)\n",
    "# - Threshold tuning F1 dari OOF ensemble (jika OOF tersedia untuk semua)\n",
    "# - Export submission ensemble (mengikuti kolom sample_submission: 'prediction' atau lainnya)\n",
    "#\n",
    "# Cara pakai (paling aman):\n",
    "# 1) Pastikan kamu sudah punya minimal 2 file OOF:\n",
    "#    - artifacts/lgbm_oof.csv\n",
    "#    - artifacts/lgbm_zaug_oof.csv  (atau oof lain)\n",
    "#    dan 2 file proba test (opsional):\n",
    "#    - artifacts/lgbm_test_proba.csv\n",
    "#    - artifacts/lgbm_zaug_test_proba.csv\n",
    "#\n",
    "# Kalau kamu belum simpan test_proba, skrip ini tetap bisa ensemble submission\n",
    "# dari file submission masing-masing (majority vote / average label),\n",
    "# tapi yang optimal adalah average PROBABILITY.\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/ens_oof.csv\n",
    "# - artifacts/ens_threshold.txt\n",
    "# - artifacts/ens_report.txt\n",
    "# - submissions/sub_ensemble_v01.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAMPLE_SUB_PATH = DATA_ROOT / \"sample_submission.csv\"\n",
    "if not SAMPLE_SUB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {SAMPLE_SUB_PATH}\")\n",
    "df_sub = pd.read_csv(SAMPLE_SUB_PATH, low_memory=False)\n",
    "\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# Pilih sumber ensemble (edit daftar ini sesuai file kamu)\n",
    "# ----------------------------\n",
    "# OOF sources (harus ada kolom: object_id, oof_proba, target)\n",
    "OOF_FILES = [\n",
    "    ART_DIR / \"lgbm_oof.csv\",\n",
    "    ART_DIR / \"lgbm_zaug_oof.csv\",\n",
    "    # ART_DIR / \"lgbm_upg_oof.csv\",   # kalau nanti kamu buat\n",
    "]\n",
    "\n",
    "# TEST proba sources (recommended) (harus ada: object_id, proba)\n",
    "# Kalau belum ada, isi [] dan pakai submission-based ensemble di bawah.\n",
    "TEST_PROBA_FILES = [\n",
    "    # ART_DIR / \"lgbm_test_proba.csv\",\n",
    "    # ART_DIR / \"lgbm_zaug_test_proba.csv\",\n",
    "]\n",
    "\n",
    "# Submission sources (fallback) (harus mengikuti sample_submission, kolom prediksi 0/1)\n",
    "SUB_FILES_FALLBACK = [\n",
    "    SUB_DIR / \"sub_lgbm_v01.csv\",\n",
    "    SUB_DIR / \"sub_lgbm_zaug_v02.csv\",\n",
    "    # SUB_DIR / \"sub_lgbm_upg_v03.csv\",\n",
    "]\n",
    "\n",
    "# Ensemble weights (opsional)\n",
    "# - Jika None, semua model bobot sama.\n",
    "# - Jika list, panjang harus sama dengan jumlah model yang dipakai.\n",
    "ENSEMBLE_WEIGHTS = None\n",
    "\n",
    "# Threshold tuning grid\n",
    "THR_STEP = 0.001\n",
    "\n",
    "# ============================================================\n",
    "# Helper loaders\n",
    "# ============================================================\n",
    "def load_oof(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    need = {\"object_id\", \"oof_proba\", \"target\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"OOF file {path} missing {need - set(df.columns)} | found={list(df.columns)}\")\n",
    "    df = df[[\"object_id\", \"oof_proba\", \"target\"]].copy()\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(str)\n",
    "    df[\"oof_proba\"] = pd.to_numeric(df[\"oof_proba\"], errors=\"coerce\")\n",
    "    df[\"target\"] = pd.to_numeric(df[\"target\"], errors=\"coerce\").astype(int)\n",
    "    df = df.dropna(subset=[\"oof_proba\"])\n",
    "    return df\n",
    "\n",
    "def load_test_proba(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    # accept proba column name variations\n",
    "    if \"object_id\" not in df.columns:\n",
    "        raise ValueError(f\"Test proba file {path} missing object_id\")\n",
    "    proba_col = None\n",
    "    for c in [\"proba\", \"pred_proba\", \"prediction_proba\", \"test_proba\"]:\n",
    "        if c in df.columns:\n",
    "            proba_col = c\n",
    "            break\n",
    "    if proba_col is None:\n",
    "        # try: any column besides object_id\n",
    "        cand = [c for c in df.columns if c != \"object_id\"]\n",
    "        if len(cand) == 1:\n",
    "            proba_col = cand[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot detect proba column in {path}. Found={list(df.columns)}\")\n",
    "    df = df[[\"object_id\", proba_col]].copy()\n",
    "    df = df.rename(columns={proba_col: \"proba\"})\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(str)\n",
    "    df[\"proba\"] = pd.to_numeric(df[\"proba\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"proba\"])\n",
    "    return df\n",
    "\n",
    "def load_submission(path: Path, pred_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    if \"object_id\" not in df.columns or pred_col not in df.columns:\n",
    "        raise ValueError(f\"Submission {path} must contain ['object_id','{pred_col}']. Found={list(df.columns)}\")\n",
    "    df = df[[\"object_id\", pred_col]].copy()\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(str)\n",
    "    df[pred_col] = pd.to_numeric(df[pred_col], errors=\"coerce\").astype(int)\n",
    "    return df\n",
    "\n",
    "def weighted_mean(mat: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    return (mat * w[None, :]).sum(axis=1) / (w.sum() + 1e-12)\n",
    "\n",
    "# ============================================================\n",
    "# 1) OOF Ensemble (threshold tuning terbaik)\n",
    "# ============================================================\n",
    "oofs = []\n",
    "for p in OOF_FILES:\n",
    "    if not p.exists():\n",
    "        print(f\"[WARN] Missing OOF: {p} (skip)\")\n",
    "        continue\n",
    "    oofs.append(load_oof(p))\n",
    "\n",
    "if len(oofs) < 2:\n",
    "    print(\"[WARN] OOF ensemble butuh >=2 OOF files. Akan lanjut ke submission-based ensemble saja.\")\n",
    "    HAVE_OOF = False\n",
    "else:\n",
    "    HAVE_OOF = True\n",
    "\n",
    "best_thr = 0.5\n",
    "\n",
    "if HAVE_OOF:\n",
    "    # Inner join by object_id so rows align\n",
    "    base = oofs[0][[\"object_id\", \"target\"]].copy()\n",
    "    for i, df in enumerate(oofs):\n",
    "        base = base.merge(df[[\"object_id\", \"oof_proba\"]].rename(columns={\"oof_proba\": f\"p{i}\"}),\n",
    "                          on=\"object_id\", how=\"inner\")\n",
    "\n",
    "    y = base[\"target\"].astype(int).values\n",
    "    p_cols = [c for c in base.columns if c.startswith(\"p\")]\n",
    "    P = base[p_cols].to_numpy(dtype=float)\n",
    "\n",
    "    m = P.shape[1]\n",
    "    if ENSEMBLE_WEIGHTS is None:\n",
    "        w = np.ones(m, dtype=float)\n",
    "    else:\n",
    "        w = np.asarray(ENSEMBLE_WEIGHTS, dtype=float)\n",
    "        if w.shape[0] != m:\n",
    "            raise ValueError(f\"ENSEMBLE_WEIGHTS length {w.shape[0]} != #models {m}\")\n",
    "\n",
    "    ens_p = weighted_mean(P, w)\n",
    "\n",
    "    # threshold tuning (fine)\n",
    "    thr_grid = np.arange(0.0, 1.0 + THR_STEP, THR_STEP)\n",
    "    best_f1 = -1.0\n",
    "    for thr in thr_grid:\n",
    "        f1 = f1_score(y, (ens_p >= thr).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = float(f1)\n",
    "            best_thr = float(thr)\n",
    "\n",
    "    pred = (ens_p >= best_thr).astype(int)\n",
    "    prec = precision_score(y, pred, zero_division=0)\n",
    "    rec = recall_score(y, pred, zero_division=0)\n",
    "\n",
    "    print(\"\\n=== OOF ENSEMBLE RESULT ===\")\n",
    "    print(f\"Models used      : {m}\")\n",
    "    print(f\"Best threshold   : {best_thr:.4f}\")\n",
    "    print(f\"OOF F1           : {best_f1:.6f}\")\n",
    "    print(f\"OOF Precision    : {prec:.6f}\")\n",
    "    print(f\"OOF Recall       : {rec:.6f}\")\n",
    "\n",
    "    # save OOF ensemble artifact\n",
    "    df_ens_oof = pd.DataFrame({\n",
    "        \"object_id\": base[\"object_id\"].astype(str),\n",
    "        \"ens_oof_proba\": ens_p.astype(np.float32),\n",
    "        \"target\": y\n",
    "    })\n",
    "    df_ens_oof.to_csv(ART_DIR / \"ens_oof.csv\", index=False)\n",
    "\n",
    "    (ART_DIR / \"ens_threshold.txt\").write_text(f\"{best_thr}\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    report = []\n",
    "    report.append(\"=== ENSEMBLE REPORT ===\")\n",
    "    report.append(f\"Models used: {m}\")\n",
    "    report.append(f\"OOF rows (inner-join): {len(base):,}\")\n",
    "    report.append(f\"Best threshold: {best_thr:.6f}\")\n",
    "    report.append(f\"OOF F1: {best_f1:.6f}\")\n",
    "    report.append(f\"OOF Precision: {prec:.6f}\")\n",
    "    report.append(f\"OOF Recall: {rec:.6f}\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"OOF files:\")\n",
    "    for p in OOF_FILES:\n",
    "        report.append(f\"- {p}\")\n",
    "    (ART_DIR / \"ens_report.txt\").write_text(\"\\n\".join(report), encoding=\"utf-8\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) TEST Ensemble: preferred from test probability files\n",
    "# ============================================================\n",
    "have_test_proba = all(p.exists() for p in TEST_PROBA_FILES) and len(TEST_PROBA_FILES) >= 2\n",
    "\n",
    "if have_test_proba:\n",
    "    test_list = [load_test_proba(p) for p in TEST_PROBA_FILES]\n",
    "\n",
    "    base = test_list[0][[\"object_id\"]].copy()\n",
    "    for i, df in enumerate(test_list):\n",
    "        base = base.merge(df.rename(columns={\"proba\": f\"p{i}\"}), on=\"object_id\", how=\"inner\")\n",
    "\n",
    "    p_cols = [c for c in base.columns if c.startswith(\"p\")]\n",
    "    P = base[p_cols].to_numpy(dtype=float)\n",
    "    m = P.shape[1]\n",
    "\n",
    "    if ENSEMBLE_WEIGHTS is None:\n",
    "        w = np.ones(m, dtype=float)\n",
    "    else:\n",
    "        w = np.asarray(ENSEMBLE_WEIGHTS, dtype=float)\n",
    "        if w.shape[0] != m:\n",
    "            raise ValueError(f\"ENSEMBLE_WEIGHTS length {w.shape[0]} != #models {m}\")\n",
    "\n",
    "    ens_test_p = weighted_mean(P, w)\n",
    "    ens_test_pred = (ens_test_p >= best_thr).astype(int)\n",
    "\n",
    "    pred_df = pd.DataFrame({\"object_id\": base[\"object_id\"].astype(str), SUB_PRED_COL: ens_test_pred})\n",
    "else:\n",
    "    # ============================================================\n",
    "    # 3) FALLBACK: Submission-based ensemble (vote / average label)\n",
    "    # ============================================================\n",
    "    subs = []\n",
    "    for p in SUB_FILES_FALLBACK:\n",
    "        if not p.exists():\n",
    "            print(f\"[WARN] Missing submission: {p} (skip)\")\n",
    "            continue\n",
    "        subs.append(load_submission(p, SUB_PRED_COL))\n",
    "\n",
    "    if len(subs) < 2:\n",
    "        raise RuntimeError(\"Butuh >=2 model untuk ensemble. Tambahkan OOF/TEST proba/submission lain.\")\n",
    "\n",
    "    base = subs[0][[\"object_id\"]].copy()\n",
    "    for i, df in enumerate(subs):\n",
    "        base = base.merge(df.rename(columns={SUB_PRED_COL: f\"y{i}\"}), on=\"object_id\", how=\"inner\")\n",
    "\n",
    "    y_cols = [c for c in base.columns if c.startswith(\"y\")]\n",
    "    Y = base[y_cols].to_numpy(dtype=float)\n",
    "\n",
    "    # average label -> then apply 0.5 vote threshold\n",
    "    avg_label = Y.mean(axis=1)\n",
    "    ens_test_pred = (avg_label >= 0.5).astype(int)\n",
    "\n",
    "    pred_df = pd.DataFrame({\"object_id\": base[\"object_id\"].astype(str), SUB_PRED_COL: ens_test_pred})\n",
    "\n",
    "# ============================================================\n",
    "# 4) Build final submission\n",
    "# ============================================================\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_ensemble_v01.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Saved submission:\", out_path)\n",
    "print(\"Used threshold   :\", best_thr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8796829",
   "metadata": {},
   "source": [
    "# Train full & buat submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL STAGE — TRAIN FULL (CPU) + EXPORT SUBMISSION\n",
    "# Default behavior (tanpa tanya-tanya):\n",
    "# - Pakai fitur paling baru jika ada: features_upg_* (STAGE 7), kalau tidak ada pakai features_merged_* (STAGE 3)\n",
    "# - Aktifkan fitur domain-shift redshift (robust) + Z noise augmentation ringan (tanpa duplikasi data)\n",
    "# - Ambil threshold terbaik dari artifacts:\n",
    "#     1) ens_threshold.txt (jika ada)\n",
    "#     2) lgbm_threshold_best.txt (STAGE 5)\n",
    "#     3) lgbm_threshold.txt / lgbm_zaug_threshold.txt (fallback)\n",
    "# - Export submission mengikuti kolom sample_submission (mis. 'prediction')\n",
    "# - Simpan juga test probabilities untuk ensemble berikutnya\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 2025\n",
    "\n",
    "# Domain-shift redshift settings (ringan)\n",
    "USE_REDSHIFT_ROBUST_FE = True\n",
    "AUG_FLOOR = 0.01   # noise minimum\n",
    "AUG_REL   = 0.02   # noise proporsional (1+z)\n",
    "\n",
    "# LightGBM base params (CPU-friendly)\n",
    "LGB_PARAMS = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=12000,      # besar dulu, nanti dipotong pakai early stopping\n",
    "    num_leaves=96,\n",
    "    min_child_samples=120,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# LightGBM import\n",
    "# ----------------------------\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"LightGBM belum terpasang. Install dulu:\\n\"\n",
    "        \"  pip install lightgbm\\n\"\n",
    "        f\"Original error: {type(e).__name__}: {e}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "train_log_path  = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path   = ART_DIR / \"test_log_clean.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "feat_tr_candidates = [ART_DIR / \"features_upg_train.csv\", ART_DIR / \"features_merged_train.csv\"]\n",
    "feat_te_candidates = [ART_DIR / \"features_upg_test.csv\",  ART_DIR / \"features_merged_test.csv\"]\n",
    "\n",
    "def pick_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "feat_tr_path = pick_existing(feat_tr_candidates)\n",
    "feat_te_path = pick_existing(feat_te_candidates)\n",
    "\n",
    "for p in [train_log_path, test_log_path, sample_sub_path, feat_tr_path, feat_te_path]:\n",
    "    if p is None or not Path(p).exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "print(\"[INFO] Feature train:\", feat_tr_path)\n",
    "print(\"[INFO] Feature test :\", feat_te_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Load data\n",
    "# ----------------------------\n",
    "df_tr_log  = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te_log  = pd.read_csv(test_log_path, low_memory=False)\n",
    "df_tr_feat = pd.read_csv(feat_tr_path, low_memory=False)\n",
    "df_te_feat = pd.read_csv(feat_te_path, low_memory=False)\n",
    "df_sub     = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# Detect submission prediction column\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# Normalize ids\n",
    "for d in [df_tr_log, df_te_log, df_tr_feat, df_te_feat, df_sub]:\n",
    "    d[\"object_id\"] = d[\"object_id\"].astype(str)\n",
    "\n",
    "# Target\n",
    "if \"target\" not in df_tr_log.columns:\n",
    "    raise ValueError(f\"train_log_clean missing 'target'. Found: {list(df_tr_log.columns)}\")\n",
    "df_tr_log[\"target\"] = pd.to_numeric(df_tr_log[\"target\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# Merge target into features\n",
    "df_train = df_tr_feat.merge(df_tr_log[[\"object_id\", \"target\"]], on=\"object_id\", how=\"left\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    bad = df_train[df_train[\"target\"].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Ada object_id train features yang tidak ketemu target. Example: {bad}\")\n",
    "\n",
    "df_test = df_te_feat.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold loader\n",
    "# ----------------------------\n",
    "def load_threshold():\n",
    "    cand = [\n",
    "        ART_DIR / \"ens_threshold.txt\",\n",
    "        ART_DIR / \"lgbm_threshold_best.txt\",\n",
    "        ART_DIR / \"lgbm_threshold.txt\",\n",
    "        ART_DIR / \"lgbm_zaug_threshold.txt\",\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists():\n",
    "            try:\n",
    "                v = float(p.read_text(encoding=\"utf-8\").strip().splitlines()[0])\n",
    "                if 0.0 <= v <= 1.0:\n",
    "                    print(\"[INFO] Using threshold from:\", p)\n",
    "                    return v\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(\"[WARN] No threshold file found. Using 0.50\")\n",
    "    return 0.5\n",
    "\n",
    "THR = load_threshold()\n",
    "\n",
    "# ----------------------------\n",
    "# Domain-shift redshift robust features\n",
    "# ----------------------------\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def get_zerr_fill_from_test(df_te_log, df_test):\n",
    "    zerr = None\n",
    "    if \"Z_err\" in df_te_log.columns:\n",
    "        zerr = _to_num(df_te_log[\"Z_err\"])\n",
    "    elif \"Z_err\" in df_test.columns:\n",
    "        zerr = _to_num(df_test[\"Z_err\"])\n",
    "    if zerr is None:\n",
    "        return 0.05\n",
    "    v = float(np.nanmedian(zerr.values))\n",
    "    if not np.isfinite(v) or v <= 0:\n",
    "        v = 0.05\n",
    "    return v\n",
    "\n",
    "def add_redshift_domainshift_features(df, zerr_fill, is_train, seed):\n",
    "    out = df.copy()\n",
    "    if \"Z\" not in out.columns:\n",
    "        out[\"Z\"] = np.nan\n",
    "    if \"Z_err\" not in out.columns:\n",
    "        out[\"Z_err\"] = np.nan\n",
    "\n",
    "    z = _to_num(out[\"Z\"]).astype(float).values\n",
    "    zerr = _to_num(out[\"Z_err\"]).astype(float).values\n",
    "\n",
    "    z_isna = ~np.isfinite(z)\n",
    "    zerr_isna = ~np.isfinite(zerr)\n",
    "\n",
    "    z_f = z.copy()\n",
    "    z_f[z_isna] = 0.0\n",
    "\n",
    "    zerr_f = zerr.copy()\n",
    "    zerr_f[zerr_isna] = zerr_fill\n",
    "    zerr_f = np.clip(zerr_f, 0.0, None)\n",
    "\n",
    "    out[\"Z_filled\"] = z_f\n",
    "    out[\"Zerr_filled\"] = zerr_f\n",
    "    out[\"Z_missing\"] = z_isna.astype(int)\n",
    "    out[\"Zerr_missing\"] = zerr_isna.astype(int)\n",
    "\n",
    "    out[\"log1pZ\"] = np.log1p(np.clip(z_f, 0.0, None))\n",
    "    out[\"log1pZerr\"] = np.log1p(np.clip(zerr_f, 0.0, None))\n",
    "    out[\"inv1pZ\"] = 1.0 / (1.0 + np.clip(z_f, 0.0, None))\n",
    "    out[\"Z_div_Zerr\"] = z_f / (zerr_f + 1e-6)\n",
    "\n",
    "    # Z noise only for train (ringan, tanpa duplikasi data)\n",
    "    if is_train:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        sigma = np.sqrt(np.square(zerr_f) + np.square(AUG_FLOOR) + np.square(AUG_REL * (1.0 + z_f)))\n",
    "        noise = rng.normal(0.0, sigma, size=z_f.shape[0])\n",
    "        z_aug = np.clip(z_f + noise, 0.0, None)\n",
    "        out[\"Z_aug\"] = z_aug\n",
    "        out[\"Z_aug_absdiff\"] = np.abs(z_aug - z_f)\n",
    "    else:\n",
    "        out[\"Z_aug\"] = z_f\n",
    "        out[\"Z_aug_absdiff\"] = 0.0\n",
    "\n",
    "    return out\n",
    "\n",
    "if USE_REDSHIFT_ROBUST_FE:\n",
    "    zerr_fill = get_zerr_fill_from_test(df_te_log, df_test)\n",
    "    print(f\"[INFO] zerr_fill (from test median) = {zerr_fill:.6f}\")\n",
    "    df_train = add_redshift_domainshift_features(df_train, zerr_fill, is_train=True, seed=SEED)\n",
    "    df_test  = add_redshift_domainshift_features(df_test,  zerr_fill, is_train=False, seed=SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# Build X/y\n",
    "# ----------------------------\n",
    "y = df_train[\"target\"].astype(int).values\n",
    "drop_cols = {\"object_id\", \"target\"}\n",
    "feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "\n",
    "# numeric coercion\n",
    "for c in feature_cols:\n",
    "    df_train[c] = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
    "    df_test[c]  = pd.to_numeric(df_test[c], errors=\"coerce\")\n",
    "\n",
    "X = df_train[feature_cols]\n",
    "X_test = df_test[feature_cols]\n",
    "\n",
    "pos = max(int((y == 1).sum()), 1)\n",
    "neg = max(int((y == 0).sum()), 1)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "params = dict(LGB_PARAMS)\n",
    "params[\"scale_pos_weight\"] = scale_pos_weight\n",
    "\n",
    "print(f\"[INFO] X_train: {X.shape} | X_test: {X_test.shape}\")\n",
    "print(f\"[INFO] pos_rate={float((y==1).mean()):.6f} | scale_pos_weight={scale_pos_weight:.4f}\")\n",
    "print(f\"[INFO] threshold={THR:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Find good best_iteration via small holdout (early stopping)\n",
    "# ----------------------------\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y, test_size=0.12, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "tmp_model = lgb.LGBMClassifier(**params)\n",
    "tmp_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_va, y_va)],\n",
    "    eval_metric=\"binary_logloss\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)]\n",
    ")\n",
    "\n",
    "best_iter = int(getattr(tmp_model, \"best_iteration_\", None) or params[\"n_estimators\"])\n",
    "print(f\"[INFO] best_iteration from holdout = {best_iter}\")\n",
    "\n",
    "# Optional sanity: holdout F1 at tuned threshold\n",
    "p_va = tmp_model.predict_proba(X_va)[:, 1]\n",
    "f1_va = f1_score(y_va, (p_va >= THR).astype(int))\n",
    "print(f\"[INFO] holdout F1@THR = {f1_va:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Train FULL with best_iter\n",
    "# ----------------------------\n",
    "final_params = dict(params)\n",
    "final_params[\"n_estimators\"] = max(200, best_iter)\n",
    "\n",
    "final_model = lgb.LGBMClassifier(**final_params)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# ----------------------------\n",
    "# Predict test (proba + label)\n",
    "# ----------------------------\n",
    "test_proba = final_model.predict_proba(X_test)[:, 1].astype(np.float32)\n",
    "test_pred  = (test_proba >= THR).astype(int)\n",
    "\n",
    "# Save test probabilities for later ensemble\n",
    "proba_path = ART_DIR / \"final_test_proba.csv\"\n",
    "pd.DataFrame({\"object_id\": df_test[\"object_id\"].astype(str), \"proba\": test_proba}).to_csv(proba_path, index=False)\n",
    "print(\"[INFO] Saved test proba:\", proba_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Build submission (match sample_submission)\n",
    "# ----------------------------\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_test[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int),\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_final_lgbm.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Saved submission:\", out_path)\n",
    "print(\"Prediction column:\", SUB_PRED_COL)\n",
    "print(\"n_estimators:\", final_params[\"n_estimators\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
