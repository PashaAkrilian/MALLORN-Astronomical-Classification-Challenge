{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ccd9f1",
   "metadata": {},
   "source": [
    "# Pahami struktur data & indeks object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd226a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\3228231385.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_index = pd.concat([df_train_idx, df_test_idx], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY ===\n",
      "Root             : D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\n",
      "Train objects    : 3,043\n",
      "Test objects     : 7,135\n",
      "Train target     : pos=148 | neg=2,895 | pos_rate=0.0486\n",
      "Splits in log    : 20 | example: ['split_01', 'split_02', 'split_03', 'split_04', 'split_05']\n",
      "\n",
      "=== SPLIT SUMMARY (top 10 rows) ===\n",
      "   split  n_objects   z_mean    z_std  ebv_mean  ebv_std  is_train  pos  neg  pos_rate\n",
      "split_01        155 0.609961 0.553551  0.061677 0.080969         1   12  143  0.077419\n",
      "split_02        170 0.677911 0.568435  0.053988 0.063161         1   12  158  0.070588\n",
      "split_03        138 0.749656 0.499070  0.057594 0.065305         1    3  135  0.021739\n",
      "split_04        145 0.763510 0.603055  0.051007 0.060881         1   12  133  0.082759\n",
      "split_05        165 0.661696 0.504670  0.050903 0.045186         1    6  159  0.036364\n",
      "split_06        155 0.611903 0.435373  0.055794 0.064640         1    9  146  0.058065\n",
      "split_07        165 0.594398 0.443501  0.050891 0.044091         1    6  159  0.036364\n",
      "split_08        162 0.634390 0.509443  0.063870 0.072503         1    3  159  0.018519\n",
      "split_09        128 0.678010 0.518778  0.053234 0.059531         1    3  125  0.023438\n",
      "split_10        144 0.771403 0.505291  0.049972 0.041083         1    5  139  0.034722\n",
      "\n",
      "=== SEQUENTIAL SCAN SPLIT FILES (chunked object_id only) ===\n",
      "[1/20] split_01 | scanning train/test lightcurves...\n",
      "[2/20] split_02 | scanning train/test lightcurves...\n",
      "[3/20] split_03 | scanning train/test lightcurves...\n",
      "[4/20] split_04 | scanning train/test lightcurves...\n",
      "[5/20] split_05 | scanning train/test lightcurves...\n",
      "[6/20] split_06 | scanning train/test lightcurves...\n",
      "[7/20] split_07 | scanning train/test lightcurves...\n",
      "[8/20] split_08 | scanning train/test lightcurves...\n",
      "[9/20] split_09 | scanning train/test lightcurves...\n",
      "[10/20] split_10 | scanning train/test lightcurves...\n",
      "[11/20] split_11 | scanning train/test lightcurves...\n",
      "[12/20] split_12 | scanning train/test lightcurves...\n",
      "[13/20] split_13 | scanning train/test lightcurves...\n",
      "[14/20] split_14 | scanning train/test lightcurves...\n",
      "[15/20] split_15 | scanning train/test lightcurves...\n",
      "[16/20] split_16 | scanning train/test lightcurves...\n",
      "[17/20] split_17 | scanning train/test lightcurves...\n",
      "[18/20] split_18 | scanning train/test lightcurves...\n",
      "[19/20] split_19 | scanning train/test lightcurves...\n",
      "[20/20] split_20 | scanning train/test lightcurves...\n",
      "\n",
      "=== SPLIT FILES SUMMARY (top 10) ===\n",
      "   split  train_lc_exists  train_lc_bytes  train_lc_total_rows  train_lc_unique_object_ids  train_lc_ok train_lc_err  train_log_objects  train_coverage_ratio  test_lc_exists  test_lc_bytes  test_lc_total_rows  test_lc_unique_object_ids  test_lc_ok test_lc_err  test_log_objects  test_coverage_ratio\n",
      "split_01             True         1417221                26324                         155         True                             155                   1.0            True        3200304               59235                        364        True                           364                  1.0\n",
      "split_02             True         1391371                25609                         170         True                             170                   1.0            True        3836326               71229                        414        True                           414                  1.0\n",
      "split_03             True         1179321                21676                         138         True                             138                   1.0            True        2905092               53751                        338        True                           338                  1.0\n",
      "split_04             True         1230455                22898                         145         True                             145                   1.0            True        2787939               51408                        332        True                           332                  1.0\n",
      "split_05             True         1386257                25934                         165         True                             165                   1.0            True        3273938               61179                        375        True                           375                  1.0\n",
      "split_06             True         1390952                25684                         155         True                             155                   1.0            True        3085635               57620                        374        True                           374                  1.0\n",
      "split_07             True         1318995                24473                         165         True                             165                   1.0            True        3514308               65101                        398        True                           398                  1.0\n",
      "split_08             True         1384373                25571                         162         True                             162                   1.0            True        3319507               61498                        387        True                           387                  1.0\n",
      "split_09             True         1058272                19690                         128         True                             128                   1.0            True        2536817               47239                        289        True                           289                  1.0\n",
      "split_10             True         1344079                25151                         144         True                             144                   1.0            True        2745692               51056                        331        True                           331                  1.0\n",
      "\n",
      "Saved artifacts to: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\n",
      " - train_log_clean.csv\n",
      " - test_log_clean.csv\n",
      " - index_object_split.csv\n",
      " - splits_summary.csv\n",
      " - split_files_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 (REVISI FULL) — Pahami struktur data & indeks object_id -> split\n",
    "# + (OPSIONAL) Sequential scan per split (chunked) untuk ringkasan lightcurve files\n",
    "#\n",
    "# Root dataset:\n",
    "#   D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\n",
    "#\n",
    "# Output (artifacts/):\n",
    "#   - train_log_clean.csv\n",
    "#   - test_log_clean.csv\n",
    "#   - index_object_split.csv\n",
    "#   - splits_summary.csv\n",
    "#   - split_files_summary.csv      (jika SCAN_SPLIT_FILES=True)\n",
    "# ============================================================\n",
    "\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "\n",
    "# Sequential scan settings (opsional)\n",
    "SCAN_SPLIT_FILES = True            # True = scan train/test_full_lightcurves.csv tiap split (chunked)\n",
    "CHUNK_ROWS = 1_000_000             # makin besar makin cepat tapi lebih berat RAM\n",
    "ONLY_SCAN_SPLITS_IN_LOG = True     # True = scan split yang muncul di log saja, bukan selalu 01..20\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS\n",
    "# ----------------------------\n",
    "PATHS = {\n",
    "    \"root\": DATA_ROOT,\n",
    "    \"train_log\": DATA_ROOT / \"train_log.csv\",\n",
    "    \"test_log\":  DATA_ROOT / \"test_log.csv\",\n",
    "    \"sample_submission\": DATA_ROOT / \"sample_submission.csv\",\n",
    "    \"artifacts\": DATA_ROOT / \"artifacts\",\n",
    "}\n",
    "PATHS[\"artifacts\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _fail(msg: str):\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "for k in [\"train_log\", \"test_log\", \"sample_submission\"]:\n",
    "    if not PATHS[k].exists():\n",
    "        _fail(f\"Missing file: {PATHS[k]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "_SPLIT_RE = re.compile(r\"(\\d+)\")\n",
    "def normalize_split(x) -> str:\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    m = _SPLIT_RE.search(s)\n",
    "    if not m:\n",
    "        return s\n",
    "    n = int(m.group(1))\n",
    "    return f\"split_{n:02d}\"\n",
    "\n",
    "def read_csv_safely(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols, name=\"df\"):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        _fail(f\"{name} missing columns: {missing}\\nFound columns: {list(df.columns)}\")\n",
    "\n",
    "def file_bytes(path: Path) -> int:\n",
    "    try:\n",
    "        return path.stat().st_size\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def scan_object_ids_csv(csv_path: Path, chunk_rows: int = 1_000_000) -> dict:\n",
    "    \"\"\"\n",
    "    Scan hanya kolom 'object_id' secara chunked.\n",
    "    Return: total_rows, unique_object_ids (count), ok (bool), err (str)\n",
    "    \"\"\"\n",
    "    res = {\n",
    "        \"path\": str(csv_path),\n",
    "        \"exists\": csv_path.exists(),\n",
    "        \"bytes\": file_bytes(csv_path),\n",
    "        \"total_rows\": 0,\n",
    "        \"unique_object_ids\": 0,\n",
    "        \"ok\": False,\n",
    "        \"err\": \"\",\n",
    "    }\n",
    "    if not csv_path.exists():\n",
    "        res[\"err\"] = \"file_missing\"\n",
    "        return res\n",
    "\n",
    "    try:\n",
    "        uniq = set()\n",
    "        total = 0\n",
    "        # Hanya baca kolom object_id agar ringan\n",
    "        for chunk in pd.read_csv(csv_path, usecols=[\"object_id\"], dtype={\"object_id\": \"string\"},\n",
    "                                 chunksize=chunk_rows, low_memory=False):\n",
    "            # dropna + convert to python str\n",
    "            vals = chunk[\"object_id\"].dropna().astype(str).tolist()\n",
    "            total += len(vals)\n",
    "            uniq.update(vals)\n",
    "\n",
    "        res[\"total_rows\"] = int(total)\n",
    "        res[\"unique_object_ids\"] = int(len(uniq))\n",
    "        res[\"ok\"] = True\n",
    "        return res\n",
    "    except ValueError as e:\n",
    "        # biasanya terjadi jika kolom object_id tidak ditemukan\n",
    "        res[\"err\"] = f\"ValueError: {e}\"\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        res[\"err\"] = f\"{type(e).__name__}: {e}\"\n",
    "        return res\n",
    "\n",
    "# ----------------------------\n",
    "# Load logs\n",
    "# ----------------------------\n",
    "df_train_log = read_csv_safely(PATHS[\"train_log\"])\n",
    "df_test_log  = read_csv_safely(PATHS[\"test_log\"])\n",
    "\n",
    "ensure_cols(df_train_log, [\"object_id\", \"Z\", \"EBV\", \"split\", \"target\"], \"train_log\")\n",
    "ensure_cols(df_test_log,  [\"object_id\", \"Z\", \"EBV\", \"split\"], \"test_log\")\n",
    "\n",
    "df_train_log = df_train_log.copy()\n",
    "df_test_log  = df_test_log.copy()\n",
    "\n",
    "# Normalize split naming\n",
    "df_train_log[\"split\"] = df_train_log[\"split\"].apply(normalize_split)\n",
    "df_test_log[\"split\"]  = df_test_log[\"split\"].apply(normalize_split)\n",
    "\n",
    "# Coerce types\n",
    "df_train_log[\"object_id\"] = df_train_log[\"object_id\"].astype(str)\n",
    "df_test_log[\"object_id\"]  = df_test_log[\"object_id\"].astype(str)\n",
    "\n",
    "for col in [\"Z\", \"EBV\"]:\n",
    "    df_train_log_CONFIRM = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n",
    "    df_test_log_CONFIRM  = pd.to_numeric(df_test_log[col], errors=\"coerce\")\n",
    "    df_train_log[col] = df_train_log_CONFIRM\n",
    "    df_test_log[col]  = df_test_log_CONFIRM\n",
    "\n",
    "if \"Z_err\" in df_train_log.columns:\n",
    "    df_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\")\n",
    "if \"Z_err\" in df_test_log.columns:\n",
    "    df_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "df_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ----------------------------\n",
    "# Basic sanity checks\n",
    "# ----------------------------\n",
    "dup_tr = int(df_train_log[\"object_id\"].duplicated().sum())\n",
    "dup_te = int(df_test_log[\"object_id\"].duplicated().sum())\n",
    "if dup_tr > 0 or dup_te > 0:\n",
    "    print(f\"[WARN] Duplicate object_id found | train={dup_tr}, test={dup_te}. Keeping first occurrence.\")\n",
    "    df_train_log = df_train_log.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "    df_test_log  = df_test_log.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "overlap = set(df_train_log[\"object_id\"]).intersection(set(df_test_log[\"object_id\"]))\n",
    "if len(overlap) > 0:\n",
    "    print(f\"[WARN] Found {len(overlap)} object_id present in BOTH train and test (unexpected). Example: {list(sorted(overlap))[:3]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build split -> ids mapping\n",
    "# ----------------------------\n",
    "split_to_train_ids = df_train_log.groupby(\"split\")[\"object_id\"].apply(list).to_dict()\n",
    "split_to_test_ids  = df_test_log.groupby(\"split\")[\"object_id\"].apply(list).to_dict()\n",
    "\n",
    "all_splits_in_log = sorted(set(df_train_log[\"split\"].dropna()).union(set(df_test_log[\"split\"].dropna())))\n",
    "\n",
    "# Optional: enforce split_01..split_20 existence check\n",
    "missing_split_dirs = [s for s in all_splits_in_log if not (PATHS[\"root\"] / s).exists()]\n",
    "if missing_split_dirs:\n",
    "    print(\"[WARN] Some split folders referenced in logs do not exist on disk:\")\n",
    "    for s in missing_split_dirs[:50]:\n",
    "        print(\"  -\", s)\n",
    "\n",
    "# ----------------------------\n",
    "# Combined index dataframe (1 row per object_id)\n",
    "# ----------------------------\n",
    "df_train_idx = df_train_log[[\"object_id\", \"split\", \"Z\", \"EBV\"]].copy()\n",
    "df_train_idx[\"is_train\"] = 1\n",
    "df_train_idx[\"target\"] = df_train_log[\"target\"]\n",
    "\n",
    "df_test_idx = df_test_log[[\"object_id\", \"split\", \"Z\", \"EBV\"]].copy()\n",
    "df_test_idx[\"is_train\"] = 0\n",
    "df_test_idx[\"target\"] = pd.NA\n",
    "\n",
    "if \"Z_err\" in df_train_log.columns or \"Z_err\" in df_test_log.columns:\n",
    "    df_train_idx[\"Z_err\"] = df_train_log[\"Z_err\"] if \"Z_err\" in df_train_log.columns else pd.NA\n",
    "    df_test_idx[\"Z_err\"]  = df_test_log[\"Z_err\"]  if \"Z_err\" in df_test_log.columns  else pd.NA\n",
    "\n",
    "df_index = pd.concat([df_train_idx, df_test_idx], ignore_index=True)\n",
    "df_index = df_index.sort_values([\"is_train\", \"split\", \"object_id\"], ascending=[False, True, True]).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Summaries\n",
    "# ----------------------------\n",
    "pos = int((df_train_log[\"target\"] == 1).sum())\n",
    "neg = int((df_train_log[\"target\"] == 0).sum())\n",
    "tot = len(df_train_log)\n",
    "pos_rate = pos / max(tot, 1)\n",
    "\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Root             : {PATHS['root']}\")\n",
    "print(f\"Train objects    : {len(df_train_log):,}\")\n",
    "print(f\"Test objects     : {len(df_test_log):,}\")\n",
    "print(f\"Train target     : pos={pos:,} | neg={neg:,} | pos_rate={pos_rate:.4f}\")\n",
    "print(f\"Splits in log    : {len(all_splits_in_log)} | example: {all_splits_in_log[:5]}\")\n",
    "\n",
    "def _split_summary(df_log: pd.DataFrame, is_train: int) -> pd.DataFrame:\n",
    "    g = df_log.groupby(\"split\").agg(\n",
    "        n_objects=(\"object_id\", \"count\"),\n",
    "        z_mean=(\"Z\", \"mean\"),\n",
    "        z_std=(\"Z\", \"std\"),\n",
    "        ebv_mean=(\"EBV\", \"mean\"),\n",
    "        ebv_std=(\"EBV\", \"std\"),\n",
    "    ).reset_index()\n",
    "    g[\"is_train\"] = is_train\n",
    "    if is_train and \"target\" in df_log.columns:\n",
    "        gg = df_log.groupby(\"split\")[\"target\"].agg(\n",
    "            pos=lambda x: int((x == 1).sum()),\n",
    "            neg=lambda x: int((x == 0).sum()),\n",
    "        ).reset_index()\n",
    "        g = g.merge(gg, on=\"split\", how=\"left\")\n",
    "        g[\"pos_rate\"] = g[\"pos\"] / g[\"n_objects\"].clip(lower=1)\n",
    "    return g\n",
    "\n",
    "df_sum_train = _split_summary(df_train_log, 1)\n",
    "df_sum_test  = _split_summary(df_test_log, 0)\n",
    "df_splits_summary = pd.concat([df_sum_train, df_sum_test], ignore_index=True).sort_values(\n",
    "    [\"is_train\",\"split\"], ascending=[False, True]\n",
    ")\n",
    "\n",
    "print(\"\\n=== SPLIT SUMMARY (top 10 rows) ===\")\n",
    "print(df_splits_summary.head(10).to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# OPTIONAL: Sequential scan split files (chunked)\n",
    "# ----------------------------\n",
    "df_split_files_summary = None\n",
    "\n",
    "if SCAN_SPLIT_FILES:\n",
    "    print(\"\\n=== SEQUENTIAL SCAN SPLIT FILES (chunked object_id only) ===\")\n",
    "    if ONLY_SCAN_SPLITS_IN_LOG:\n",
    "        splits_to_scan = all_splits_in_log\n",
    "    else:\n",
    "        splits_to_scan = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "\n",
    "    rows = []\n",
    "    for i, sp in enumerate(splits_to_scan, start=1):\n",
    "        sp_dir = PATHS[\"root\"] / sp\n",
    "        train_lc = sp_dir / \"train_full_lightcurves.csv\"\n",
    "        test_lc  = sp_dir / \"test_full_lightcurves.csv\"\n",
    "\n",
    "        print(f\"[{i}/{len(splits_to_scan)}] {sp} | scanning train/test lightcurves...\")\n",
    "\n",
    "        tr_scan = scan_object_ids_csv(train_lc, chunk_rows=CHUNK_ROWS)\n",
    "        te_scan = scan_object_ids_csv(test_lc,  chunk_rows=CHUNK_ROWS)\n",
    "\n",
    "        # Compare to logs (berapa object log yang seharusnya ada)\n",
    "        tr_log_n = len(split_to_train_ids.get(sp, []))\n",
    "        te_log_n = len(split_to_test_ids.get(sp, []))\n",
    "\n",
    "        rows.append({\n",
    "            \"split\": sp,\n",
    "            \"train_lc_exists\": tr_scan[\"exists\"],\n",
    "            \"train_lc_bytes\": tr_scan[\"bytes\"],\n",
    "            \"train_lc_total_rows\": tr_scan[\"total_rows\"],\n",
    "            \"train_lc_unique_object_ids\": tr_scan[\"unique_object_ids\"],\n",
    "            \"train_lc_ok\": tr_scan[\"ok\"],\n",
    "            \"train_lc_err\": tr_scan[\"err\"],\n",
    "            \"train_log_objects\": tr_log_n,\n",
    "            \"train_coverage_ratio\": (tr_scan[\"unique_object_ids\"] / tr_log_n) if tr_log_n > 0 else pd.NA,\n",
    "\n",
    "            \"test_lc_exists\": te_scan[\"exists\"],\n",
    "            \"test_lc_bytes\": te_scan[\"bytes\"],\n",
    "            \"test_lc_total_rows\": te_scan[\"total_rows\"],\n",
    "            \"test_lc_unique_object_ids\": te_scan[\"unique_object_ids\"],\n",
    "            \"test_lc_ok\": te_scan[\"ok\"],\n",
    "            \"test_lc_err\": te_scan[\"err\"],\n",
    "            \"test_log_objects\": te_log_n,\n",
    "            \"test_coverage_ratio\": (te_scan[\"unique_object_ids\"] / te_log_n) if te_log_n > 0 else pd.NA,\n",
    "        })\n",
    "\n",
    "    df_split_files_summary = pd.DataFrame(rows).sort_values(\"split\").reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== SPLIT FILES SUMMARY (top 10) ===\")\n",
    "    print(df_split_files_summary.head(10).to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# Save artifacts\n",
    "# ----------------------------\n",
    "(df_train_log).to_csv(PATHS[\"artifacts\"] / \"train_log_clean.csv\", index=False)\n",
    "(df_test_log).to_csv(PATHS[\"artifacts\"] / \"test_log_clean.csv\", index=False)\n",
    "(df_index).to_csv(PATHS[\"artifacts\"] / \"index_object_split.csv\", index=False)\n",
    "(df_splits_summary).to_csv(PATHS[\"artifacts\"] / \"splits_summary.csv\", index=False)\n",
    "\n",
    "if df_split_files_summary is not None:\n",
    "    df_split_files_summary.to_csv(PATHS[\"artifacts\"] / \"split_files_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts to:\", PATHS[\"artifacts\"])\n",
    "print(\" - train_log_clean.csv\")\n",
    "print(\" - test_log_clean.csv\")\n",
    "print(\" - index_object_split.csv\")\n",
    "print(\" - splits_summary.csv\")\n",
    "if df_split_files_summary is not None:\n",
    "    print(\" - split_files_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a69fa",
   "metadata": {},
   "source": [
    "# Baseline super cepat (cek pipeline benar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f260f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sample_submission prediction column = 'prediction'\n",
      "[FOLD 1/5] F1@0.50 = 0.10160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 2/5] F1@0.50 = 0.08673\n",
      "[FOLD 3/5] F1@0.50 = 0.10918\n",
      "[FOLD 4/5] F1@0.50 = 0.08696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 5/5] F1@0.50 = 0.08978\n",
      "\n",
      "=== CV SUMMARY (threshold=0.50) ===\n",
      "Mean F1@0.50: 0.09485 | Std: 0.00900\n",
      "\n",
      "=== OOF THRESHOLD TUNING ===\n",
      "Best threshold = 0.44\n",
      "OOF F1(best)   = 0.10441\n",
      "\n",
      "Confusion Matrix (OOF):\n",
      "[[ 574 2321]\n",
      " [  12  136]]\n",
      "\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9795    0.1983    0.3298      2895\n",
      "           1     0.0554    0.9189    0.1044       148\n",
      "\n",
      "    accuracy                         0.2333      3043\n",
      "   macro avg     0.5174    0.5586    0.2171      3043\n",
      "weighted avg     0.9346    0.2333    0.3188      3043\n",
      "\n",
      "\n",
      "=== DONE ===\n",
      "Saved: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\submissions\\sub_baseline_logreg.csv\n",
      "Saved: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\baseline_oof.csv\n",
      "Saved: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\baseline_threshold.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err' 'Zerr_clip']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — BASELINE SUPER CEPAT (CEK PIPELINE BENAR) [REVISI]\n",
    "# Model: Logistic Regression (CPU cepat)\n",
    "# Fitur: hanya dari log (Z, EBV, Z_err + missing flags)\n",
    "#\n",
    "# Fix:\n",
    "# - sample_submission.csv di dataset kamu pakai kolom 'prediction' (bukan 'target')\n",
    "# - kode ini auto-detect nama kolom prediksi dari sample_submission\n",
    "#\n",
    "# Input:\n",
    "#   artifacts/train_log_clean.csv\n",
    "#   artifacts/test_log_clean.csv\n",
    "#   sample_submission.csv\n",
    "#\n",
    "# Output:\n",
    "#   artifacts/baseline_oof.csv\n",
    "#   artifacts/baseline_threshold.txt\n",
    "#   submissions/sub_baseline_logreg.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 2025\n",
    "N_FOLDS = 5\n",
    "\n",
    "# ----------------------------\n",
    "# Load artifacts from Stage 1\n",
    "# ----------------------------\n",
    "train_path = ART_DIR / \"train_log_clean.csv\"\n",
    "test_path  = ART_DIR / \"test_log_clean.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {train_path}. Jalankan Stage 1 dulu.\")\n",
    "if not test_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_path}. Jalankan Stage 1 dulu.\")\n",
    "if not sample_sub_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {sample_sub_path}\")\n",
    "\n",
    "df_tr = pd.read_csv(train_path, low_memory=False)\n",
    "df_te = pd.read_csv(test_path, low_memory=False)\n",
    "df_sub = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Detect submission prediction column name\n",
    "# ----------------------------\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "SUB_PRED_COL = pred_cols[0]  # di kasus kamu: 'prediction'\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# Minimal checks\n",
    "# ----------------------------\n",
    "need_tr = [\"object_id\", \"Z\", \"EBV\", \"split\", \"target\"]\n",
    "need_te = [\"object_id\", \"Z\", \"EBV\", \"split\"]\n",
    "\n",
    "for c in need_tr:\n",
    "    if c not in df_tr.columns:\n",
    "        raise ValueError(f\"train missing col: {c} | found={list(df_tr.columns)}\")\n",
    "for c in need_te:\n",
    "    if c not in df_te.columns:\n",
    "        raise ValueError(f\"test missing col: {c} | found={list(df_te.columns)}\")\n",
    "\n",
    "# Ensure numeric\n",
    "for col in [\"Z\", \"EBV\"]:\n",
    "    df_tr[col] = pd.to_numeric(df_tr[col], errors=\"coerce\")\n",
    "    df_te[col] = pd.to_numeric(df_te[col], errors=\"coerce\")\n",
    "\n",
    "# Z_err optional: train biasanya kosong/tidak ada, test ada\n",
    "if \"Z_err\" not in df_tr.columns:\n",
    "    df_tr[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_tr[\"Z_err\"] = pd.to_numeric(df_tr[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "if \"Z_err\" not in df_te.columns:\n",
    "    df_te[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_te[\"Z_err\"] = pd.to_numeric(df_te[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "# Target\n",
    "y = pd.to_numeric(df_tr[\"target\"], errors=\"coerce\").astype(int).values\n",
    "\n",
    "# ----------------------------\n",
    "# Feature set (log-only)\n",
    "# ----------------------------\n",
    "def build_log_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = pd.DataFrame({\n",
    "        \"Z\": df[\"Z\"].astype(float),\n",
    "        \"EBV\": df[\"EBV\"].astype(float),\n",
    "        \"Z_err\": df[\"Z_err\"].astype(float),\n",
    "        \"Z_isna\": df[\"Z\"].isna().astype(int),\n",
    "        \"EBV_isna\": df[\"EBV\"].isna().astype(int),\n",
    "        \"Zerr_isna\": df[\"Z_err\"].isna().astype(int),\n",
    "    })\n",
    "\n",
    "    # stabilisasi sederhana (optional)\n",
    "    def _p99(a):\n",
    "        a = np.asarray(a, dtype=float)\n",
    "        a = a[np.isfinite(a)]\n",
    "        if a.size == 0:\n",
    "            return np.nan\n",
    "        return float(np.nanpercentile(a, 99))\n",
    "\n",
    "    z99 = _p99(X[\"Z\"].values)\n",
    "    e99 = _p99(X[\"EBV\"].values)\n",
    "    zerr99 = _p99(X[\"Z_err\"].values)\n",
    "\n",
    "    X[\"Z_clip\"] = X[\"Z\"].clip(lower=0, upper=z99 if np.isfinite(z99) else 10.0)\n",
    "    X[\"EBV_clip\"] = X[\"EBV\"].clip(lower=0, upper=e99 if np.isfinite(e99) else 1.0)\n",
    "    X[\"Zerr_clip\"] = X[\"Z_err\"].clip(lower=0, upper=zerr99 if np.isfinite(zerr99) else 1.0)\n",
    "    return X\n",
    "\n",
    "X_tr = build_log_features(df_tr)\n",
    "X_te = build_log_features(df_te)\n",
    "\n",
    "# ----------------------------\n",
    "# Model pipeline (fast CPU)\n",
    "# ----------------------------\n",
    "clf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=500,\n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# CV + OOF\n",
    "# ----------------------------\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "oof_proba = np.zeros(len(X_tr), dtype=np.float32)\n",
    "\n",
    "fold_scores = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tr, y), start=1):\n",
    "    X_train, X_val = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
    "    y_train, y_val = y[tr_idx], y[va_idx]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    p_val = clf.predict_proba(X_val)[:, 1]\n",
    "    oof_proba[va_idx] = p_val\n",
    "\n",
    "    f1_05 = f1_score(y_val, (p_val >= 0.5).astype(int))\n",
    "    fold_scores.append(f1_05)\n",
    "    print(f\"[FOLD {fold}/{N_FOLDS}] F1@0.50 = {f1_05:.5f}\")\n",
    "\n",
    "print(\"\\n=== CV SUMMARY (threshold=0.50) ===\")\n",
    "print(f\"Mean F1@0.50: {float(np.mean(fold_scores)):.5f} | Std: {float(np.std(fold_scores)):.5f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold tuning (global OOF)\n",
    "# ----------------------------\n",
    "thr_grid = np.linspace(0.01, 0.99, 99)\n",
    "best_thr, best_f1 = 0.5, -1.0\n",
    "for thr in thr_grid:\n",
    "    f1 = f1_score(y, (oof_proba >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "print(\"\\n=== OOF THRESHOLD TUNING ===\")\n",
    "print(f\"Best threshold = {best_thr:.2f}\")\n",
    "print(f\"OOF F1(best)   = {best_f1:.5f}\")\n",
    "\n",
    "y_hat = (oof_proba >= best_thr).astype(int)\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(confusion_matrix(y, y_hat))\n",
    "\n",
    "print(\"\\nClassification Report (OOF):\")\n",
    "print(classification_report(y, y_hat, digits=4))\n",
    "\n",
    "# Save OOF artifact\n",
    "df_oof = pd.DataFrame({\n",
    "    \"object_id\": df_tr[\"object_id\"].astype(str),\n",
    "    \"oof_proba\": oof_proba,\n",
    "    \"target\": y\n",
    "})\n",
    "df_oof.to_csv(ART_DIR / \"baseline_oof.csv\", index=False)\n",
    "\n",
    "with open(ART_DIR / \"baseline_threshold.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{best_thr}\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train full + predict test + export submission\n",
    "# ----------------------------\n",
    "clf.fit(X_tr, y)\n",
    "test_proba = clf.predict_proba(X_te)[:, 1]\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_te[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int)\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_baseline_logreg.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Saved:\", ART_DIR / \"baseline_oof.csv\")\n",
    "print(\"Saved:\", ART_DIR / \"baseline_threshold.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f488b",
   "metadata": {},
   "source": [
    "# Koreksi extinction (de-extinct flux) + fitur statistik per band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad19f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Tidak berhasil ekstrak koefisien extinction dari Using_the_Data notebook.\n",
      "       Pakai FALLBACK_EXT_COEFF. Jika mau 100% sama, ganti nilai dict ini sesuai notebook.\n",
      "[INFO] EXT_COEFF = {'u': 4.2, 'g': 3.3, 'r': 2.3, 'i': 1.7, 'z': 1.3, 'y': 1.1}\n",
      "[INFO] Total splits to process: 20 | example: ['split_01', 'split_02', 'split_03', 'split_04', 'split_05']\n",
      "\n",
      "[1/20] split_01\n",
      "  train_ids=155 | test_ids=364\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[2/20] split_02\n",
      "  train_ids=170 | test_ids=414\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[3/20] split_03\n",
      "  train_ids=138 | test_ids=338\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[4/20] split_04\n",
      "  train_ids=145 | test_ids=332\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[5/20] split_05\n",
      "  train_ids=165 | test_ids=375\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[6/20] split_06\n",
      "  train_ids=155 | test_ids=374\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[7/20] split_07\n",
      "  train_ids=165 | test_ids=398\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[8/20] split_08\n",
      "  train_ids=162 | test_ids=387\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[9/20] split_09\n",
      "  train_ids=128 | test_ids=289\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[10/20] split_10\n",
      "  train_ids=144 | test_ids=331\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[11/20] split_11\n",
      "  train_ids=146 | test_ids=325\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[12/20] split_12\n",
      "  train_ids=155 | test_ids=353\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[13/20] split_13\n",
      "  train_ids=143 | test_ids=379\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[14/20] split_14\n",
      "  train_ids=154 | test_ids=351\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[15/20] split_15\n",
      "  train_ids=158 | test_ids=342\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[16/20] split_16\n",
      "  train_ids=155 | test_ids=354\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[17/20] split_17\n",
      "  train_ids=153 | test_ids=351\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[18/20] split_18\n",
      "  train_ids=152 | test_ids=345\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[19/20] split_19\n",
      "  train_ids=147 | test_ids=375\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[20/20] split_20\n",
      "  train_ids=153 | test_ids=358\n",
      "  files: train_exists=True | test_exists=True\n",
      "\n",
      "[INFO] Lightcurve features built:\n",
      "  train rows: 3043 | cols: 61\n",
      "  test  rows: 7135 | cols: 61\n",
      "\n",
      "=== STAGE 3 DONE ===\n",
      "Saved:\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_lc_train.csv\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_lc_test.csv\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_log.csv\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_merged_train.csv\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_merged_test.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — DE-EXTINCT FLUX + FITUR STATISTIK PER BAND (CHUNKED)\n",
    "#\n",
    "# Output:\n",
    "#   artifacts/features_lc_train.csv\n",
    "#   artifacts/features_lc_test.csv\n",
    "#   artifacts/features_log.csv              (log features per object)\n",
    "#   artifacts/features_merged_train.csv     (log + lc)\n",
    "#   artifacts/features_merged_test.csv      (log + lc)\n",
    "#\n",
    "# Notes:\n",
    "# - Hemat RAM: baca lightcurve per split, per chunk\n",
    "# - Statistik yang dihitung (per object_id x band):\n",
    "#   n_obs, flux_mean, flux_std, flux_min, flux_max, amp, snr_mean, frac_snr_gt3, frac_snr_gt5, time_span\n",
    "# ============================================================\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_ROWS = 1_000_000\n",
    "BANDS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Load logs (clean)\n",
    "# ----------------------------\n",
    "train_log_path = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path  = ART_DIR / \"test_log_clean.csv\"\n",
    "if not train_log_path.exists() or not test_log_path.exists():\n",
    "    raise FileNotFoundError(\"Missing cleaned logs. Jalankan Stage 1 dulu.\")\n",
    "\n",
    "df_tr = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te = pd.read_csv(test_log_path, low_memory=False)\n",
    "\n",
    "# numeric\n",
    "for col in [\"Z\", \"EBV\"]:\n",
    "    df_tr[col] = pd.to_numeric(df_tr[col], errors=\"coerce\")\n",
    "    df_te[col] = pd.to_numeric(df_te[col], errors=\"coerce\")\n",
    "\n",
    "if \"Z_err\" not in df_tr.columns:\n",
    "    df_tr[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_tr[\"Z_err\"] = pd.to_numeric(df_tr[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "if \"Z_err\" not in df_te.columns:\n",
    "    df_te[\"Z_err\"] = np.nan\n",
    "else:\n",
    "    df_te[\"Z_err\"] = pd.to_numeric(df_te[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "df_tr[\"object_id\"] = df_tr[\"object_id\"].astype(str)\n",
    "df_te[\"object_id\"] = df_te[\"object_id\"].astype(str)\n",
    "\n",
    "# ----------------------------\n",
    "# EBV map for de-extinction\n",
    "# ----------------------------\n",
    "ebv_map = pd.concat([\n",
    "    df_tr[[\"object_id\", \"EBV\"]],\n",
    "    df_te[[\"object_id\", \"EBV\"]],\n",
    "], ignore_index=True).drop_duplicates(\"object_id\")\n",
    "ebv_dict = dict(zip(ebv_map[\"object_id\"].values, ebv_map[\"EBV\"].values))\n",
    "\n",
    "# ----------------------------\n",
    "# Auto-extract extinction coefficients from Using_the_Data notebook (if exists)\n",
    "# ----------------------------\n",
    "def try_extract_extinction_coeffs(root: Path):\n",
    "    \"\"\"\n",
    "    Cari file *Using_the_Data*.ipynb, lalu coba ekstrak dict yang berisi key u,g,r,i,z,y dan value float.\n",
    "    \"\"\"\n",
    "    ipynbs = list(root.rglob(\"*Using_the_Data*.ipynb\"))\n",
    "    if not ipynbs:\n",
    "        return None, None\n",
    "\n",
    "    for nb_path in ipynbs[:5]:\n",
    "        try:\n",
    "            nb = json.loads(nb_path.read_text(encoding=\"utf-8\"))\n",
    "            cells = nb.get(\"cells\", [])\n",
    "            text = []\n",
    "            for c in cells:\n",
    "                if c.get(\"cell_type\") in (\"code\", \"markdown\"):\n",
    "                    src = c.get(\"source\", [])\n",
    "                    if isinstance(src, list):\n",
    "                        text.append(\"\".join(src))\n",
    "                    elif isinstance(src, str):\n",
    "                        text.append(src)\n",
    "            blob = \"\\n\".join(text)\n",
    "\n",
    "            # cari pattern dict: 'u': 4.0, 'g': 3.0, ...\n",
    "            pairs = re.findall(r\"['\\\"]([ugrizy])['\\\"]\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)\", blob)\n",
    "            if not pairs:\n",
    "                continue\n",
    "\n",
    "            d = {}\n",
    "            for k, v in pairs:\n",
    "                d[k] = float(v)\n",
    "\n",
    "            # harus punya semua bands\n",
    "            if all(b in d for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]):\n",
    "                return d, nb_path\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return None, None\n",
    "\n",
    "EXT_COEFF, SRC_NB = try_extract_extinction_coeffs(DATA_ROOT)\n",
    "\n",
    "# fallback (jika notebook tidak ketemu / gagal parse)\n",
    "# kamu boleh ganti angka ini bila ingin sama persis dengan notebook Using_the_Data.\n",
    "FALLBACK_EXT_COEFF = {\n",
    "    \"u\": 4.2,\n",
    "    \"g\": 3.3,\n",
    "    \"r\": 2.3,\n",
    "    \"i\": 1.7,\n",
    "    \"z\": 1.3,\n",
    "    \"y\": 1.1,\n",
    "}\n",
    "\n",
    "if EXT_COEFF is None:\n",
    "    EXT_COEFF = FALLBACK_EXT_COEFF\n",
    "    print(\"[WARN] Tidak berhasil ekstrak koefisien extinction dari Using_the_Data notebook.\")\n",
    "    print(\"       Pakai FALLBACK_EXT_COEFF. Jika mau 100% sama, ganti nilai dict ini sesuai notebook.\")\n",
    "else:\n",
    "    print(f\"[INFO] Extinction coeffs loaded from: {SRC_NB}\")\n",
    "print(\"[INFO] EXT_COEFF =\", EXT_COEFF)\n",
    "\n",
    "def de_extinct_flux(flux: np.ndarray, ebv: np.ndarray, band: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    flux_corr = flux * 10^(0.4 * A_lambda), dengan A_lambda = EXT_COEFF[band] * EBV\n",
    "    \"\"\"\n",
    "    R = float(EXT_COEFF.get(band, 0.0))\n",
    "    A = R * ebv\n",
    "    factor = np.power(10.0, 0.4 * A)\n",
    "    return flux * factor\n",
    "\n",
    "# ----------------------------\n",
    "# Lightcurve column detection (robust to naming)\n",
    "# ----------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\s\\(\\)\\[\\]\\-]+\", \"\", s)\n",
    "    s = s.replace(\"_\", \"\")\n",
    "    return s\n",
    "\n",
    "def detect_lc_columns(csv_path: Path):\n",
    "    head = pd.read_csv(csv_path, nrows=0)\n",
    "    cols = list(head.columns)\n",
    "    ncols = {_norm(c): c for c in cols}\n",
    "\n",
    "    def pick(cands):\n",
    "        for c in cands:\n",
    "            if c in ncols:\n",
    "                return ncols[c]\n",
    "        return None\n",
    "\n",
    "    col_object = pick([\"objectid\", \"object_id\"])\n",
    "    col_time   = pick([\"timemjd\", \"time\", \"mjd\", \"timemodifiedjuliandate\"])\n",
    "    col_flux   = pick([\"flux\"])\n",
    "    col_ferr   = pick([\"fluxerr\", \"flux_err\", \"fluxerror\", \"fluxunc\", \"fluxuncertainty\"])\n",
    "    col_filt   = pick([\"filter\", \"band\", \"passband\"])\n",
    "\n",
    "    got = {\n",
    "        \"object_id\": col_object,\n",
    "        \"mjd\": col_time,\n",
    "        \"flux\": col_flux,\n",
    "        \"flux_err\": col_ferr,\n",
    "        \"filter\": col_filt,\n",
    "    }\n",
    "    if any(v is None for v in got.values()):\n",
    "        raise ValueError(f\"Kolom lightcurve tidak terdeteksi lengkap di {csv_path}\\n\"\n",
    "                         f\"Detected mapping: {got}\\nAll columns: {cols}\")\n",
    "    return got\n",
    "\n",
    "# ----------------------------\n",
    "# Chunked scan + partial aggregation\n",
    "# ----------------------------\n",
    "def partial_agg_from_file(csv_path: Path, object_ids_set: set, ebv_dict: dict, chunk_rows: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return partial aggregated stats per (object_id, filter) dari 1 file CSV lightcurve.\n",
    "    \"\"\"\n",
    "    if not csv_path.exists():\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    colmap = detect_lc_columns(csv_path)\n",
    "    usecols = list(colmap.values())\n",
    "\n",
    "    parts = []\n",
    "    reader = pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_rows, low_memory=False)\n",
    "\n",
    "    for chunk in reader:\n",
    "        chunk = chunk.rename(columns={v: k for k, v in colmap.items()})\n",
    "\n",
    "        # types\n",
    "        chunk[\"object_id\"] = chunk[\"object_id\"].astype(str)\n",
    "        chunk = chunk[chunk[\"object_id\"].isin(object_ids_set)]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        chunk[\"filter\"] = chunk[\"filter\"].astype(str).str.strip().str.lower()\n",
    "        chunk = chunk[chunk[\"filter\"].isin(BANDS)]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        chunk[\"mjd\"] = pd.to_numeric(chunk[\"mjd\"], errors=\"coerce\")\n",
    "        chunk[\"flux\"] = pd.to_numeric(chunk[\"flux\"], errors=\"coerce\")\n",
    "        chunk[\"flux_err\"] = pd.to_numeric(chunk[\"flux_err\"], errors=\"coerce\")\n",
    "\n",
    "        chunk = chunk.dropna(subset=[\"mjd\", \"flux\", \"flux_err\"])\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # EBV mapping\n",
    "        ebv = chunk[\"object_id\"].map(ebv_dict).astype(float).values\n",
    "        flx = chunk[\"flux\"].astype(float).values\n",
    "\n",
    "        # de-extinct per band (vectorized per group band)\n",
    "        # lebih cepat: apply per band mask\n",
    "        flux_corr = flx.copy()\n",
    "        for b in BANDS:\n",
    "            m = (chunk[\"filter\"].values == b)\n",
    "            if m.any():\n",
    "                flux_corr[m] = de_extinct_flux(flux_corr[m], ebv[m], b)\n",
    "\n",
    "        chunk[\"flux_corr\"] = flux_corr\n",
    "\n",
    "        # SNR\n",
    "        ferr = chunk[\"flux_err\"].astype(float).values\n",
    "        snr = np.zeros_like(ferr, dtype=float)\n",
    "        mpos = ferr > 0\n",
    "        snr[mpos] = np.abs(flux_corr[mpos]) / ferr[mpos]\n",
    "        chunk[\"snr\"] = snr\n",
    "        chunk[\"snr_gt3\"] = (snr > 3.0).astype(int)\n",
    "        chunk[\"snr_gt5\"] = (snr > 5.0).astype(int)\n",
    "\n",
    "        # partial groupby stats\n",
    "        g = chunk.groupby([\"object_id\", \"filter\"]).agg(\n",
    "            n_obs=(\"flux_corr\", \"size\"),\n",
    "            sum_flux=(\"flux_corr\", \"sum\"),\n",
    "            sum_flux2=(\"flux_corr\", lambda x: float(np.sum(np.square(x.values)))),\n",
    "            min_flux=(\"flux_corr\", \"min\"),\n",
    "            max_flux=(\"flux_corr\", \"max\"),\n",
    "            sum_snr=(\"snr\", \"sum\"),\n",
    "            cnt_snr_gt3=(\"snr_gt3\", \"sum\"),\n",
    "            cnt_snr_gt5=(\"snr_gt5\", \"sum\"),\n",
    "            min_time=(\"mjd\", \"min\"),\n",
    "            max_time=(\"mjd\", \"max\"),\n",
    "        ).reset_index()\n",
    "\n",
    "        parts.append(g)\n",
    "\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "def combine_partials(df_partials: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine partial stats menjadi final stats per (object_id, filter).\n",
    "    \"\"\"\n",
    "    if df_partials.empty:\n",
    "        return df_partials\n",
    "\n",
    "    g = df_partials.groupby([\"object_id\", \"filter\"]).agg(\n",
    "        n_obs=(\"n_obs\", \"sum\"),\n",
    "        sum_flux=(\"sum_flux\", \"sum\"),\n",
    "        sum_flux2=(\"sum_flux2\", \"sum\"),\n",
    "        min_flux=(\"min_flux\", \"min\"),\n",
    "        max_flux=(\"max_flux\", \"max\"),\n",
    "        sum_snr=(\"sum_snr\", \"sum\"),\n",
    "        cnt_snr_gt3=(\"cnt_snr_gt3\", \"sum\"),\n",
    "        cnt_snr_gt5=(\"cnt_snr_gt5\", \"sum\"),\n",
    "        min_time=(\"min_time\", \"min\"),\n",
    "        max_time=(\"max_time\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # derive features\n",
    "    n = g[\"n_obs\"].astype(float).values\n",
    "    mean = g[\"sum_flux\"].values / np.clip(n, 1.0, None)\n",
    "    var = (g[\"sum_flux2\"].values / np.clip(n, 1.0, None)) - np.square(mean)\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var)\n",
    "\n",
    "    g[\"flux_mean\"] = mean\n",
    "    g[\"flux_std\"] = std\n",
    "    g[\"amp\"] = g[\"max_flux\"] - g[\"min_flux\"]\n",
    "    g[\"snr_mean\"] = g[\"sum_snr\"].values / np.clip(n, 1.0, None)\n",
    "    g[\"frac_snr_gt3\"] = g[\"cnt_snr_gt3\"].values / np.clip(n, 1.0, None)\n",
    "    g[\"frac_snr_gt5\"] = g[\"cnt_snr_gt5\"].values / np.clip(n, 1.0, None)\n",
    "    g[\"time_span\"] = g[\"max_time\"] - g[\"min_time\"]\n",
    "\n",
    "    # keep only final columns\n",
    "    keep = [\n",
    "        \"object_id\", \"filter\",\n",
    "        \"n_obs\", \"flux_mean\", \"flux_std\", \"min_flux\", \"max_flux\",\n",
    "        \"amp\", \"snr_mean\", \"frac_snr_gt3\", \"frac_snr_gt5\", \"time_span\"\n",
    "    ]\n",
    "    return g[keep]\n",
    "\n",
    "def pivot_band_features(df_band: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pivot per band → 1 row per object_id (wide).\n",
    "    \"\"\"\n",
    "    if df_band.empty:\n",
    "        return pd.DataFrame(columns=[\"object_id\"])\n",
    "\n",
    "    feats = [c for c in df_band.columns if c not in (\"object_id\", \"filter\")]\n",
    "    wide = df_band.pivot(index=\"object_id\", columns=\"filter\", values=feats)\n",
    "\n",
    "    # flatten columns: (feat, band) -> f\"{band}__{feat}\"\n",
    "    wide.columns = [f\"{band}__{feat}\" for (feat, band) in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "    return wide\n",
    "\n",
    "# ----------------------------\n",
    "# Process all splits (train + test)\n",
    "# ----------------------------\n",
    "all_splits = sorted(set(df_tr[\"split\"].dropna().astype(str)).union(set(df_te[\"split\"].dropna().astype(str))))\n",
    "print(f\"[INFO] Total splits to process: {len(all_splits)} | example: {all_splits[:5]}\")\n",
    "\n",
    "train_wides = []\n",
    "test_wides = []\n",
    "\n",
    "for i, sp in enumerate(all_splits, start=1):\n",
    "    sp_dir = DATA_ROOT / sp\n",
    "    tr_file = sp_dir / \"train_full_lightcurves.csv\"\n",
    "    te_file = sp_dir / \"test_full_lightcurves.csv\"\n",
    "\n",
    "    tr_ids = set(df_tr.loc[df_tr[\"split\"].astype(str).eq(sp), \"object_id\"].astype(str).tolist())\n",
    "    te_ids = set(df_te.loc[df_te[\"split\"].astype(str).eq(sp), \"object_id\"].astype(str).tolist())\n",
    "\n",
    "    print(f\"\\n[{i}/{len(all_splits)}] {sp}\")\n",
    "    print(f\"  train_ids={len(tr_ids):,} | test_ids={len(te_ids):,}\")\n",
    "    print(f\"  files: train_exists={tr_file.exists()} | test_exists={te_file.exists()}\")\n",
    "\n",
    "    # TRAIN split\n",
    "    if len(tr_ids) > 0 and tr_file.exists():\n",
    "        part_tr = partial_agg_from_file(tr_file, tr_ids, ebv_dict, CHUNK_ROWS)\n",
    "        band_tr = combine_partials(part_tr)\n",
    "        wide_tr = pivot_band_features(band_tr)\n",
    "        train_wides.append(wide_tr)\n",
    "\n",
    "    # TEST split\n",
    "    if len(te_ids) > 0 and te_file.exists():\n",
    "        part_te = partial_agg_from_file(te_file, te_ids, ebv_dict, CHUNK_ROWS)\n",
    "        band_te = combine_partials(part_te)\n",
    "        wide_te = pivot_band_features(band_te)\n",
    "        test_wides.append(wide_te)\n",
    "\n",
    "# concat all splits\n",
    "df_feat_tr = pd.concat(train_wides, ignore_index=True) if train_wides else pd.DataFrame(columns=[\"object_id\"])\n",
    "df_feat_te = pd.concat(test_wides, ignore_index=True) if test_wides else pd.DataFrame(columns=[\"object_id\"])\n",
    "\n",
    "# de-dup (safety)\n",
    "df_feat_tr = df_feat_tr.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "df_feat_te = df_feat_te.drop_duplicates(\"object_id\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n[INFO] Lightcurve features built:\")\n",
    "print(\"  train rows:\", len(df_feat_tr), \"| cols:\", df_feat_tr.shape[1])\n",
    "print(\"  test  rows:\", len(df_feat_te), \"| cols:\", df_feat_te.shape[1])\n",
    "\n",
    "# ----------------------------\n",
    "# Log features (per object_id)\n",
    "# ----------------------------\n",
    "def build_log_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = pd.DataFrame({\n",
    "        \"object_id\": df[\"object_id\"].astype(str),\n",
    "        \"Z\": df[\"Z\"].astype(float),\n",
    "        \"EBV\": df[\"EBV\"].astype(float),\n",
    "        \"Z_err\": df[\"Z_err\"].astype(float),\n",
    "        \"Z_isna\": df[\"Z\"].isna().astype(int),\n",
    "        \"EBV_isna\": df[\"EBV\"].isna().astype(int),\n",
    "        \"Zerr_isna\": df[\"Z_err\"].isna().astype(int),\n",
    "    })\n",
    "    return out\n",
    "\n",
    "df_log_tr = build_log_features(df_tr)\n",
    "df_log_te = build_log_features(df_te)\n",
    "\n",
    "df_log_all = pd.concat([df_log_tr.assign(is_train=1), df_log_te.assign(is_train=0)], ignore_index=True)\n",
    "df_log_all.to_csv(ART_DIR / \"features_log.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Merge log + lc features\n",
    "# ----------------------------\n",
    "df_train_merged = df_log_tr.merge(df_feat_tr, on=\"object_id\", how=\"left\")\n",
    "df_test_merged  = df_log_te.merge(df_feat_te, on=\"object_id\", how=\"left\")\n",
    "\n",
    "# save\n",
    "df_feat_tr.to_csv(ART_DIR / \"features_lc_train.csv\", index=False)\n",
    "df_feat_te.to_csv(ART_DIR / \"features_lc_test.csv\", index=False)\n",
    "df_train_merged.to_csv(ART_DIR / \"features_merged_train.csv\", index=False)\n",
    "df_test_merged.to_csv(ART_DIR / \"features_merged_test.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== STAGE 3 DONE ===\")\n",
    "print(\"Saved:\")\n",
    "print(\" -\", ART_DIR / \"features_lc_train.csv\")\n",
    "print(\" -\", ART_DIR / \"features_lc_test.csv\")\n",
    "print(\" -\", ART_DIR / \"features_log.csv\")\n",
    "print(\" -\", ART_DIR / \"features_merged_train.csv\")\n",
    "print(\" -\", ART_DIR / \"features_merged_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae998a3",
   "metadata": {},
   "source": [
    "# Model utama CPU: LightGBM + CV yang benar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a4e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Backend used: sklearn_hgb\n",
      "[INFO] sample_submission prediction column = 'prediction'\n",
      "[INFO] X_train shape: (3043, 66) | X_test shape: (7135, 66)\n",
      "[INFO] Pos rate: 0.048636 | scale_pos_weight=19.5608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 1/5] best_iter=82 | F1@0.50=0.38835\n",
      "[FOLD 2/5] best_iter=81 | F1@0.50=0.35789\n",
      "[FOLD 3/5] best_iter=70 | F1@0.50=0.45833\n",
      "[FOLD 4/5] best_iter=96 | F1@0.50=0.43038\n",
      "[FOLD 5/5] best_iter=76 | F1@0.50=0.35955\n",
      "\n",
      "=== CV SUMMARY (threshold=0.50) ===\n",
      "Mean F1@0.50: 0.39890 | Std: 0.03966\n",
      "Best iters   : min=70 | mean=81.0 | max=96\n",
      "\n",
      "=== OOF THRESHOLD TUNING ===\n",
      "Best threshold = 0.6680\n",
      "OOF F1(best)   = 0.422535\n",
      "\n",
      "Confusion Matrix (OOF):\n",
      "[[2763  132]\n",
      " [  73   75]]\n",
      "\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9743    0.9544    0.9642      2895\n",
      "           1     0.3623    0.5068    0.4225       148\n",
      "\n",
      "    accuracy                         0.9326      3043\n",
      "   macro avg     0.6683    0.7306    0.6934      3043\n",
      "weighted avg     0.9445    0.9326    0.9379      3043\n",
      "\n",
      "\n",
      "Saved artifacts:\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_oof.csv\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_threshold.txt\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_cv_report.txt\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_feature_importance.csv\n",
      "\n",
      "=== DONE ===\n",
      "Backend: sklearn_hgb\n",
      "Final n_estimators (if applicable): 200\n",
      "Saved submission: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\submissions\\sub_lgbm_v01.csv\n",
      "Saved test proba: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_test_proba.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — MODEL UTAMA CPU: CV + Threshold tuning (F1) [REVISI FULL: AUTO-FALLBACK]\n",
    "#\n",
    "# Backend priority:\n",
    "# 1) lightgbm (jika terinstall)\n",
    "# 2) xgboost  (jika terinstall)\n",
    "# 3) sklearn HistGradientBoostingClassifier (fallback, no extra install)\n",
    "#\n",
    "# Output (tetap pakai nama file yang sama):\n",
    "# - artifacts/lgbm_oof.csv\n",
    "# - artifacts/lgbm_threshold.txt\n",
    "# - artifacts/lgbm_cv_report.txt\n",
    "# - artifacts/lgbm_feature_importance.csv\n",
    "# - artifacts/lgbm_test_proba.csv\n",
    "# - submissions/sub_lgbm_v01.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED    = 2025\n",
    "N_FOLDS = 5\n",
    "\n",
    "THR_STEP = 0.001  # threshold tuning grid step (F1)\n",
    "\n",
    "# ----------------------------\n",
    "# Backend selection (no crash)\n",
    "# ----------------------------\n",
    "BACKEND = None\n",
    "lgb = None\n",
    "xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb  # type: ignore\n",
    "    BACKEND = \"lightgbm\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import xgboost as xgb  # type: ignore\n",
    "        BACKEND = \"xgboost\"\n",
    "    except Exception:\n",
    "        BACKEND = \"sklearn_hgb\"\n",
    "\n",
    "print(f\"[INFO] Backend used: {BACKEND}\")\n",
    "\n",
    "# sklearn fallback model\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# Load inputs\n",
    "# ----------------------------\n",
    "train_log_path  = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path   = ART_DIR / \"test_log_clean.csv\"\n",
    "feat_tr_path    = ART_DIR / \"features_merged_train.csv\"\n",
    "feat_te_path    = ART_DIR / \"features_merged_test.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "for p in [train_log_path, test_log_path, feat_tr_path, feat_te_path, sample_sub_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p}. Pastikan Stage 1 & 3 sudah dijalankan.\")\n",
    "\n",
    "df_tr_log  = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te_log  = pd.read_csv(test_log_path, low_memory=False)\n",
    "df_tr_feat = pd.read_csv(feat_tr_path, low_memory=False)\n",
    "df_te_feat = pd.read_csv(feat_te_path, low_memory=False)\n",
    "df_sub     = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Detect submission prediction column name\n",
    "# ----------------------------\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]  # contoh: 'prediction'\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare train/test tables (merge target)\n",
    "# ----------------------------\n",
    "need_log_cols = [\"object_id\", \"target\"]\n",
    "for c in need_log_cols:\n",
    "    if c not in df_tr_log.columns:\n",
    "        raise ValueError(f\"train_log_clean missing '{c}'. Found: {list(df_tr_log.columns)}\")\n",
    "\n",
    "for d in [df_tr_log, df_te_log, df_tr_feat, df_te_feat]:\n",
    "    d[\"object_id\"] = d[\"object_id\"].astype(str)\n",
    "\n",
    "df_tr_log[\"target\"] = pd.to_numeric(df_tr_log[\"target\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "df_train = df_tr_feat.merge(df_tr_log[[\"object_id\", \"target\"]], on=\"object_id\", how=\"left\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    bad = df_train[df_train[\"target\"].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Ada object_id di features_merged_train yang tidak ketemu target. Example: {bad}\")\n",
    "\n",
    "df_test = df_te_feat.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Define features\n",
    "# ----------------------------\n",
    "drop_cols = {\"object_id\", \"target\"}\n",
    "feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "\n",
    "# Force numeric\n",
    "for c in feature_cols:\n",
    "    df_train[c] = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
    "    df_test[c]  = pd.to_numeric(df_test[c], errors=\"coerce\")\n",
    "\n",
    "X_df = df_train[feature_cols]\n",
    "y = df_train[\"target\"].astype(int).values\n",
    "X_test_df = df_test[feature_cols]\n",
    "\n",
    "print(f\"[INFO] X_train shape: {X_df.shape} | X_test shape: {X_test_df.shape}\")\n",
    "pos = max(int((y == 1).sum()), 1)\n",
    "neg = max(int((y == 0).sum()), 1)\n",
    "scale_pos_weight = neg / pos\n",
    "print(f\"[INFO] Pos rate: {(y==1).mean():.6f} | scale_pos_weight={scale_pos_weight:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Imputer (shared)\n",
    "# ----------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Fit imputer on full train (ok for CV karena median-only; kalau mau strict, fit per-fold, tapi lebih lambat)\n",
    "X_all = imputer.fit_transform(X_df)\n",
    "X_test_all = imputer.transform(X_test_df)\n",
    "\n",
    "# Cast to float32 for speed/memory\n",
    "X_all = X_all.astype(np.float32, copy=False)\n",
    "X_test_all = X_test_all.astype(np.float32, copy=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Model factory\n",
    "# ----------------------------\n",
    "def make_model(seed: int):\n",
    "    if BACKEND == \"lightgbm\":\n",
    "        params = dict(\n",
    "            objective=\"binary\",\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=7000,\n",
    "            num_leaves=64,\n",
    "            min_child_samples=150,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "        )\n",
    "        return lgb.LGBMClassifier(**params)\n",
    "\n",
    "    if BACKEND == \"xgboost\":\n",
    "        # xgboost CPU params\n",
    "        params = dict(\n",
    "            objective=\"binary:logistic\",\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=8000,\n",
    "            max_depth=6,\n",
    "            min_child_weight=3.0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            tree_method=\"hist\",\n",
    "        )\n",
    "        return xgb.XGBClassifier(**params)\n",
    "\n",
    "    # sklearn fallback (no extra install)\n",
    "    # class imbalance handled via sample_weight in fit()\n",
    "    return HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_leaf_nodes=64,\n",
    "        min_samples_leaf=60,\n",
    "        l2_regularization=0.0,\n",
    "        max_iter=2500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=50,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "def fit_model(model, X_tr, y_tr, X_va, y_va, seed: int):\n",
    "    if BACKEND == \"lightgbm\":\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration_\", None) or model.n_estimators)\n",
    "        return model, best_it\n",
    "\n",
    "    if BACKEND == \"xgboost\":\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration\", None) or model.n_estimators)\n",
    "        return model, best_it\n",
    "\n",
    "    # sklearn fallback: use sample_weight for imbalance\n",
    "    sw = np.ones_like(y_tr, dtype=np.float32)\n",
    "    sw[y_tr == 1] = float(scale_pos_weight)\n",
    "    model.fit(X_tr, y_tr, sample_weight=sw)\n",
    "    # best iteration approximation\n",
    "    best_it = int(getattr(model, \"n_iter_\", None) or getattr(model, \"max_iter\", 0) or 0)\n",
    "    return model, best_it\n",
    "\n",
    "def predict_proba_pos(model, X):\n",
    "    if BACKEND in (\"lightgbm\", \"xgboost\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    # sklearn hgb has predict_proba\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# CV + OOF\n",
    "# ----------------------------\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_proba = np.zeros(X_all.shape[0], dtype=np.float32)\n",
    "best_iters = []\n",
    "fold_f1_05 = []\n",
    "\n",
    "# feature importance accum (if backend supports)\n",
    "feat_importance_accum = np.zeros(len(feature_cols), dtype=np.float64)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y), start=1):\n",
    "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    model = make_model(SEED + fold)\n",
    "    model, best_it = fit_model(model, X_tr, y_tr, X_va, y_va, seed=SEED + fold)\n",
    "\n",
    "    p_va = predict_proba_pos(model, X_va).astype(np.float32)\n",
    "    oof_proba[va_idx] = p_va\n",
    "\n",
    "    f1_at_05 = f1_score(y_va, (p_va >= 0.5).astype(int))\n",
    "    fold_f1_05.append(float(f1_at_05))\n",
    "    best_iters.append(int(best_it))\n",
    "\n",
    "    # importance (gain) if available\n",
    "    try:\n",
    "        if BACKEND == \"lightgbm\":\n",
    "            fi = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "            feat_importance_accum += fi\n",
    "        elif BACKEND == \"xgboost\":\n",
    "            booster = model.get_booster()\n",
    "            score = booster.get_score(importance_type=\"gain\")\n",
    "            fi = np.array([score.get(f\"f{i}\", 0.0) for i in range(len(feature_cols))], dtype=np.float64)\n",
    "            feat_importance_accum += fi\n",
    "        else:\n",
    "            # sklearn fallback: no native importance\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[FOLD {fold}/{N_FOLDS}] best_iter={best_it} | F1@0.50={f1_at_05:.5f}\")\n",
    "\n",
    "print(\"\\n=== CV SUMMARY (threshold=0.50) ===\")\n",
    "print(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.5f} | Std: {float(np.std(fold_f1_05)):.5f}\")\n",
    "print(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.1f} | max={int(np.max(best_iters))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold tuning (global OOF) for F1\n",
    "# ----------------------------\n",
    "thr_grid = np.arange(0.0, 1.0 + THR_STEP, THR_STEP)\n",
    "best_thr, best_f1 = 0.5, -1.0\n",
    "for thr in thr_grid:\n",
    "    f1 = f1_score(y, (oof_proba >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "print(\"\\n=== OOF THRESHOLD TUNING ===\")\n",
    "print(f\"Best threshold = {best_thr:.4f}\")\n",
    "print(f\"OOF F1(best)   = {best_f1:.6f}\")\n",
    "\n",
    "y_hat = (oof_proba >= best_thr).astype(int)\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(confusion_matrix(y, y_hat))\n",
    "\n",
    "print(\"\\nClassification Report (OOF):\")\n",
    "print(classification_report(y, y_hat, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# Save OOF + threshold + report\n",
    "# ----------------------------\n",
    "df_oof = pd.DataFrame({\n",
    "    \"object_id\": df_train[\"object_id\"].astype(str),\n",
    "    \"oof_proba\": oof_proba,\n",
    "    \"target\": y\n",
    "})\n",
    "df_oof.to_csv(ART_DIR / \"lgbm_oof.csv\", index=False)\n",
    "\n",
    "with open(ART_DIR / \"lgbm_threshold.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{best_thr}\\n\")\n",
    "\n",
    "with open(ART_DIR / \"lgbm_cv_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"BACKEND={BACKEND}\\n\")\n",
    "    f.write(\"=== CV SUMMARY (threshold=0.50) ===\\n\")\n",
    "    f.write(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.6f} | Std: {float(np.std(fold_f1_05)):.6f}\\n\")\n",
    "    f.write(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.2f} | max={int(np.max(best_iters))}\\n\\n\")\n",
    "    f.write(\"=== OOF THRESHOLD TUNING ===\\n\")\n",
    "    f.write(f\"Best threshold = {best_thr:.6f}\\n\")\n",
    "    f.write(f\"OOF F1(best)   = {best_f1:.6f}\\n\")\n",
    "\n",
    "# Feature importance (may be zeros if sklearn fallback)\n",
    "fi_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance_gain_sum\": feat_importance_accum\n",
    "}).sort_values(\"importance_gain_sum\", ascending=False).reset_index(drop=True)\n",
    "fi_df.to_csv(ART_DIR / \"lgbm_feature_importance.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\" -\", ART_DIR / \"lgbm_oof.csv\")\n",
    "print(\" -\", ART_DIR / \"lgbm_threshold.txt\")\n",
    "print(\" -\", ART_DIR / \"lgbm_cv_report.txt\")\n",
    "print(\" -\", ART_DIR / \"lgbm_feature_importance.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train final model on full train\n",
    "# ----------------------------\n",
    "final_model = make_model(SEED + 999)\n",
    "\n",
    "# For final training:\n",
    "# - lightgbm/xgboost: use mean best_iter\n",
    "# - sklearn_hgb: keep its internal early stopping\n",
    "mean_best_iter = int(max(200, round(float(np.mean(best_iters)))))\n",
    "\n",
    "if BACKEND == \"lightgbm\":\n",
    "    final_model.set_params(n_estimators=mean_best_iter)\n",
    "    final_model.fit(X_all, y)\n",
    "elif BACKEND == \"xgboost\":\n",
    "    final_model.set_params(n_estimators=mean_best_iter)\n",
    "    final_model.fit(X_all, y, verbose=False)\n",
    "else:\n",
    "    sw = np.ones_like(y, dtype=np.float32)\n",
    "    sw[y == 1] = float(scale_pos_weight)\n",
    "    final_model.fit(X_all, y, sample_weight=sw)\n",
    "\n",
    "test_proba = predict_proba_pos(final_model, X_test_all).astype(np.float32)\n",
    "test_pred  = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "# Save test probabilities (berguna untuk ensemble proba)\n",
    "pd.DataFrame({\"object_id\": df_test[\"object_id\"].astype(str), \"proba\": test_proba}).to_csv(\n",
    "    ART_DIR / \"lgbm_test_proba.csv\", index=False\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Build submission\n",
    "# ----------------------------\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_test[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int),\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_lgbm_v01.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Backend:\", BACKEND)\n",
    "print(\"Final n_estimators (if applicable):\", mean_best_iter)\n",
    "print(\"Saved submission:\", out_path)\n",
    "print(\"Saved test proba:\", ART_DIR / \"lgbm_test_proba.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3042a969",
   "metadata": {},
   "source": [
    "# Threshold tuning khusus F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510947f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THRESHOLD TUNING (GLOBAL OOF) ===\n",
      "Best threshold (grid 0.001) = 0.668\n",
      "Best OOF F1                 = 0.422535\n",
      "\n",
      "=== FINE SEARCH ===\n",
      "Best threshold (fine) = 0.6680\n",
      "Best OOF F1 (fine)    = 0.422535\n",
      "Precision           : 0.362319\n",
      "Recall              : 0.506757\n",
      "\n",
      "Saved files:\n",
      "- D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_threshold_grid.csv\n",
      "- D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_threshold_best.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — THRESHOLD TUNING KHUSUS F1 (LEBIH HALUS)\n",
    "#\n",
    "# Tujuan:\n",
    "# - Cari threshold terbaik untuk memaksimalkan F1 berdasarkan OOF proba\n",
    "# - Opsional: threshold per-fold (lebih robust), lalu voting / averaging threshold\n",
    "#\n",
    "# Input:\n",
    "# - artifacts/lgbm_oof.csv            (from STAGE 4)\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/lgbm_threshold_grid.csv\n",
    "# - artifacts/lgbm_threshold_best.txt\n",
    "# - artifacts/lgbm_threshold_report.txt\n",
    "# - (opsional) artifacts/lgbm_threshold_per_fold.csv  (jika fold tersedia di oof)\n",
    "#\n",
    "# Catatan:\n",
    "# - STAGE 4 di atas menyimpan oof_proba + target, tapi tidak simpan fold.\n",
    "# - Jadi tuning per-fold hanya bisa jika kamu juga punya kolom fold.\n",
    "#   Kalau belum ada, skrip ini fokus global OOF threshold (yang sudah cukup kuat).\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "\n",
    "OOF_PATH = ART_DIR / \"lgbm_oof.csv\"\n",
    "if not OOF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {OOF_PATH}. Jalankan STAGE 4 dulu.\")\n",
    "\n",
    "df_oof = pd.read_csv(OOF_PATH, low_memory=False)\n",
    "\n",
    "need = [\"oof_proba\", \"target\"]\n",
    "for c in need:\n",
    "    if c not in df_oof.columns:\n",
    "        raise ValueError(f\"OOF missing '{c}'. Found: {list(df_oof.columns)}\")\n",
    "\n",
    "p = pd.to_numeric(df_oof[\"oof_proba\"], errors=\"coerce\").astype(float).values\n",
    "y = pd.to_numeric(df_oof[\"target\"], errors=\"coerce\").astype(int).values\n",
    "\n",
    "# Drop NaN safety\n",
    "mask = np.isfinite(p)\n",
    "if mask.mean() < 0.999:\n",
    "    df_oof = df_oof.loc[mask].reset_index(drop=True)\n",
    "    p = p[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Coarse grid (0.00..1.00 step 0.01)\n",
    "# ----------------------------\n",
    "thr_grid_coarse = np.linspace(0.0, 1.0, 1001)  # step 0.001 (lebih halus dari 0.01)\n",
    "rows = []\n",
    "\n",
    "best_thr = 0.5\n",
    "best_f1 = -1.0\n",
    "\n",
    "for thr in thr_grid_coarse:\n",
    "    pred = (p >= thr).astype(int)\n",
    "    f1 = f1_score(y, pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "    # simpan ringkas (biar file tidak kegedean, simpan tiap 0.005)\n",
    "    # tapi kita tetap cari best pakai full 0.001 grid\n",
    "    if abs((thr * 1000) % 5) < 1e-9:  # every 0.005\n",
    "        prec = precision_score(y, pred, zero_division=0)\n",
    "        rec  = recall_score(y, pred, zero_division=0)\n",
    "        rows.append((thr, f1, prec, rec))\n",
    "\n",
    "df_grid = pd.DataFrame(rows, columns=[\"threshold\", \"f1\", \"precision\", \"recall\"])\n",
    "df_grid.to_csv(ART_DIR / \"lgbm_threshold_grid.csv\", index=False)\n",
    "\n",
    "print(\"=== THRESHOLD TUNING (GLOBAL OOF) ===\")\n",
    "print(f\"Best threshold (grid 0.001) = {best_thr:.3f}\")\n",
    "print(f\"Best OOF F1                 = {best_f1:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Fine local search around best (optional)\n",
    "#    - cari di sekitar best_thr ± 0.02 dengan step 0.0002\n",
    "# ----------------------------\n",
    "lo = max(0.0, best_thr - 0.02)\n",
    "hi = min(1.0, best_thr + 0.02)\n",
    "thr_grid_fine = np.linspace(lo, hi, int(round((hi - lo) / 0.0002)) + 1)\n",
    "\n",
    "best_thr2 = best_thr\n",
    "best_f12 = best_f1\n",
    "\n",
    "for thr in thr_grid_fine:\n",
    "    pred = (p >= thr).astype(int)\n",
    "    f1 = f1_score(y, pred)\n",
    "    if f1 > best_f12:\n",
    "        best_f12 = float(f1)\n",
    "        best_thr2 = float(thr)\n",
    "\n",
    "print(\"\\n=== FINE SEARCH ===\")\n",
    "print(f\"Best threshold (fine) = {best_thr2:.4f}\")\n",
    "print(f\"Best OOF F1 (fine)    = {best_f12:.6f}\")\n",
    "\n",
    "# Save best threshold\n",
    "with open(ART_DIR / \"lgbm_threshold_best.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{best_thr2}\\n\")\n",
    "\n",
    "# Report\n",
    "pred_best = (p >= best_thr2).astype(int)\n",
    "prec_best = precision_score(y, pred_best, zero_division=0)\n",
    "rec_best  = recall_score(y, pred_best, zero_division=0)\n",
    "\n",
    "report = []\n",
    "report.append(\"=== THRESHOLD TUNING REPORT (GLOBAL OOF) ===\")\n",
    "report.append(f\"OOF samples         : {len(y)}\")\n",
    "report.append(f\"Positive rate (y=1) : {float((y==1).mean()):.6f}\")\n",
    "report.append(\"\")\n",
    "report.append(f\"Best threshold      : {best_thr2:.6f}\")\n",
    "report.append(f\"F1                  : {best_f12:.6f}\")\n",
    "report.append(f\"Precision           : {float(prec_best):.6f}\")\n",
    "report.append(f\"Recall              : {float(rec_best):.6f}\")\n",
    "report.append(\"\")\n",
    "report.append(\"Saved files:\")\n",
    "report.append(f\"- {ART_DIR / 'lgbm_threshold_grid.csv'}\")\n",
    "report.append(f\"- {ART_DIR / 'lgbm_threshold_best.txt'}\")\n",
    "\n",
    "(ART_DIR / \"lgbm_threshold_report.txt\").write_text(\"\\n\".join(report), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n\".join(report[-6:]))\n",
    "\n",
    "# ----------------------------\n",
    "# (Optional) If user later wants: apply this threshold to create a new submission\n",
    "# - Submission dibuat di STAGE 4. Kalau kamu ingin regenerate submission\n",
    "#   dengan threshold baru, tinggal rerun STAGE 4 bagian inferensi,\n",
    "#   atau aku buatkan STAGE 5b khusus \"re-export submission from saved proba\".\n",
    "# ----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72ba03",
   "metadata": {},
   "source": [
    "# Tangani “domain shift” redshift (train spec-z vs test photo-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1c37cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Backend used: sklearn_hgb\n",
      "[INFO] sample_submission prediction column = 'prediction'\n",
      "[INFO] zerr_fill (median test Z_err) = 0.029650\n",
      "[INFO] Augmented train rows: 6,086 (AUG_COPIES=1)\n",
      "[INFO] X_train shape: (6086, 76) | X_test shape: (7135, 76)\n",
      "[INFO] Pos rate: 0.048636 | scale_pos_weight=19.5608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOLD 1/5] best_iter=160 | F1@0.50=0.79688\n",
      "[FOLD 2/5] best_iter=179 | F1@0.50=0.77519\n",
      "[FOLD 3/5] best_iter=254 | F1@0.50=0.85965\n",
      "[FOLD 4/5] best_iter=193 | F1@0.50=0.79365\n",
      "[FOLD 5/5] best_iter=169 | F1@0.50=0.71318\n",
      "\n",
      "=== CV SUMMARY (threshold=0.50) ===\n",
      "Mean F1@0.50: 0.78771 | Std: 0.04692\n",
      "Best iters   : min=160 | mean=191.0 | max=254\n",
      "\n",
      "=== OOF THRESHOLD TUNING ===\n",
      "Best threshold = 0.7400\n",
      "OOF F1(best)   = 0.846011\n",
      "\n",
      "Confusion Matrix (OOF):\n",
      "[[5775   15]\n",
      " [  68  228]]\n",
      "\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9884    0.9974    0.9929      5790\n",
      "           1     0.9383    0.7703    0.8460       296\n",
      "\n",
      "    accuracy                         0.9864      6086\n",
      "   macro avg     0.9633    0.8838    0.9194      6086\n",
      "weighted avg     0.9859    0.9864    0.9857      6086\n",
      "\n",
      "\n",
      "Saved artifacts:\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_zaug_oof.csv\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_zaug_threshold.txt\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_zaug_cv_report.txt\n",
      " - D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_zaug_feature_importance.csv\n",
      "\n",
      "=== DONE ===\n",
      "Backend: sklearn_hgb\n",
      "Final n_estimators (if applicable): 300\n",
      "Saved submission: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\submissions\\sub_lgbm_zaug_v02.csv\n",
      "Saved test proba: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\lgbm_zaug_test_proba.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 — DOMAIN SHIFT REDSHIFT (TRAIN spec-z vs TEST photo-z)\n",
    "# [REVISI FULL: AUTO-FALLBACK TANPA LIGHTGBM]\n",
    "#\n",
    "# Backend priority:\n",
    "# 1) lightgbm (jika terinstall)\n",
    "# 2) xgboost  (jika terinstall)\n",
    "# 3) sklearn HistGradientBoostingClassifier (fallback, no extra install)\n",
    "#\n",
    "# Fitur domain-shift:\n",
    "# - Z_filled, Zerr_filled, Z_missing, Zerr_missing\n",
    "# - log1pZ, log1pZerr, inv1pZ, Z_div_Zerr\n",
    "# - Z_aug (train noisy), Z_aug_absdiff\n",
    "#\n",
    "# Prasyarat:\n",
    "# - artifacts/train_log_clean.csv\n",
    "# - artifacts/test_log_clean.csv\n",
    "# - artifacts/features_merged_train.csv  (STAGE 3)\n",
    "# - artifacts/features_merged_test.csv   (STAGE 3)\n",
    "# - sample_submission.csv\n",
    "#\n",
    "# Output (nama tetap):\n",
    "# - artifacts/lgbm_zaug_oof.csv\n",
    "# - artifacts/lgbm_zaug_threshold.txt\n",
    "# - artifacts/lgbm_zaug_cv_report.txt\n",
    "# - artifacts/lgbm_zaug_feature_importance.csv\n",
    "# - artifacts/lgbm_zaug_test_proba.csv\n",
    "# - submissions/sub_lgbm_zaug_v02.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED    = 2025\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Noise augmentation strength\n",
    "AUG_COPIES = 1          # 0=off, 1=duplikasi 1x (jadi 2x data), 2=jadi 3x data, dst (CPU lebih berat)\n",
    "AUG_FLOOR  = 0.01       # floor noise (absolute)\n",
    "AUG_REL    = 0.02       # noise tambahan proporsional (1+z)\n",
    "\n",
    "# Threshold tuning grid\n",
    "THR_STEP = 0.001\n",
    "\n",
    "# ----------------------------\n",
    "# Backend selection (no crash)\n",
    "# ----------------------------\n",
    "BACKEND = None\n",
    "lgb = None\n",
    "xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb  # type: ignore\n",
    "    BACKEND = \"lightgbm\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import xgboost as xgb  # type: ignore\n",
    "        BACKEND = \"xgboost\"\n",
    "    except Exception:\n",
    "        BACKEND = \"sklearn_hgb\"\n",
    "\n",
    "print(f\"[INFO] Backend used: {BACKEND}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load inputs\n",
    "# ----------------------------\n",
    "train_log_path  = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path   = ART_DIR / \"test_log_clean.csv\"\n",
    "feat_tr_path    = ART_DIR / \"features_merged_train.csv\"\n",
    "feat_te_path    = ART_DIR / \"features_merged_test.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "for p in [train_log_path, test_log_path, feat_tr_path, feat_te_path, sample_sub_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p}. Jalankan STAGE 1 & 3 dulu.\")\n",
    "\n",
    "df_tr_log  = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te_log  = pd.read_csv(test_log_path, low_memory=False)\n",
    "df_tr_feat = pd.read_csv(feat_tr_path, low_memory=False)\n",
    "df_te_feat = pd.read_csv(feat_te_path, low_memory=False)\n",
    "df_sub     = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# Detect submission prediction column name\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# Normalize ids\n",
    "for d in [df_tr_log, df_te_log, df_tr_feat, df_te_feat]:\n",
    "    d[\"object_id\"] = d[\"object_id\"].astype(str)\n",
    "\n",
    "# Target\n",
    "if \"target\" not in df_tr_log.columns:\n",
    "    raise ValueError(f\"train_log_clean missing target. Found: {list(df_tr_log.columns)}\")\n",
    "df_tr_log[\"target\"] = pd.to_numeric(df_tr_log[\"target\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "df_train_base = df_tr_feat.merge(df_tr_log[[\"object_id\", \"target\"]], on=\"object_id\", how=\"left\")\n",
    "if df_train_base[\"target\"].isna().any():\n",
    "    bad = df_train_base[df_train_base[\"target\"].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Ada object_id train features yang tidak ketemu target. Example: {bad}\")\n",
    "\n",
    "df_test_base = df_te_feat.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Compute Z_err reference from TEST (photo-z error distribution)\n",
    "# ----------------------------\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "zerr_test = None\n",
    "if \"Z_err\" in df_te_log.columns:\n",
    "    zerr_test = _to_num(df_te_log[\"Z_err\"])\n",
    "elif \"Z_err\" in df_test_base.columns:\n",
    "    zerr_test = _to_num(df_test_base[\"Z_err\"])\n",
    "\n",
    "if zerr_test is None:\n",
    "    zerr_fill = 0.05\n",
    "    print(\"[WARN] Tidak menemukan Z_err di test. Pakai zerr_fill=0.05\")\n",
    "else:\n",
    "    zerr_fill = float(np.nanmedian(zerr_test.astype(float).values))\n",
    "    if not np.isfinite(zerr_fill) or zerr_fill <= 0:\n",
    "        zerr_fill = 0.05\n",
    "    print(f\"[INFO] zerr_fill (median test Z_err) = {zerr_fill:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Feature engineering: robust Z/Z_err + Z augmentation column\n",
    "# ----------------------------\n",
    "def add_redshift_domainshift_features(df: pd.DataFrame, zerr_fill: float, is_train: bool, rng: np.random.Generator):\n",
    "    out = df.copy()\n",
    "\n",
    "    if \"Z\" not in out.columns:\n",
    "        out[\"Z\"] = np.nan\n",
    "    if \"Z_err\" not in out.columns:\n",
    "        out[\"Z_err\"] = np.nan\n",
    "\n",
    "    out[\"Z\"] = pd.to_numeric(out[\"Z\"], errors=\"coerce\")\n",
    "    out[\"Z_err\"] = pd.to_numeric(out[\"Z_err\"], errors=\"coerce\")\n",
    "\n",
    "    z = out[\"Z\"].astype(float).values\n",
    "    zerr = out[\"Z_err\"].astype(float).values\n",
    "\n",
    "    z_isna = ~np.isfinite(z)\n",
    "    zerr_isna = ~np.isfinite(zerr)\n",
    "\n",
    "    z_f = z.copy()\n",
    "    z_f[z_isna] = 0.0\n",
    "\n",
    "    zerr_f = zerr.copy()\n",
    "    zerr_f[zerr_isna] = zerr_fill\n",
    "    zerr_f = np.clip(zerr_f, 0.0, None)\n",
    "\n",
    "    out[\"Z_filled\"] = z_f\n",
    "    out[\"Zerr_filled\"] = zerr_f\n",
    "    out[\"Z_missing\"] = z_isna.astype(int)\n",
    "    out[\"Zerr_missing\"] = zerr_isna.astype(int)\n",
    "\n",
    "    out[\"log1pZ\"] = np.log1p(np.clip(z_f, 0.0, None))\n",
    "    out[\"log1pZerr\"] = np.log1p(np.clip(zerr_f, 0.0, None))\n",
    "    out[\"inv1pZ\"] = 1.0 / (1.0 + np.clip(z_f, 0.0, None))\n",
    "    out[\"Z_div_Zerr\"] = z_f / (zerr_f + 1e-6)\n",
    "\n",
    "    if is_train:\n",
    "        sigma = np.sqrt(np.square(zerr_f) + np.square(AUG_FLOOR) + np.square(AUG_REL * (1.0 + z_f)))\n",
    "        noise = rng.normal(loc=0.0, scale=sigma, size=z_f.shape[0])\n",
    "        z_aug = np.clip(z_f + noise, 0.0, None)\n",
    "        out[\"Z_aug\"] = z_aug\n",
    "        out[\"Z_aug_absdiff\"] = np.abs(z_aug - z_f)\n",
    "    else:\n",
    "        out[\"Z_aug\"] = z_f\n",
    "        out[\"Z_aug_absdiff\"] = 0.0\n",
    "\n",
    "    return out\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "df_train_fe = add_redshift_domainshift_features(df_train_base, zerr_fill=zerr_fill, is_train=True, rng=rng)\n",
    "df_test_fe  = add_redshift_domainshift_features(df_test_base,  zerr_fill=zerr_fill, is_train=False, rng=rng)\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: Data augmentation by duplicating TRAIN with different noise draws\n",
    "# ----------------------------\n",
    "if AUG_COPIES > 0:\n",
    "    aug_list = [df_train_fe]\n",
    "    for k in range(AUG_COPIES):\n",
    "        rng_k = np.random.default_rng(SEED + 1000 * (k + 1))\n",
    "        df_k = add_redshift_domainshift_features(df_train_base, zerr_fill=zerr_fill, is_train=True, rng=rng_k)\n",
    "        aug_list.append(df_k)\n",
    "    df_train_fe = pd.concat(aug_list, ignore_index=True)\n",
    "    print(f\"[INFO] Augmented train rows: {len(df_train_fe):,} (AUG_COPIES={AUG_COPIES})\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build X/y\n",
    "# ----------------------------\n",
    "y = df_train_fe[\"target\"].astype(int).values\n",
    "drop_cols = {\"object_id\", \"target\"}\n",
    "feature_cols = [c for c in df_train_fe.columns if c not in drop_cols]\n",
    "\n",
    "# Numeric coercion\n",
    "for c in feature_cols:\n",
    "    df_train_fe[c] = pd.to_numeric(df_train_fe[c], errors=\"coerce\")\n",
    "    df_test_fe[c]  = pd.to_numeric(df_test_fe[c], errors=\"coerce\")\n",
    "\n",
    "X_df = df_train_fe[feature_cols]\n",
    "X_test_df = df_test_fe[feature_cols]\n",
    "\n",
    "pos = max(int((y == 1).sum()), 1)\n",
    "neg = max(int((y == 0).sum()), 1)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "print(f\"[INFO] X_train shape: {X_df.shape} | X_test shape: {X_test_df.shape}\")\n",
    "print(f\"[INFO] Pos rate: {(y==1).mean():.6f} | scale_pos_weight={scale_pos_weight:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Imputer\n",
    "# ----------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_all = imputer.fit_transform(X_df).astype(np.float32, copy=False)\n",
    "X_test_all = imputer.transform(X_test_df).astype(np.float32, copy=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Model factory\n",
    "# ----------------------------\n",
    "def make_model(seed: int):\n",
    "    if BACKEND == \"lightgbm\":\n",
    "        params = dict(\n",
    "            objective=\"binary\",\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=9000,\n",
    "            num_leaves=96,\n",
    "            min_child_samples=120,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "        )\n",
    "        return lgb.LGBMClassifier(**params)\n",
    "\n",
    "    if BACKEND == \"xgboost\":\n",
    "        params = dict(\n",
    "            objective=\"binary:logistic\",\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=10000,\n",
    "            max_depth=6,\n",
    "            min_child_weight=3.0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            tree_method=\"hist\",\n",
    "        )\n",
    "        return xgb.XGBClassifier(**params)\n",
    "\n",
    "    return HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_leaf_nodes=96,\n",
    "        min_samples_leaf=60,\n",
    "        l2_regularization=0.0,\n",
    "        max_iter=3500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=80,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "def fit_model(model, X_tr, y_tr, X_va, y_va):\n",
    "    if BACKEND == \"lightgbm\":\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)]\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration_\", None) or model.n_estimators)\n",
    "        return model, best_it\n",
    "\n",
    "    if BACKEND == \"xgboost\":\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=250\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration\", None) or model.n_estimators)\n",
    "        return model, best_it\n",
    "\n",
    "    sw = np.ones_like(y_tr, dtype=np.float32)\n",
    "    sw[y_tr == 1] = float(scale_pos_weight)\n",
    "    model.fit(X_tr, y_tr, sample_weight=sw)\n",
    "    best_it = int(getattr(model, \"n_iter_\", None) or getattr(model, \"max_iter\", 0) or 0)\n",
    "    return model, best_it\n",
    "\n",
    "def predict_proba_pos(model, X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# CV + OOF\n",
    "# ----------------------------\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_proba = np.zeros(X_all.shape[0], dtype=np.float32)\n",
    "best_iters = []\n",
    "fold_f1_05 = []\n",
    "feat_importance_accum = np.zeros(len(feature_cols), dtype=np.float64)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y), start=1):\n",
    "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    model = make_model(SEED + 10 * fold)\n",
    "    model, best_it = fit_model(model, X_tr, y_tr, X_va, y_va)\n",
    "\n",
    "    p_va = predict_proba_pos(model, X_va).astype(np.float32)\n",
    "    oof_proba[va_idx] = p_va\n",
    "\n",
    "    f1_at_05 = f1_score(y_va, (p_va >= 0.5).astype(int))\n",
    "    fold_f1_05.append(float(f1_at_05))\n",
    "    best_iters.append(int(best_it))\n",
    "\n",
    "    # importance (gain)\n",
    "    try:\n",
    "        if BACKEND == \"lightgbm\":\n",
    "            fi = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "            feat_importance_accum += fi\n",
    "        elif BACKEND == \"xgboost\":\n",
    "            booster = model.get_booster()\n",
    "            score = booster.get_score(importance_type=\"gain\")\n",
    "            fi = np.array([score.get(f\"f{i}\", 0.0) for i in range(len(feature_cols))], dtype=np.float64)\n",
    "            feat_importance_accum += fi\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[FOLD {fold}/{N_FOLDS}] best_iter={best_it} | F1@0.50={f1_at_05:.5f}\")\n",
    "\n",
    "print(\"\\n=== CV SUMMARY (threshold=0.50) ===\")\n",
    "print(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.5f} | Std: {float(np.std(fold_f1_05)):.5f}\")\n",
    "print(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.1f} | max={int(np.max(best_iters))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold tuning (global OOF) for F1\n",
    "# ----------------------------\n",
    "thr_grid = np.arange(0.0, 1.0 + THR_STEP, THR_STEP)\n",
    "best_thr, best_f1 = 0.5, -1.0\n",
    "for thr in thr_grid:\n",
    "    f1 = f1_score(y, (oof_proba >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = float(f1)\n",
    "        best_thr = float(thr)\n",
    "\n",
    "print(\"\\n=== OOF THRESHOLD TUNING ===\")\n",
    "print(f\"Best threshold = {best_thr:.4f}\")\n",
    "print(f\"OOF F1(best)   = {best_f1:.6f}\")\n",
    "\n",
    "y_hat = (oof_proba >= best_thr).astype(int)\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(confusion_matrix(y, y_hat))\n",
    "print(\"\\nClassification Report (OOF):\")\n",
    "print(classification_report(y, y_hat, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# Save artifacts\n",
    "# ----------------------------\n",
    "oof_path = ART_DIR / \"lgbm_zaug_oof.csv\"\n",
    "pd.DataFrame({\n",
    "    \"object_id\": df_train_fe[\"object_id\"].astype(str),\n",
    "    \"oof_proba\": oof_proba,\n",
    "    \"target\": y\n",
    "}).to_csv(oof_path, index=False)\n",
    "\n",
    "thr_path = ART_DIR / \"lgbm_zaug_threshold.txt\"\n",
    "thr_path.write_text(f\"{best_thr}\\n\", encoding=\"utf-8\")\n",
    "\n",
    "rep_path = ART_DIR / \"lgbm_zaug_cv_report.txt\"\n",
    "with open(rep_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"BACKEND={BACKEND}\\n\")\n",
    "    f.write(\"=== CV SUMMARY (threshold=0.50) ===\\n\")\n",
    "    f.write(f\"Mean F1@0.50: {float(np.mean(fold_f1_05)):.6f} | Std: {float(np.std(fold_f1_05)):.6f}\\n\")\n",
    "    f.write(f\"Best iters   : min={int(np.min(best_iters))} | mean={float(np.mean(best_iters)):.2f} | max={int(np.max(best_iters))}\\n\\n\")\n",
    "    f.write(\"=== OOF THRESHOLD TUNING ===\\n\")\n",
    "    f.write(f\"Best threshold = {best_thr:.6f}\\n\")\n",
    "    f.write(f\"OOF F1(best)   = {best_f1:.6f}\\n\")\n",
    "\n",
    "fi_path = ART_DIR / \"lgbm_zaug_feature_importance.csv\"\n",
    "pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance_gain_sum\": feat_importance_accum\n",
    "}).sort_values(\"importance_gain_sum\", ascending=False).reset_index(drop=True).to_csv(fi_path, index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\" -\", oof_path)\n",
    "print(\" -\", thr_path)\n",
    "print(\" -\", rep_path)\n",
    "print(\" -\", fi_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Train final model on full train\n",
    "# ----------------------------\n",
    "final_model = make_model(SEED + 999)\n",
    "\n",
    "mean_best_iter = int(max(300, round(float(np.mean(best_iters)))))\n",
    "\n",
    "if BACKEND == \"lightgbm\":\n",
    "    final_model.set_params(n_estimators=mean_best_iter)\n",
    "    final_model.fit(X_all, y)\n",
    "elif BACKEND == \"xgboost\":\n",
    "    final_model.set_params(n_estimators=mean_best_iter)\n",
    "    final_model.fit(X_all, y, verbose=False)\n",
    "else:\n",
    "    sw = np.ones_like(y, dtype=np.float32)\n",
    "    sw[y == 1] = float(scale_pos_weight)\n",
    "    final_model.fit(X_all, y, sample_weight=sw)\n",
    "\n",
    "test_proba = predict_proba_pos(final_model, X_test_all).astype(np.float32)\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "# Save test proba (buat ensemble proba)\n",
    "proba_path = ART_DIR / \"lgbm_zaug_test_proba.csv\"\n",
    "pd.DataFrame({\"object_id\": df_test_fe[\"object_id\"].astype(str), \"proba\": test_proba}).to_csv(proba_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Build submission\n",
    "# ----------------------------\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_test_fe[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int),\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_lgbm_zaug_v02.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Backend:\", BACKEND)\n",
    "print(\"Final n_estimators (if applicable):\", mean_best_iter)\n",
    "print(\"Saved submission:\", out_path)\n",
    "print(\"Saved test proba:\", proba_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67815b",
   "metadata": {},
   "source": [
    "# Feature upgrade yang sering menang di lightcurve (masih CPU-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f1cb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"present\")] = present\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cv_absmean\")] = sd / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_pos\")] = peak_pos\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"log1p_peak_pos\")] = np.log1p(peak_pos)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_over_absmean\")] = mx / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_absmean\")] = amp / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_std\")] = amp / (sd + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"present\")] = present\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cv_absmean\")] = sd / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_pos\")] = peak_pos\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"log1p_peak_pos\")] = np.log1p(peak_pos)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_over_absmean\")] = mx / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_absmean\")] = amp / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_std\")] = amp / (sd + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"present\")] = present\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cv_absmean\")] = sd / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_pos\")] = peak_pos\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"log1p_peak_pos\")] = np.log1p(peak_pos)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_over_absmean\")] = mx / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_absmean\")] = amp / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_std\")] = amp / (sd + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"present\")] = present\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cv_absmean\")] = sd / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_pos\")] = peak_pos\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"log1p_peak_pos\")] = np.log1p(peak_pos)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"peak_over_absmean\")] = mx / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_absmean\")] = amp / (np.abs(mu) + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"amp_over_std\")] = amp / (sd + EPS)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_12532\\388144649.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 7 DONE ===\n",
      "Saved: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_upg_train.csv\n",
      "Saved: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_upg_test.csv\n",
      "Train shape: (3043, 167) | Test shape: (7135, 167)\n",
      "Added features: 100\n",
      "Example added: ['amp_concentration', 'amp_ratio_gr', 'amp_ratio_iz', 'amp_ratio_ri', 'amp_ratio_ug', 'amp_ratio_zy', 'g__amp_over_absmean', 'g__amp_over_std', 'g__cadence_proxy', 'g__cv_absmean', 'g__has_negative', 'g__log1p_peak_pos', 'g__peak_over_absmean', 'g__peak_pos', 'g__present', 'g__snr_per_obs', 'i__amp_over_absmean', 'i__amp_over_std', 'i__cadence_proxy', 'i__cv_absmean', 'i__has_negative', 'i__log1p_peak_pos', 'i__peak_over_absmean', 'i__peak_pos', 'i__present', 'i__snr_per_obs', 'log_amp_ratio_gr', 'log_amp_ratio_iz', 'log_amp_ratio_ri', 'log_amp_ratio_ug']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 — FEATURE UPGRADE (CPU-FRIENDLY)\n",
    "# Upgrade fitur lightcurve TANPA baca ulang CSV besar:\n",
    "# - Derive \"shape-ish\" features per band dari fitur STAGE 3 (mean/std/min/max/amp/snr/time_span)\n",
    "# - Tambah cross-band features (ratio/diff antar band)\n",
    "# - Tambah global aggregation across bands (jumlah band hadir, total n_obs, peak band, dll)\n",
    "#\n",
    "# Input (STAGE 3):\n",
    "#   artifacts/features_merged_train.csv\n",
    "#   artifacts/features_merged_test.csv\n",
    "#\n",
    "# Output:\n",
    "#   artifacts/features_upg_train.csv\n",
    "#   artifacts/features_upg_test.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "\n",
    "IN_TR = ART_DIR / \"features_merged_train.csv\"\n",
    "IN_TE = ART_DIR / \"features_merged_test.csv\"\n",
    "OUT_TR = ART_DIR / \"features_upg_train.csv\"\n",
    "OUT_TE = ART_DIR / \"features_upg_test.csv\"\n",
    "\n",
    "BANDS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "BASE_FEATS = [\"n_obs\", \"flux_mean\", \"flux_std\", \"min_flux\", \"max_flux\", \"amp\",\n",
    "              \"snr_mean\", \"frac_snr_gt3\", \"frac_snr_gt5\", \"time_span\"]\n",
    "\n",
    "EPS = 1e-9\n",
    "\n",
    "for p in [IN_TR, IN_TE]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p}. Jalankan STAGE 3 dulu.\")\n",
    "\n",
    "df_tr = pd.read_csv(IN_TR, low_memory=False)\n",
    "df_te = pd.read_csv(IN_TE, low_memory=False)\n",
    "\n",
    "if \"object_id\" not in df_tr.columns or \"object_id\" not in df_te.columns:\n",
    "    raise ValueError(\"features_merged_* wajib punya kolom object_id\")\n",
    "\n",
    "df_tr[\"object_id\"] = df_tr[\"object_id\"].astype(str)\n",
    "df_te[\"object_id\"] = df_te[\"object_id\"].astype(str)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def colname(band: str, feat: str) -> str:\n",
    "    return f\"{band}__{feat}\"\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols: list):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "def safe_series(df: pd.DataFrame, c: str, default=np.nan):\n",
    "    if c in df.columns:\n",
    "        return pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return pd.Series(default, index=df.index, dtype=\"float64\")\n",
    "\n",
    "def add_band_derived(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Pastikan base columns numeric\n",
    "    cols = [colname(b, f) for b in BANDS for f in BASE_FEATS if colname(b, f) in out.columns]\n",
    "    ensure_numeric(out, cols)\n",
    "\n",
    "    for b in BANDS:\n",
    "        n_obs = safe_series(out, colname(b, \"n_obs\"), default=0.0).fillna(0.0)\n",
    "        mu    = safe_series(out, colname(b, \"flux_mean\"))\n",
    "        sd    = safe_series(out, colname(b, \"flux_std\"))\n",
    "        mn    = safe_series(out, colname(b, \"min_flux\"))\n",
    "        mx    = safe_series(out, colname(b, \"max_flux\"))\n",
    "        amp   = safe_series(out, colname(b, \"amp\"))\n",
    "        snr_m = safe_series(out, colname(b, \"snr_mean\"))\n",
    "        tspan = safe_series(out, colname(b, \"time_span\"))\n",
    "\n",
    "        present = (n_obs > 0).astype(int)\n",
    "        out[colname(b, \"present\")] = present\n",
    "\n",
    "        # Coef of variation (robust)\n",
    "        out[colname(b, \"cv_absmean\")] = sd / (np.abs(mu) + EPS)\n",
    "\n",
    "        # Peak positivity (untuk ratio/cross-band yang lebih stabil)\n",
    "        peak_pos = np.clip(mx, 0.0, None)\n",
    "        out[colname(b, \"peak_pos\")] = peak_pos\n",
    "        out[colname(b, \"log1p_peak_pos\")] = np.log1p(peak_pos)\n",
    "\n",
    "        # Peak-to-mean ratio (pakai abs mean biar stabil)\n",
    "        out[colname(b, \"peak_over_absmean\")] = mx / (np.abs(mu) + EPS)\n",
    "\n",
    "        # Amp normalized\n",
    "        out[colname(b, \"amp_over_absmean\")] = amp / (np.abs(mu) + EPS)\n",
    "        out[colname(b, \"amp_over_std\")] = amp / (sd + EPS)\n",
    "\n",
    "        # Negative flux indicator\n",
    "        out[colname(b, \"has_negative\")] = (mn < 0).astype(int)\n",
    "\n",
    "        # Time density proxy (karena gap exact sulit tanpa sort)\n",
    "        out[colname(b, \"cadence_proxy\")] = tspan / (n_obs.clip(lower=1.0))\n",
    "\n",
    "        # SNR density proxy\n",
    "        out[colname(b, \"snr_per_obs\")] = snr_m  # already per obs-ish, keep alias\n",
    "\n",
    "    return out\n",
    "\n",
    "def add_cross_band(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Convenience arrays for cross-band operations\n",
    "    peak_pos_cols = [colname(b, \"peak_pos\") for b in BANDS]\n",
    "    amp_cols      = [colname(b, \"amp\") for b in BANDS]\n",
    "    nobs_cols     = [colname(b, \"n_obs\") for b in BANDS]\n",
    "    pres_cols     = [colname(b, \"present\") for b in BANDS]\n",
    "    tspan_cols    = [colname(b, \"time_span\") for b in BANDS]\n",
    "\n",
    "    ensure_numeric(out, [c for c in peak_pos_cols+amp_cols+nobs_cols+tspan_cols if c in out.columns])\n",
    "\n",
    "    # Global counts\n",
    "    pres_mat = np.column_stack([safe_series(out, c, default=0.0).fillna(0.0).values for c in pres_cols])\n",
    "    out[\"n_bands_present\"] = pres_mat.sum(axis=1).astype(int)\n",
    "\n",
    "    nobs_mat = np.column_stack([safe_series(out, c, default=0.0).fillna(0.0).values for c in nobs_cols])\n",
    "    out[\"total_n_obs\"] = nobs_mat.sum(axis=1).astype(float)\n",
    "    out[\"mean_n_obs_per_band_present\"] = out[\"total_n_obs\"] / (out[\"n_bands_present\"].clip(lower=1).astype(float))\n",
    "\n",
    "    # Peak across bands (positive peak)\n",
    "    peak_mat = np.column_stack([safe_series(out, c, default=np.nan).values for c in peak_pos_cols])\n",
    "    peak_mat = np.where(np.isfinite(peak_mat), peak_mat, -np.inf)\n",
    "\n",
    "    max_peak = np.max(peak_mat, axis=1)\n",
    "    sum_peak = np.sum(np.where(np.isfinite(peak_mat) & (peak_mat > -np.inf), np.clip(peak_mat, 0.0, None), 0.0), axis=1)\n",
    "    argmax_peak = np.argmax(peak_mat, axis=1)\n",
    "\n",
    "    out[\"max_peak_pos\"] = np.where(np.isfinite(max_peak), max_peak, np.nan)\n",
    "    out[\"sum_peak_pos\"] = sum_peak\n",
    "    out[\"peak_concentration\"] = out[\"max_peak_pos\"] / (out[\"sum_peak_pos\"] + EPS)\n",
    "\n",
    "    # Encode peak band as integer 0..5 + one-hot\n",
    "    out[\"peak_band_idx\"] = argmax_peak.astype(int)\n",
    "    for i, b in enumerate(BANDS):\n",
    "        out[f\"peak_band_is_{b}\"] = (out[\"peak_band_idx\"] == i).astype(int)\n",
    "\n",
    "    # Amp across bands\n",
    "    amp_mat = np.column_stack([safe_series(out, c, default=np.nan).values for c in amp_cols])\n",
    "    amp_mat = np.where(np.isfinite(amp_mat), amp_mat, -np.inf)\n",
    "    out[\"max_amp\"] = np.where(np.isfinite(np.max(amp_mat, axis=1)), np.max(amp_mat, axis=1), np.nan)\n",
    "    out[\"mean_amp\"] = np.nanmean(np.where(amp_mat > -np.inf, amp_mat, np.nan), axis=1)\n",
    "    out[\"amp_concentration\"] = out[\"max_amp\"] / (np.nansum(np.where(np.isfinite(amp_mat) & (amp_mat > -np.inf), np.clip(amp_mat, 0.0, None), 0.0), axis=1) + EPS)\n",
    "\n",
    "    # Time span across bands\n",
    "    tspan_mat = np.column_stack([safe_series(out, c, default=np.nan).values for c in tspan_cols])\n",
    "    out[\"max_time_span\"] = np.nanmax(tspan_mat, axis=1)\n",
    "    out[\"mean_time_span\"] = np.nanmean(tspan_mat, axis=1)\n",
    "\n",
    "    # Adjacent band ratios (peak_pos & amp) + log ratios (lebih stabil)\n",
    "    adj_pairs = [(\"u\",\"g\"), (\"g\",\"r\"), (\"r\",\"i\"), (\"i\",\"z\"), (\"z\",\"y\")]\n",
    "    for b1, b2 in adj_pairs:\n",
    "        p1 = safe_series(out, colname(b1, \"peak_pos\"))\n",
    "        p2 = safe_series(out, colname(b2, \"peak_pos\"))\n",
    "        out[f\"peakpos_ratio_{b1}{b2}\"] = (p1 + EPS) / (p2 + EPS)\n",
    "        out[f\"log_peakpos_ratio_{b1}{b2}\"] = np.log1p(np.clip(p1, 0.0, None)) - np.log1p(np.clip(p2, 0.0, None))\n",
    "\n",
    "        a1 = safe_series(out, colname(b1, \"amp\"))\n",
    "        a2 = safe_series(out, colname(b2, \"amp\"))\n",
    "        out[f\"amp_ratio_{b1}{b2}\"] = (a1 + EPS) / (a2 + EPS)\n",
    "        out[f\"log_amp_ratio_{b1}{b2}\"] = np.log1p(np.clip(a1, 0.0, None)) - np.log1p(np.clip(a2, 0.0, None))\n",
    "\n",
    "    # Broad color-like: blue vs red (g+r) vs (i+z+y)\n",
    "    blue = safe_series(out, colname(\"g\",\"peak_pos\")) + safe_series(out, colname(\"r\",\"peak_pos\"))\n",
    "    red  = safe_series(out, colname(\"i\",\"peak_pos\")) + safe_series(out, colname(\"z\",\"peak_pos\")) + safe_series(out, colname(\"y\",\"peak_pos\"))\n",
    "    out[\"peakpos_blue_over_red\"] = (blue + EPS) / (red + EPS)\n",
    "    out[\"log_peakpos_blue_over_red\"] = np.log1p(np.clip(blue, 0.0, None)) - np.log1p(np.clip(red, 0.0, None))\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Build upgraded features\n",
    "# ----------------------------\n",
    "df_tr_upg = add_cross_band(add_band_derived(df_tr))\n",
    "df_te_upg = add_cross_band(add_band_derived(df_te))\n",
    "\n",
    "# Final sanity: keep column order stable-ish (object_id first)\n",
    "def reorder(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = list(df.columns)\n",
    "    if \"object_id\" in cols:\n",
    "        cols = [\"object_id\"] + [c for c in cols if c != \"object_id\"]\n",
    "        return df[cols]\n",
    "    return df\n",
    "\n",
    "df_tr_upg = reorder(df_tr_upg)\n",
    "df_te_upg = reorder(df_te_upg)\n",
    "\n",
    "df_tr_upg.to_csv(OUT_TR, index=False)\n",
    "df_te_upg.to_csv(OUT_TE, index=False)\n",
    "\n",
    "print(\"=== STAGE 7 DONE ===\")\n",
    "print(\"Saved:\", OUT_TR)\n",
    "print(\"Saved:\", OUT_TE)\n",
    "print(\"Train shape:\", df_tr_upg.shape, \"| Test shape:\", df_te_upg.shape)\n",
    "\n",
    "# Quick peek of new columns count\n",
    "new_cols = set(df_tr_upg.columns) - set(df_tr.columns)\n",
    "print(\"Added features:\", len(new_cols))\n",
    "print(\"Example added:\", sorted(list(new_cols))[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40617a3",
   "metadata": {},
   "source": [
    "# Ensemble ringan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4f3ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sample_submission prediction column = 'prediction'\n",
      "\n",
      "=== OOF ENSEMBLE RESULT ===\n",
      "Models used      : 2\n",
      "Best threshold   : 0.6310\n",
      "OOF F1           : 0.687943\n",
      "OOF Precision    : 0.723881\n",
      "OOF Recall       : 0.655405\n",
      "\n",
      "=== DONE ===\n",
      "Saved submission: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\submissions\\sub_ensemble_v01.csv\n",
      "Used threshold   : 0.631\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 8 — ENSEMBLE RINGAN (CPU-FRIENDLY)\n",
    "# Tujuan:\n",
    "# - Gabungkan prediksi dari beberapa model/varian (mis: LGBM v01, LGBM z-aug v02, LGBM fitur upgrade)\n",
    "# - Ensemble dengan average probability (lebih stabil)\n",
    "# - Threshold tuning F1 dari OOF ensemble (jika OOF tersedia untuk semua)\n",
    "# - Export submission ensemble (mengikuti kolom sample_submission: 'prediction' atau lainnya)\n",
    "#\n",
    "# Cara pakai (paling aman):\n",
    "# 1) Pastikan kamu sudah punya minimal 2 file OOF:\n",
    "#    - artifacts/lgbm_oof.csv\n",
    "#    - artifacts/lgbm_zaug_oof.csv  (atau oof lain)\n",
    "#    dan 2 file proba test (opsional):\n",
    "#    - artifacts/lgbm_test_proba.csv\n",
    "#    - artifacts/lgbm_zaug_test_proba.csv\n",
    "#\n",
    "# Kalau kamu belum simpan test_proba, skrip ini tetap bisa ensemble submission\n",
    "# dari file submission masing-masing (majority vote / average label),\n",
    "# tapi yang optimal adalah average PROBABILITY.\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/ens_oof.csv\n",
    "# - artifacts/ens_threshold.txt\n",
    "# - artifacts/ens_report.txt\n",
    "# - submissions/sub_ensemble_v01.csv\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAMPLE_SUB_PATH = DATA_ROOT / \"sample_submission.csv\"\n",
    "if not SAMPLE_SUB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {SAMPLE_SUB_PATH}\")\n",
    "df_sub = pd.read_csv(SAMPLE_SUB_PATH, low_memory=False)\n",
    "\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# Pilih sumber ensemble (edit daftar ini sesuai file kamu)\n",
    "# ----------------------------\n",
    "# OOF sources (harus ada kolom: object_id, oof_proba, target)\n",
    "OOF_FILES = [\n",
    "    ART_DIR / \"lgbm_oof.csv\",\n",
    "    ART_DIR / \"lgbm_zaug_oof.csv\",\n",
    "    # ART_DIR / \"lgbm_upg_oof.csv\",   # kalau nanti kamu buat\n",
    "]\n",
    "\n",
    "# TEST proba sources (recommended) (harus ada: object_id, proba)\n",
    "# Kalau belum ada, isi [] dan pakai submission-based ensemble di bawah.\n",
    "TEST_PROBA_FILES = [\n",
    "    # ART_DIR / \"lgbm_test_proba.csv\",\n",
    "    # ART_DIR / \"lgbm_zaug_test_proba.csv\",\n",
    "]\n",
    "\n",
    "# Submission sources (fallback) (harus mengikuti sample_submission, kolom prediksi 0/1)\n",
    "SUB_FILES_FALLBACK = [\n",
    "    SUB_DIR / \"sub_lgbm_v01.csv\",\n",
    "    SUB_DIR / \"sub_lgbm_zaug_v02.csv\",\n",
    "    # SUB_DIR / \"sub_lgbm_upg_v03.csv\",\n",
    "]\n",
    "\n",
    "# Ensemble weights (opsional)\n",
    "# - Jika None, semua model bobot sama.\n",
    "# - Jika list, panjang harus sama dengan jumlah model yang dipakai.\n",
    "ENSEMBLE_WEIGHTS = None\n",
    "\n",
    "# Threshold tuning grid\n",
    "THR_STEP = 0.001\n",
    "\n",
    "# ============================================================\n",
    "# Helper loaders\n",
    "# ============================================================\n",
    "def load_oof(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    need = {\"object_id\", \"oof_proba\", \"target\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"OOF file {path} missing {need - set(df.columns)} | found={list(df.columns)}\")\n",
    "    df = df[[\"object_id\", \"oof_proba\", \"target\"]].copy()\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(str)\n",
    "    df[\"oof_proba\"] = pd.to_numeric(df[\"oof_proba\"], errors=\"coerce\")\n",
    "    df[\"target\"] = pd.to_numeric(df[\"target\"], errors=\"coerce\").astype(int)\n",
    "    df = df.dropna(subset=[\"oof_proba\"])\n",
    "    return df\n",
    "\n",
    "def load_test_proba(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    # accept proba column name variations\n",
    "    if \"object_id\" not in df.columns:\n",
    "        raise ValueError(f\"Test proba file {path} missing object_id\")\n",
    "    proba_col = None\n",
    "    for c in [\"proba\", \"pred_proba\", \"prediction_proba\", \"test_proba\"]:\n",
    "        if c in df.columns:\n",
    "            proba_col = c\n",
    "            break\n",
    "    if proba_col is None:\n",
    "        # try: any column besides object_id\n",
    "        cand = [c for c in df.columns if c != \"object_id\"]\n",
    "        if len(cand) == 1:\n",
    "            proba_col = cand[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot detect proba column in {path}. Found={list(df.columns)}\")\n",
    "    df = df[[\"object_id\", proba_col]].copy()\n",
    "    df = df.rename(columns={proba_col: \"proba\"})\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(str)\n",
    "    df[\"proba\"] = pd.to_numeric(df[\"proba\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"proba\"])\n",
    "    return df\n",
    "\n",
    "def load_submission(path: Path, pred_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    if \"object_id\" not in df.columns or pred_col not in df.columns:\n",
    "        raise ValueError(f\"Submission {path} must contain ['object_id','{pred_col}']. Found={list(df.columns)}\")\n",
    "    df = df[[\"object_id\", pred_col]].copy()\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(str)\n",
    "    df[pred_col] = pd.to_numeric(df[pred_col], errors=\"coerce\").astype(int)\n",
    "    return df\n",
    "\n",
    "def weighted_mean(mat: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    return (mat * w[None, :]).sum(axis=1) / (w.sum() + 1e-12)\n",
    "\n",
    "# ============================================================\n",
    "# 1) OOF Ensemble (threshold tuning terbaik)\n",
    "# ============================================================\n",
    "oofs = []\n",
    "for p in OOF_FILES:\n",
    "    if not p.exists():\n",
    "        print(f\"[WARN] Missing OOF: {p} (skip)\")\n",
    "        continue\n",
    "    oofs.append(load_oof(p))\n",
    "\n",
    "if len(oofs) < 2:\n",
    "    print(\"[WARN] OOF ensemble butuh >=2 OOF files. Akan lanjut ke submission-based ensemble saja.\")\n",
    "    HAVE_OOF = False\n",
    "else:\n",
    "    HAVE_OOF = True\n",
    "\n",
    "best_thr = 0.5\n",
    "\n",
    "if HAVE_OOF:\n",
    "    # Inner join by object_id so rows align\n",
    "    base = oofs[0][[\"object_id\", \"target\"]].copy()\n",
    "    for i, df in enumerate(oofs):\n",
    "        base = base.merge(df[[\"object_id\", \"oof_proba\"]].rename(columns={\"oof_proba\": f\"p{i}\"}),\n",
    "                          on=\"object_id\", how=\"inner\")\n",
    "\n",
    "    y = base[\"target\"].astype(int).values\n",
    "    p_cols = [c for c in base.columns if c.startswith(\"p\")]\n",
    "    P = base[p_cols].to_numpy(dtype=float)\n",
    "\n",
    "    m = P.shape[1]\n",
    "    if ENSEMBLE_WEIGHTS is None:\n",
    "        w = np.ones(m, dtype=float)\n",
    "    else:\n",
    "        w = np.asarray(ENSEMBLE_WEIGHTS, dtype=float)\n",
    "        if w.shape[0] != m:\n",
    "            raise ValueError(f\"ENSEMBLE_WEIGHTS length {w.shape[0]} != #models {m}\")\n",
    "\n",
    "    ens_p = weighted_mean(P, w)\n",
    "\n",
    "    # threshold tuning (fine)\n",
    "    thr_grid = np.arange(0.0, 1.0 + THR_STEP, THR_STEP)\n",
    "    best_f1 = -1.0\n",
    "    for thr in thr_grid:\n",
    "        f1 = f1_score(y, (ens_p >= thr).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = float(f1)\n",
    "            best_thr = float(thr)\n",
    "\n",
    "    pred = (ens_p >= best_thr).astype(int)\n",
    "    prec = precision_score(y, pred, zero_division=0)\n",
    "    rec = recall_score(y, pred, zero_division=0)\n",
    "\n",
    "    print(\"\\n=== OOF ENSEMBLE RESULT ===\")\n",
    "    print(f\"Models used      : {m}\")\n",
    "    print(f\"Best threshold   : {best_thr:.4f}\")\n",
    "    print(f\"OOF F1           : {best_f1:.6f}\")\n",
    "    print(f\"OOF Precision    : {prec:.6f}\")\n",
    "    print(f\"OOF Recall       : {rec:.6f}\")\n",
    "\n",
    "    # save OOF ensemble artifact\n",
    "    df_ens_oof = pd.DataFrame({\n",
    "        \"object_id\": base[\"object_id\"].astype(str),\n",
    "        \"ens_oof_proba\": ens_p.astype(np.float32),\n",
    "        \"target\": y\n",
    "    })\n",
    "    df_ens_oof.to_csv(ART_DIR / \"ens_oof.csv\", index=False)\n",
    "\n",
    "    (ART_DIR / \"ens_threshold.txt\").write_text(f\"{best_thr}\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    report = []\n",
    "    report.append(\"=== ENSEMBLE REPORT ===\")\n",
    "    report.append(f\"Models used: {m}\")\n",
    "    report.append(f\"OOF rows (inner-join): {len(base):,}\")\n",
    "    report.append(f\"Best threshold: {best_thr:.6f}\")\n",
    "    report.append(f\"OOF F1: {best_f1:.6f}\")\n",
    "    report.append(f\"OOF Precision: {prec:.6f}\")\n",
    "    report.append(f\"OOF Recall: {rec:.6f}\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"OOF files:\")\n",
    "    for p in OOF_FILES:\n",
    "        report.append(f\"- {p}\")\n",
    "    (ART_DIR / \"ens_report.txt\").write_text(\"\\n\".join(report), encoding=\"utf-8\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) TEST Ensemble: preferred from test probability files\n",
    "# ============================================================\n",
    "have_test_proba = all(p.exists() for p in TEST_PROBA_FILES) and len(TEST_PROBA_FILES) >= 2\n",
    "\n",
    "if have_test_proba:\n",
    "    test_list = [load_test_proba(p) for p in TEST_PROBA_FILES]\n",
    "\n",
    "    base = test_list[0][[\"object_id\"]].copy()\n",
    "    for i, df in enumerate(test_list):\n",
    "        base = base.merge(df.rename(columns={\"proba\": f\"p{i}\"}), on=\"object_id\", how=\"inner\")\n",
    "\n",
    "    p_cols = [c for c in base.columns if c.startswith(\"p\")]\n",
    "    P = base[p_cols].to_numpy(dtype=float)\n",
    "    m = P.shape[1]\n",
    "\n",
    "    if ENSEMBLE_WEIGHTS is None:\n",
    "        w = np.ones(m, dtype=float)\n",
    "    else:\n",
    "        w = np.asarray(ENSEMBLE_WEIGHTS, dtype=float)\n",
    "        if w.shape[0] != m:\n",
    "            raise ValueError(f\"ENSEMBLE_WEIGHTS length {w.shape[0]} != #models {m}\")\n",
    "\n",
    "    ens_test_p = weighted_mean(P, w)\n",
    "    ens_test_pred = (ens_test_p >= best_thr).astype(int)\n",
    "\n",
    "    pred_df = pd.DataFrame({\"object_id\": base[\"object_id\"].astype(str), SUB_PRED_COL: ens_test_pred})\n",
    "else:\n",
    "    # ============================================================\n",
    "    # 3) FALLBACK: Submission-based ensemble (vote / average label)\n",
    "    # ============================================================\n",
    "    subs = []\n",
    "    for p in SUB_FILES_FALLBACK:\n",
    "        if not p.exists():\n",
    "            print(f\"[WARN] Missing submission: {p} (skip)\")\n",
    "            continue\n",
    "        subs.append(load_submission(p, SUB_PRED_COL))\n",
    "\n",
    "    if len(subs) < 2:\n",
    "        raise RuntimeError(\"Butuh >=2 model untuk ensemble. Tambahkan OOF/TEST proba/submission lain.\")\n",
    "\n",
    "    base = subs[0][[\"object_id\"]].copy()\n",
    "    for i, df in enumerate(subs):\n",
    "        base = base.merge(df.rename(columns={SUB_PRED_COL: f\"y{i}\"}), on=\"object_id\", how=\"inner\")\n",
    "\n",
    "    y_cols = [c for c in base.columns if c.startswith(\"y\")]\n",
    "    Y = base[y_cols].to_numpy(dtype=float)\n",
    "\n",
    "    # average label -> then apply 0.5 vote threshold\n",
    "    avg_label = Y.mean(axis=1)\n",
    "    ens_test_pred = (avg_label >= 0.5).astype(int)\n",
    "\n",
    "    pred_df = pd.DataFrame({\"object_id\": base[\"object_id\"].astype(str), SUB_PRED_COL: ens_test_pred})\n",
    "\n",
    "# ============================================================\n",
    "# 4) Build final submission\n",
    "# ============================================================\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_ensemble_v01.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Saved submission:\", out_path)\n",
    "print(\"Used threshold   :\", best_thr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8796829",
   "metadata": {},
   "source": [
    "# Train full & buat submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8162338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Backend used: sklearn_hgb\n",
      "[INFO] Feature train: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_upg_train.csv\n",
      "[INFO] Feature test : D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\features_upg_test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sample_submission prediction column = 'prediction'\n",
      "[INFO] Using threshold from: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\ens_threshold.txt\n",
      "[INFO] zerr_fill (from test median) = 0.029650\n",
      "[INFO] X_train: (3043, 176) | X_test: (7135, 176)\n",
      "[INFO] pos_rate=0.048636 | scale_pos_weight=19.5608\n",
      "[INFO] threshold=0.631000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Z_err']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] best_iteration (holdout) = 134\n",
      "[INFO] holdout F1@THR = 0.611111\n",
      "[INFO] Saved test proba: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\artifacts\\final_test_proba.csv\n",
      "\n",
      "=== DONE ===\n",
      "Backend: sklearn_hgb\n",
      "Prediction column: prediction\n",
      "Saved submission: D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\\submissions\\sub_final_cpu.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL STAGE — TRAIN FULL (CPU) + EXPORT SUBMISSION  [REVISI FULL: AUTO-FALLBACK]\n",
    "#\n",
    "# Default behavior:\n",
    "# - Pakai fitur paling baru jika ada: features_upg_* (STAGE 7), kalau tidak ada pakai features_merged_* (STAGE 3)\n",
    "# - Aktifkan fitur domain-shift redshift (robust) + Z noise augmentation ringan\n",
    "# - Ambil threshold terbaik dari artifacts:\n",
    "#     1) ens_threshold.txt (jika ada)\n",
    "#     2) lgbm_threshold_best.txt\n",
    "#     3) lgbm_threshold.txt / lgbm_zaug_threshold.txt\n",
    "#     4) fallback 0.50\n",
    "# - Export submission mengikuti kolom sample_submission (mis. 'prediction')\n",
    "# - Simpan juga test probabilities untuk ensemble berikutnya\n",
    "#\n",
    "# Backend priority (tanpa crash kalau lgbm belum ada):\n",
    "# 1) lightgbm\n",
    "# 2) xgboost\n",
    "# 3) sklearn HistGradientBoostingClassifier (no extra install)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(r\"D:\\MALLORN Astronomical Classification Challenge\\mallorn-astronomical-classification-challenge\")\n",
    "ART_DIR   = DATA_ROOT / \"artifacts\"\n",
    "SUB_DIR   = DATA_ROOT / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 2025\n",
    "\n",
    "# Domain-shift redshift settings (ringan)\n",
    "USE_REDSHIFT_ROBUST_FE = True\n",
    "AUG_FLOOR = 0.01\n",
    "AUG_REL   = 0.02\n",
    "\n",
    "# Threshold tuning fallback if none exists\n",
    "DEFAULT_THR = 0.50\n",
    "\n",
    "# ----------------------------\n",
    "# Backend selection (no crash)\n",
    "# ----------------------------\n",
    "BACKEND = None\n",
    "lgb = None\n",
    "xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb  # type: ignore\n",
    "    BACKEND = \"lightgbm\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import xgboost as xgb  # type: ignore\n",
    "        BACKEND = \"xgboost\"\n",
    "    except Exception:\n",
    "        BACKEND = \"sklearn_hgb\"\n",
    "\n",
    "print(f\"[INFO] Backend used: {BACKEND}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "train_log_path  = ART_DIR / \"train_log_clean.csv\"\n",
    "test_log_path   = ART_DIR / \"test_log_clean.csv\"\n",
    "sample_sub_path = DATA_ROOT / \"sample_submission.csv\"\n",
    "\n",
    "feat_tr_candidates = [ART_DIR / \"features_upg_train.csv\", ART_DIR / \"features_merged_train.csv\"]\n",
    "feat_te_candidates = [ART_DIR / \"features_upg_test.csv\",  ART_DIR / \"features_merged_test.csv\"]\n",
    "\n",
    "def pick_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "feat_tr_path = pick_existing(feat_tr_candidates)\n",
    "feat_te_path = pick_existing(feat_te_candidates)\n",
    "\n",
    "for p in [train_log_path, test_log_path, sample_sub_path, feat_tr_path, feat_te_path]:\n",
    "    if p is None or not Path(p).exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "print(\"[INFO] Feature train:\", feat_tr_path)\n",
    "print(\"[INFO] Feature test :\", feat_te_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Load data\n",
    "# ----------------------------\n",
    "df_tr_log  = pd.read_csv(train_log_path, low_memory=False)\n",
    "df_te_log  = pd.read_csv(test_log_path, low_memory=False)\n",
    "df_tr_feat = pd.read_csv(feat_tr_path, low_memory=False)\n",
    "df_te_feat = pd.read_csv(feat_te_path, low_memory=False)\n",
    "df_sub     = pd.read_csv(sample_sub_path, low_memory=False)\n",
    "\n",
    "# Detect submission prediction column\n",
    "if \"object_id\" not in df_sub.columns:\n",
    "    raise ValueError(f\"sample_submission tidak punya 'object_id'. Found: {list(df_sub.columns)}\")\n",
    "pred_cols = [c for c in df_sub.columns if c != \"object_id\"]\n",
    "if len(pred_cols) != 1:\n",
    "    raise ValueError(f\"sample_submission harus punya 1 kolom prediksi selain object_id. Found: {list(df_sub.columns)}\")\n",
    "SUB_PRED_COL = pred_cols[0]\n",
    "print(f\"[INFO] sample_submission prediction column = '{SUB_PRED_COL}'\")\n",
    "\n",
    "# Normalize ids\n",
    "for d in [df_tr_log, df_te_log, df_tr_feat, df_te_feat, df_sub]:\n",
    "    d[\"object_id\"] = d[\"object_id\"].astype(str)\n",
    "\n",
    "# Target\n",
    "if \"target\" not in df_tr_log.columns:\n",
    "    raise ValueError(f\"train_log_clean missing 'target'. Found: {list(df_tr_log.columns)}\")\n",
    "df_tr_log[\"target\"] = pd.to_numeric(df_tr_log[\"target\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "df_train = df_tr_feat.merge(df_tr_log[[\"object_id\", \"target\"]], on=\"object_id\", how=\"left\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    bad = df_train[df_train[\"target\"].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Ada object_id train features yang tidak ketemu target. Example: {bad}\")\n",
    "\n",
    "df_test = df_te_feat.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold loader\n",
    "# ----------------------------\n",
    "def load_threshold():\n",
    "    cand = [\n",
    "        ART_DIR / \"ens_threshold.txt\",\n",
    "        ART_DIR / \"lgbm_threshold_best.txt\",\n",
    "        ART_DIR / \"lgbm_threshold.txt\",\n",
    "        ART_DIR / \"lgbm_zaug_threshold.txt\",\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists():\n",
    "            try:\n",
    "                v = float(p.read_text(encoding=\"utf-8\").strip().splitlines()[0])\n",
    "                if 0.0 <= v <= 1.0:\n",
    "                    print(\"[INFO] Using threshold from:\", p)\n",
    "                    return v\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(f\"[WARN] No threshold file found. Using {DEFAULT_THR:.2f}\")\n",
    "    return float(DEFAULT_THR)\n",
    "\n",
    "THR = load_threshold()\n",
    "\n",
    "# ----------------------------\n",
    "# Domain-shift redshift robust features\n",
    "# ----------------------------\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def get_zerr_fill_from_test(df_te_log, df_test):\n",
    "    zerr = None\n",
    "    if \"Z_err\" in df_te_log.columns:\n",
    "        zerr = _to_num(df_te_log[\"Z_err\"])\n",
    "    elif \"Z_err\" in df_test.columns:\n",
    "        zerr = _to_num(df_test[\"Z_err\"])\n",
    "    if zerr is None:\n",
    "        return 0.05\n",
    "    v = float(np.nanmedian(zerr.values))\n",
    "    if not np.isfinite(v) or v <= 0:\n",
    "        v = 0.05\n",
    "    return v\n",
    "\n",
    "def add_redshift_domainshift_features(df, zerr_fill, is_train, seed):\n",
    "    out = df.copy()\n",
    "\n",
    "    if \"Z\" not in out.columns:\n",
    "        out[\"Z\"] = np.nan\n",
    "    if \"Z_err\" not in out.columns:\n",
    "        out[\"Z_err\"] = np.nan\n",
    "\n",
    "    z = _to_num(out[\"Z\"]).astype(float).values\n",
    "    zerr = _to_num(out[\"Z_err\"]).astype(float).values\n",
    "\n",
    "    z_isna = ~np.isfinite(z)\n",
    "    zerr_isna = ~np.isfinite(zerr)\n",
    "\n",
    "    z_f = z.copy()\n",
    "    z_f[z_isna] = 0.0\n",
    "\n",
    "    zerr_f = zerr.copy()\n",
    "    zerr_f[zerr_isna] = zerr_fill\n",
    "    zerr_f = np.clip(zerr_f, 0.0, None)\n",
    "\n",
    "    out[\"Z_filled\"] = z_f\n",
    "    out[\"Zerr_filled\"] = zerr_f\n",
    "    out[\"Z_missing\"] = z_isna.astype(int)\n",
    "    out[\"Zerr_missing\"] = zerr_isna.astype(int)\n",
    "\n",
    "    out[\"log1pZ\"] = np.log1p(np.clip(z_f, 0.0, None))\n",
    "    out[\"log1pZerr\"] = np.log1p(np.clip(zerr_f, 0.0, None))\n",
    "    out[\"inv1pZ\"] = 1.0 / (1.0 + np.clip(z_f, 0.0, None))\n",
    "    out[\"Z_div_Zerr\"] = z_f / (zerr_f + 1e-6)\n",
    "\n",
    "    if is_train:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        sigma = np.sqrt(np.square(zerr_f) + np.square(AUG_FLOOR) + np.square(AUG_REL * (1.0 + z_f)))\n",
    "        noise = rng.normal(0.0, sigma, size=z_f.shape[0])\n",
    "        z_aug = np.clip(z_f + noise, 0.0, None)\n",
    "        out[\"Z_aug\"] = z_aug\n",
    "        out[\"Z_aug_absdiff\"] = np.abs(z_aug - z_f)\n",
    "    else:\n",
    "        out[\"Z_aug\"] = z_f\n",
    "        out[\"Z_aug_absdiff\"] = 0.0\n",
    "\n",
    "    return out\n",
    "\n",
    "if USE_REDSHIFT_ROBUST_FE:\n",
    "    zerr_fill = get_zerr_fill_from_test(df_te_log, df_test)\n",
    "    print(f\"[INFO] zerr_fill (from test median) = {zerr_fill:.6f}\")\n",
    "    df_train = add_redshift_domainshift_features(df_train, zerr_fill, is_train=True, seed=SEED)\n",
    "    df_test  = add_redshift_domainshift_features(df_test,  zerr_fill, is_train=False, seed=SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# Build X/y\n",
    "# ----------------------------\n",
    "y = df_train[\"target\"].astype(int).values\n",
    "drop_cols = {\"object_id\", \"target\"}\n",
    "feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "\n",
    "for c in feature_cols:\n",
    "    df_train[c] = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
    "    df_test[c]  = pd.to_numeric(df_test[c], errors=\"coerce\")\n",
    "\n",
    "X_df = df_train[feature_cols]\n",
    "X_test_df = df_test[feature_cols]\n",
    "\n",
    "pos = max(int((y == 1).sum()), 1)\n",
    "neg = max(int((y == 0).sum()), 1)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "print(f\"[INFO] X_train: {X_df.shape} | X_test: {X_test_df.shape}\")\n",
    "print(f\"[INFO] pos_rate={float((y==1).mean()):.6f} | scale_pos_weight={scale_pos_weight:.4f}\")\n",
    "print(f\"[INFO] threshold={THR:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Imputer (median) + float32\n",
    "# ----------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_all = imputer.fit_transform(X_df).astype(np.float32, copy=False)\n",
    "X_test_all = imputer.transform(X_test_df).astype(np.float32, copy=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Model factory + helpers\n",
    "# ----------------------------\n",
    "def make_model(seed: int):\n",
    "    if BACKEND == \"lightgbm\":\n",
    "        params = dict(\n",
    "            objective=\"binary\",\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=12000,\n",
    "            num_leaves=96,\n",
    "            min_child_samples=120,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "        )\n",
    "        return lgb.LGBMClassifier(**params)\n",
    "\n",
    "    if BACKEND == \"xgboost\":\n",
    "        params = dict(\n",
    "            objective=\"binary:logistic\",\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=15000,\n",
    "            max_depth=6,\n",
    "            min_child_weight=3.0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            tree_method=\"hist\",\n",
    "        )\n",
    "        return xgb.XGBClassifier(**params)\n",
    "\n",
    "    return HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_leaf_nodes=96,\n",
    "        min_samples_leaf=60,\n",
    "        l2_regularization=0.0,\n",
    "        max_iter=4000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.12,\n",
    "        n_iter_no_change=100,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "def predict_proba_pos(model, X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# Find best_iter via small holdout (early stop)\n",
    "# - lightgbm/xgboost: early stopping -> best_iteration\n",
    "# - sklearn_hgb: internal early stopping -> n_iter_\n",
    "# ----------------------------\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X_all, y, test_size=0.12, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "tmp_model = make_model(SEED + 7)\n",
    "\n",
    "if BACKEND == \"lightgbm\":\n",
    "    tmp_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)]\n",
    "    )\n",
    "    best_iter = int(getattr(tmp_model, \"best_iteration_\", None) or tmp_model.n_estimators)\n",
    "\n",
    "elif BACKEND == \"xgboost\":\n",
    "    tmp_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=300\n",
    "    )\n",
    "    best_iter = int(getattr(tmp_model, \"best_iteration\", None) or tmp_model.n_estimators)\n",
    "\n",
    "else:\n",
    "    sw = np.ones_like(y_tr, dtype=np.float32)\n",
    "    sw[y_tr == 1] = float(scale_pos_weight)\n",
    "    tmp_model.fit(X_tr, y_tr, sample_weight=sw)\n",
    "    best_iter = int(getattr(tmp_model, \"n_iter_\", None) or getattr(tmp_model, \"max_iter\", 2000) or 2000)\n",
    "\n",
    "print(f\"[INFO] best_iteration (holdout) = {best_iter}\")\n",
    "\n",
    "p_va = predict_proba_pos(tmp_model, X_va)\n",
    "f1_va = f1_score(y_va, (p_va >= THR).astype(int))\n",
    "print(f\"[INFO] holdout F1@THR = {f1_va:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train FULL with best_iter (if applicable)\n",
    "# ----------------------------\n",
    "final_model = make_model(SEED + 999)\n",
    "\n",
    "if BACKEND == \"lightgbm\":\n",
    "    final_model.set_params(n_estimators=max(200, best_iter))\n",
    "    final_model.fit(X_all, y)\n",
    "\n",
    "elif BACKEND == \"xgboost\":\n",
    "    final_model.set_params(n_estimators=max(200, best_iter))\n",
    "    final_model.fit(X_all, y, verbose=False)\n",
    "\n",
    "else:\n",
    "    sw = np.ones_like(y, dtype=np.float32)\n",
    "    sw[y == 1] = float(scale_pos_weight)\n",
    "    final_model.fit(X_all, y, sample_weight=sw)\n",
    "\n",
    "# ----------------------------\n",
    "# Predict test (proba + label)\n",
    "# ----------------------------\n",
    "test_proba = predict_proba_pos(final_model, X_test_all).astype(np.float32)\n",
    "test_pred  = (test_proba >= THR).astype(int)\n",
    "\n",
    "# Save test probabilities (buat ensemble berikutnya)\n",
    "proba_path = ART_DIR / \"final_test_proba.csv\"\n",
    "pd.DataFrame({\"object_id\": df_test[\"object_id\"].astype(str), \"proba\": test_proba}).to_csv(proba_path, index=False)\n",
    "print(\"[INFO] Saved test proba:\", proba_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Build submission\n",
    "# ----------------------------\n",
    "sub = df_sub[[\"object_id\"]].copy()\n",
    "sub[\"object_id\"] = sub[\"object_id\"].astype(str)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"object_id\": df_test[\"object_id\"].astype(str),\n",
    "    SUB_PRED_COL: test_pred.astype(int),\n",
    "})\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if sub[SUB_PRED_COL].isna().any():\n",
    "    missing = sub[sub[SUB_PRED_COL].isna()][\"object_id\"].head(10).tolist()\n",
    "    raise RuntimeError(f\"Submission has NaN after merge. Example missing object_id: {missing}\")\n",
    "\n",
    "out_path = SUB_DIR / \"sub_final_cpu.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Backend:\", BACKEND)\n",
    "print(\"Prediction column:\", SUB_PRED_COL)\n",
    "print(\"Saved submission:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
