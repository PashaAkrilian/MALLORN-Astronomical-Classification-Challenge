{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14010596,"sourceType":"datasetVersion","datasetId":8925232}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:29:08.638492Z","iopub.execute_input":"2026-01-03T13:29:08.638737Z","iopub.status.idle":"2026-01-03T13:29:10.193621Z","shell.execute_reply.started":"2026-01-03T13:29:08.638707Z","shell.execute_reply":"2026-01-03T13:29:10.192139Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mallorn-dataset/sample_submission.csv\n/kaggle/input/mallorn-dataset/test_log.csv\n/kaggle/input/mallorn-dataset/train_log.csv\n/kaggle/input/mallorn-dataset/split_17/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_17/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_01/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_01/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_02/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_02/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_08/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_08/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_04/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_04/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_07/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_07/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_15/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_15/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_20/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_20/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_06/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_06/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_19/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_19/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_09/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_09/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_12/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_12/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_16/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_16/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_10/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_10/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_18/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_18/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_03/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_03/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_11/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_11/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_14/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_14/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_05/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_05/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_13/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_13/test_full_lightcurves.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":" # Kaggle CPU Environment Setup","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Kaggle CPU Environment Setup (ONE CELL) — REVISI FULL v6\n# Fokus v6:\n# - Iterasi split pakai PATHS[\"SPLITS\"] (bukan set) -> deterministik\n# - Validasi: test_log == sample_submission (set & count)\n# - Siapkan mapping: object_id -> split & meta\n# - Guard untuk stage-stage berikutnya (resume friendly)\n# ============================================================\n\nimport os, sys, gc, json, time, random, hashlib, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\nSEED = 2025\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\n# ----------------------------\n# CPU thread limits (anti-freeze)\n# ----------------------------\nTHREADS = 2\nfor k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n    os.environ.setdefault(k, str(THREADS))\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\ntry:\n    import torch\n    torch.manual_seed(SEED)\n    torch.set_num_threads(THREADS)\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    torch = None\n\n# ----------------------------\n# PATHS\n# ----------------------------\nDATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\nPATHS = {\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n}\n\n# ----------------------------\n# WORKDIR (versioned run)\n# ----------------------------\nWORKDIR = Path(\"/kaggle/working\")\nBASE_RUN_DIR = WORKDIR / \"mallorn_run\"\nBASE_RUN_DIR.mkdir(parents=True, exist_ok=True)\n\nCFG = {\n    # Pipeline toggles\n    \"USE_GBDT\": True,\n    \"USE_DEEP_LITE\": False,      # CNN binned (CPU-friendly). Nyalakan kalau mau hybrid.\n    \"USE_HYBRID_BLEND\": False,   # blend GBDT + DEEP_LITE\n    \"USE_THRESHOLD_TUNING\": True,\n\n    # Feature settings\n    \"USE_ASINH_FLUX\": True,\n    \"SNR_CLIP\": 30.0,\n    \"SNR_DET_THR\": 3.0,\n    \"SNR_STRONG_THR\": 5.0,\n    \"MIN_FLUXERR\": 1e-6,\n\n    # Streaming\n    \"CHUNK_ROWS\": 200_000,\n\n    # CV\n    \"N_FOLDS\": 5,\n    \"CV_STRATIFY\": True,\n    \"CV_USE_SPLIT_COL\": True,    # pakai group=split agar anti leakage split\n    # Note: jika sklearn tidak punya StratifiedGroupKFold, fallback GroupKFold\n\n    # Deep-lite (binned)\n    \"BINS\": 48,                  # time bins\n    \"DEEP_EPOCHS\": 8,            # CPU-friendly\n    \"DEEP_BS\": 128,\n}\n\ndef _hash_cfg(d: dict) -> str:\n    s = json.dumps(d, sort_keys=True)\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:10]\n\nCFG_HASH = _hash_cfg(CFG)\nRUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\nRUN_DIR = BASE_RUN_DIR / f\"run_{RUN_TAG}_{CFG_HASH}\"\n\nART_DIR   = RUN_DIR / \"artifacts\"\nCACHE_DIR = RUN_DIR / \"cache\"\nOOF_DIR   = RUN_DIR / \"oof\"\nSUB_DIR   = RUN_DIR / \"submissions\"\nLOG_DIR   = RUN_DIR / \"logs\"\nFEAT_DIR  = CACHE_DIR / \"features\"\nSEQ_DIR   = CACHE_DIR / \"seq\"      # untuk deep-lite bins\n\nfor d in [RUN_DIR, ART_DIR, CACHE_DIR, OOF_DIR, SUB_DIR, LOG_DIR, FEAT_DIR, SEQ_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\ndef _must_exist(p: Path, what: str):\n    if not p.exists():\n        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n\n_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n_must_exist(PATHS[\"TRAIN_LOG\"],  \"train_log.csv\")\n_must_exist(PATHS[\"TEST_LOG\"],   \"test_log.csv\")\n\nfor sd in PATHS[\"SPLITS\"]:\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    _must_exist(tr, f\"{sd.name}/train_full_lightcurves.csv\")\n    _must_exist(te, f\"{sd.name}/test_full_lightcurves.csv\")\n\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\n# ----------------------------\n# Load logs + sample\n# ----------------------------\ndf_sub = _norm_cols(pd.read_csv(PATHS[\"SAMPLE_SUB\"], dtype={\"object_id\": \"string\"}, **SAFE_READ_KW))\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission must have object_id,prediction. Found: {list(df_sub.columns)}\")\n\ndf_train_log = _norm_cols(pd.read_csv(PATHS[\"TRAIN_LOG\"], dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\ndf_test_log  = _norm_cols(pd.read_csv(PATHS[\"TEST_LOG\"],  dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n\nneed_train = {\"object_id\",\"EBV\",\"Z\",\"split\",\"target\"}\nneed_test  = {\"object_id\",\"EBV\",\"Z\",\"split\"}\nif not need_train.issubset(df_train_log.columns):\n    raise ValueError(f\"train_log missing: {sorted(list(need_train-set(df_train_log.columns)))}\")\nif not need_test.issubset(df_test_log.columns):\n    raise ValueError(f\"test_log missing: {sorted(list(need_test-set(df_test_log.columns)))}\")\n\ndef _normalize_split(x):\n    if pd.isna(x): return \"\"\n    s = str(x).strip()\n    if not s: return \"\"\n    if s.isdigit(): return f\"split_{int(s):02d}\"\n    s = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s.startswith(\"split_\"):\n        tail = s.split(\"split_\",1)[1].strip(\"_\")\n        if tail.isdigit():\n            return f\"split_{int(tail):02d}\"\n    return s\n\ndf_train_log[\"split\"] = df_train_log[\"split\"].map(_normalize_split)\ndf_test_log[\"split\"]  = df_test_log[\"split\"].map(_normalize_split)\n\nvalid_splits = {f\"split_{i:02d}\" for i in range(1,21)}\nbad_tr = sorted(set(df_train_log[\"split\"]) - valid_splits)\nbad_te = sorted(set(df_test_log[\"split\"]) - valid_splits)\nif bad_tr: raise ValueError(f\"Invalid split in train_log: {bad_tr[:10]}\")\nif bad_te: raise ValueError(f\"Invalid split in test_log:  {bad_te[:10]}\")\n\nfor col in [\"EBV\",\"Z\"]:\n    df_train_log[col] = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n    df_test_log[col]  = pd.to_numeric(df_test_log[col],  errors=\"coerce\")\n    if df_train_log[col].isna().any():\n        raise ValueError(f\"train_log {col} has NaN after numeric coercion: {int(df_train_log[col].isna().sum())}\")\n    if df_test_log[col].isna().any():\n        raise ValueError(f\"test_log {col} has NaN after numeric coercion: {int(df_test_log[col].isna().sum())}\")\n\ndf_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\")\nif df_train_log[\"target\"].isna().any():\n    raise ValueError(f\"train_log target NaN after coercion: {int(df_train_log['target'].isna().sum())}\")\nu = set(pd.unique(df_train_log[\"target\"]).tolist())\nif not u.issubset({0,1}):\n    raise ValueError(f\"train_log target must be 0/1. Found: {sorted(list(u))}\")\n\n# Z_err handling\nif \"Z_err\" not in df_test_log.columns:\n    df_test_log[\"Z_err\"] = np.nan\ndf_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\ndf_test_log[\"has_zerr\"] = (~df_test_log[\"Z_err\"].isna()).astype(\"int8\")\ndf_test_log[\"Z_err\"] = df_test_log[\"Z_err\"].fillna(0.0)\n\nif \"Z_err\" not in df_train_log.columns:\n    df_train_log[\"Z_err\"] = 0.0\ndf_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\").fillna(0.0)\ndf_train_log[\"has_zerr\"] = 0\n\n# Uniqueness\nif df_train_log[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in train_log: {int(df_train_log['object_id'].duplicated().sum())}\")\nif df_test_log[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in test_log:  {int(df_test_log['object_id'].duplicated().sum())}\")\n\n# sample_submission alignment check\nsub_ids = df_sub[\"object_id\"].astype(\"string\")\ntest_ids = df_test_log[\"object_id\"].astype(\"string\")\nif len(sub_ids) != len(test_ids):\n    raise ValueError(f\"Row mismatch: sample_submission={len(sub_ids)} vs test_log={len(test_ids)}\")\n\ns_sub = set(sub_ids.tolist())\ns_tst = set(test_ids.tolist())\nif s_sub != s_tst:\n    missing_in_test = list(s_sub - s_tst)[:5]\n    missing_in_sub  = list(s_tst - s_sub)[:5]\n    raise ValueError(\n        \"sample_submission and test_log object_id set mismatch.\\n\"\n        f\"- sample not in test_log (up to5): {missing_in_test}\\n\"\n        f\"- test_log not in sample (up to5): {missing_in_sub}\"\n    )\n\n# Basic counts\npos = int((df_train_log[\"target\"]==1).sum())\nneg = int((df_train_log[\"target\"]==0).sum())\ntot = int(len(df_train_log))\n\nprint(\"ENV OK (Kaggle CPU)\")\nprint(f\"- Python: {sys.version.split()[0]}\")\nprint(f\"- Numpy:  {np.__version__}\")\nprint(f\"- Pandas: {pd.__version__}\")\nif torch is not None:\n    print(f\"- Torch:  {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\nelse:\n    print(\"- Torch:  not available\")\n\nprint(\"\\nDATA OK\")\nprint(f\"- train_log objects: {tot:,} | pos={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.2f}%\")\nprint(f\"- test_log objects:  {len(df_test_log):,}\")\nprint(f\"- sample_submission: {len(df_sub):,}\")\nprint(f\"- splits: {len(PATHS['SPLITS'])} folders (01..20)\")\n\n# Save snapshot\nsnap = {\n    \"SEED\": SEED,\n    \"CFG\": CFG,\n    \"CFG_HASH\": CFG_HASH,\n    \"RUN_DIR\": str(RUN_DIR),\n    \"DATA_ROOT\": str(DATA_ROOT),\n}\nwith open(RUN_DIR / \"config_stage0.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(snap, f, indent=2)\n\nglobals().update({\n    \"SEED\": SEED, \"THREADS\": THREADS, \"CFG\": CFG, \"CFG_HASH\": CFG_HASH,\n    \"PATHS\": PATHS, \"DATA_ROOT\": DATA_ROOT, \"RUN_DIR\": RUN_DIR,\n    \"ART_DIR\": ART_DIR, \"CACHE_DIR\": CACHE_DIR, \"OOF_DIR\": OOF_DIR,\n    \"SUB_DIR\": SUB_DIR, \"LOG_DIR\": LOG_DIR, \"FEAT_DIR\": FEAT_DIR, \"SEQ_DIR\": SEQ_DIR,\n    \"df_sub\": df_sub, \"df_train_log\": df_train_log, \"df_test_log\": df_test_log\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:40:47.249946Z","iopub.execute_input":"2026-01-03T13:40:47.250228Z","iopub.status.idle":"2026-01-03T13:40:52.126428Z","shell.execute_reply.started":"2026-01-03T13:40:47.250205Z","shell.execute_reply":"2026-01-03T13:40:52.125534Z"}},"outputs":[{"name":"stdout","text":"ENV OK (Kaggle CPU)\n- Python: 3.12.12\n- Numpy:  2.0.2\n- Pandas: 2.2.2\n- Torch:  2.8.0+cu126 | CUDA: False\n\nDATA OK\n- train_log objects: 3,043 | pos=148 | neg=2,895 | pos%=4.86%\n- test_log objects:  7,135\n- sample_submission: 7,135\n- splits: 20 folders (01..20)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"63"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Verify Dataset Paths & Split Discovery","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — Split Routing + Lightcurve Micro-Profiling (ONE CELL, CPU-SAFE)\n# REVISI FULL v3 (IO-LEBIH IRIT + DETERM + LEBIH BANYAK STATS)\n#\n# Output:\n# - logs/split_routing.csv\n# - logs/lc_sample_stats.csv\n# - logs/stage1_summary.json\n#\n# Catatan:\n# - Stage ini tidak \"meningkatkan akurasi\" langsung, tapi memastikan ALL SPLITS kebaca\n#   dan memberi statistik penting untuk tuning stage modelling berikutnya.\n# ============================================================\n\nimport re, gc, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require STAGE 0 globals\n# ----------------------------\nneed0 = [\"PATHS\", \"df_train_log\", \"df_test_log\"]\nfor need in need0:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n\nDATA_ROOT = Path(PATHS[\"DATA_ROOT\"])\n\n# deterministic split list (JANGAN set)\nSPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\nVALID_SPLITS = set(SPLIT_LIST)\n\n# split dirs mapping (deterministic)\nSPLIT_DIRS = {p.name: p for p in PATHS[\"SPLITS\"]}\n# Optional dirs from stage 0\nRUN_DIR = Path(globals().get(\"RUN_DIR\", \"/kaggle/working/mallorn_run\"))\nLOG_DIR = Path(globals().get(\"LOG_DIR\", RUN_DIR / \"logs\"))\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nCFG_LOCAL = globals().get(\"CFG\", {})\nSEED = int(globals().get(\"SEED\", 2025))\n\n# ----------------------------\n# 1) Safe read config (konsisten dengan STAGE 0)\n# ----------------------------\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\n# sampling knobs (CPU-safe)\nHEAD_ROWS = int(CFG_LOCAL.get(\"LC_HEAD_ROWS\", 2000))           # sample rows per file\nSAMPLE_ID_PER_SPLIT = int(CFG_LOCAL.get(\"SAMPLE_ID_PER_SPLIT\", 5))\nCHUNK_ROWS = int(CFG_LOCAL.get(\"CHUNK_ROWS\", 200_000))\nMAX_CHUNKS_PER_FILE = int(CFG_LOCAL.get(\"MAX_CHUNKS_PER_FILE\", 6))\n\n# numeric policy thresholds (ketat untuk Time/Flux_err)\nMAX_TIME_NA_FRAC = float(CFG_LOCAL.get(\"MAX_TIME_NA_FRAC\", 0.02))\nMAX_FERR_NA_FRAC = float(CFG_LOCAL.get(\"MAX_FERR_NA_FRAC\", 0.02))\nID_MISS_FAIL_FRAC = float(CFG_LOCAL.get(\"ID_MISS_FAIL_FRAC\", 0.80))  # fail jika >= 80% ID sample tidak ketemu (scan cap)\nMIN_SAMPLE_ROWS = int(CFG_LOCAL.get(\"MIN_SAMPLE_ROWS\", 100))         # fail kalau sample terlalu kosong\n\n# ----------------------------\n# 2) Helpers\n# ----------------------------\nREQ_LC_COLS = [\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"]\nREQ_LC_COLS_SET = set(REQ_LC_COLS)\nALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef normalize_split_name(x) -> str:\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s2.isdigit():\n        return f\"split_{int(s2):02d}\"\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    return s2\n\ndef sizeof_mb(p: Path) -> float:\n    try:\n        return p.stat().st_size / (1024**2)\n    except Exception:\n        return float(\"nan\")\n\ndef _read_sample_df(p: Path, nrows: int):\n    \"\"\"\n    Single read for:\n    - schema presence (via usecols)\n    - filter sanity\n    - numeric coercion sample stats\n    \"\"\"\n    try:\n        dfh = pd.read_csv(p, usecols=REQ_LC_COLS, nrows=nrows, **SAFE_READ_KW)\n    except ValueError as e:\n        # Usually \"Usecols do not match columns\"\n        # Provide clearer diagnostics\n        df0 = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n        cols = [c.strip() for c in df0.columns]\n        miss = sorted(list(REQ_LC_COLS_SET - set(cols)))\n        raise ValueError(\n            f\"[LC SCHEMA] {p} missing required columns: {miss}. Found columns: {cols}\"\n        ) from e\n    return _norm_cols(dfh)\n\ndef _numeric_and_filter_stats(dfh: pd.DataFrame):\n    out = {\"n_sample\": int(len(dfh))}\n    if len(dfh) == 0:\n        out.update({\n            \"time_na_frac\": 1.0, \"flux_na_frac\": 1.0, \"ferr_na_frac\": 1.0,\n            \"filter_bad\": \"\", \"filter_sample\": \"\"\n        })\n        return out\n\n    # Filter values\n    filt = dfh[\"Filter\"].astype(\"string\").str.strip().str.lower()\n    filt = filt[~filt.isna()]\n    uniq = sorted(set(filt.tolist()))\n    bad = sorted([v for v in uniq if v not in ALLOWED_FILTERS])\n    out[\"filter_bad\"] = \",\".join(bad[:10]) if bad else \"\"\n    out[\"filter_sample\"] = \",\".join(uniq[:10]) if uniq else \"\"\n\n    # Band coverage from sample\n    if len(filt) > 0:\n        vc = filt.value_counts()\n        denom = float(vc.sum())\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            out[f\"frac_{b}\"] = float(vc.get(b, 0) / denom)\n    else:\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            out[f\"frac_{b}\"] = 0.0\n\n    # Numeric coercion\n    t = pd.to_numeric(dfh[\"Time (MJD)\"], errors=\"coerce\")\n    f = pd.to_numeric(dfh[\"Flux\"], errors=\"coerce\")\n    e = pd.to_numeric(dfh[\"Flux_err\"], errors=\"coerce\")\n\n    out[\"time_na_frac\"] = float(t.isna().mean())\n    out[\"flux_na_frac\"] = float(f.isna().mean())\n    out[\"ferr_na_frac\"] = float(e.isna().mean())\n\n    # Quick stats (ignore NaN)\n    if (~t.isna()).any():\n        out[\"time_min\"] = float(t.min())\n        out[\"time_max\"] = float(t.max())\n        out[\"time_span\"] = float(t.max() - t.min())\n    else:\n        out[\"time_min\"] = np.nan\n        out[\"time_max\"] = np.nan\n        out[\"time_span\"] = np.nan\n\n    if (~f.isna()).any():\n        fd = f.dropna()\n        out[\"flux_neg_frac\"] = float((fd < 0).mean())\n        out[\"flux_p01\"] = float(np.quantile(fd, 0.01))\n        out[\"flux_p50\"] = float(np.quantile(fd, 0.50))\n        out[\"flux_p99\"] = float(np.quantile(fd, 0.99))\n    else:\n        out[\"flux_neg_frac\"] = np.nan\n        out[\"flux_p01\"] = np.nan\n        out[\"flux_p50\"] = np.nan\n        out[\"flux_p99\"] = np.nan\n\n    if (~e.isna()).any():\n        ed = e.dropna()\n        out[\"ferr_min\"] = float(ed.min())\n        out[\"ferr_p50\"] = float(np.quantile(ed, 0.50))\n        out[\"ferr_p99\"] = float(np.quantile(ed, 0.99))\n        out[\"ferr_neg_any\"] = int((ed < 0).any())\n    else:\n        out[\"ferr_min\"] = np.nan\n        out[\"ferr_p50\"] = np.nan\n        out[\"ferr_p99\"] = np.nan\n        out[\"ferr_neg_any\"] = 0\n\n    return out\n\ndef _sample_id_presence(csv_path: Path, want_ids: set, chunk_rows: int, max_chunks: int):\n    \"\"\"\n    Limited scan memastikan beberapa object_id dari log benar-benar muncul di file.\n    Scan hanya kolom object_id (lebih murah).\n    \"\"\"\n    if not want_ids:\n        return 0, set(), 0\n    remaining = set(want_ids)\n    found = set()\n    nread_chunks = 0\n\n    for i, chunk in enumerate(pd.read_csv(csv_path, usecols=[\"object_id\"], chunksize=chunk_rows, **SAFE_READ_KW)):\n        nread_chunks += 1\n        ids = set(chunk[\"object_id\"].astype(\"string\"))\n        hit = remaining & ids\n        if hit:\n            found |= hit\n            remaining -= hit\n        if not remaining:\n            break\n        if i + 1 >= max_chunks:\n            break\n\n    return len(found), remaining, nread_chunks\n\n# ----------------------------\n# 3) Normalize split col in logs (idempotent)\n# ----------------------------\nfor df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n    if \"split\" not in df.columns:\n        raise ValueError(f\"{name} missing 'split' column.\")\n    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n\nbad_train_split = sorted(set(df_train_log[\"split\"].unique()) - VALID_SPLITS)\nbad_test_split  = sorted(set(df_test_log[\"split\"].unique())  - VALID_SPLITS)\nif bad_train_split:\n    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\nif bad_test_split:\n    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n\n# ----------------------------\n# 4) Verify disk splits set + required files exist\n# ----------------------------\ndisk_splits = set(SPLIT_DIRS.keys())\nmissing_dirs = sorted(list(VALID_SPLITS - disk_splits))\nextra_dirs   = sorted(list(disk_splits - VALID_SPLITS))\nif missing_dirs or extra_dirs:\n    msg = []\n    if missing_dirs: msg.append(f\"Missing split folders: {missing_dirs[:10]}\")\n    if extra_dirs:   msg.append(f\"Unexpected split folders: {extra_dirs[:10]}\")\n    raise RuntimeError(\"Split folder set mismatch.\\n\" + \"\\n\".join(msg))\n\nmissing_files = []\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    for kind in [\"train\", \"test\"]:\n        p = sd / f\"{kind}_full_lightcurves.csv\"\n        if not p.exists():\n            missing_files.append(str(p))\nif missing_files:\n    raise FileNotFoundError(\"Some lightcurve files missing (showing up to 10):\\n\" + \"\\n\".join(missing_files[:10]))\n\n# ----------------------------\n# 5) Build routing manifest (40 file)\n# ----------------------------\ntrain_counts = df_train_log[\"split\"].value_counts().to_dict()\ntest_counts  = df_test_log[\"split\"].value_counts().to_dict()\n\nrouting_rows = []\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    routing_rows.append({\n        \"split\": sp,\n        \"train_csv\": str(tr),\n        \"test_csv\": str(te),\n        \"train_mb\": sizeof_mb(tr),\n        \"test_mb\": sizeof_mb(te),\n        \"n_train_objects_log\": int(train_counts.get(sp, 0)),\n        \"n_test_objects_log\":  int(test_counts.get(sp, 0)),\n    })\n\ndf_routing = pd.DataFrame(routing_rows)\nrouting_path = LOG_DIR / \"split_routing.csv\"\ndf_routing.to_csv(routing_path, index=False)\n\n# ----------------------------\n# 6) Sample profiling + ID crosscheck (single read sample)\n# ----------------------------\nstats_rows = []\nwarn_flux_na_files = 0\n\nt0 = time.time()\n\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    for kind in [\"train\", \"test\"]:\n        p = sd / f\"{kind}_full_lightcurves.csv\"\n\n        # read sample once\n        dfh = _read_sample_df(p, nrows=HEAD_ROWS)\n\n        # minimum sanity: sample should not be empty\n        if len(dfh) < MIN_SAMPLE_ROWS:\n            raise ValueError(f\"[LC SAMPLE] Too few rows sampled from {p} (n={len(dfh)}). Possible read issue.\")\n\n        # compute stats\n        st = _numeric_and_filter_stats(dfh)\n\n        # Filter sanity\n        if st.get(\"filter_bad\", \"\"):\n            raise ValueError(f\"[LC FILTER] Unexpected Filter values in {p}: {st['filter_bad']} (sample={st.get('filter_sample','')})\")\n\n        # numeric policy\n        if st.get(\"time_na_frac\", 0.0) > MAX_TIME_NA_FRAC:\n            raise ValueError(f\"[LC NUM] Time(MJD) NaN too high in sample: {p} frac={st['time_na_frac']:.4f}\")\n        if st.get(\"ferr_na_frac\", 0.0) > MAX_FERR_NA_FRAC:\n            raise ValueError(f\"[LC NUM] Flux_err NaN too high in sample: {p} frac={st['ferr_na_frac']:.4f}\")\n        if int(st.get(\"ferr_neg_any\", 0)) == 1:\n            raise ValueError(f\"[LC NUM] Negative Flux_err detected in sample of {p} (should be >=0).\")\n\n        if st.get(\"flux_na_frac\", 0.0) > 0:\n            warn_flux_na_files += 1\n\n        # sample ID crosscheck (limited scan)\n        if kind == \"train\":\n            ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(\"string\")\n        else:\n            ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(\"string\")\n\n        k = min(SAMPLE_ID_PER_SPLIT, len(ids))\n        want = set(ids.sample(n=k, random_state=SEED).tolist()) if k > 0 else set()\n\n        found_n, missing_ids, nread_chunks = _sample_id_presence(p, want, CHUNK_ROWS, MAX_CHUNKS_PER_FILE)\n        miss_frac = (len(missing_ids) / max(len(want), 1)) if want else 0.0\n\n        if want and miss_frac >= ID_MISS_FAIL_FRAC:\n            raise ValueError(\n                f\"[LC ID] Severe mismatch within limited scan: {p} missing {len(missing_ids)}/{len(want)} \"\n                f\"(chunks_read={nread_chunks}). Example missing: {list(missing_ids)[:3]}\"\n            )\n        if want and missing_ids:\n            print(f\"[WARN] ID limited-scan miss: split={sp} kind={kind} miss {len(missing_ids)}/{len(want)} (chunks_read={nread_chunks})\")\n\n        row = {\n            \"split\": sp,\n            \"kind\": kind,\n            \"file\": str(p),\n            \"file_mb\": sizeof_mb(p),\n            \"n_sample\": st.get(\"n_sample\", 0),\n            \"time_na_frac\": st.get(\"time_na_frac\", np.nan),\n            \"flux_na_frac\": st.get(\"flux_na_frac\", np.nan),\n            \"ferr_na_frac\": st.get(\"ferr_na_frac\", np.nan),\n            \"time_min\": st.get(\"time_min\", np.nan),\n            \"time_max\": st.get(\"time_max\", np.nan),\n            \"time_span\": st.get(\"time_span\", np.nan),\n            \"flux_neg_frac\": st.get(\"flux_neg_frac\", np.nan),\n            \"flux_p01\": st.get(\"flux_p01\", np.nan),\n            \"flux_p50\": st.get(\"flux_p50\", np.nan),\n            \"flux_p99\": st.get(\"flux_p99\", np.nan),\n            \"ferr_min\": st.get(\"ferr_min\", np.nan),\n            \"ferr_p50\": st.get(\"ferr_p50\", np.nan),\n            \"ferr_p99\": st.get(\"ferr_p99\", np.nan),\n            \"filter_sample\": st.get(\"filter_sample\", \"\"),\n            \"id_check_k\": int(len(want)),\n            \"id_found\": int(found_n),\n            \"id_missing\": int(len(missing_ids)),\n            \"id_scan_chunks\": int(nread_chunks),\n        }\n        # band fractions\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            row[f\"frac_{b}\"] = st.get(f\"frac_{b}\", 0.0)\n\n        stats_rows.append(row)\n\ndf_lc_stats = pd.DataFrame(stats_rows)\nlc_stats_path = LOG_DIR / \"lc_sample_stats.csv\"\ndf_lc_stats.to_csv(lc_stats_path, index=False)\n\n# ----------------------------\n# 7) Summary prints + JSON summary\n# ----------------------------\nelapsed = time.time() - t0\n\n# worst by flux_na_frac (informational only)\nworst_flux_na = (\n    df_lc_stats.sort_values(\"flux_na_frac\", ascending=False)\n    .head(8)[[\"split\",\"kind\",\"flux_na_frac\",\"time_na_frac\",\"ferr_na_frac\",\"file_mb\"]]\n)\n\n# worst by time_na_frac (hard policy already, but show)\nworst_time_na = (\n    df_lc_stats.sort_values(\"time_na_frac\", ascending=False)\n    .head(8)[[\"split\",\"kind\",\"time_na_frac\",\"flux_na_frac\",\"ferr_na_frac\",\"file_mb\"]]\n)\n\nsummary = {\n    \"stage\": \"stage1\",\n    \"data_root\": str(DATA_ROOT),\n    \"log_dir\": str(LOG_DIR),\n    \"head_rows\": HEAD_ROWS,\n    \"sample_id_per_split\": SAMPLE_ID_PER_SPLIT,\n    \"chunk_rows\": CHUNK_ROWS,\n    \"max_chunks_per_file\": MAX_CHUNKS_PER_FILE,\n    \"thresholds\": {\n        \"MAX_TIME_NA_FRAC\": MAX_TIME_NA_FRAC,\n        \"MAX_FERR_NA_FRAC\": MAX_FERR_NA_FRAC,\n        \"ID_MISS_FAIL_FRAC\": ID_MISS_FAIL_FRAC,\n        \"MIN_SAMPLE_ROWS\": MIN_SAMPLE_ROWS\n    },\n    \"warn_flux_na_files\": int(warn_flux_na_files),\n    \"routing_csv\": str(routing_path),\n    \"lc_sample_stats_csv\": str(lc_stats_path),\n    \"elapsed_sec\": float(elapsed),\n}\n\nsummary_path = LOG_DIR / \"stage1_summary.json\"\nwith open(summary_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"STAGE 1 OK — SPLIT ROUTING READY\")\nprint(f\"- routing saved: {routing_path}\")\nprint(f\"- lc sample stats saved: {lc_stats_path}\")\nprint(f\"- summary json saved: {summary_path}\")\nprint(f\"- elapsed: {elapsed/60:.2f} min | warn_flux_na_files={warn_flux_na_files}\")\n\nprint(\"\\nOBJECT COUNTS (from logs)\")\nfor sp in SPLIT_LIST:\n    print(f\"- {sp}: train={int(train_counts.get(sp,0)):,} | test={int(test_counts.get(sp,0)):,}\")\n\nprint(\"\\nWORST SAMPLE (highest flux_na_frac in sample head)\")\nprint(worst_flux_na.to_string(index=False))\n\nprint(\"\\nWORST SAMPLE (highest time_na_frac in sample head)\")\nprint(worst_time_na.to_string(index=False))\n\n# ----------------------------\n# 8) Export to globals (dipakai stage berikutnya)\n# ----------------------------\nglobals().update({\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SPLIT_DIRS\": SPLIT_DIRS,\n    \"SPLIT_LIST\": SPLIT_LIST,\n    \"df_split_routing\": df_routing,\n    \"df_lc_sample_stats\": df_lc_stats,\n    \"STAGE1_SUMMARY_PATH\": summary_path,\n})\n\ngc.collect()\nprint(\"\\nStage 1 complete: splits verified + routing/stats exported.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:45:04.736622Z","iopub.execute_input":"2026-01-03T13:45:04.737525Z","iopub.status.idle":"2026-01-03T13:45:07.742022Z","shell.execute_reply.started":"2026-01-03T13:45:04.737495Z","shell.execute_reply":"2026-01-03T13:45:07.740807Z"}},"outputs":[{"name":"stdout","text":"STAGE 1 OK — SPLIT ROUTING READY\n- routing saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/logs/split_routing.csv\n- lc sample stats saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/logs/lc_sample_stats.csv\n- summary json saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/logs/stage1_summary.json\n- elapsed: 0.04 min | warn_flux_na_files=18\n\nOBJECT COUNTS (from logs)\n- split_01: train=155 | test=364\n- split_02: train=170 | test=414\n- split_03: train=138 | test=338\n- split_04: train=145 | test=332\n- split_05: train=165 | test=375\n- split_06: train=155 | test=374\n- split_07: train=165 | test=398\n- split_08: train=162 | test=387\n- split_09: train=128 | test=289\n- split_10: train=144 | test=331\n- split_11: train=146 | test=325\n- split_12: train=155 | test=353\n- split_13: train=143 | test=379\n- split_14: train=154 | test=351\n- split_15: train=158 | test=342\n- split_16: train=155 | test=354\n- split_17: train=153 | test=351\n- split_18: train=152 | test=345\n- split_19: train=147 | test=375\n- split_20: train=153 | test=358\n\nWORST SAMPLE (highest flux_na_frac in sample head)\n   split  kind  flux_na_frac  time_na_frac  ferr_na_frac  file_mb\nsplit_07 train         0.095           0.0           0.0 1.257892\nsplit_04 train         0.002           0.0           0.0 1.173453\nsplit_19  test         0.002           0.0           0.0 2.904259\nsplit_17 train         0.002           0.0           0.0 1.173508\nsplit_05  test         0.001           0.0           0.0 3.122271\nsplit_11 train         0.001           0.0           0.0 1.180762\nsplit_14 train         0.001           0.0           0.0 1.316985\nsplit_19 train         0.001           0.0           0.0 1.140910\n\nWORST SAMPLE (highest time_na_frac in sample head)\n   split  kind  time_na_frac  flux_na_frac  ferr_na_frac  file_mb\nsplit_01 train           0.0        0.0000           0.0 1.351567\nsplit_01  test           0.0        0.0000           0.0 3.052048\nsplit_02 train           0.0        0.0000           0.0 1.326915\nsplit_02  test           0.0        0.0000           0.0 3.658606\nsplit_03 train           0.0        0.0005           0.0 1.124688\nsplit_03  test           0.0        0.0000           0.0 2.770512\nsplit_04 train           0.0        0.0020           0.0 1.173453\nsplit_04  test           0.0        0.0000           0.0 2.658786\n\nStage 1 complete: splits verified + routing/stats exported.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Load and Validate Train/Test Logs","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — Clean Meta Logs + CV Fold Assignment (ONE CELL, CPU-SAFE)\n# REVISI FULL v4 (FIX: 5-FOLD TERPAKAI + Z_ERR TIDAK MATI)\n#\n# Output:\n#   * df_train_meta, df_test_meta (index=object_id)\n#   * id2split_train, id2split_test\n#   * artifacts: train_meta.(parquet|csv), test_meta.(parquet|csv)\n#   * artifacts: split_stats.csv, train_folds.csv\n#   * artifacts: id2split_train.json, id2split_test.json\n#   * artifacts: split2fold.json (kalau CV_USE_SPLIT_COL=True)\n#\n# Notes:\n# - Tidak load full lightcurves.\n# - EBV/Z clip pakai TRAIN saja (anti leakage).\n# - Z_err clip pakai TEST quantiles (tanpa label) -> supaya tidak 0..0.\n# - Fold strategy:\n#     * Jika CFG[\"CV_USE_SPLIT_COL\"]=True -> assign split->fold dengan quota 4 split/fold\n#     * else -> StratifiedKFold object-level\n# ============================================================\n\nimport re, gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 0/1 globals\n# ----------------------------\nfor need in [\"PATHS\", \"ART_DIR\", \"SPLIT_DIRS\", \"CFG\", \"SEED\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 & STAGE 1 dulu.\")\n\nTRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\nTEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n\nART_DIR = Path(ART_DIR)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nSEED = int(SEED)\nN_FOLDS = int(CFG.get(\"N_FOLDS\", 5))\nCV_USE_SPLIT_COL = bool(CFG.get(\"CV_USE_SPLIT_COL\", True))\n\nSPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\nVALID_SPLITS = set(SPLIT_LIST)\n\ndisk_splits = set(SPLIT_DIRS.keys())\nif disk_splits != VALID_SPLITS:\n    miss = sorted(list(VALID_SPLITS - disk_splits))\n    extra = sorted(list(disk_splits - VALID_SPLITS))\n    raise RuntimeError(f\"SPLIT_DIRS mismatch. missing={miss[:5]} extra={extra[:5]} (jalankan ulang STAGE 1)\")\n\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\ndef normalize_split_name(x) -> str:\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s2.isdigit():\n        return f\"split_{int(s2):02d}\"\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    return s2\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef _coerce_float32(df: pd.DataFrame, col: str):\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\")\n\ndef _safe_clip(series: pd.Series, lo: float, hi: float) -> pd.Series:\n    return series.clip(lower=np.float32(lo), upper=np.float32(hi)).astype(\"float32\")\n\ndef _qclip_bounds(arr: np.ndarray, qlo=0.001, qhi=0.999, default=(0.0, 0.0)):\n    x = np.asarray(arr, dtype=float)\n    x = x[np.isfinite(x)]\n    if len(x) == 0:\n        return float(default[0]), float(default[1])\n    lo, hi = np.quantile(x, [qlo, qhi])\n    return float(lo), float(hi)\n\ndef _load_or_use_global(global_name: str, path: Path) -> pd.DataFrame:\n    if global_name in globals() and isinstance(globals()[global_name], pd.DataFrame):\n        return _norm_cols(globals()[global_name].copy())\n    return _norm_cols(pd.read_csv(path, dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n\n# ----------------------------\n# 2) Load logs\n# ----------------------------\ndf_train = _load_or_use_global(\"df_train_log\", TRAIN_LOG_PATH)\ndf_test  = _load_or_use_global(\"df_test_log\",  TEST_LOG_PATH)\n\n# ----------------------------\n# 3) Required columns check\n# ----------------------------\nreq_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\nreq_train  = req_common | {\"target\"}\nreq_test   = req_common\n\nmiss_train = sorted(list(req_train - set(df_train.columns)))\nmiss_test  = sorted(list(req_test  - set(df_test.columns)))\nif miss_train:\n    raise ValueError(f\"train_log missing columns: {miss_train} | found={list(df_train.columns)}\")\nif miss_test:\n    raise ValueError(f\"test_log missing columns: {miss_test} | found={list(df_test.columns)}\")\n\n# ----------------------------\n# 4) Basic cleaning\n# ----------------------------\ndf_train[\"object_id\"] = df_train[\"object_id\"].astype(\"string\").str.strip()\ndf_test[\"object_id\"]  = df_test[\"object_id\"].astype(\"string\").str.strip()\n\ndf_train[\"split\"] = df_train[\"split\"].astype(\"string\").map(normalize_split_name)\ndf_test[\"split\"]  = df_test[\"split\"].astype(\"string\").map(normalize_split_name)\n\nbad_train_split = sorted(set(df_train[\"split\"].unique()) - VALID_SPLITS)\nbad_test_split  = sorted(set(df_test[\"split\"].unique())  - VALID_SPLITS)\nif bad_train_split:\n    raise ValueError(f\"train_log invalid split values: {bad_train_split[:10]}\")\nif bad_test_split:\n    raise ValueError(f\"test_log invalid split values: {bad_test_split[:10]}\")\n\n# ----------------------------\n# 5) Ensure Z_err exists + numeric coercion\n# ----------------------------\nif \"Z_err\" not in df_train.columns:\n    df_train[\"Z_err\"] = np.nan\nif \"Z_err\" not in df_test.columns:\n    df_test[\"Z_err\"] = np.nan\n\n# has_zerr BEFORE fill\ndf_train[\"has_zerr\"] = (~pd.to_numeric(df_train[\"Z_err\"], errors=\"coerce\").isna()).astype(\"int8\")\ndf_test[\"has_zerr\"]  = (~pd.to_numeric(df_test[\"Z_err\"],  errors=\"coerce\").isna()).astype(\"int8\")\n\nfor c in [\"EBV\",\"Z\",\"Z_err\"]:\n    _coerce_float32(df_train, c)\n    _coerce_float32(df_test, c)\n\n# ----------------------------\n# 6) Duplicate / overlap checks\n# ----------------------------\nif df_train[\"object_id\"].duplicated().any():\n    ex = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n    raise ValueError(f\"Duplicated object_id in train_log (examples): {ex}\")\nif df_test[\"object_id\"].duplicated().any():\n    ex = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n    raise ValueError(f\"Duplicated object_id in test_log (examples): {ex}\")\n\noverlap = set(df_train[\"object_id\"].tolist()) & set(df_test[\"object_id\"].tolist())\nif overlap:\n    raise ValueError(f\"object_id overlap train vs test (examples): {list(overlap)[:5]}\")\n\n# ----------------------------\n# 7) Target validation\n# ----------------------------\ndf_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\nif df_train[\"target\"].isna().any():\n    raise ValueError(f\"train_log target has NaN after coercion: {int(df_train['target'].isna().sum())} rows.\")\nuniq_t = set(pd.unique(df_train[\"target\"]).tolist())\nif not uniq_t.issubset({0,1}):\n    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\ndf_train[\"target\"] = df_train[\"target\"].astype(\"int8\")\n\n# ----------------------------\n# 8) Missing flags + fills\n# ----------------------------\nfor df in [df_train, df_test]:\n    df[\"EBV_missing\"] = df[\"EBV\"].isna().astype(\"int8\")\n    df[\"Z_missing\"]   = df[\"Z\"].isna().astype(\"int8\")\n    df[\"Zerr_missing\"]= df[\"Z_err\"].isna().astype(\"int8\")\n\ndf_train[\"EBV\"] = df_train[\"EBV\"].fillna(np.float32(0.0)).astype(\"float32\")\ndf_test[\"EBV\"]  = df_test[\"EBV\"].fillna(np.float32(0.0)).astype(\"float32\")\n\n# Z fill pakai TRAIN stats\ntrain_split_med = df_train.groupby(\"split\")[\"Z\"].median().to_dict()\ntrain_gmed = float(np.nanmedian(df_train[\"Z\"].values.astype(float))) if np.isfinite(df_train[\"Z\"].values.astype(float)).any() else 0.0\ntrain_gmed = np.float32(train_gmed)\n\ndef _fill_z(df: pd.DataFrame, split_med: dict, gmed: np.float32):\n    z = df[\"Z\"].copy()\n    if z.isna().any():\n        z = z.fillna(df[\"split\"].map(split_med))\n        z = z.fillna(gmed)\n    return z.astype(\"float32\")\n\ndf_train[\"Z\"] = _fill_z(df_train, train_split_med, train_gmed)\ndf_test[\"Z\"]  = _fill_z(df_test,  train_split_med, train_gmed)\n\n# Z_err fill NaN -> 0 (train mostly 0, test sebagian ada)\ndf_train[\"Z_err\"] = df_train[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\ndf_test[\"Z_err\"]  = df_test[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n\n# photo-z indicator (dataset-level)\ndf_train[\"is_photoz\"] = np.int8(0)\ndf_test[\"is_photoz\"]  = np.int8(1)\n\n# ----------------------------\n# 9) Clipping + derived meta features\n# ----------------------------\n# EBV/Z clip pakai TRAIN (anti leakage)\nEBV_LO, EBV_HI = _qclip_bounds(df_train[\"EBV\"].values, 0.001, 0.999)\nZ_LO,   Z_HI   = _qclip_bounds(df_train[\"Z\"].values,   0.001, 0.999)\n\ndf_train[\"EBV_clip\"] = _safe_clip(df_train[\"EBV\"], EBV_LO, EBV_HI)\ndf_test[\"EBV_clip\"]  = _safe_clip(df_test[\"EBV\"],  EBV_LO, EBV_HI)\n\ndf_train[\"Z_clip\"] = _safe_clip(df_train[\"Z\"], Z_LO, Z_HI)\ndf_test[\"Z_clip\"]  = _safe_clip(df_test[\"Z\"],  Z_LO, Z_HI)\n\n# Z_err clip: PAKAI TEST quantiles supaya tidak 0..0\n# (tanpa label, aman)\n# guard: kalau test juga semua 0, tetap aman -> hi=0\nZE_LO = 0.0\n_, ZE_HI = _qclip_bounds(df_test[\"Z_err\"].values, 0.001, 0.999, default=(0.0, 0.0))\nZE_HI = max(float(ZE_HI), 0.0)\n\ndf_train[\"Zerr_clip\"] = _safe_clip(df_train[\"Z_err\"], ZE_LO, ZE_HI)\ndf_test[\"Zerr_clip\"]  = _safe_clip(df_test[\"Z_err\"],  ZE_LO, ZE_HI)\n\ndf_train[\"log1pZ\"] = np.log1p(df_train[\"Z_clip\"].astype(\"float32\")).astype(\"float32\")\ndf_test[\"log1pZ\"]  = np.log1p(df_test[\"Z_clip\"].astype(\"float32\")).astype(\"float32\")\n\ndf_train[\"log1pZerr\"] = np.log1p(df_train[\"Zerr_clip\"].astype(\"float32\")).astype(\"float32\")\ndf_test[\"log1pZerr\"]  = np.log1p(df_test[\"Zerr_clip\"].astype(\"float32\")).astype(\"float32\")\n\neps = np.float32(1e-6)\ndf_train[\"zerr_rel\"] = (df_train[\"Zerr_clip\"] / (df_train[\"Z_clip\"] + eps)).astype(\"float32\")\ndf_test[\"zerr_rel\"]  = (df_test[\"Zerr_clip\"]  / (df_test[\"Z_clip\"]  + eps)).astype(\"float32\")\n\nsplit2id = {f\"split_{i:02d}\": i for i in range(1, 21)}\ndf_train[\"split_id\"] = df_train[\"split\"].map(split2id).astype(\"int16\")\ndf_test[\"split_id\"]  = df_test[\"split\"].map(split2id).astype(\"int16\")\n\n# ----------------------------\n# 10) Fold assignment\n# ----------------------------\ndf_train[\"fold\"] = -1\n\nif CV_USE_SPLIT_COL:\n    # Quota per fold: 20 split / 5 fold = 4 split/fold\n    total_splits = len(SPLIT_LIST)\n    if total_splits % N_FOLDS != 0:\n        # tetap bisa jalan, tapi quota pakai ceil\n        quota = int(np.ceil(total_splits / N_FOLDS))\n    else:\n        quota = int(total_splits / N_FOLDS)\n\n    sp_stat = (\n        df_train.groupby(\"split\")[\"target\"]\n        .agg([\"count\",\"sum\"])\n        .rename(columns={\"count\":\"n\",\"sum\":\"pos\"})\n        .reindex(SPLIT_LIST)\n        .fillna(0)\n        .astype({\"n\":int,\"pos\":int})\n        .reset_index()\n    )\n    sp_stat[\"neg\"] = sp_stat[\"n\"] - sp_stat[\"pos\"]\n    sp_stat[\"pos_rate\"] = sp_stat[\"pos\"] / sp_stat[\"n\"].clip(lower=1)\n\n    # sort: split paling \"berat\" dulu\n    sp_stat = sp_stat.sort_values([\"pos\",\"n\"], ascending=False).reset_index(drop=True)\n\n    global_pos_rate = float(df_train[\"target\"].mean())\n    target_fold_n = float(len(df_train) / max(N_FOLDS, 1))\n\n    fold_n = np.zeros(N_FOLDS, dtype=float)\n    fold_pos = np.zeros(N_FOLDS, dtype=float)\n    fold_k = np.zeros(N_FOLDS, dtype=int)\n    split2fold = {}\n\n    rng = np.random.default_rng(SEED)\n\n    for _, r in sp_stat.iterrows():\n        sp = r[\"split\"]\n        n  = float(r[\"n\"])\n        p  = float(r[\"pos\"])\n\n        # candidate folds yang masih belum penuh quota split\n        cand = np.where(fold_k < quota)[0]\n        if len(cand) == 0:\n            # fallback: semua sudah quota (harusnya tidak terjadi jika quota benar)\n            cand = np.arange(N_FOLDS)\n\n        scores = []\n        for f in cand:\n            n2 = fold_n[f] + n\n            p2 = fold_pos[f] + p\n            pr2 = (p2 / n2) if n2 > 0 else global_pos_rate\n\n            # score = keseimbangan class + keseimbangan size + penalti kalau fold mendekati penuh\n            score = abs(pr2 - global_pos_rate) \\\n                    + 0.20 * abs(n2 - target_fold_n) / max(target_fold_n, 1.0) \\\n                    + 0.05 * (fold_k[f] / max(quota, 1))\n\n            scores.append(score)\n\n        scores = np.asarray(scores, dtype=float)\n        best_idx = np.where(scores == scores.min())[0]\n        choose = int(cand[int(rng.choice(best_idx))]) if len(best_idx) > 1 else int(cand[int(best_idx[0])])\n\n        split2fold[sp] = choose\n        fold_n[choose] += n\n        fold_pos[choose] += p\n        fold_k[choose] += 1\n\n    # apply\n    df_train[\"fold\"] = df_train[\"split\"].map(split2fold).astype(\"int16\")\n\n    # HARD GUARD: semua fold 0..K-1 harus muncul dan tidak kosong\n    uniq_folds = sorted(df_train[\"fold\"].unique().tolist())\n    if uniq_folds != list(range(N_FOLDS)):\n        print(f\"[WARN] split->fold tidak memakai semua fold. uniq_folds={uniq_folds}. Fallback ke StratifiedKFold object-level.\")\n        CV_USE_SPLIT_COL = False\n    else:\n        # save mapping untuk audit\n        with open(ART_DIR / \"split2fold.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump({k:int(v) for k,v in split2fold.items()}, f)\n\nif not CV_USE_SPLIT_COL:\n    try:\n        from sklearn.model_selection import StratifiedKFold\n        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n        y = df_train[\"target\"].to_numpy()\n        idx = np.arange(len(df_train))\n        for fold_id, (_, va_idx) in enumerate(skf.split(idx, y)):\n            df_train.iloc[va_idx, df_train.columns.get_loc(\"fold\")] = fold_id\n        df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n    except Exception as e:\n        print(f\"[WARN] StratifiedKFold unavailable ({type(e).__name__}). Using round-robin fallback.\")\n        df_train[\"fold\"] = -1\n        rng = np.random.default_rng(SEED)\n        pos_idx = df_train.index[df_train[\"target\"] == 1].to_numpy()\n        neg_idx = df_train.index[df_train[\"target\"] == 0].to_numpy()\n        rng.shuffle(pos_idx); rng.shuffle(neg_idx)\n        for j, ii in enumerate(pos_idx):\n            df_train.at[ii, \"fold\"] = int(j % N_FOLDS)\n        for j, ii in enumerate(neg_idx):\n            df_train.at[ii, \"fold\"] = int(j % N_FOLDS)\n        df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n\n# hard check\nif (df_train[\"fold\"] < 0).any():\n    n_bad = int((df_train[\"fold\"] < 0).sum())\n    raise RuntimeError(f\"Fold assignment gagal: ada {n_bad} baris fold=-1\")\n\n# ----------------------------\n# 11) Build meta tables (index=object_id)\n# ----------------------------\nkeep_train = [\n    \"object_id\",\"split\",\"split_id\",\n    \"EBV\",\"EBV_clip\",\"Z\",\"Z_clip\",\"log1pZ\",\n    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_rel\",\n    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n    \"fold\",\"target\"\n]\nkeep_test = [\n    \"object_id\",\"split\",\"split_id\",\n    \"EBV\",\"EBV_clip\",\"Z\",\"Z_clip\",\"log1pZ\",\n    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_rel\",\n    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\"\n]\n\nif \"SpecType\" in df_train.columns:\n    keep_train.append(\"SpecType\")\n\ndf_train_meta = df_train[keep_train].copy().set_index(\"object_id\", drop=True).sort_index()\ndf_test_meta  = df_test[keep_test].copy().set_index(\"object_id\", drop=True).sort_index()\n\nid2split_train = df_train_meta[\"split\"].to_dict()\nid2split_test  = df_test_meta[\"split\"].to_dict()\n\n# ----------------------------\n# 12) Save artifacts\n# ----------------------------\ntrain_pq = ART_DIR / \"train_meta.parquet\"\ntest_pq  = ART_DIR / \"test_meta.parquet\"\ntrain_csv = ART_DIR / \"train_meta.csv\"\ntest_csv  = ART_DIR / \"test_meta.csv\"\n\ntry:\n    df_train_meta.to_parquet(train_pq, index=True)\n    df_test_meta.to_parquet(test_pq, index=True)\n    saved_train, saved_test = str(train_pq), str(test_pq)\nexcept Exception:\n    df_train_meta.to_csv(train_csv, index=True)\n    df_test_meta.to_csv(test_csv, index=True)\n    saved_train, saved_test = str(train_csv), str(test_csv)\n\nsplit_stats = pd.DataFrame({\n    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n})\nsplit_stats.index.name = \"split\"\npos_by_split = df_train_meta.groupby(\"split\")[\"target\"].sum().reindex(SPLIT_LIST).fillna(0).astype(int)\nsplit_stats[\"train_pos\"] = pos_by_split.values\nsplit_stats[\"train_pos_rate\"] = (split_stats[\"train_pos\"] / split_stats[\"train_objects\"].clip(lower=1)).astype(\"float32\")\n\nsplit_stats_path = ART_DIR / \"split_stats.csv\"\nsplit_stats.to_csv(split_stats_path)\n\nfold_path = ART_DIR / \"train_folds.csv\"\ndf_train_meta.reset_index()[[\"object_id\",\"split\",\"fold\",\"target\"]].to_csv(fold_path, index=False)\n\nwith open(ART_DIR / \"id2split_train.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(id2split_train, f)\nwith open(ART_DIR / \"id2split_test.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(id2split_test, f)\n\n# ----------------------------\n# 13) Print summary\n# ----------------------------\npos = int((df_train_meta[\"target\"] == 1).sum())\nneg = int((df_train_meta[\"target\"] == 0).sum())\ntot = int(len(df_train_meta))\npos_rate = pos / max(tot, 1)\nscale_pos_weight = float(neg / max(pos, 1))\n\nprint(\"STAGE 2 OK — META READY (clean + folds)\")\nprint(f\"- CV_USE_SPLIT_COL: {CV_USE_SPLIT_COL} | N_FOLDS={N_FOLDS}\")\nprint(f\"- train objects: {tot:,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={pos_rate*100:.3f}%\")\nprint(f\"- test objects : {len(df_test_meta):,}\")\nprint(f\"- saved train  : {saved_train}\")\nprint(f\"- saved test   : {saved_test}\")\nprint(f\"- saved stats  : {split_stats_path}\")\nprint(f\"- saved folds  : {fold_path}\")\nprint(f\"- scale_pos_weight (neg/pos): {scale_pos_weight:.3f}\")\n\nprint(\"\\nCLIP RANGES\")\nprint(f\"- EBV clip (train): [{EBV_LO:.6f}, {EBV_HI:.6f}]\")\nprint(f\"- Z   clip (train): [{Z_LO:.6f}, {Z_HI:.6f}]\")\nprint(f\"- Zerr clip (test ): [{ZE_LO:.6f}, {ZE_HI:.6f}]\")\n\nfold_tab = (\n    df_train_meta.reset_index().groupby(\"fold\")[\"target\"]\n    .agg([\"count\",\"sum\"]).rename(columns={\"sum\":\"pos\"})\n    .reindex(range(N_FOLDS)).fillna(0)\n)\nfold_tab[\"pos_rate\"] = fold_tab[\"pos\"] / fold_tab[\"count\"].clip(lower=1)\nprint(\"\\nFOLD BALANCE (count/pos/pos_rate) — MUST SHOW 0..K-1\")\nprint(fold_tab.to_string())\n\n# ----------------------------\n# 14) Export globals\n# ----------------------------\nglobals().update({\n    \"df_train_meta\": df_train_meta,\n    \"df_test_meta\": df_test_meta,\n    \"id2split_train\": id2split_train,\n    \"id2split_test\": id2split_test,\n    \"split_stats\": split_stats,\n    \"split2id\": split2id,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"CV_USE_SPLIT_COL_USED\": CV_USE_SPLIT_COL,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:58:42.469764Z","iopub.execute_input":"2026-01-03T13:58:42.470089Z","iopub.status.idle":"2026-01-03T13:58:42.777942Z","shell.execute_reply.started":"2026-01-03T13:58:42.470052Z","shell.execute_reply":"2026-01-03T13:58:42.776595Z"}},"outputs":[{"name":"stdout","text":"STAGE 2 OK — META READY (clean + folds)\n- CV_USE_SPLIT_COL: True | N_FOLDS=5\n- train objects: 3,043 | pos(TDE)=148 | neg=2,895 | pos%=4.864%\n- test objects : 7,135\n- saved train  : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/train_meta.parquet\n- saved test   : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/test_meta.parquet\n- saved stats  : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/split_stats.csv\n- saved folds  : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/train_folds.csv\n- scale_pos_weight (neg/pos): 19.561\n\nCLIP RANGES\n- EBV clip (train): [0.005042, 0.581790]\n- Z   clip (train): [0.044923, 4.032352]\n- Zerr clip (test ): [0.000000, 0.106846]\n\nFOLD BALANCE (count/pos/pos_rate) — MUST SHOW 0..K-1\n      count  pos  pos_rate\nfold                      \n0       607   20  0.032949\n1       565    6  0.010619\n2       632   53  0.083861\n3       641   27  0.042122\n4       598   42  0.070234\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"64"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Lightcurve Loading Strategy","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — Robust Lightcurve Loader Utilities (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v3.1 (FIX IndexError + routing by split lebih aman)\n#\n# FIX utama:\n# - groupby().groups menghasilkan label object_id (string), bukan integer positions\n#   -> jangan pakai df.index[idx]; pakai idx langsung / idx.astype(str).tolist()\n# ============================================================\n\nimport gc, re\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"CFG\", \"SEED\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n\nART_DIR = Path(ART_DIR)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nSEED = int(SEED)\nMIN_FLUXERR = float(CFG.get(\"MIN_FLUXERR\", 1e-6))\n\n# konsisten dengan STAGE 0/2\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\nREQ_LC_KEYS = [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]\nALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\nFILTER_ORDER = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\n\n# ----------------------------\n# 1) Build split file mapping (train/test lightcurves)\n# ----------------------------\nSPLIT_FILES = {}\nfor s in SPLIT_LIST:\n    sd = Path(SPLIT_DIRS[s])\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    if (not tr.exists()) or (not te.exists()):\n        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n\n# Save split file manifest\nmanifest = []\nfor s in SPLIT_LIST:\n    p_tr = SPLIT_FILES[s][\"train\"]\n    p_te = SPLIT_FILES[s][\"test\"]\n    manifest.append({\n        \"split\": s,\n        \"train_path\": str(p_tr),\n        \"test_path\": str(p_te),\n        \"train_mb\": float(p_tr.stat().st_size) / (1024**2),\n        \"test_mb\":  float(p_te.stat().st_size) / (1024**2),\n    })\ndf_manifest = pd.DataFrame(manifest).sort_values(\"split\")\nmanifest_path = ART_DIR / \"split_file_manifest.csv\"\ndf_manifest.to_csv(manifest_path, index=False)\n\n# ----------------------------\n# 2) Build object routing by split (FIX IndexError)\n# ----------------------------\ntrain_ids_by_split = {s: [] for s in SPLIT_LIST}\ntest_ids_by_split  = {s: [] for s in SPLIT_LIST}\n\n# groups: split -> Index(labels=object_id)\ntr_groups = df_train_meta.groupby(\"split\").groups\nte_groups = df_test_meta.groupby(\"split\").groups\n\nfor sp, idx in tr_groups.items():\n    if sp in train_ids_by_split:\n        # idx sudah berisi object_id labels\n        train_ids_by_split[sp] = pd.Index(idx).astype(str).tolist()\n\nfor sp, idx in te_groups.items():\n    if sp in test_ids_by_split:\n        test_ids_by_split[sp] = pd.Index(idx).astype(str).tolist()\n\n# sanity\nif sum(len(v) for v in train_ids_by_split.values()) != len(df_train_meta):\n    raise RuntimeError(\"Routing train_ids_by_split mismatch total vs df_train_meta length.\")\nif sum(len(v) for v in test_ids_by_split.values()) != len(df_test_meta):\n    raise RuntimeError(\"Routing test_ids_by_split mismatch total vs df_test_meta length.\")\n\ndf_counts = pd.DataFrame({\n    \"split\": SPLIT_LIST,\n    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n})\ncounts_path = ART_DIR / \"object_counts_by_split.csv\"\ndf_counts.to_csv(counts_path, index=False)\n\n# ----------------------------\n# 3) Robust header mapping -> canonical columns\n# ----------------------------\n_LC_CFG_CACHE = {}  # (split_name, which) -> cfg dict\n\ndef _canon_col(x: str) -> str:\n    s = str(x).strip().lower()\n    s = s.replace(\"\\ufeff\", \"\")\n    s = re.sub(r\"\\s+\", \"\", s)\n    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n    s = s.replace(\"-\", \"_\")\n    return s\n\ndef _build_lc_read_cfg(p: Path):\n    h = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n    orig_cols = list(h.columns)\n\n    c2o = {}\n    for c in orig_cols:\n        k = _canon_col(c)\n        if k not in c2o:\n            c2o[k] = c\n\n    obj_col = c2o.get(\"object_id\", None)\n\n    time_col = None\n    for k in [\"time_mjd\", \"timemjd\", \"mjd\", \"time\"]:\n        if k in c2o:\n            time_col = c2o[k]\n            break\n\n    flux_col = c2o.get(\"flux\", None)\n\n    ferr_col = None\n    for k in [\"flux_err\", \"fluxerr\", \"fluxerror\"]:\n        if k in c2o:\n            ferr_col = c2o[k]\n            break\n\n    filt_col = c2o.get(\"filter\", None)\n\n    missing = []\n    if obj_col is None:  missing.append(\"object_id\")\n    if time_col is None: missing.append(\"Time (MJD)\")\n    if flux_col is None: missing.append(\"Flux\")\n    if ferr_col is None: missing.append(\"Flux_err\")\n    if filt_col is None: missing.append(\"Filter\")\n    if missing:\n        raise ValueError(\n            f\"Missing required lightcurve columns in {p.name}: {missing}. \"\n            f\"Header sample: {orig_cols[:20]}\"\n        )\n\n    usecols = [obj_col, time_col, flux_col, ferr_col, filt_col]\n    rename = {obj_col:\"object_id\", time_col:\"mjd\", flux_col:\"flux\", ferr_col:\"flux_err\", filt_col:\"filter\"}\n\n    # enforce string only for id/filter (numeric we'll coerce later)\n    dtypes = {obj_col:\"string\", filt_col:\"string\"}\n\n    return {\"usecols\": usecols, \"dtype\": dtypes, \"rename\": rename}\n\ndef _normalize_lc_chunk(df: pd.DataFrame, drop_bad_filter: bool = True, drop_bad_mjd: bool = True):\n    df = df[[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"]].copy()\n\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n    df[\"filter\"] = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n\n    df.loc[~df[\"filter\"].isin(list(ALLOWED_FILTERS)), \"filter\"] = pd.NA\n\n    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(\"float32\")\n    df[\"flux\"] = pd.to_numeric(df[\"flux\"], errors=\"coerce\").astype(\"float32\")\n    df[\"flux_err\"] = pd.to_numeric(df[\"flux_err\"], errors=\"coerce\").astype(\"float32\")\n\n    fe = df[\"flux_err\"]\n    if MIN_FLUXERR > 0:\n        df.loc[fe.notna() & (fe < MIN_FLUXERR), \"flux_err\"] = np.float32(MIN_FLUXERR)\n\n    df = df[df[\"object_id\"].notna() & (df[\"object_id\"] != \"\")]\n    if drop_bad_filter:\n        df = df[df[\"filter\"].notna()]\n    if drop_bad_mjd:\n        df = df[df[\"mjd\"].notna()]\n\n    return df[REQ_LC_KEYS]\n\n# ----------------------------\n# 4) Chunked readers\n# ----------------------------\ndef iter_lightcurve_chunks(\n    split_name: str,\n    which: str,\n    chunksize: int = 400_000,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True\n):\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Unknown split_name={split_name}.\")\n    if which not in (\"train\", \"test\"):\n        raise ValueError(\"which must be 'train' or 'test'\")\n\n    p = SPLIT_FILES[split_name][which]\n    key = (split_name, which)\n    if key not in _LC_CFG_CACHE:\n        _LC_CFG_CACHE[key] = _build_lc_read_cfg(p)\n    cfg = _LC_CFG_CACHE[key]\n\n    reader = pd.read_csv(\n        p,\n        usecols=cfg[\"usecols\"],\n        dtype=cfg[\"dtype\"],\n        chunksize=int(chunksize),\n        **SAFE_READ_KW\n    )\n    for chunk in reader:\n        chunk = chunk.rename(columns=cfg[\"rename\"])\n        yield _normalize_lc_chunk(chunk, drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd)\n\ndef load_object_lightcurve(\n    object_id: str,\n    which: str,\n    chunksize: int = 400_000,\n    sort_time: bool = True,\n    max_chunks: int = None,\n    stop_after_found_block: bool = True\n):\n    object_id = str(object_id).strip()\n\n    if which == \"train\":\n        if object_id not in df_train_meta.index:\n            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n        split_name = str(df_train_meta.loc[object_id, \"split\"])\n    elif which == \"test\":\n        if object_id not in df_test_meta.index:\n            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n        split_name = str(df_test_meta.loc[object_id, \"split\"])\n    else:\n        raise ValueError(\"which must be 'train' or 'test'\")\n\n    pieces = []\n    seen = 0\n    found_any = False\n    last_hit = False\n\n    for ch in iter_lightcurve_chunks(split_name, which, chunksize=chunksize):\n        seen += 1\n        sub = ch[ch[\"object_id\"] == object_id]\n        hit = (len(sub) > 0)\n        if hit:\n            pieces.append(sub)\n            found_any = True\n\n        if stop_after_found_block and found_any and last_hit and (not hit):\n            break\n        last_hit = hit\n\n        if max_chunks is not None and seen >= int(max_chunks):\n            break\n\n    if not pieces:\n        out = pd.DataFrame(columns=REQ_LC_KEYS)\n    else:\n        out = pd.concat(pieces, ignore_index=True)\n        if sort_time and len(out) > 1:\n            out[\"filter_ord\"] = out[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n            out = out.sort_values([\"mjd\", \"filter_ord\"], kind=\"mergesort\").drop(columns=[\"filter_ord\"]).reset_index(drop=True)\n\n    return out\n\n# ----------------------------\n# 5) Quick smoke test\n# ----------------------------\n_smoke_splits = [\"split_01\", \"split_08\", \"split_17\"]\nfor s in _smoke_splits:\n    if len(train_ids_by_split.get(s, [])) == 0 or len(test_ids_by_split.get(s, [])) == 0:\n        raise RuntimeError(f\"Split {s} has 0 objects in train/test meta (unexpected).\")\n\n    ch_tr = next(iter_lightcurve_chunks(s, \"train\", chunksize=50_000))\n    ch_te = next(iter_lightcurve_chunks(s, \"test\",  chunksize=50_000))\n\n    if list(ch_tr.columns) != REQ_LC_KEYS:\n        raise RuntimeError(f\"Train chunk schema mismatch in {s}: {list(ch_tr.columns)}\")\n    if list(ch_te.columns) != REQ_LC_KEYS:\n        raise RuntimeError(f\"Test chunk schema mismatch in {s}: {list(ch_te.columns)}\")\n\n    badf_tr = sorted(set(ch_tr[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n    badf_te = sorted(set(ch_te[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n    if badf_tr or badf_te:\n        raise ValueError(f\"Unexpected filter values in smoke chunk split={s}: train_bad={badf_tr} test_bad={badf_te}\")\n\nprint(\"STAGE 3 OK — LIGHTCURVE LOADING UTILITIES READY\")\nprint(f\"- Saved manifest: {manifest_path}\")\nprint(f\"- Saved counts  : {counts_path}\")\n\nglobals().update({\n    \"SPLIT_FILES\": SPLIT_FILES,\n    \"train_ids_by_split\": train_ids_by_split,\n    \"test_ids_by_split\": test_ids_by_split,\n    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n    \"load_object_lightcurve\": load_object_lightcurve,\n    \"REQ_LC_KEYS\": REQ_LC_KEYS,\n    \"ALLOWED_FILTERS\": ALLOWED_FILTERS,\n    \"FILTER_ORDER\": FILTER_ORDER,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:13:10.548619Z","iopub.execute_input":"2026-01-03T14:13:10.548955Z","iopub.status.idle":"2026-01-03T14:13:11.209243Z","shell.execute_reply.started":"2026-01-03T14:13:10.548928Z","shell.execute_reply":"2026-01-03T14:13:11.208072Z"}},"outputs":[{"name":"stdout","text":"STAGE 3 OK — LIGHTCURVE LOADING UTILITIES READY\n- Saved manifest: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/split_file_manifest.csv\n- Saved counts  : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/object_counts_by_split.csv\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"303"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — Photometric Cleaning (FORCE OVERWRITE) — REVISI FULL v6.2\n# FIX v6.2:\n# - Safety guard: Path.is_relative_to (anti false-positive substring)\n# - Atomic parquet: tmp file tetap .parquet (part_0000.tmp.parquet)\n# - Filter normalization tanpa np.char (aman untuk <NA>/mixed dtype)\n# - EBV pakai EBV_clip jika tersedia (lebih stabil), fallback EBV\n# ============================================================\n\nimport gc, json, warnings, time, shutil\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n\nART_DIR = Path(ART_DIR)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 1) Settings\n# ----------------------------\nCHUNKSIZE   = 350_000\nERR_EPS     = 1e-6\nSNR_DET     = 3.0\nDET_SIGMA   = 3.0\n\nMIN_FLUX_POS_UJY   = 1e-6\nMAG_MIN, MAG_MAX   = -10.0, 50.0\nMAGERR_FLOOR_DET   = 1e-3\nMAGERR_FLOOR_ND    = 0.75\nMAGERR_CAP         = 10.0\n\nWRITE_FORMAT = \"parquet\"   # \"parquet\" or \"csv.gz\"\nONLY_SPLITS  = None        # e.g. [\"split_01\"] untuk test cepat\nKEEP_FLUX_DEBUG = False\nDROP_BAD_TIME_ROWS = True\n\n# FORCE overwrite\nREBUILD_MODE = \"wipe_all\"  # \"wipe_all\" | \"wipe_parts_only\"\n\n# ----------------------------\n# 2) Extinction coefficients (placeholder; ganti kalau punya nilai resmi)\n# ----------------------------\nEXT_RLAMBDA = {\"u\": 4.8, \"g\": 3.6, \"r\": 2.7, \"i\": 2.1, \"z\": 1.6, \"y\": 1.3}\nBAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\nID2BAND = {v: k for k, v in BAND2ID.items()}\n\n# pakai EBV_clip jika ada (lebih stabil), fallback EBV\nEBV_TRAIN_SER = df_train_meta[\"EBV_clip\"] if \"EBV_clip\" in df_train_meta.columns else df_train_meta[\"EBV\"]\nEBV_TEST_SER  = df_test_meta[\"EBV_clip\"]  if \"EBV_clip\"  in df_test_meta.columns  else df_test_meta[\"EBV\"]\n\nMAG_ZP = float(2.5 * np.log10(3631e6))  # ~23.9\n\n# ----------------------------\n# 3) Output root + WIPE (with safety guard)\n# ----------------------------\nLC_CLEAN_DIR = ART_DIR / \"lc_clean_mag\"\n\nart_abs = ART_DIR.resolve()\nlc_abs  = LC_CLEAN_DIR.resolve()\n\n# robust safety guard\ntry:\n    ok_rel = lc_abs.is_relative_to(art_abs)\nexcept AttributeError:\n    # very old python fallback (shouldn't happen on Kaggle py3.12)\n    ok_rel = str(lc_abs).startswith(str(art_abs) + \"/\") or str(lc_abs).startswith(str(art_abs) + \"\\\\\")\n\nif not ok_rel:\n    raise RuntimeError(f\"Safety guard failed: LC_CLEAN_DIR bukan turunan ART_DIR.\\nART_DIR={art_abs}\\nLC_CLEAN_DIR={lc_abs}\")\n\nif REBUILD_MODE == \"wipe_all\":\n    if LC_CLEAN_DIR.exists():\n        shutil.rmtree(LC_CLEAN_DIR, ignore_errors=True)\n    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\nelif REBUILD_MODE == \"wipe_parts_only\":\n    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\nelse:\n    raise ValueError(\"REBUILD_MODE must be 'wipe_all' or 'wipe_parts_only'\")\n\n# ----------------------------\n# 4) Atomic writer (tmp -> rename)\n# ----------------------------\ndef _atomic_write_parquet(df: pd.DataFrame, out_path: Path):\n    # tmp tetap berakhiran .parquet agar engine tidak rewel\n    tmp = out_path.with_name(out_path.stem + \".tmp\" + out_path.suffix)  # part_0000.tmp.parquet\n    try:\n        df.to_parquet(tmp, index=False)\n        tmp.replace(out_path)\n    finally:\n        # cleanup kalau gagal sebelum replace\n        if tmp.exists() and (not out_path.exists()):\n            try:\n                tmp.unlink()\n            except Exception:\n                pass\n\ndef _atomic_write_csv_gz(df: pd.DataFrame, out_path: Path):\n    final_path = out_path.with_suffix(\".csv.gz\")\n    tmp = final_path.with_name(final_path.stem + \".tmp\" + \"\".join(final_path.suffixes))  # keep .csv.gz\n    try:\n        df.to_csv(tmp, index=False, compression=\"gzip\")\n        tmp.replace(final_path)\n    finally:\n        if tmp.exists() and (not final_path.exists()):\n            try:\n                tmp.unlink()\n            except Exception:\n                pass\n    return final_path\n\ndef write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    if fmt == \"parquet\":\n        try:\n            _atomic_write_parquet(df, out_path)\n            return \"parquet\", out_path\n        except Exception as e:\n            alt = _atomic_write_csv_gz(df, out_path)\n            return f\"csv.gz (fallback from parquet: {type(e).__name__})\", alt\n    elif fmt == \"csv.gz\":\n        alt = _atomic_write_csv_gz(df, out_path)\n        return \"csv.gz\", alt\n    else:\n        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n\n# ----------------------------\n# 5) Core cleaning (NaN/negative-safe)\n# ----------------------------\ndef clean_chunk_to_mag(ch: pd.DataFrame, ebv_ser: pd.Series):\n    # normalize id/filter safely (tanpa np.char)\n    oid_ser = ch[\"object_id\"].astype(\"string\").str.strip()\n    filt_ser = ch[\"filter\"].astype(\"string\").str.strip().str.lower()\n\n    # numeric\n    mjd = ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n    flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n    err  = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)\n\n    # sanitize err\n    err = np.nan_to_num(err, nan=np.float32(ERR_EPS), posinf=np.float32(ERR_EPS), neginf=np.float32(ERR_EPS))\n    err = np.maximum(err, np.float32(ERR_EPS))\n\n    # sanitize flux: inf -> NaN (NaN tetap)\n    flux = flux.astype(np.float32, copy=False)\n    flux[~np.isfinite(flux)] = np.float32(np.nan)\n\n    # band_id mapping (vectorized masks; aman walau filt_ser ada <NA>)\n    filt = filt_ser.fillna(\"\").to_numpy(dtype=object, copy=False)\n    band_id = np.full(len(ch), -1, dtype=np.int8)\n    for b, bid in BAND2ID.items():\n        band_id[filt == b] = np.int8(bid)\n\n    if np.any(band_id < 0):\n        bad = pd.Series(filt[band_id < 0]).value_counts().head(10).index.tolist()\n        raise ValueError(f\"Unknown/invalid filter values encountered (top examples): {bad}\")\n\n    # EBV lookup (missing -> 0.0)\n    ebv = oid_ser.map(ebv_ser).fillna(0.0).to_numpy(dtype=np.float32)\n    ebv[~np.isfinite(ebv)] = np.float32(0.0)\n\n    # R_lambda lookup\n    rlam = np.zeros(len(ch), dtype=np.float32)\n    for b, rv in EXT_RLAMBDA.items():\n        rlam[filt == b] = np.float32(rv)\n\n    A = (rlam * ebv).astype(np.float32)\n    mul = np.power(np.float32(10.0), (np.float32(0.4) * A)).astype(np.float32)\n\n    flux_deext = (flux * mul).astype(np.float32)\n    err_deext  = (err  * mul).astype(np.float32)\n\n    # snr (NaN flux -> 0)\n    okf = np.isfinite(flux_deext)\n    snr = np.zeros_like(err_deext, dtype=np.float32)\n    snr[okf] = (flux_deext[okf] / np.maximum(err_deext[okf], np.float32(ERR_EPS))).astype(np.float32)\n\n    detected = (snr > np.float32(SNR_DET)).astype(np.int8)\n\n    nan_flux_rows = int((~okf).sum())\n    if nan_flux_rows:\n        detected[~okf] = np.int8(0)\n        snr[~okf] = np.float32(0.0)\n\n    flux_detlim = (np.float32(DET_SIGMA) * err_deext).astype(np.float32)\n\n    flux_for_mag = np.where(\n        detected == 1,\n        np.maximum(flux_deext, np.float32(MIN_FLUX_POS_UJY)),\n        np.maximum(flux_detlim, np.float32(MIN_FLUX_POS_UJY)),\n    ).astype(np.float32)\n\n    mag = (np.float32(MAG_ZP) - np.float32(2.5) * np.log10(flux_for_mag)).astype(np.float32)\n    mag = np.clip(mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32)\n\n    mag_err = (np.float32(1.0857362) * (err_deext / flux_for_mag)).astype(np.float32)\n    mag_err = np.clip(mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32)\n\n    if MAGERR_FLOOR_ND is not None and float(MAGERR_FLOOR_ND) > 0:\n        mag_err = np.where(\n            detected == 1,\n            mag_err,\n            np.maximum(mag_err, np.float32(MAGERR_FLOOR_ND))\n        ).astype(np.float32)\n\n    out = pd.DataFrame({\n        \"object_id\": pd.array(oid_ser.to_numpy(copy=False), dtype=\"string\"),\n        \"mjd\": mjd.astype(np.float32, copy=False),\n        \"band_id\": band_id.astype(np.int8, copy=False),\n        \"mag\": mag.astype(np.float32, copy=False),\n        \"mag_err\": mag_err.astype(np.float32, copy=False),\n        \"snr\": snr.astype(np.float32, copy=False),\n        \"detected\": detected.astype(np.int8, copy=False),\n    })\n\n    dropped_time = 0\n    if DROP_BAD_TIME_ROWS:\n        t = out[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n        keep = np.isfinite(t)\n        dropped_time = int((~keep).sum())\n        if dropped_time:\n            out = out[keep]\n\n    if KEEP_FLUX_DEBUG:\n        out[\"flux_deext\"] = pd.Series(np.nan_to_num(flux_deext, nan=0.0), dtype=\"float32\")\n        out[\"err_deext\"]  = pd.Series(err_deext, dtype=\"float32\")\n\n    return out, dropped_time, nan_flux_rows\n\n# ----------------------------\n# 6) Process split-wise\n# ----------------------------\nsplits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else list(SPLIT_LIST)\n\nsummary_rows, manifest_rows = [], []\n\ndef _wipe_parts_dir(out_dir: Path):\n    if out_dir.exists():\n        for pat in [\"part_*.parquet\", \"part_*.csv.gz\", \"*.tmp\", \"*.tmp.parquet\", \"*.tmp.csv.gz\"]:\n            for f in out_dir.glob(pat):\n                try:\n                    f.unlink()\n                except Exception:\n                    pass\n\ndef process_split(split_name: str, which: str):\n    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n    out_dir = LC_CLEAN_DIR / split_name / which\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    if REBUILD_MODE == \"wipe_parts_only\":\n        _wipe_parts_dir(out_dir)\n\n    t0 = time.time()\n    part_idx = 0\n    n_rows_total = 0\n    n_det = 0\n    n_finite_mag = 0\n    mag_min = np.inf\n    mag_max = -np.inf\n    dropped_time_total = 0\n    nan_flux_total = 0\n\n    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n        cleaned, dropped_time, nan_flux = clean_chunk_to_mag(ch, ebv_ser)\n\n        dropped_time_total += int(dropped_time)\n        nan_flux_total += int(nan_flux)\n\n        n_rows = int(len(cleaned))\n        n_rows_total += n_rows\n\n        det_arr = cleaned[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n        n_det += int(det_arr.sum())\n\n        mag_arr = cleaned[\"mag\"].to_numpy(dtype=np.float32, copy=False)\n        fin = np.isfinite(mag_arr)\n        n_finite_mag += int(fin.sum())\n        if fin.any():\n            mag_min = float(min(mag_min, float(np.min(mag_arr[fin]))))\n            mag_max = float(max(mag_max, float(np.max(mag_arr[fin]))))\n\n        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n        used_fmt, final_path = write_part(cleaned, out_path, WRITE_FORMAT)\n\n        manifest_rows.append({\n            \"split\": split_name,\n            \"which\": which,\n            \"part\": int(part_idx),\n            \"path\": str(final_path),\n            \"rows\": int(n_rows),\n            \"format\": str(used_fmt),\n        })\n\n        part_idx += 1\n        del cleaned, ch\n        if part_idx % 10 == 0:\n            gc.collect()\n\n    dt = time.time() - t0\n    summary_rows.append({\n        \"split\": split_name,\n        \"which\": which,\n        \"parts\": int(part_idx),\n        \"rows\": int(n_rows_total),\n        \"det_frac_snr_gt_thr\": float(n_det / max(n_rows_total, 1)),\n        \"finite_mag_frac\": float(n_finite_mag / max(n_rows_total, 1)),\n        \"mag_min\": (mag_min if np.isfinite(mag_min) else np.nan),\n        \"mag_max\": (mag_max if np.isfinite(mag_max) else np.nan),\n        \"dropped_time_rows\": int(dropped_time_total),\n        \"nan_flux_rows\": int(nan_flux_total),\n        \"sec\": float(dt),\n    })\n\n    print(\n        f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | \"\n        f\"det%={100*(n_det/max(n_rows_total,1)):.2f}% | \"\n        f\"nan_flux={nan_flux_total:,} | drop_time={dropped_time_total:,} | \"\n        f\"mag_range=[{(mag_min if np.isfinite(mag_min) else np.nan):.2f}, {(mag_max if np.isfinite(mag_max) else np.nan):.2f}] | \"\n        f\"time={dt:.1f}s\"\n    )\n\nprint(f\"[Stage 4] REBUILD_MODE={REBUILD_MODE} | Writing to: {LC_CLEAN_DIR}\")\nfor s in splits_to_use:\n    process_split(s, \"train\")\n    process_split(s, \"test\")\n\n# ----------------------------\n# 7) Save manifests + summary + config\n# ----------------------------\ndf_parts_manifest = pd.DataFrame(manifest_rows)\ndf_summary  = pd.DataFrame(summary_rows)\n\nmanifest_path = LC_CLEAN_DIR / \"lc_clean_mag_manifest.csv\"\nsummary_path  = LC_CLEAN_DIR / \"lc_clean_mag_summary.csv\"\n\ndf_parts_manifest.to_csv(manifest_path, index=False)\ndf_summary.to_csv(summary_path, index=False)\n\ncfg_path = LC_CLEAN_DIR / \"photometric_config_mag.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\n        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n        \"SNR_DET\": float(SNR_DET),\n        \"DET_SIGMA\": float(DET_SIGMA),\n        \"ERR_EPS\": float(ERR_EPS),\n        \"MIN_FLUX_POS_UJY\": float(MIN_FLUX_POS_UJY),\n        \"MAG_ZP\": float(MAG_ZP),\n        \"MAG_MIN\": float(MAG_MIN),\n        \"MAG_MAX\": float(MAG_MAX),\n        \"MAGERR_FLOOR_DET\": float(MAGERR_FLOOR_DET),\n        \"MAGERR_FLOOR_ND\": float(MAGERR_FLOOR_ND),\n        \"MAGERR_CAP\": float(MAGERR_CAP),\n        \"CHUNKSIZE\": int(CHUNKSIZE),\n        \"WRITE_FORMAT\": str(WRITE_FORMAT),\n        \"ONLY_SPLITS\": list(splits_to_use),\n        \"KEEP_FLUX_DEBUG\": bool(KEEP_FLUX_DEBUG),\n        \"DROP_BAD_TIME_ROWS\": bool(DROP_BAD_TIME_ROWS),\n        \"REBUILD_MODE\": str(REBUILD_MODE),\n        \"EBV_SOURCE\": (\"EBV_clip\" if (\"EBV_clip\" in df_train_meta.columns and \"EBV_clip\" in df_test_meta.columns) else \"EBV\"),\n    }, f, indent=2)\n\nprint(\"\\n[Stage 4] Done.\")\nprint(f\"- LC_CLEAN_DIR  : {LC_CLEAN_DIR}\")\nprint(f\"- Saved manifest: {manifest_path}\")\nprint(f\"- Saved summary : {summary_path}\")\nprint(f\"- Saved config  : {cfg_path}\")\n\n# ----------------------------\n# 8) Helper for next stages\n# ----------------------------\ndef get_clean_parts(split_name: str, which: str):\n    m = df_parts_manifest[(df_parts_manifest[\"split\"] == split_name) & (df_parts_manifest[\"which\"] == which)].sort_values(\"part\")\n    return m[\"path\"].astype(str).tolist()\n\nglobals().update({\n    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n    \"BAND2ID\": BAND2ID,\n    \"ID2BAND\": ID2BAND,\n    \"MAG_ZP\": MAG_ZP,\n    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n    \"lc_clean_mag_manifest\": df_parts_manifest,\n    \"lc_clean_mag_summary\": df_summary,\n    \"get_clean_parts\": get_clean_parts,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:18:10.166103Z","iopub.execute_input":"2026-01-03T14:18:10.166430Z","iopub.status.idle":"2026-01-03T14:18:15.453444Z","shell.execute_reply.started":"2026-01-03T14:18:10.166407Z","shell.execute_reply":"2026-01-03T14:18:15.452880Z"}},"outputs":[{"name":"stdout","text":"[Stage 4] REBUILD_MODE=wipe_all | Writing to: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/lc_clean_mag\n[Stage 4] split_01/train: parts=1 | rows=26,324 | det%=19.34% | nan_flux=11 | drop_time=0 | mag_range=[19.72, 25.96] | time=0.1s\n[Stage 4] split_01/test: parts=1 | rows=59,235 | det%=23.02% | nan_flux=23 | drop_time=0 | mag_range=[19.61, 26.20] | time=0.2s\n[Stage 4] split_02/train: parts=1 | rows=25,609 | det%=24.45% | nan_flux=6 | drop_time=0 | mag_range=[20.10, 26.04] | time=0.1s\n[Stage 4] split_02/test: parts=1 | rows=71,229 | det%=21.69% | nan_flux=8 | drop_time=0 | mag_range=[18.77, 26.32] | time=0.2s\n[Stage 4] split_03/train: parts=1 | rows=21,676 | det%=21.65% | nan_flux=5 | drop_time=0 | mag_range=[20.17, 26.23] | time=0.1s\n[Stage 4] split_03/test: parts=1 | rows=53,751 | det%=21.90% | nan_flux=8 | drop_time=0 | mag_range=[19.61, 26.37] | time=0.2s\n[Stage 4] split_04/train: parts=1 | rows=22,898 | det%=21.11% | nan_flux=12 | drop_time=0 | mag_range=[20.38, 26.16] | time=0.1s\n[Stage 4] split_04/test: parts=1 | rows=51,408 | det%=21.70% | nan_flux=50 | drop_time=0 | mag_range=[19.66, 26.25] | time=0.2s\n[Stage 4] split_05/train: parts=1 | rows=25,934 | det%=18.33% | nan_flux=15 | drop_time=0 | mag_range=[18.82, 26.33] | time=0.1s\n[Stage 4] split_05/test: parts=1 | rows=61,179 | det%=18.21% | nan_flux=42 | drop_time=0 | mag_range=[19.09, 26.23] | time=0.2s\n[Stage 4] split_06/train: parts=1 | rows=25,684 | det%=18.85% | nan_flux=11 | drop_time=0 | mag_range=[19.80, 26.15] | time=0.1s\n[Stage 4] split_06/test: parts=1 | rows=57,620 | det%=19.94% | nan_flux=21 | drop_time=0 | mag_range=[19.28, 26.04] | time=0.2s\n[Stage 4] split_07/train: parts=1 | rows=24,473 | det%=21.44% | nan_flux=306 | drop_time=0 | mag_range=[20.05, 26.32] | time=0.1s\n[Stage 4] split_07/test: parts=1 | rows=65,101 | det%=19.10% | nan_flux=904 | drop_time=0 | mag_range=[19.97, 26.32] | time=0.2s\n[Stage 4] split_08/train: parts=1 | rows=25,571 | det%=22.80% | nan_flux=6 | drop_time=0 | mag_range=[18.59, 26.30] | time=0.1s\n[Stage 4] split_08/test: parts=1 | rows=61,498 | det%=24.50% | nan_flux=47 | drop_time=0 | mag_range=[19.13, 25.96] | time=0.2s\n[Stage 4] split_09/train: parts=1 | rows=19,690 | det%=21.13% | nan_flux=315 | drop_time=0 | mag_range=[19.83, 26.31] | time=0.1s\n[Stage 4] split_09/test: parts=1 | rows=47,239 | det%=22.70% | nan_flux=514 | drop_time=0 | mag_range=[18.88, 26.11] | time=0.1s\n[Stage 4] split_10/train: parts=1 | rows=25,151 | det%=20.86% | nan_flux=7 | drop_time=0 | mag_range=[19.52, 26.21] | time=0.1s\n[Stage 4] split_10/test: parts=1 | rows=51,056 | det%=21.21% | nan_flux=51 | drop_time=0 | mag_range=[16.62, 26.43] | time=0.2s\n[Stage 4] split_11/train: parts=1 | rows=22,927 | det%=19.59% | nan_flux=4 | drop_time=0 | mag_range=[20.57, 26.42] | time=0.1s\n[Stage 4] split_11/test: parts=1 | rows=49,723 | det%=20.17% | nan_flux=12 | drop_time=0 | mag_range=[20.31, 26.42] | time=0.2s\n[Stage 4] split_12/train: parts=1 | rows=25,546 | det%=19.64% | nan_flux=12 | drop_time=0 | mag_range=[20.40, 26.32] | time=0.1s\n[Stage 4] split_12/test: parts=1 | rows=54,499 | det%=19.29% | nan_flux=13 | drop_time=0 | mag_range=[19.98, 26.26] | time=0.2s\n[Stage 4] split_13/train: parts=1 | rows=23,203 | det%=20.64% | nan_flux=2 | drop_time=0 | mag_range=[19.46, 26.05] | time=0.1s\n[Stage 4] split_13/test: parts=1 | rows=63,653 | det%=19.56% | nan_flux=15 | drop_time=0 | mag_range=[18.30, 26.15] | time=0.2s\n[Stage 4] split_14/train: parts=1 | rows=25,706 | det%=20.36% | nan_flux=23 | drop_time=0 | mag_range=[20.52, 26.40] | time=0.1s\n[Stage 4] split_14/test: parts=1 | rows=58,643 | det%=17.91% | nan_flux=43 | drop_time=0 | mag_range=[20.41, 26.04] | time=0.2s\n[Stage 4] split_15/train: parts=1 | rows=23,972 | det%=19.09% | nan_flux=116 | drop_time=0 | mag_range=[20.11, 26.04] | time=0.1s\n[Stage 4] split_15/test: parts=1 | rows=52,943 | det%=20.03% | nan_flux=197 | drop_time=0 | mag_range=[19.91, 26.24] | time=0.2s\n[Stage 4] split_16/train: parts=1 | rows=25,173 | det%=21.42% | nan_flux=3 | drop_time=0 | mag_range=[20.00, 26.26] | time=0.1s\n[Stage 4] split_16/test: parts=1 | rows=58,192 | det%=20.12% | nan_flux=13 | drop_time=0 | mag_range=[19.57, 26.17] | time=0.2s\n[Stage 4] split_17/train: parts=1 | rows=22,705 | det%=22.09% | nan_flux=12 | drop_time=0 | mag_range=[19.64, 26.17] | time=0.1s\n[Stage 4] split_17/test: parts=1 | rows=59,482 | det%=19.59% | nan_flux=21 | drop_time=0 | mag_range=[19.96, 26.42] | time=0.2s\n[Stage 4] split_18/train: parts=1 | rows=21,536 | det%=23.77% | nan_flux=7 | drop_time=0 | mag_range=[20.49, 26.05] | time=0.1s\n[Stage 4] split_18/test: parts=1 | rows=53,887 | det%=23.88% | nan_flux=14 | drop_time=0 | mag_range=[20.60, 26.44] | time=0.2s\n[Stage 4] split_19/train: parts=1 | rows=22,087 | det%=23.73% | nan_flux=8 | drop_time=0 | mag_range=[19.98, 26.31] | time=0.1s\n[Stage 4] split_19/test: parts=1 | rows=56,355 | det%=24.17% | nan_flux=16 | drop_time=0 | mag_range=[20.30, 26.34] | time=0.2s\n[Stage 4] split_20/train: parts=1 | rows=23,519 | det%=20.45% | nan_flux=10 | drop_time=0 | mag_range=[19.80, 25.93] | time=0.1s\n[Stage 4] split_20/test: parts=1 | rows=58,432 | det%=19.65% | nan_flux=10 | drop_time=0 | mag_range=[19.87, 26.36] | time=0.2s\n\n[Stage 4] Done.\n- LC_CLEAN_DIR  : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/lc_clean_mag\n- Saved manifest: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n- Saved summary : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/lc_clean_mag/lc_clean_mag_summary.csv\n- Saved config  : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/lc_clean_mag/photometric_config_mag.json\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"273"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Sequence Tokenization (Event-based Tokens)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v5.2 (PATH+META SYNC HARDENED + MISSING-OBJECT SAFE + BUCKET ROBUST)\n#\n# FIX UTAMA v5.2:\n# - Auto-find STAGE 4 manifest dari run manapun.\n# - Sync path benar: RUN_DIR/ART_DIR/LC_CLEAN_DIR dari manifest.\n# - Auto-reload df_train_meta/df_test_meta dari ART_DIR synced jika mismatch.\n# - Validasi semua part path exists.\n# - Jika ada object_id tidak muncul di cleaned parts -> build EMPTY sequence (len=1) agar built==expected.\n# - Bucket writer aman (try/finally close) + cleanup tmp dir rmtree.\n#\n# OUTPUT:\n# - artifacts/seq_tokens/split_XX/{train|test}/shard_*.npz\n# - artifacts/seq_tokens/seq_manifest_{train|test}.csv\n# - artifacts/seq_tokens/seq_build_stats.csv\n# - artifacts/seq_tokens/seq_config.json\n# ============================================================\n\nimport gc, json, warnings, time, shutil\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nfor need in [\"ART_DIR\", \"df_train_meta\", \"df_test_meta\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 0 + STAGE 2 dulu (ART_DIR + meta).\")\n\nART_DIR = Path(ART_DIR)\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\ndef _safe_string_series(s: pd.Series) -> pd.Series:\n    try:\n        return s.astype(\"string\").str.strip()\n    except Exception:\n        return s.astype(str).str.strip()\n\ndef _find_stage4_manifest(art_dir: Path):\n    cand = art_dir / \"lc_clean_mag\" / \"lc_clean_mag_manifest.csv\"\n    if cand.exists():\n        return cand\n\n    root = Path(\"/kaggle/working/mallorn_run\")\n    if not root.exists():\n        return None\n\n    cands = list(root.glob(\"run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\"))\n    if not cands:\n        cands = list(root.glob(\"run_*/**/lc_clean_mag_manifest.csv\"))\n    if not cands:\n        return None\n\n    cands = sorted(cands, key=lambda p: p.stat().st_mtime, reverse=True)\n    return cands[0]\n\ndef _sync_dirs_from_manifest(manifest_csv: Path):\n    lc_clean_dir = manifest_csv.parent\n    art_dir_new  = lc_clean_dir.parent\n    run_dir_new  = art_dir_new.parent\n    return run_dir_new, art_dir_new, lc_clean_dir\n\ndef _load_meta_if_needed(art_dir_synced: Path):\n    \"\"\"\n    Jika df_train_meta/df_test_meta yg ada di memori mismatch dengan file meta di run synced,\n    maka reload dari file agar routing split/object_id konsisten.\n    \"\"\"\n    global df_train_meta, df_test_meta\n\n    tr_pq = art_dir_synced / \"train_meta.parquet\"\n    te_pq = art_dir_synced / \"test_meta.parquet\"\n    tr_csv = art_dir_synced / \"train_meta.csv\"\n    te_csv = art_dir_synced / \"test_meta.csv\"\n\n    # kalau file meta tidak ada, pakai yang di memori\n    if not (tr_pq.exists() or tr_csv.exists()) or not (te_pq.exists() or te_csv.exists()):\n        return False, \"meta file not found in synced ART_DIR; keep in-memory\"\n\n    def _read_meta(pq, csv):\n        if pq.exists():\n            return pd.read_parquet(pq).set_index(\"object_id\") if \"object_id\" in pd.read_parquet(pq, columns=None).columns else pd.read_parquet(pq)\n        else:\n            return pd.read_csv(csv).set_index(\"object_id\")\n\n    # load candidate (safe, minimal)\n    try:\n        if tr_pq.exists():\n            cand_train = pd.read_parquet(tr_pq)\n        else:\n            cand_train = pd.read_csv(tr_csv)\n\n        if te_pq.exists():\n            cand_test = pd.read_parquet(te_pq)\n        else:\n            cand_test = pd.read_csv(te_csv)\n\n        # ensure index=object_id\n        if \"object_id\" in cand_train.columns:\n            cand_train = cand_train.set_index(\"object_id\", drop=True)\n        if \"object_id\" in cand_test.columns:\n            cand_test = cand_test.set_index(\"object_id\", drop=True)\n\n        cand_train.index = cand_train.index.astype(\"string\")\n        cand_test.index = cand_test.index.astype(\"string\")\n\n        # mismatch check: size + a few ids overlap\n        mem_train_n = int(len(df_train_meta))\n        mem_test_n  = int(len(df_test_meta))\n        cand_train_n = int(len(cand_train))\n        cand_test_n  = int(len(cand_test))\n\n        if (mem_train_n != cand_train_n) or (mem_test_n != cand_test_n):\n            df_train_meta = cand_train\n            df_test_meta = cand_test\n            return True, f\"reloaded meta due to size mismatch: mem({mem_train_n},{mem_test_n}) -> file({cand_train_n},{cand_test_n})\"\n\n        # also check sample ids\n        sample_ids = df_train_meta.index[:5].astype(str).tolist()\n        ok = all((sid in cand_train.index) for sid in sample_ids)\n        if not ok:\n            df_train_meta = cand_train\n            df_test_meta = cand_test\n            return True, \"reloaded meta due to id mismatch\"\n\n        return False, \"meta already consistent\"\n    except Exception as e:\n        return False, f\"meta reload skipped (error: {type(e).__name__}: {e})\"\n\n# ----------------------------\n# 2) Locate STAGE 4 output (robust)\n# ----------------------------\nmanifest_csv = _find_stage4_manifest(ART_DIR)\nif manifest_csv is None:\n    root = Path(\"/kaggle/working/mallorn_run\")\n    runs = sorted([p.name for p in root.glob(\"run_*\") if p.is_dir()])[-15:] if root.exists() else []\n    raise RuntimeError(\n        \"Output STAGE 4 (lc_clean_mag_manifest.csv) tidak ditemukan.\\n\"\n        f\"- ART_DIR saat ini: {ART_DIR}\\n\"\n        f\"- Expected: {ART_DIR/'lc_clean_mag'}\\n\"\n        f\"- Runs available (last 15): {runs}\\n\"\n        \"Solusi: pastikan STAGE 4 benar-benar selesai dan menulis artifacts/lc_clean_mag.\"\n    )\n\nRUN_DIR, ART_DIR, LC_CLEAN_DIR = _sync_dirs_from_manifest(manifest_csv)\n\nprint(\"STAGE 5 ROUTING SYNC OK\")\nprint(f\"- RUN_DIR      : {RUN_DIR}\")\nprint(f\"- ART_DIR      : {ART_DIR}\")\nprint(f\"- LC_CLEAN_DIR : {LC_CLEAN_DIR}\")\nprint(f\"- manifest_csv : {manifest_csv}\")\n\nreloaded, msg = _load_meta_if_needed(ART_DIR)\nprint(f\"- meta_sync    : {msg}\")\n\n# ----------------------------\n# 3) Load & validate Stage4 manifest\n# ----------------------------\n_df_clean_manifest = pd.read_csv(manifest_csv)\n_df_clean_manifest.columns = [c.strip() for c in _df_clean_manifest.columns]\n\nneed_cols = {\"split\", \"which\", \"part\", \"path\"}\nmiss = sorted(list(need_cols - set(_df_clean_manifest.columns)))\nif miss:\n    raise RuntimeError(f\"Manifest STAGE 4 missing columns: {miss} | cols={list(_df_clean_manifest.columns)}\")\n\npaths = _df_clean_manifest[\"path\"].astype(str).tolist()\nmissing_paths = [p for p in paths if not Path(p).exists()]\nif missing_paths:\n    ex = missing_paths[:10]\n    raise RuntimeError(\n        \"Ada file part STAGE 4 yang hilang (manifest ada tapi file tidak ada).\\n\"\n        f\"Missing count={len(missing_paths)} | contoh={ex}\\n\"\n        \"Solusi: rerun STAGE 4 dengan mode rebuild/wipe untuk regenerasi cache.\"\n    )\n\ndef get_clean_parts(split_name: str, which: str):\n    m = _df_clean_manifest[(_df_clean_manifest[\"split\"] == split_name) & (_df_clean_manifest[\"which\"] == which)]\n    if m.empty:\n        return []\n    return m.sort_values(\"part\")[\"path\"].astype(str).tolist()\n\n# ----------------------------\n# 4) Recover SPLIT_LIST + routing ids (force stable)\n# ----------------------------\nSPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\nsplits_in_manifest = sorted(set(_df_clean_manifest[\"split\"].astype(str).tolist()))\n# pakai intersection agar tidak nyasar split yang tidak ada part-nya\nSPLITS_TO_CONSIDER = [s for s in SPLIT_LIST if s in splits_in_manifest]\n\ntrain_ids_by_split = {s: [] for s in SPLIT_LIST}\nfor oid, sp in df_train_meta[\"split\"].astype(str).items():\n    train_ids_by_split[str(sp)].append(str(oid))\n\ntest_ids_by_split = {s: [] for s in SPLIT_LIST}\nfor oid, sp in df_test_meta[\"split\"].astype(str).items():\n    test_ids_by_split[str(sp)].append(str(oid))\n\n# ----------------------------\n# 5) Settings\n# ----------------------------\nONLY_SPLITS = None                 # None=all; contoh: [\"split_01\"] untuk test cepat\nREBUILD_MODE = \"wipe_all\"          # \"wipe_all\" or \"reuse_if_exists\"\n\nCOMPRESS_NPZ = False\nSHARD_MAX_OBJECTS = 1500\n\nSNR_TANH_SCALE = 10.0\nTIME_CLIP_MAX_DAYS = None\nDROP_BAD_TIME_ROWS = True\n\nL_MAX = int(CFG.get(\"L_MAX\", 256)) if \"CFG\" in globals() else 256\nTRUNC_POLICY = str(CFG.get(\"TRUNC_POLICY\", \"smart\")) if \"CFG\" in globals() else \"smart\"  # smart/head/none\nKEEP_DET_FRAC = float(CFG.get(\"KEEP_DET_FRAC\", 0.70)) if \"CFG\" in globals() else 0.70\nKEEP_EDGE = True\nUSE_RESTFRAME_TIME = bool(CFG.get(\"USE_RESTFRAME_TIME\", True)) if \"CFG\" in globals() else True\n\nNUM_BUCKETS = 64\n\nSEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\nSEQ_DIR.mkdir(parents=True, exist_ok=True)\n\nTOKEN_MODE = None\nFEATURE_NAMES = None\nFEATURE_DIM = None\n\nBASE_COLS = {\"object_id\", \"mjd\", \"band_id\", \"snr\", \"detected\"}\nMODE_COLS = {\"mag\": {\"mag\", \"mag_err\"}, \"asinh\": {\"flux_asinh\", \"err_log1p\"}}\n\n# ----------------------------\n# 6) Reader for cleaned parts\n# ----------------------------\ndef _read_clean_part(path: str) -> pd.DataFrame:\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(f\"Clean part missing: {p}\")\n\n    if p.suffix == \".parquet\":\n        df = pd.read_parquet(p)\n    elif p.name.endswith(\".csv.gz\"):\n        df = pd.read_csv(p, compression=\"gzip\")\n    else:\n        df = pd.read_csv(p)\n\n    df.columns = [c.strip() for c in df.columns]\n\n    global TOKEN_MODE, FEATURE_NAMES, FEATURE_DIM\n    if TOKEN_MODE is None:\n        cols = set(df.columns)\n        if BASE_COLS.issubset(cols) and MODE_COLS[\"mag\"].issubset(cols):\n            TOKEN_MODE = \"mag\"\n            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"mag\", \"mag_err_log\", \"snr_tanh\", \"detected\"]\n        elif BASE_COLS.issubset(cols) and MODE_COLS[\"asinh\"].issubset(cols):\n            TOKEN_MODE = \"asinh\"\n            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\n        else:\n            raise RuntimeError(\n                \"Cannot detect cleaned schema.\\n\"\n                f\"Found cols={list(df.columns)}\\n\"\n                \"Expected MAG or ASINH schema from STAGE 4.\"\n            )\n        FEATURE_DIM = len(FEATURE_NAMES)\n\n    req = set(BASE_COLS) | set(MODE_COLS[TOKEN_MODE])\n    miss = sorted(list(req - set(df.columns)))\n    if miss:\n        raise RuntimeError(f\"Clean part missing columns: {miss} | file={p}\")\n\n    df[\"object_id\"] = _safe_string_series(df[\"object_id\"])\n    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n    df[\"snr\"] = pd.to_numeric(df[\"snr\"], errors=\"coerce\").astype(np.float32)\n    df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n\n    if TOKEN_MODE == \"mag\":\n        df[\"mag\"] = pd.to_numeric(df[\"mag\"], errors=\"coerce\").astype(np.float32)\n        df[\"mag_err\"] = pd.to_numeric(df[\"mag_err\"], errors=\"coerce\").astype(np.float32)\n    else:\n        df[\"flux_asinh\"] = pd.to_numeric(df[\"flux_asinh\"], errors=\"coerce\").astype(np.float32)\n        df[\"err_log1p\"] = pd.to_numeric(df[\"err_log1p\"], errors=\"coerce\").astype(np.float32)\n\n    if DROP_BAD_TIME_ROWS:\n        df = df[np.isfinite(df[\"mjd\"].to_numpy())]\n\n    return df\n\n# ----------------------------\n# 7) Truncation\n# ----------------------------\ndef _smart_truncate(mjd, det, snr, Lmax: int):\n    n = len(mjd)\n    if n <= Lmax:\n        return np.arange(n, dtype=np.int64)\n\n    idx_all = np.arange(n, dtype=np.int64)\n    keep = set()\n    if KEEP_EDGE:\n        keep.add(0); keep.add(n - 1)\n\n    det_idx = idx_all[det.astype(bool)]\n    k_det = int(max(0, min(len(det_idx), int(np.floor(Lmax * KEEP_DET_FRAC)))))\n    if k_det > 0 and len(det_idx) > 0:\n        score = np.abs(snr[det_idx])\n        top = det_idx[np.argsort(-score)[:k_det]]\n        for i in top.tolist():\n            keep.add(int(i))\n\n    if len(keep) < Lmax:\n        rem = [i for i in idx_all.tolist() if i not in keep]\n        need = Lmax - len(keep)\n        if rem and need > 0:\n            pick = np.linspace(0, len(rem) - 1, num=need, dtype=int)\n            for p in pick.tolist():\n                keep.add(int(rem[p]))\n\n    out = np.array(sorted(keep), dtype=np.int64)\n    if len(out) > Lmax:\n        pos = np.linspace(0, len(out) - 1, num=Lmax, dtype=int)\n        out = out[pos]\n    return out\n\n# ----------------------------\n# 8) Build tokens per object (empty-safe)\n# ----------------------------\ndef build_empty_tokens():\n    # token kosong (len=1) agar object tetap ada\n    X = np.zeros((1, int(FEATURE_DIM)), dtype=np.float32)\n    B = np.full((1,), -1, dtype=np.int8)\n    return X, B, 0, 1\n\ndef build_object_tokens(df_obj: pd.DataFrame, z_val: float = 0.0):\n    if df_obj is None or df_obj.empty:\n        return build_empty_tokens()\n\n    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n\n    order = np.lexsort((band, mjd))\n    mjd = mjd[order]; band = band[order]; snr = snr[order]; det = det[order]\n\n    z = float(z_val) if (z_val is not None and np.isfinite(z_val)) else 0.0\n    denom = (1.0 + max(z, 0.0)) if USE_RESTFRAME_TIME else 1.0\n\n    t0 = mjd[0]\n    t_rel = (mjd - t0) / np.float32(denom)\n    dt = np.empty_like(t_rel); dt[0] = 0.0\n    if len(t_rel) > 1:\n        dt[1:] = np.maximum(t_rel[1:] - t_rel[:-1], 0.0)\n\n    if TIME_CLIP_MAX_DAYS is not None:\n        mx = np.float32(TIME_CLIP_MAX_DAYS)\n        t_rel = np.clip(t_rel, 0.0, mx)\n        dt    = np.clip(dt,    0.0, mx)\n\n    t_rel_log = np.log1p(t_rel).astype(np.float32)\n    dt_log    = np.log1p(dt).astype(np.float32)\n\n    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n    det_f = det.astype(np.float32)\n\n    if TOKEN_MODE == \"mag\":\n        mag = df_obj[\"mag\"].to_numpy(dtype=np.float32, copy=False)[order]\n        mag_err = df_obj[\"mag_err\"].to_numpy(dtype=np.float32, copy=False)[order]\n        mag = np.nan_to_num(mag, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        mag_err = np.nan_to_num(mag_err, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        mag_err = np.maximum(mag_err, np.float32(0.0))\n        mag_err_log = np.log1p(mag_err).astype(np.float32)\n        X = np.stack([t_rel_log, dt_log, mag, mag_err_log, snr_tanh, det_f], axis=1).astype(np.float32)\n    else:\n        flux = df_obj[\"flux_asinh\"].to_numpy(dtype=np.float32, copy=False)[order]\n        elog = df_obj[\"err_log1p\"].to_numpy(dtype=np.float32, copy=False)[order]\n        flux = np.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        elog = np.nan_to_num(elog, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        X = np.stack([t_rel_log, dt_log, flux, elog, snr_tanh, det_f], axis=1).astype(np.float32)\n\n    L0 = int(X.shape[0])\n    if L_MAX and int(L_MAX) > 0 and X.shape[0] > int(L_MAX):\n        if TRUNC_POLICY == \"smart\":\n            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n        elif TRUNC_POLICY == \"head\":\n            keep = np.arange(int(L_MAX), dtype=np.int64)\n        else:\n            keep = np.arange(X.shape[0], dtype=np.int64)\n\n        if len(keep) != X.shape[0]:\n            X = X[keep]\n            band = band[keep]\n\n            # recompute dt_log (stabil)\n            sel_mjd = mjd[keep]\n            sel_t = (sel_mjd - sel_mjd[0]) / np.float32(denom)\n            sel_dt = np.empty_like(sel_t); sel_dt[0] = 0.0\n            if len(sel_t) > 1:\n                sel_dt[1:] = np.maximum(sel_t[1:] - sel_t[:-1], 0.0)\n            X[:, 1] = np.log1p(sel_dt).astype(np.float32)\n\n    return X, band.astype(np.int8), L0, int(X.shape[0])\n\n# ----------------------------\n# 9) Shard writer\n# ----------------------------\ndef save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    obj_arr = np.asarray(object_ids, dtype=\"S\")\n    if COMPRESS_NPZ:\n        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n    else:\n        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n\n# ----------------------------\n# 10) Robust builder: bucketize -> groupby object -> shard (missing-safe)\n# ----------------------------\ndef build_sequences_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 64):\n    try:\n        import pyarrow as pa\n        import pyarrow.parquet as pq\n    except Exception as e:\n        raise RuntimeError(\"pyarrow tidak tersedia. Di Kaggle biasanya ada.\") from e\n\n    parts = get_clean_parts(split_name, which)\n    if not parts:\n        raise RuntimeError(f\"Tidak ada cleaned parts untuk {split_name}/{which}. Cek STAGE 4 output.\")\n\n    tmp_dir = Path(ART_DIR) / \"tmp_seq_buckets\" / split_name / which\n    if tmp_dir.exists():\n        shutil.rmtree(tmp_dir, ignore_errors=True)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n\n    writers = {}\n    kept_rows = 0\n    t0 = time.time()\n\n    def bucket_idx(series_objid: pd.Series) -> np.ndarray:\n        h = pd.util.hash_pandas_object(series_objid, index=False).to_numpy(dtype=np.uint64, copy=False)\n        return (h % np.uint64(num_buckets)).astype(np.int16)\n\n    try:\n        # 1) write buckets\n        for p in parts:\n            df = _read_clean_part(p)\n            if df.empty:\n                continue\n\n            df = df[df[\"object_id\"].isin(expected_ids)]\n            if df.empty:\n                continue\n\n            kept_rows += int(len(df))\n            bidx = bucket_idx(df[\"object_id\"])\n            df[\"_b\"] = bidx\n\n            for b in np.unique(bidx):\n                sub = df[df[\"_b\"] == b].drop(columns=[\"_b\"])\n                if sub.empty:\n                    continue\n                fp = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n                table = pa.Table.from_pandas(sub, preserve_index=False)\n                if int(b) not in writers:\n                    writers[int(b)] = pq.ParquetWriter(fp, table.schema, compression=\"snappy\")\n                writers[int(b)].write_table(table)\n\n            del df\n            gc.collect()\n\n    finally:\n        for w in list(writers.values()):\n            try:\n                w.close()\n            except Exception:\n                pass\n\n    meta = df_train_meta if which == \"train\" else df_test_meta\n\n    manifest_rows = []\n    shard_idx = 0\n    batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n    built_ids = set()\n\n    len_before, len_after = [], []\n\n    def flush_shard_local():\n        nonlocal shard_idx, batch_obj_ids, batch_X, batch_B, batch_len, manifest_rows\n        if not batch_obj_ids:\n            return\n        lengths = np.asarray(batch_len, dtype=np.int64)\n        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n        offsets[1:] = np.cumsum(lengths)\n\n        Xc = np.concatenate(batch_X, axis=0).astype(np.float32)\n        Bc = np.concatenate(batch_B, axis=0).astype(np.int8)\n\n        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n        save_shard(shard_path, batch_obj_ids, Xc, Bc, offsets)\n\n        for i, oid in enumerate(batch_obj_ids):\n            manifest_rows.append({\n                \"object_id\": oid,\n                \"split\": split_name,\n                \"which\": which,\n                \"shard\": str(shard_path),\n                \"start\": int(offsets[i]),\n                \"length\": int(lengths[i]),\n            })\n\n        shard_idx += 1\n        batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n        gc.collect()\n\n    # 2) read buckets -> groupby object\n    for bf in sorted(tmp_dir.glob(\"bucket_*.parquet\")):\n        dfb = pd.read_parquet(bf)\n        if dfb.empty:\n            continue\n\n        for oid, g in dfb.groupby(\"object_id\", sort=False):\n            oid = str(oid)\n            if oid in built_ids:\n                continue\n\n            z_val = float(meta.loc[oid, \"Z\"]) if (USE_RESTFRAME_TIME and oid in meta.index) else 0.0\n            X, B, lb, la = build_object_tokens(g, z_val=z_val)\n\n            len_before.append(lb)\n            len_after.append(la)\n\n            batch_obj_ids.append(oid)\n            batch_X.append(X)\n            batch_B.append(B)\n            batch_len.append(int(X.shape[0]))\n            built_ids.add(oid)\n\n            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                flush_shard_local()\n\n        del dfb\n        gc.collect()\n\n    # 3) fill missing objects (empty sequences)\n    missing_ids = list(expected_ids - built_ids)\n    if missing_ids:\n        for oid in missing_ids:\n            oid = str(oid)\n            X, B, lb, la = build_empty_tokens()\n            len_before.append(lb)\n            len_after.append(la)\n\n            batch_obj_ids.append(oid)\n            batch_X.append(X)\n            batch_B.append(B)\n            batch_len.append(int(X.shape[0]))\n            built_ids.add(oid)\n\n            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                flush_shard_local()\n\n    flush_shard_local()\n\n    # cleanup tmp\n    shutil.rmtree(tmp_dir, ignore_errors=True)\n\n    st = {\n        \"kept_rows\": int(kept_rows),\n        \"built_objects\": int(len(built_ids)),\n        \"missing_filled\": int(len(missing_ids)),\n        \"len_before_mean\": float(np.mean(len_before)) if len_before else 0.0,\n        \"len_before_p95\": float(np.quantile(len_before, 0.95)) if len_before else 0.0,\n        \"len_after_mean\": float(np.mean(len_after)) if len_after else 0.0,\n        \"len_after_p95\": float(np.quantile(len_after, 0.95)) if len_after else 0.0,\n        \"truncated_frac\": float(np.mean([a < b for a, b in zip(len_after, len_before)])) if len_before else 0.0,\n        \"time_s\": float(time.time() - t0),\n    }\n    return manifest_rows, st\n\n# ----------------------------\n# 11) RUN\n# ----------------------------\nsplits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLITS_TO_CONSIDER\nall_manifest_train, all_manifest_test, split_run_stats = [], [], []\n\ndef expected_set_for(split_name: str, which: str) -> set:\n    return set(train_ids_by_split.get(split_name, [])) if which == \"train\" else set(test_ids_by_split.get(split_name, []))\n\nfor split_name in splits_to_run:\n    for which in [\"train\", \"test\"]:\n        out_dir = SEQ_DIR / split_name / which\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        expected_ids = expected_set_for(split_name, which)\n        if len(expected_ids) == 0:\n            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}.\")\n\n        shard_exists = any(out_dir.glob(\"shard_*.npz\"))\n        if REBUILD_MODE == \"reuse_if_exists\" and shard_exists:\n            print(f\"\\n[Stage 5] SKIP (exists): {split_name}/{which}\")\n            continue\n        else:\n            for f in out_dir.glob(\"shard_*.npz\"):\n                try: f.unlink()\n                except Exception: pass\n\n        print(f\"\\n[Stage 5] {split_name}/{which} | expected={len(expected_ids):,} | L_MAX={L_MAX} | TRUNC={TRUNC_POLICY}\")\n\n        manifest_rows, st = build_sequences_bucket(\n            split_name=split_name,\n            which=which,\n            expected_ids=expected_ids,\n            out_dir=out_dir,\n            num_buckets=NUM_BUCKETS\n        )\n\n        print(f\"[Stage 5] OK: built={st['built_objects']:,} (missing_filled={st['missing_filled']:,}) | \"\n              f\"kept_rows={st['kept_rows']:,} | \"\n              f\"len_mean {st['len_before_mean']:.1f}->{st['len_after_mean']:.1f} | \"\n              f\"p95 {st['len_before_p95']:.1f}->{st['len_after_p95']:.1f} | \"\n              f\"trunc%={st['truncated_frac']*100:.1f}% | \"\n              f\"time={st['time_s']:.2f}s | mode={TOKEN_MODE}\")\n\n        split_run_stats.append({\"split\": split_name, \"which\": which, **st})\n\n        if which == \"train\":\n            all_manifest_train.extend(manifest_rows)\n        else:\n            all_manifest_test.extend(manifest_rows)\n\n        gc.collect()\n\n# ----------------------------\n# 12) Save manifests + stats + config\n# ----------------------------\ndf_m_train = pd.DataFrame(all_manifest_train).sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\ndf_m_test  = pd.DataFrame(all_manifest_test).sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n\nmtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\nmtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\ndf_m_train.to_csv(mtrain_path, index=False)\ndf_m_test.to_csv(mtest_path, index=False)\n\ndf_stats = pd.DataFrame(split_run_stats)\nstats_path = SEQ_DIR / \"seq_build_stats.csv\"\ndf_stats.to_csv(stats_path, index=False)\n\ncfg = {\n    \"token_mode\": TOKEN_MODE,\n    \"feature_names\": FEATURE_NAMES,\n    \"feature_dim\": int(FEATURE_DIM),\n    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n    \"compress_npz\": bool(COMPRESS_NPZ),\n    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n    \"num_buckets\": int(NUM_BUCKETS),\n    \"L_MAX\": int(L_MAX),\n    \"TRUNC_POLICY\": str(TRUNC_POLICY),\n    \"KEEP_DET_FRAC\": float(KEEP_DET_FRAC),\n    \"USE_RESTFRAME_TIME\": bool(USE_RESTFRAME_TIME),\n    \"REBUILD_MODE\": str(REBUILD_MODE),\n    \"RUN_DIR_USED\": str(RUN_DIR),\n    \"ART_DIR_USED\": str(ART_DIR),\n    \"LC_CLEAN_DIR_USED\": str(LC_CLEAN_DIR),\n    \"manifest_csv\": str(manifest_csv),\n}\ncfg_path = SEQ_DIR / \"seq_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(cfg, f, indent=2)\n\nprint(\"\\n[Stage 5] DONE\")\nprint(f\"- token_mode : {TOKEN_MODE}\")\nprint(f\"- features   : {FEATURE_NAMES}\")\nprint(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\nprint(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\nprint(f\"- Saved: {stats_path}\")\nprint(f\"- Saved: {cfg_path}\")\n\n# ----------------------------\n# 13) Smoke test\n# ----------------------------\ndef load_sequence(object_id: str, which: str):\n    object_id = str(object_id).strip()\n    m = df_m_train if which == \"train\" else df_m_test\n    row = m[m[\"object_id\"] == object_id]\n    if row.empty:\n        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n    r = row.iloc[0]\n    data = np.load(r[\"shard\"], allow_pickle=False)\n    start = int(r[\"start\"]); length = int(r[\"length\"])\n    X = data[\"x\"][start:start+length]\n    B = data[\"band\"][start:start+length]\n    return X, B\n\n_smoke_oid = str(df_train_meta.index[0])\nX_sm, B_sm = load_sequence(_smoke_oid, \"train\")\nprint(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\nprint(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n\n# ----------------------------\n# 14) Export globals\n# ----------------------------\nglobals().update({\n    \"RUN_DIR\": RUN_DIR,\n    \"ART_DIR\": ART_DIR,\n    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n    \"SEQ_DIR\": SEQ_DIR,\n    \"seq_manifest_train\": df_m_train,\n    \"seq_manifest_test\": df_m_test,\n    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n    \"SEQ_FEATURE_DIM\": int(FEATURE_DIM),\n    \"SEQ_TOKEN_MODE\": TOKEN_MODE,\n    \"get_clean_parts\": get_clean_parts,\n    \"load_sequence\": load_sequence,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:28:45.389499Z","iopub.execute_input":"2026-01-03T14:28:45.389795Z","iopub.status.idle":"2026-01-03T14:35:15.983963Z","shell.execute_reply.started":"2026-01-03T14:28:45.389772Z","shell.execute_reply":"2026-01-03T14:35:15.983196Z"}},"outputs":[{"name":"stdout","text":"STAGE 5 ROUTING SYNC OK\n- RUN_DIR      : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76\n- ART_DIR      : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts\n- LC_CLEAN_DIR : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/lc_clean_mag\n- manifest_csv : /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n- meta_sync    : meta already consistent\n\n[Stage 5] split_01/train | expected=155 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=26,324 | len_mean 169.8->145.3 | p95 191.7->191.7 | trunc%=3.9% | time=9.83s | mode=mag\n\n[Stage 5] split_01/test | expected=364 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=364 (missing_filled=0) | kept_rows=59,235 | len_mean 162.7->148.3 | p95 193.8->193.8 | trunc%=2.2% | time=10.13s | mode=mag\n\n[Stage 5] split_02/train | expected=170 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=170 (missing_filled=0) | kept_rows=25,609 | len_mean 150.6->145.9 | p95 195.5->195.5 | trunc%=1.2% | time=9.14s | mode=mag\n\n[Stage 5] split_02/test | expected=414 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=414 (missing_filled=0) | kept_rows=71,229 | len_mean 172.1->149.5 | p95 201.0->201.0 | trunc%=3.4% | time=9.90s | mode=mag\n\n[Stage 5] split_03/train | expected=138 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=138 (missing_filled=0) | kept_rows=21,676 | len_mean 157.1->145.1 | p95 191.8->191.8 | trunc%=2.2% | time=8.48s | mode=mag\n\n[Stage 5] split_03/test | expected=338 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=338 (missing_filled=0) | kept_rows=53,751 | len_mean 159.0->147.0 | p95 194.3->194.3 | trunc%=2.4% | time=10.20s | mode=mag\n\n[Stage 5] split_04/train | expected=145 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=145 (missing_filled=0) | kept_rows=22,898 | len_mean 157.9->139.7 | p95 187.0->187.0 | trunc%=2.8% | time=8.66s | mode=mag\n\n[Stage 5] split_04/test | expected=332 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=332 (missing_filled=0) | kept_rows=51,408 | len_mean 154.8->140.9 | p95 195.0->195.0 | trunc%=3.0% | time=9.69s | mode=mag\n\n[Stage 5] split_05/train | expected=165 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=165 (missing_filled=0) | kept_rows=25,934 | len_mean 157.2->146.9 | p95 191.6->191.6 | trunc%=2.4% | time=9.59s | mode=mag\n\n[Stage 5] split_05/test | expected=375 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=375 (missing_filled=0) | kept_rows=61,179 | len_mean 163.1->146.0 | p95 194.0->194.0 | trunc%=2.7% | time=9.95s | mode=mag\n\n[Stage 5] split_06/train | expected=155 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,684 | len_mean 165.7->145.8 | p95 198.0->198.0 | trunc%=3.2% | time=8.67s | mode=mag\n\n[Stage 5] split_06/test | expected=374 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=374 (missing_filled=0) | kept_rows=57,620 | len_mean 154.1->143.4 | p95 189.3->189.3 | trunc%=1.9% | time=9.89s | mode=mag\n\n[Stage 5] split_07/train | expected=165 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=165 (missing_filled=0) | kept_rows=24,473 | len_mean 148.3->146.9 | p95 191.6->191.6 | trunc%=0.6% | time=9.41s | mode=mag\n\n[Stage 5] split_07/test | expected=398 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=398 (missing_filled=0) | kept_rows=65,101 | len_mean 163.6->147.9 | p95 194.1->194.1 | trunc%=2.3% | time=9.83s | mode=mag\n\n[Stage 5] split_08/train | expected=162 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=162 (missing_filled=0) | kept_rows=25,571 | len_mean 157.8->144.1 | p95 185.9->185.9 | trunc%=1.9% | time=9.28s | mode=mag\n\n[Stage 5] split_08/test | expected=387 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=387 (missing_filled=0) | kept_rows=61,498 | len_mean 158.9->148.4 | p95 194.4->194.4 | trunc%=1.6% | time=10.60s | mode=mag\n\n[Stage 5] split_09/train | expected=128 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=128 (missing_filled=0) | kept_rows=19,690 | len_mean 153.8->144.8 | p95 192.3->192.3 | trunc%=1.6% | time=8.40s | mode=mag\n\n[Stage 5] split_09/test | expected=289 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=289 (missing_filled=0) | kept_rows=47,239 | len_mean 163.5->147.4 | p95 196.0->196.0 | trunc%=3.1% | time=9.90s | mode=mag\n\n[Stage 5] split_10/train | expected=144 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=144 (missing_filled=0) | kept_rows=25,151 | len_mean 174.7->150.1 | p95 204.4->204.4 | trunc%=4.2% | time=9.40s | mode=mag\n\n[Stage 5] split_10/test | expected=331 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=331 (missing_filled=0) | kept_rows=51,056 | len_mean 154.2->143.7 | p95 188.0->188.0 | trunc%=2.1% | time=10.58s | mode=mag\n\n[Stage 5] split_11/train | expected=146 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=146 (missing_filled=0) | kept_rows=22,927 | len_mean 157.0->146.2 | p95 192.0->192.0 | trunc%=1.4% | time=9.14s | mode=mag\n\n[Stage 5] split_11/test | expected=325 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=325 (missing_filled=0) | kept_rows=49,723 | len_mean 153.0->144.9 | p95 189.6->189.6 | trunc%=1.2% | time=9.82s | mode=mag\n\n[Stage 5] split_12/train | expected=155 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,546 | len_mean 164.8->149.3 | p95 193.9->193.9 | trunc%=3.2% | time=9.70s | mode=mag\n\n[Stage 5] split_12/test | expected=353 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=353 (missing_filled=0) | kept_rows=54,499 | len_mean 154.4->145.4 | p95 185.0->185.0 | trunc%=1.1% | time=9.88s | mode=mag\n\n[Stage 5] split_13/train | expected=143 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=143 (missing_filled=0) | kept_rows=23,203 | len_mean 162.3->149.3 | p95 190.9->190.9 | trunc%=2.1% | time=9.62s | mode=mag\n\n[Stage 5] split_13/test | expected=379 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=379 (missing_filled=0) | kept_rows=63,653 | len_mean 167.9->149.3 | p95 195.1->195.1 | trunc%=2.9% | time=10.65s | mode=mag\n\n[Stage 5] split_14/train | expected=154 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=154 (missing_filled=0) | kept_rows=25,706 | len_mean 166.9->151.1 | p95 198.0->198.0 | trunc%=2.6% | time=9.24s | mode=mag\n\n[Stage 5] split_14/test | expected=351 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=351 (missing_filled=0) | kept_rows=58,643 | len_mean 167.1->150.2 | p95 193.0->193.0 | trunc%=2.8% | time=9.83s | mode=mag\n\n[Stage 5] split_15/train | expected=158 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=158 (missing_filled=0) | kept_rows=23,972 | len_mean 151.7->143.7 | p95 195.0->195.0 | trunc%=2.5% | time=8.83s | mode=mag\n\n[Stage 5] split_15/test | expected=342 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=342 (missing_filled=0) | kept_rows=52,943 | len_mean 154.8->143.9 | p95 193.0->193.0 | trunc%=2.3% | time=10.17s | mode=mag\n\n[Stage 5] split_16/train | expected=155 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,173 | len_mean 162.4->149.0 | p95 199.9->199.9 | trunc%=1.9% | time=9.02s | mode=mag\n\n[Stage 5] split_16/test | expected=354 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=354 (missing_filled=0) | kept_rows=58,192 | len_mean 164.4->148.1 | p95 193.3->193.3 | trunc%=2.8% | time=10.04s | mode=mag\n\n[Stage 5] split_17/train | expected=153 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=153 (missing_filled=0) | kept_rows=22,705 | len_mean 148.4->145.0 | p95 190.4->190.4 | trunc%=1.3% | time=8.72s | mode=mag\n\n[Stage 5] split_17/test | expected=351 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=351 (missing_filled=0) | kept_rows=59,482 | len_mean 169.5->147.3 | p95 198.0->198.0 | trunc%=3.4% | time=9.90s | mode=mag\n\n[Stage 5] split_18/train | expected=152 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=152 (missing_filled=0) | kept_rows=21,536 | len_mean 141.7->137.8 | p95 191.2->191.2 | trunc%=1.3% | time=9.56s | mode=mag\n\n[Stage 5] split_18/test | expected=345 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=345 (missing_filled=0) | kept_rows=53,887 | len_mean 156.2->141.2 | p95 188.8->188.8 | trunc%=2.3% | time=10.29s | mode=mag\n\n[Stage 5] split_19/train | expected=147 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=147 (missing_filled=0) | kept_rows=22,087 | len_mean 150.3->142.0 | p95 188.4->188.4 | trunc%=1.4% | time=8.70s | mode=mag\n\n[Stage 5] split_19/test | expected=375 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=375 (missing_filled=0) | kept_rows=56,355 | len_mean 150.3->143.1 | p95 191.0->191.0 | trunc%=1.1% | time=9.86s | mode=mag\n\n[Stage 5] split_20/train | expected=153 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=153 (missing_filled=0) | kept_rows=23,519 | len_mean 153.7->143.2 | p95 187.8->187.8 | trunc%=1.3% | time=9.84s | mode=mag\n\n[Stage 5] split_20/test | expected=358 | L_MAX=256 | TRUNC=smart\n[Stage 5] OK: built=358 (missing_filled=0) | kept_rows=58,432 | len_mean 163.2->148.5 | p95 190.0->190.0 | trunc%=2.5% | time=10.04s | mode=mag\n\n[Stage 5] DONE\n- token_mode : mag\n- features   : ['t_rel_log', 'dt_log', 'mag', 'mag_err_log', 'snr_tanh', 'detected']\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/seq_tokens/seq_manifest_train.csv (rows=3,043)\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/seq_tokens/seq_manifest_test.csv  (rows=7,135)\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/seq_tokens/seq_build_stats.csv\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/seq_tokens/seq_config.json\n\n[Stage 5] Smoke test object_id=Dornhoth_anwar_melethron\n- seq_len=184 | X_shape=(184, 6) | bands_unique=[0, 1, 2, 3, 4, 5]\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"55"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Sequence Length Policy (Padding, Truncation, Windowing)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n# ONE CELL, Kaggle CPU-SAFE — REVISI FULL v2.2 (MAG/ASINH COMPAT, HARDENED)\n#\n# Output:\n# - artifacts/fixed_seq/{train|test}_{X|B|M}.dat  (memmap)\n# - artifacts/fixed_seq/{train|test}_ids.npy\n# - artifacts/fixed_seq/train_y.npy\n# - artifacts/fixed_seq/{train|test}_origlen.npy, {train|test}_winstart.npy, {train|test}_winend.npy\n# - artifacts/fixed_seq/length_policy_config.json\n# ============================================================\n\nimport gc, json, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"seq_manifest_train\", \"seq_manifest_test\", \"SEQ_FEATURE_NAMES\",\n             \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 -> 5 dulu.\")\n\nART_DIR = Path(ART_DIR)\n\nm_train = seq_manifest_train.copy()\nm_test  = seq_manifest_test.copy()\n\nSEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\nfeat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n\n# ----------------------------\n# 0b) Detect token_mode (MAG vs ASINH)\n# ----------------------------\nSEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\nif SEQ_TOKEN_MODE is None:\n    if (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n        SEQ_TOKEN_MODE = \"asinh\"\n    elif (\"mag\" in feat) and (\"mag_err_log\" in feat):\n        SEQ_TOKEN_MODE = \"mag\"\n    else:\n        raise ValueError(\n            \"Cannot infer SEQ_TOKEN_MODE from SEQ_FEATURE_NAMES.\\n\"\n            f\"SEQ_FEATURE_NAMES={SEQ_FEATURE_NAMES}\\n\"\n            \"Expected either (flux_asinh, err_log1p) or (mag, mag_err_log).\"\n        )\n\nREQ_COMMON = [\"t_rel_log\", \"dt_log\", \"snr_tanh\", \"detected\"]\nfor k in REQ_COMMON:\n    if k not in feat:\n        raise ValueError(f\"SEQ_FEATURE_NAMES must include '{k}'. Found: {SEQ_FEATURE_NAMES}\")\n\nif SEQ_TOKEN_MODE == \"asinh\":\n    if \"flux_asinh\" not in feat:\n        raise ValueError(\"token_mode=asinh requires 'flux_asinh'.\")\n    SCORE_VALUE_FEAT = \"flux_asinh\"\nelif SEQ_TOKEN_MODE == \"mag\":\n    if \"mag\" not in feat:\n        raise ValueError(\"token_mode=mag requires 'mag'.\")\n    SCORE_VALUE_FEAT = \"mag\"\nelse:\n    raise ValueError(f\"Unknown SEQ_TOKEN_MODE={SEQ_TOKEN_MODE}\")\n\nprint(f\"[Stage 6] token_mode={SEQ_TOKEN_MODE} | score_value_feat={SCORE_VALUE_FEAT} | F={len(SEQ_FEATURE_NAMES)}\")\n\n# ----------------------------\n# 1) Settings\n# ----------------------------\nFORCE_MAX_LEN = None          # e.g. 256 (kalau mau paksa)\nMAXLEN_CAPS = (256, 384, 512) # CPU-safe choices\n\n# Score weights\nW_SNR = 1.00\nW_VAL = 0.35\nW_DET = 0.25\n\n# Padding policy\nPAD_BAND_ID = 0\n\n# AUTO: kalau shard punya band negatif (mis. -1 dari empty token), shift band ids otomatis\nSHIFT_BAND_IDS = False\nAUTO_SHIFT_IF_NEGATIVE_BANDS = True\n\n# Build policy\nREBUILD_MODE = \"wipe_all\"     # \"wipe_all\" atau \"reuse_if_exists\"\nDTYPE_X = np.float32          # bisa fp16 kalau disk ketat\n\n# ----------------------------\n# 2) Inspect length distribution -> choose MAX_LEN\n# ----------------------------\ndef describe_lengths(m: pd.DataFrame, name: str):\n    L = pd.to_numeric(m[\"length\"], errors=\"coerce\").fillna(0).to_numpy(dtype=np.int32, copy=False)\n    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n    print(f\"\\n{name} length stats\")\n    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[7])} | p95={int(q[8])} | p99={int(q[10])} | max={int(q[-1])}\")\n    return q\n\nq_tr = describe_lengths(m_train, \"TRAIN\")\nq_te = describe_lengths(m_test,  \"TEST\")\n\np95 = int(max(q_tr[8], q_te[8]))\nif FORCE_MAX_LEN is not None:\n    MAX_LEN = int(FORCE_MAX_LEN)\nelse:\n    if p95 <= 256:\n        MAX_LEN = 256\n    elif p95 <= 384:\n        MAX_LEN = 384\n    else:\n        MAX_LEN = 512\n\nif MAX_LEN not in MAXLEN_CAPS and FORCE_MAX_LEN is None:\n    MAX_LEN = int(min(MAXLEN_CAPS, key=lambda x: abs(x - MAX_LEN)))\n\nprint(f\"\\n[Stage 6] MAX_LEN={MAX_LEN} (based on p95={p95})\")\n\n# ----------------------------\n# 3) Window scoring (adaptive)\n# ----------------------------\ndef _brightness_proxy_from_mag(mag: np.ndarray) -> np.ndarray:\n    mag = np.nan_to_num(mag, nan=np.float32(0.0), posinf=np.float32(0.0), neginf=np.float32(0.0)).astype(np.float32, copy=False)\n    if mag.size == 0:\n        return np.zeros_like(mag, dtype=np.float32)\n    med = np.float32(np.median(mag))\n    br = np.maximum(med - mag, np.float32(0.0))\n    br = np.log1p(br).astype(np.float32, copy=False)\n    return br\n\ndef _score_tokens(X: np.ndarray) -> np.ndarray:\n    snr = np.abs(X[:, feat[\"snr_tanh\"]]).astype(np.float32, copy=False)\n    det = X[:, feat[\"detected\"]].astype(np.float32, copy=False)\n\n    if SEQ_TOKEN_MODE == \"asinh\":\n        val = np.abs(X[:, feat[\"flux_asinh\"]]).astype(np.float32, copy=False)\n    else:\n        mag = X[:, feat[\"mag\"]].astype(np.float32, copy=False)\n        val = _brightness_proxy_from_mag(mag)\n\n    score = (np.float32(W_SNR) * snr) + (np.float32(W_VAL) * val) + (np.float32(W_DET) * det)\n    score = np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n    return score\n\ndef select_best_window(score: np.ndarray, max_len: int) -> tuple[int, int]:\n    L = int(score.shape[0])\n    if L <= max_len:\n        return 0, L\n\n    cs = np.empty(L + 1, dtype=np.float32)\n    cs[0] = 0.0\n    np.cumsum(score.astype(np.float32, copy=False), out=cs[1:])\n\n    ws = cs[max_len:] - cs[:-max_len]\n    if not np.isfinite(ws).any():\n        start = (L - max_len) // 2\n    else:\n        start = int(np.argmax(ws))\n    end = start + max_len\n    return start, end\n\ndef pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n    L = int(X.shape[0])\n    F = int(X.shape[1])\n\n    Xp = np.zeros((max_len, F), dtype=DTYPE_X)\n    Bp = np.full((max_len,), PAD_BAND_ID, dtype=np.int8)\n    Mp = np.zeros((max_len,), dtype=np.int8)\n\n    if L <= 0:\n        return Xp, Bp, Mp, 0, 0, 0\n\n    if L <= max_len:\n        Xw = X\n        Bw = B\n        ws, we = 0, L\n    else:\n        sc = _score_tokens(X)\n        ws, we = select_best_window(sc, max_len=max_len)\n        Xw = X[ws:we]\n        Bw = B[ws:we]\n\n    lw = int(Xw.shape[0])\n    Xp[:lw] = Xw.astype(DTYPE_X, copy=False)\n\n    if SHIFT_BAND_IDS:\n        # token band 1..K; pad=0; band=-1 -> 0\n        Bw16 = Bw.astype(np.int16, copy=False)\n        Bp[:lw] = (Bw16 + 1).astype(np.int8, copy=False)\n    else:\n        Bp[:lw] = Bw.astype(np.int8, copy=False)\n\n    Mp[:lw] = 1\n    return Xp, Bp, Mp, L, int(ws), int(we)\n\n# ----------------------------\n# 4) Fixed cache builder setup\n# ----------------------------\nFIX_DIR = Path(ART_DIR) / \"fixed_seq\"\nFIX_DIR.mkdir(parents=True, exist_ok=True)\n\n# robust ordering\ntrain_ids = df_train_meta.index.astype(\"string\").str.strip().astype(str).to_list()\n\n# y column robust\n_y_col = None\nfor cand in [\"target\", \"y\", \"label\", \"class\", \"target_id\"]:\n    if cand in df_train_meta.columns:\n        _y_col = cand\n        break\nif _y_col is None:\n    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols={list(df_train_meta.columns)[:30]}\")\n\ny_train = pd.to_numeric(df_train_meta[_y_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n\ndef _try_load_sample_sub_ids():\n    # 1) df_sub (kalau ada)\n    if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in globals()[\"df_sub\"].columns:\n        return globals()[\"df_sub\"][\"object_id\"].astype(str).str.strip().to_list()\n\n    # 2) PATHS sample_submission\n    if \"PATHS\" in globals() and isinstance(PATHS, dict):\n        keys = [\"SAMPLE_SUB\", \"SAMPLE_SUBMISSION\", \"sample_submission\", \"sample_sub\", \"SAMPLE\"]\n        for k in keys:\n            p = PATHS.get(k, None)\n            if p and Path(p).exists():\n                df = pd.read_csv(p)\n                if \"object_id\" in df.columns:\n                    return df[\"object_id\"].astype(str).str.strip().to_list()\n    return None\n\ntest_ids = _try_load_sample_sub_ids()\nif test_ids is None:\n    test_ids = df_test_meta.index.astype(\"string\").str.strip().astype(str).to_list()\n\n# strict unique ids\nif len(set(train_ids)) != len(train_ids):\n    raise RuntimeError(\"train_ids contains duplicates. Check df_train_meta.index.\")\nif len(set(test_ids)) != len(test_ids):\n    raise RuntimeError(\"test_ids contains duplicates. Check ordering source (df_sub/sample_sub/df_test_meta).\")\n\ntrain_row = {oid: i for i, oid in enumerate(train_ids)}\ntest_row  = {oid: i for i, oid in enumerate(test_ids)}\n\nNTR = len(train_ids)\nNTE = len(test_ids)\nF = len(SEQ_FEATURE_NAMES)\n\ndef _gb(nbytes): return float(nbytes) / (1024**3)\nsize_tr = NTR * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\nsize_te = NTE * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\nprint(f\"\\n[Stage 6] Memmap X sizes approx: train={_gb(size_tr):.2f} GB | test={_gb(size_te):.2f} GB | dtype={DTYPE_X}\")\n\n# memmap paths\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\ntest_X_path  = FIX_DIR / \"test_X.dat\"\ntest_B_path  = FIX_DIR / \"test_B.dat\"\ntest_M_path  = FIX_DIR / \"test_M.dat\"\n\ntrain_len_path = FIX_DIR / \"train_origlen.npy\"\ntrain_ws_path  = FIX_DIR / \"train_winstart.npy\"\ntrain_we_path  = FIX_DIR / \"train_winend.npy\"\ntest_len_path  = FIX_DIR / \"test_origlen.npy\"\ntest_ws_path   = FIX_DIR / \"test_winstart.npy\"\ntest_we_path   = FIX_DIR / \"test_winend.npy\"\n\n# ----------------------------\n# 4b) Auto shift band ids if needed\n# ----------------------------\nif AUTO_SHIFT_IF_NEGATIVE_BANDS and (not SHIFT_BAND_IDS):\n    try:\n        # ambil 1 shard train untuk cek min band\n        sp0 = str(m_train[\"shard\"].astype(str).iloc[0])\n        d0 = np.load(sp0, allow_pickle=False)\n        b0 = d0[\"band\"]\n        bmin = int(np.min(b0)) if b0.size else 0\n        del d0\n        if bmin < 0:\n            SHIFT_BAND_IDS = True\n            print(f\"[Stage 6] AUTO SHIFT_BAND_IDS=True (detected band min={bmin} in shard sample)\")\n    except Exception as e:\n        print(f\"[Stage 6] AUTO SHIFT check skipped ({type(e).__name__}: {e})\")\n\n# ----------------------------\n# 4c) Rebuild handling\n# ----------------------------\ndef _all_exist(paths):\n    return all(Path(p).exists() for p in paths)\n\nreuse_paths = [\n    train_X_path, train_B_path, train_M_path,\n    test_X_path, test_B_path, test_M_path,\n    FIX_DIR / \"train_ids.npy\", FIX_DIR / \"test_ids.npy\", FIX_DIR / \"train_y.npy\",\n    train_len_path, train_ws_path, train_we_path,\n    test_len_path, test_ws_path, test_we_path,\n    FIX_DIR / \"length_policy_config.json\"\n]\n\nif REBUILD_MODE == \"reuse_if_exists\" and _all_exist(reuse_paths):\n    print(\"[Stage 6] REUSE (exists): fixed_seq cache already present.\")\n    globals().update({\n        \"FIX_DIR\": FIX_DIR, \"MAX_LEN\": MAX_LEN,\n        \"FIX_TRAIN_X_PATH\": train_X_path, \"FIX_TRAIN_B_PATH\": train_B_path, \"FIX_TRAIN_M_PATH\": train_M_path,\n        \"FIX_TEST_X_PATH\": test_X_path,  \"FIX_TEST_B_PATH\": test_B_path,  \"FIX_TEST_M_PATH\": test_M_path,\n        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n        \"FIX_POLICY_CFG_PATH\": FIX_DIR / \"length_policy_config.json\",\n        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n    })\n    raise SystemExit\n\n# ----------------------------\n# 5) Create memmaps\n# ----------------------------\nXtr = np.memmap(train_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTR, MAX_LEN, F))\nBtr = np.memmap(train_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\nMtr = np.memmap(train_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n\nXte = np.memmap(test_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTE, MAX_LEN, F))\nBte = np.memmap(test_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\nMte = np.memmap(test_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n\noriglen_tr = np.zeros((NTR,), dtype=np.int32)\nwinstart_tr = np.zeros((NTR,), dtype=np.int32)\nwinend_tr   = np.zeros((NTR,), dtype=np.int32)\n\noriglen_te = np.zeros((NTE,), dtype=np.int32)\nwinstart_te = np.zeros((NTE,), dtype=np.int32)\nwinend_te   = np.zeros((NTE,), dtype=np.int32)\n\nfilled_tr = np.zeros((NTR,), dtype=np.uint8)\nfilled_te = np.zeros((NTE,), dtype=np.uint8)\n\n# ----------------------------\n# 6) Fill memmaps per shard (fast path)\n# ----------------------------\ndef process_manifest_into_memmap(m: pd.DataFrame, which: str):\n    if which == \"train\":\n        row_map = train_row\n        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n        origlen, ws_arr, we_arr = origlen_tr, winstart_tr, winend_tr\n        filled_mask = filled_tr\n        expected_n = NTR\n    else:\n        row_map = test_row\n        Xmm, Bmm, Mmm = Xte, Bte, Mte\n        origlen, ws_arr, we_arr = origlen_te, winstart_te, winend_te\n        filled_mask = filled_te\n        expected_n = NTE\n\n    for c in [\"object_id\", \"shard\", \"start\", \"length\"]:\n        if c not in m.columns:\n            raise RuntimeError(f\"Manifest missing column '{c}'. cols={list(m.columns)}\")\n\n    shard_paths = m[\"shard\"].astype(str).unique().tolist()\n    miss_sh = [p for p in shard_paths if not Path(p).exists()]\n    if miss_sh:\n        raise RuntimeError(f\"Missing shard files ({which}): count={len(miss_sh)} | ex={miss_sh[:5]}\")\n\n    filled = 0\n    dup = 0\n    empty = 0\n\n    t0 = time.time()\n    for shard_path in sorted(shard_paths):\n        g = m[m[\"shard\"].astype(str) == shard_path]\n        if g.empty:\n            continue\n\n        data = np.load(shard_path, allow_pickle=False)\n        x_all = data[\"x\"]\n        b_all = data[\"band\"]\n\n        # mapping object_id -> row index (robust int, na=-1)\n        oids = g[\"object_id\"].astype(str).to_numpy()\n        idxs = pd.Series(oids).map(row_map).astype(\"Int64\").to_numpy(dtype=np.int64, na_value=-1)\n\n        starts = pd.to_numeric(g[\"start\"], errors=\"coerce\").fillna(-1).to_numpy(dtype=np.int64, copy=False)\n        lens   = pd.to_numeric(g[\"length\"], errors=\"coerce\").fillna(0).to_numpy(dtype=np.int64, copy=False)\n\n        valid = (idxs >= 0) & (starts >= 0) & (lens >= 0)\n        if not valid.any():\n            del data\n            continue\n\n        idxs_v = idxs[valid]\n        starts_v = starts[valid]\n        lens_v = lens[valid]\n        oids_v = oids[valid]\n\n        for oid, idx, st, ln in zip(oids_v, idxs_v, starts_v, lens_v):\n            if ln <= 0:\n                empty += 1\n                continue\n            if filled_mask[idx]:\n                dup += 1\n                continue\n\n            end = int(st + ln)\n            if st < 0 or end > x_all.shape[0] or end > b_all.shape[0]:\n                raise RuntimeError(\n                    f\"[Stage 6] Out-of-range slice in shard={shard_path}\\n\"\n                    f\"- oid={oid} idx={idx} start={st} len={ln} end={end}\\n\"\n                    f\"- shard_x_len={x_all.shape[0]} shard_b_len={b_all.shape[0]}\"\n                )\n\n            X = x_all[st:end]\n            B = b_all[st:end]\n\n            Xp, Bp, Mp, L0, ws, we = pad_to_fixed(X, B, max_len=MAX_LEN)\n\n            Xmm[idx, :, :] = Xp\n            Bmm[idx, :] = Bp\n            Mmm[idx, :] = Mp\n            origlen[idx] = int(L0)\n            ws_arr[idx] = int(ws)\n            we_arr[idx] = int(we)\n            filled_mask[idx] = 1\n            filled += 1\n\n        del data\n        if filled % 2000 == 0:\n            gc.collect()\n\n    elapsed = time.time() - t0\n    return {\"filled\": int(filled), \"dup_skipped\": int(dup), \"empty_len\": int(empty), \"time_s\": float(elapsed), \"expected\": int(expected_n)}\n\nprint(\"\\n[Stage 6] Building fixed cache (TRAIN)...\")\nst_tr = process_manifest_into_memmap(m_train, \"train\")\nprint(f\"[Stage 6] TRAIN filled={st_tr['filled']:,}/{st_tr['expected']:,} | dup={st_tr['dup_skipped']:,} | empty={st_tr['empty_len']:,} | time={st_tr['time_s']:.2f}s\")\n\nprint(\"\\n[Stage 6] Building fixed cache (TEST)...\")\nst_te = process_manifest_into_memmap(m_test, \"test\")\nprint(f\"[Stage 6] TEST  filled={st_te['filled']:,}/{st_te['expected']:,} | dup={st_te['dup_skipped']:,} | empty={st_te['empty_len']:,} | time={st_te['time_s']:.2f}s\")\n\nXtr.flush(); Btr.flush(); Mtr.flush()\nXte.flush(); Bte.flush(); Mte.flush()\n\n# ----------------------------\n# 7) Hard sanity: must be 100% filled\n# ----------------------------\nmiss_tr = np.where(filled_tr == 0)[0]\nmiss_te = np.where(filled_te == 0)[0]\nif len(miss_tr) > 0:\n    ex = [train_ids[i] for i in miss_tr[:10]]\n    raise RuntimeError(f\"[Stage 6] TRAIN missing filled rows: {len(miss_tr):,}/{NTR:,} | ex={ex}\")\nif len(miss_te) > 0:\n    ex = [test_ids[i] for i in miss_te[:10]]\n    raise RuntimeError(f\"[Stage 6] TEST missing filled rows: {len(miss_te):,}/{NTE:,} | ex={ex}\")\n\n# ----------------------------\n# 8) Save ids + y + meta arrays\n# ----------------------------\nnp.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\nnp.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids, dtype=\"S\"))\nnp.save(FIX_DIR / \"train_y.npy\",   y_train)\n\nnp.save(train_len_path, origlen_tr)\nnp.save(train_ws_path,  winstart_tr)\nnp.save(train_we_path,  winend_tr)\n\nnp.save(test_len_path,  origlen_te)\nnp.save(test_ws_path,   winstart_te)\nnp.save(test_we_path,   winend_te)\n\n# ----------------------------\n# 9) Quick sanity samples\n# ----------------------------\ndef sanity_samples(which: str, n_show: int = 3, seed: int = 2025):\n    rng = np.random.default_rng(seed)\n    if which == \"train\":\n        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n        ids = train_ids\n        ol = origlen_tr\n    else:\n        Xmm, Bmm, Mmm = Xte, Bte, Mte\n        ids = test_ids\n        ol = origlen_te\n\n    idxs = rng.choice(len(ids), size=min(n_show, len(ids)), replace=False)\n    print(f\"\\n[Stage 6] Sanity samples ({which}):\")\n    for i in idxs:\n        kept = int(Mmm[i].sum())\n        bands = sorted(set(Bmm[i, :kept].tolist())) if kept > 0 else []\n        print(f\"- idx={i} oid={ids[i]} orig_len={int(ol[i])} kept={kept} bands_unique={bands}\")\n\nsanity_samples(\"train\", 3)\nsanity_samples(\"test\", 3)\n\n# ----------------------------\n# 10) Save config\n# ----------------------------\npolicy_cfg = {\n    \"token_mode\": SEQ_TOKEN_MODE,\n    \"max_len\": int(MAX_LEN),\n    \"feature_names\": list(SEQ_FEATURE_NAMES),\n    \"score_weights\": {\"W_SNR\": float(W_SNR), \"W_VAL\": float(W_VAL), \"W_DET\": float(W_DET)},\n    \"score_value_feat\": SCORE_VALUE_FEAT,\n    \"window_policy\": \"best_contiguous_window_by_max_sum(score)\",\n    \"padding\": {\"PAD_BAND_ID\": int(PAD_BAND_ID), \"SHIFT_BAND_IDS\": bool(SHIFT_BAND_IDS)},\n    \"dtype_X\": str(DTYPE_X),\n    \"order\": {\n        \"train\": \"df_train_meta.index\",\n        \"test\": (\"df_sub.object_id\" if (\"df_sub\" in globals() and isinstance(df_sub, pd.DataFrame) and \"object_id\" in df_sub.columns) else \"df_test_meta.index / sample_submission fallback\"),\n        \"y_col\": str(_y_col),\n    },\n    \"stats\": {\"train\": st_tr, \"test\": st_te},\n    \"files\": {\n        \"train_X\": str(train_X_path), \"train_B\": str(train_B_path), \"train_M\": str(train_M_path),\n        \"test_X\": str(test_X_path),   \"test_B\": str(test_B_path),   \"test_M\": str(test_M_path),\n        \"train_y\": str(FIX_DIR / \"train_y.npy\"),\n        \"train_ids\": str(FIX_DIR / \"train_ids.npy\"),\n        \"test_ids\": str(FIX_DIR / \"test_ids.npy\"),\n        \"train_origlen\": str(train_len_path), \"train_winstart\": str(train_ws_path), \"train_winend\": str(train_we_path),\n        \"test_origlen\": str(test_len_path),   \"test_winstart\": str(test_ws_path),   \"test_winend\": str(test_we_path),\n    }\n}\ncfg_path = FIX_DIR / \"length_policy_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(policy_cfg, f, indent=2)\n\nprint(\"\\n[Stage 6] DONE\")\nprint(f\"- FIX_DIR: {FIX_DIR}\")\nprint(f\"- Saved config: {cfg_path}\")\n\nglobals().update({\n    \"FIX_DIR\": FIX_DIR,\n    \"MAX_LEN\": MAX_LEN,\n    \"FIX_TRAIN_X_PATH\": train_X_path,\n    \"FIX_TRAIN_B_PATH\": train_B_path,\n    \"FIX_TRAIN_M_PATH\": train_M_path,\n    \"FIX_TEST_X_PATH\": test_X_path,\n    \"FIX_TEST_B_PATH\": test_B_path,\n    \"FIX_TEST_M_PATH\": test_M_path,\n    \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n    \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n    \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n    \"FIX_POLICY_CFG_PATH\": cfg_path,\n    \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n    \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:35:15.986095Z","iopub.execute_input":"2026-01-03T14:35:15.986476Z","iopub.status.idle":"2026-01-03T14:35:16.937716Z","shell.execute_reply.started":"2026-01-03T14:35:15.986440Z","shell.execute_reply":"2026-01-03T14:35:16.936425Z"}},"outputs":[{"name":"stdout","text":"[Stage 6] token_mode=mag | score_value_feat=mag | F=6\n\nTRAIN length stats\n- n_objects=3,043 | min=17 | p50=150 | p90=183 | p95=194 | p99=256 | max=256\n\nTEST length stats\n- n_objects=7,135 | min=18 | p50=152 | p90=183 | p95=193 | p99=256 | max=256\n\n[Stage 6] MAX_LEN=256 (based on p95=194)\n\n[Stage 6] Memmap X sizes approx: train=0.02 GB | test=0.04 GB | dtype=<class 'numpy.float32'>\n\n[Stage 6] Building fixed cache (TRAIN)...\n[Stage 6] TRAIN filled=3,043/3,043 | dup=0 | empty=0 | time=0.15s\n\n[Stage 6] Building fixed cache (TEST)...\n[Stage 6] TEST  filled=7,135/7,135 | dup=0 | empty=0 | time=0.29s\n\n[Stage 6] Sanity samples (train):\n- idx=1360 oid=gwilwileth_adel_amloth orig_len=157 kept=157 bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=3020 oid=vin_araf_gwador orig_len=151 kept=151 bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=3025 oid=ylf_alph_mindon orig_len=167 kept=167 bands_unique=[0, 1, 2, 3, 4, 5]\n\n[Stage 6] Sanity samples (test):\n- idx=3191 oid=rom_bellas_lebdas orig_len=142 kept=142 bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=7082 oid=nim_nestad_thor orig_len=161 kept=161 bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=7094 oid=rach_bellas_dol orig_len=156 kept=156 bands_unique=[0, 1, 2, 3, 4, 5]\n\n[Stage 6] DONE\n- FIX_DIR: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/fixed_seq\n- Saved config: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/fixed_seq/length_policy_config.json\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"616"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# CV Split (Object-Level, Stratified)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v2.2 (HARDENED + HOLDOUT FALLBACK)\n#\n# Output:\n# - artifacts/cv/cv_folds.csv\n# - artifacts/cv/cv_folds.npz   (train_idx_f + val_idx_f)\n# - artifacts/cv/cv_report.txt\n# - artifacts/cv/cv_config.json\n# - globals: fold_assign, folds, n_splits, CV_DIR\n# ============================================================\n\nimport gc, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nfor need in [\"df_train_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 2 dulu (df_train_meta & ART_DIR).\")\n\nSEED = int(globals().get(\"SEED\", 2025))\nART_DIR = Path(ART_DIR)\n\n# ----------------------------\n# 1) CV Settings\n# ----------------------------\nDEFAULT_SPLITS = 5\nFORCE_N_SPLITS = None              # set int kalau mau paksa (mis. 3), else None\nMIN_POS_PER_FOLD = 3               # stabilitas; 3–10 umum\nENFORCE_MIN_POS_PER_FOLD = True    # kalau True: n_splits turun otomatis sampai min_pos>=MIN_POS_PER_FOLD (atau fallback holdout)\nUSE_GROUP_BY_SPLIT = False         # True => prefer StratifiedGroupKFold (groups=df_train_meta[\"split\"])\nAUTO_FALLBACK_GROUP = True         # True => kalau group-cv tidak bisa, fallback ke StratifiedKFold\nHOLDOUT_FALLBACK = True            # True => kalau CV tidak mungkin, pakai 1 fold holdout (n_splits=1)\nHOLDOUT_FRAC = 0.20                # target val fraction untuk holdout\n\nprint(f\"[Stage 7] seed={SEED} | default_splits={DEFAULT_SPLITS} | MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} \"\n      f\"| enforce_minpos={ENFORCE_MIN_POS_PER_FOLD} | group_by_split={USE_GROUP_BY_SPLIT} | fallback_group={AUTO_FALLBACK_GROUP}\")\n\n# ----------------------------\n# 2) Helpers\n# ----------------------------\ndef _decode_ids(arr) -> list:\n    out = []\n    for x in arr.tolist():\n        if isinstance(x, (bytes, bytearray)):\n            s = x.decode(\"utf-8\", errors=\"ignore\")\n        else:\n            s = str(x)\n        out.append(s.strip())\n    return out\n\ndef _find_train_ids_npy(art_dir: Path) -> Path | None:\n    # priority 1: FIX_DIR\n    if \"FIX_DIR\" in globals():\n        p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n        if p.exists():\n            return p\n\n    # priority 2: ART_DIR/fixed_seq\n    p = art_dir / \"fixed_seq\" / \"train_ids.npy\"\n    if p.exists():\n        return p\n\n    # priority 3: scan mallorn_run runs (latest mtime)\n    root = Path(\"/kaggle/working/mallorn_run\")\n    if root.exists():\n        cands = list(root.glob(\"run_*/artifacts/fixed_seq/train_ids.npy\"))\n        if cands:\n            cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n            return cands[0]\n    return None\n\ndef _safe_str_index(idx: pd.Index) -> pd.Index:\n    return pd.Index([str(x).strip() for x in idx], dtype=\"object\")\n\n# ----------------------------\n# 3) Determine train_ids ordering (prefer fixed cache from STAGE 6)\n# ----------------------------\ntrain_ids = None\norder_source = \"df_train_meta.index\"\n\np_ids = _find_train_ids_npy(ART_DIR)\nif p_ids is not None:\n    raw = np.load(p_ids, allow_pickle=False)\n    train_ids = _decode_ids(raw)\n    order_source = str(p_ids)\nelse:\n    train_ids = [str(x).strip() for x in df_train_meta.index.astype(str).tolist()]\n    order_source = \"df_train_meta.index\"\n\n# uniqueness check train_ids\nif len(train_ids) != len(set(train_ids)):\n    s = pd.Series(train_ids)\n    dup = s[s.duplicated()].iloc[:10].tolist()\n    raise RuntimeError(f\"[Stage 7] train_ids has duplicates (examples): {dup}\")\n\nN = len(train_ids)\n\n# ----------------------------\n# 4) Build robust mapping: train_id -> row position in df_train_meta\n#    (tidak mengharuskan df_train_meta.index sudah string)\n# ----------------------------\nmeta = df_train_meta\n\nmeta_idx_str = _safe_str_index(meta.index)\nif meta_idx_str.has_duplicates:\n    d = pd.Series(meta_idx_str).value_counts()\n    dup = d[d > 1].index.tolist()[:10]\n    raise RuntimeError(f\"[Stage 7] df_train_meta index has duplicates after str/strip (examples): {dup}\")\n\npos_map = pd.Series(np.arange(len(meta), dtype=np.int32), index=meta_idx_str)\n\npos_s = pos_map.reindex(train_ids)\nmissing = pos_s[pos_s.isna()].index.tolist()\nif missing:\n    ex = missing[:10]\n    raise RuntimeError(\n        \"[Stage 7] Some train_ids not found in df_train_meta (after str/strip index).\\n\"\n        f\"Missing count={len(missing)} | ex={ex}\\n\"\n        \"Solusi: pastikan df_train_meta memang object-level meta dan index-nya object_id.\"\n    )\n\npos_idx = pos_s.astype(np.int32).to_numpy()\n\n# ----------------------------\n# 5) Robust target column -> y (ordered by train_ids)\n# ----------------------------\ntarget_col = None\nfor cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n    if cand in meta.columns:\n        target_col = cand\n        break\nif target_col is None:\n    raise RuntimeError(f\"[Stage 7] Cannot find target column in df_train_meta. cols(sample)={list(meta.columns)[:40]}\")\n\ny_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\ny = y_all[pos_idx]\ny = (y > 0).astype(np.int8)  # force 0/1\n\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\nif pos == 0 or neg == 0:\n    raise RuntimeError(f\"[Stage 7] Invalid class distribution: pos={pos}, neg={neg}. Cannot do stratified split.\")\n\n# ----------------------------\n# 6) Optional groups (by split)\n# ----------------------------\ngroups = None\ngroup_col = None\n\nif USE_GROUP_BY_SPLIT:\n    for cand in [\"split\", \"split_id\", \"split_name\", \"split_idx\"]:\n        if cand in meta.columns:\n            group_col = cand\n            break\n\n    if group_col is None:\n        if not AUTO_FALLBACK_GROUP:\n            raise RuntimeError(\"[Stage 7] USE_GROUP_BY_SPLIT=True but no split column found in df_train_meta.\")\n        print(\"[Stage 7] WARN: split column not found; fallback to StratifiedKFold.\")\n        USE_GROUP_BY_SPLIT = False\n    else:\n        g_all = meta[group_col].astype(str).to_numpy()\n        groups = g_all[pos_idx]\n\n# ----------------------------\n# 7) Choose n_splits safely + auto-adjust\n# ----------------------------\nmax_splits_by_pos = pos\nmax_splits_by_neg = neg\nmax_splits_by_minpos = max(1, pos // max(int(MIN_POS_PER_FOLD), 1))\n\nn0 = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg, max_splits_by_minpos)\nif FORCE_N_SPLITS is not None:\n    n0 = int(FORCE_N_SPLITS)\n\nprint(f\"[Stage 7] Candidate n_splits={n0} | N={N:,} pos={pos:,} neg={neg:,} pos%={pos/max(N,1)*100:.6f}% | order_source={order_source}\")\n\n# ----------------------------\n# 8) Build folds (sklearn) with robust fallback\n# ----------------------------\ntry:\n    from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n    try:\n        from sklearn.model_selection import StratifiedGroupKFold\n    except Exception:\n        StratifiedGroupKFold = None\nexcept Exception as e:\n    raise RuntimeError(\"scikit-learn is not available in this environment.\") from e\n\ndef _try_split_kfold(k: int, use_group: bool):\n    fold_assign = np.full(N, -1, dtype=np.int16)\n    folds = []\n    per = []\n\n    if use_group:\n        if StratifiedGroupKFold is None:\n            return (False, \"StratifiedGroupKFold(unavailable)\", None, None, None)\n        splitter = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(np.zeros(N), y, groups=groups)\n        cv_type = f\"StratifiedGroupKFold({group_col})\"\n    else:\n        splitter = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(np.zeros(N), y)\n        cv_type = \"StratifiedKFold\"\n\n    try:\n        for fold, (tr_idx, val_idx) in enumerate(split_iter):\n            fold_assign[val_idx] = fold\n            yf = y[val_idx]\n            pf = int((yf == 1).sum())\n            nf = int((yf == 0).sum())\n            per.append((len(val_idx), pf, nf))\n            folds.append({\n                \"fold\": int(fold),\n                \"train_idx\": tr_idx.astype(np.int32),\n                \"val_idx\": val_idx.astype(np.int32),\n            })\n    except Exception as e:\n        return (False, f\"{cv_type} (error: {type(e).__name__})\", None, None, None)\n\n    if (fold_assign < 0).any():\n        return (False, f\"{cv_type} (unassigned)\", None, None, None)\n\n    # hard check: each fold must have pos>=1 and neg>=1\n    for (_, pf, nf) in per:\n        if pf == 0 or nf == 0:\n            return (False, f\"{cv_type} (empty class in fold)\", None, None, None)\n\n    return (True, cv_type, fold_assign, folds, per)\n\ndef _make_holdout():\n    # 1 split holdout with StratifiedShuffleSplit\n    # choose val size that ensures at least 1 pos and 1 neg in val (and in train)\n    n_pos = pos\n    n_neg = neg\n    n = N\n\n    # start from HOLDOUT_FRAC\n    val_n = int(round(n * float(HOLDOUT_FRAC)))\n    val_n = max(val_n, 2)  # at least 2 samples\n    val_n = min(val_n, n - 2)\n\n    # ensure possible: need at least 1 pos and 1 neg in val\n    val_n = max(val_n, 2)\n    if n_pos == 1 or n_neg == 1:\n        # still possible but fragile, keep val small\n        val_n = 2\n\n    # loop adjust if impossible\n    def feasible(vn: int) -> bool:\n        # need vn >=2 and vn <= n-2\n        if vn < 2 or vn > n - 2:\n            return False\n        # can we place at least 1 pos and 1 neg into val and keep at least 1 pos/neg in train?\n        return (n_pos >= 2 and n_neg >= 2) or ((n_pos >= 1 and n_neg >= 1) and (n_pos - 1 >= 1) and (n_neg - 1 >= 1))\n\n    if not feasible(val_n):\n        # minimal viable with both classes in train and val requires pos>=2 and neg>=2\n        if n_pos < 2 or n_neg < 2:\n            raise RuntimeError(\n                f\"[Stage 7] Cannot build even holdout split safely. Need pos>=2 and neg>=2. Got pos={n_pos}, neg={n_neg}.\"\n            )\n        val_n = 2\n\n    test_size = val_n / n\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=SEED)\n    tr_idx, val_idx = next(splitter.split(np.zeros(n), y))\n\n    fold_assign = np.full(n, -1, dtype=np.int16)\n    fold_assign[val_idx] = 0\n\n    yf = y[val_idx]\n    per = [(len(val_idx), int((yf == 1).sum()), int((yf == 0).sum()))]\n\n    folds = [{\n        \"fold\": 0,\n        \"train_idx\": tr_idx.astype(np.int32),\n        \"val_idx\": val_idx.astype(np.int32),\n    }]\n    return 1, \"Holdout(StratifiedShuffleSplit)\", fold_assign, folds, per\n\nbest = None\nused_group = bool(USE_GROUP_BY_SPLIT)\n\nif n0 >= 2:\n    for k in range(n0, 1, -1):\n        ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=used_group)\n\n        if (not ok) and used_group and AUTO_FALLBACK_GROUP:\n            ok2, cv_type2, fa2, folds2, per2 = _try_split_kfold(k, use_group=False)\n            if ok2:\n                ok, cv_type, fa, folds, per = ok2, cv_type2, fa2, folds2, per2\n                used_group = False\n\n        if not ok:\n            continue\n\n        min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n        if ENFORCE_MIN_POS_PER_FOLD and (min_pos_seen < MIN_POS_PER_FOLD) and (FORCE_N_SPLITS is None):\n            continue\n\n        best = (k, cv_type, fa, folds, per, min_pos_seen)\n        break\n\n# if enforce failed completely, pick first valid (pos>=1 in each fold)\nif best is None and n0 >= 2:\n    for k in range(n0, 1, -1):\n        ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=bool(USE_GROUP_BY_SPLIT))\n        if (not ok) and USE_GROUP_BY_SPLIT and AUTO_FALLBACK_GROUP:\n            ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=False)\n        if ok:\n            min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n            best = (k, cv_type, fa, folds, per, min_pos_seen)\n            print(f\"[Stage 7] NOTE: Could not satisfy MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}. Using k={k} with min_pos={min_pos_seen}.\")\n            break\n\n# fallback to holdout if still none\nif best is None:\n    if HOLDOUT_FALLBACK:\n        n_splits, cv_type, fold_assign, folds, per = _make_holdout()\n        min_pos_seen = per[0][1] if per else 0\n        best = (n_splits, cv_type, fold_assign, folds, per, min_pos_seen)\n        print(f\"[Stage 7] FALLBACK -> {cv_type} | val_pos={min_pos_seen}\")\n    else:\n        raise RuntimeError(\"[Stage 7] Failed to build a valid CV split. Try smaller DEFAULT_SPLITS / FORCE_N_SPLITS, or enable HOLDOUT_FALLBACK.\")\n\nn_splits, cv_type, fold_assign, folds, per, min_pos_seen = best\n\n# ----------------------------\n# 9) Report + validation\n# ----------------------------\nlines = []\nlines.append(f\"CV={cv_type} n_splits={n_splits} seed={SEED}\")\nlines.append(f\"Order source: {order_source}\")\nlines.append(f\"Target column: {target_col}\")\nlines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\nif USE_GROUP_BY_SPLIT:\n    lines.append(f\"Group col requested: {group_col} | used_group={('Group' in cv_type)}\")\nlines.append(\"Per-fold distribution (val):\")\n\nok = True\nfor f in range(n_splits):\n    idx = np.where(fold_assign == f)[0]\n    yf = y[idx]\n    pf = int((yf == 1).sum())\n    nf = int((yf == 0).sum())\n    lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:9.6f}%\")\n    if pf == 0 or nf == 0:\n        ok = False\n\nif not ok:\n    raise RuntimeError(\"[Stage 7] A fold has pos=0 or neg=0 after selection (should not happen).\")\n\nif (fold_assign < 0).any():\n    bad = np.where(fold_assign < 0)[0][:10]\n    ex = [train_ids[i] for i in bad]\n    raise RuntimeError(f\"[Stage 7] Unassigned fold entries detected: count={(fold_assign<0).sum()} | ex={ex}\")\n\nif min_pos_seen < MIN_POS_PER_FOLD and n_splits >= 2:\n    lines.append(f\"NOTE: min positives in a fold = {min_pos_seen} (< MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}). \"\n                 \"Threshold/F1 tuning bisa noisy; pertimbangkan n_splits lebih kecil.\")\n\nprint(f\"[Stage 7] FINAL: n_splits={n_splits} | cv_type={cv_type} | min_pos_in_fold={min_pos_seen}\")\n\n# ----------------------------\n# 10) Save artifacts\n# ----------------------------\nCV_DIR = ART_DIR / \"cv\"\nCV_DIR.mkdir(parents=True, exist_ok=True)\n\ndf_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\nfolds_csv = CV_DIR / \"cv_folds.csv\"\ndf_folds.to_csv(folds_csv, index=False)\n\nnpz_path = CV_DIR / \"cv_folds.npz\"\nnpz_kwargs = {}\nfor f in range(n_splits):\n    npz_kwargs[f\"train_idx_{f}\"] = folds[f][\"train_idx\"]\n    npz_kwargs[f\"val_idx_{f}\"]   = folds[f][\"val_idx\"]\nnp.savez(npz_path, **npz_kwargs)\n\nreport_path = CV_DIR / \"cv_report.txt\"\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\ncfg_path = CV_DIR / \"cv_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"seed\": SEED,\n            \"n_splits\": int(n_splits),\n            \"cv_type\": cv_type,\n            \"min_pos_per_fold\": int(MIN_POS_PER_FOLD),\n            \"enforce_min_pos_per_fold\": bool(ENFORCE_MIN_POS_PER_FOLD),\n            \"use_group_by_split_requested\": bool(USE_GROUP_BY_SPLIT),\n            \"auto_fallback_group\": bool(AUTO_FALLBACK_GROUP),\n            \"holdout_fallback\": bool(HOLDOUT_FALLBACK),\n            \"order_source\": order_source,\n            \"target_col\": target_col,\n            \"group_col\": group_col,\n            \"artifacts\": {\n                \"folds_csv\": str(folds_csv),\n                \"folds_npz\": str(npz_path),\n                \"report_txt\": str(report_path),\n            },\n        },\n        f,\n        indent=2,\n    )\n\nprint(\"\\n[Stage 7] CV split OK\")\nprint(f\"- Saved: {folds_csv}\")\nprint(f\"- Saved: {npz_path}\")\nprint(f\"- Saved: {report_path}\")\nprint(f\"- Saved: {cfg_path}\")\n\n# print tail report lines\ntail_n = min(len(lines), n_splits + 6)\nprint(\"\\n\".join(lines[-tail_n:]))\n\n# ----------------------------\n# 11) Export globals for next stage\n# ----------------------------\nglobals().update({\n    \"CV_DIR\": CV_DIR,\n    \"n_splits\": int(n_splits),\n    \"train_ids_ordered\": train_ids,\n    \"y_ordered\": y,\n    \"fold_assign\": fold_assign,\n    \"folds\": folds,\n    \"CV_FOLDS_CSV\": folds_csv,\n    \"CV_FOLDS_NPZ\": npz_path,\n    \"CV_CFG_PATH\": cfg_path,\n    \"CV_TYPE\": cv_type,\n    \"CV_ORDER_SOURCE\": order_source,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:39:31.482663Z","iopub.execute_input":"2026-01-03T14:39:31.482986Z","iopub.status.idle":"2026-01-03T14:39:32.304755Z","shell.execute_reply.started":"2026-01-03T14:39:31.482959Z","shell.execute_reply":"2026-01-03T14:39:32.303971Z"}},"outputs":[{"name":"stdout","text":"[Stage 7] seed=2025 | default_splits=5 | MIN_POS_PER_FOLD=3 | enforce_minpos=True | group_by_split=False | fallback_group=True\n[Stage 7] Candidate n_splits=5 | N=3,043 pos=148 neg=2,895 pos%=4.863621% | order_source=/kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/fixed_seq/train_ids.npy\n[Stage 7] FINAL: n_splits=5 | cv_type=StratifiedKFold | min_pos_in_fold=29\n\n[Stage 7] CV split OK\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/cv/cv_folds.csv\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/cv/cv_folds.npz\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/cv/cv_report.txt\n- Saved: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/cv/cv_config.json\nCV=StratifiedKFold n_splits=5 seed=2025\nOrder source: /kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/artifacts/fixed_seq/train_ids.npy\nTarget column: target\nTotal: N=3043 | pos=148 | neg=2895 | pos%=4.863621%\nPer-fold distribution (val):\n- fold 0: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n- fold 1: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n- fold 2: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n- fold 3: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n- fold 4: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"33"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Train Model (CPU-Safe Configuration)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 8 — Train Multiband Event Transformer (CPU-Safe)\n# REVISI FULL v3.1 (BOOST + NO-LEAK + OneCycle FIX + SHIFT_BAND_IDS OK)\n#\n# Output:\n# - checkpoints/fold_*.pt\n# - oof/oof_prob.npy + oof/oof_prob.csv\n# - oof/fold_metrics.json\n# - logs/train_cfg_stage8.json + global_feature_spec.json\n# ============================================================\n\nimport os, gc, json, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require minimal previous stages\n# ----------------------------\nneed_min = [\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_train_meta\",\"n_splits\",\"folds\"]\nfor k in need_min:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n\n# ----------------------------\n# 0a) Resolve train_ids ordering + labels (robust)\n# ----------------------------\ndef _decode_ids(arr):\n    out = []\n    for x in arr.tolist():\n        if isinstance(x, (bytes, bytearray)):\n            s = x.decode(\"utf-8\", errors=\"ignore\")\n        else:\n            s = str(x)\n        out.append(s.strip())\n    return out\n\n# ordering\nif \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n    train_ids = [str(x).strip() for x in list(globals()[\"train_ids_ordered\"])]\nelse:\n    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n    if p.exists():\n        raw = np.load(p, allow_pickle=False)\n        train_ids = _decode_ids(raw if raw.dtype.kind in (\"S\",\"O\") else raw.astype(str))\n    else:\n        train_ids = [str(x).strip() for x in df_train_meta.index.astype(str).tolist()]\n\n# target column robust\ntarget_col = None\nfor cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n    if cand in df_train_meta.columns:\n        target_col = cand\n        break\nif target_col is None:\n    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols(sample)={list(df_train_meta.columns)[:40]}\")\n\n# robust mapping by stringified index (avoid index dtype mismatch)\nmeta_idx_str = pd.Index([str(x).strip() for x in df_train_meta.index], dtype=\"object\")\npos_map = pd.Series(np.arange(len(df_train_meta), dtype=np.int32), index=meta_idx_str)\npos_idx = pos_map.reindex(train_ids).to_numpy()\nif np.isnan(pos_idx).any():\n    miss = [train_ids[i] for i in np.where(np.isnan(pos_idx))[0][:10]]\n    raise RuntimeError(f\"Some train_ids not found in df_train_meta.index (string-mapped). ex={miss}\")\n\npos_idx = pos_idx.astype(np.int32)\n\ny_all = pd.to_numeric(df_train_meta[target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy()\ny = y_all[pos_idx]\ny = (y > 0).astype(np.int8)\n\n# ----------------------------\n# 0b) Ensure output dirs exist\n# ----------------------------\nif \"RUN_DIR\" in globals() and globals()[\"RUN_DIR\"] is not None:\n    RUN_DIR = Path(globals()[\"RUN_DIR\"])\nelse:\n    if \"ART_DIR\" in globals() and globals()[\"ART_DIR\"] is not None:\n        RUN_DIR = Path(globals()[\"ART_DIR\"]).parent\n    else:\n        RUN_DIR = Path(\"/kaggle/working/mallorn_run\")\n\nART_DIR = Path(globals().get(\"ART_DIR\", RUN_DIR / \"artifacts\"))\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nCKPT_DIR = Path(globals().get(\"CKPT_DIR\", RUN_DIR / \"checkpoints\"))\nOOF_DIR  = Path(globals().get(\"OOF_DIR\",  RUN_DIR / \"oof\"))\nLOG_DIR  = Path(globals().get(\"LOG_DIR\",  RUN_DIR / \"logs\"))\nCKPT_DIR.mkdir(parents=True, exist_ok=True)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nglobals().update({\"RUN_DIR\": RUN_DIR, \"ART_DIR\": ART_DIR, \"CKPT_DIR\": CKPT_DIR, \"OOF_DIR\": OOF_DIR, \"LOG_DIR\": LOG_DIR})\n\n# ----------------------------\n# 1) Torch imports + CPU safety\n# ----------------------------\ntry:\n    import torch\n    import torch.nn as nn\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\ndevice = torch.device(\"cpu\")\n\n# thread guard\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\ntry:\n    from sklearn.metrics import roc_auc_score\nexcept Exception as e:\n    raise RuntimeError(\"scikit-learn metrics tidak tersedia.\") from e\n\n# ----------------------------\n# 2) Open memmaps (fixed seq) — NO RAM load\n# ----------------------------\nFIX_DIR = Path(globals()[\"FIX_DIR\"])\nN = len(train_ids)\nL = int(globals()[\"MAX_LEN\"])\nSEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\nFdim = len(SEQ_FEATURE_NAMES)\nfeat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\n\nfor p in [train_X_path, train_B_path, train_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\nX_mm = np.memmap(train_X_path, dtype=np.float32, mode=\"r\", shape=(N, L, Fdim))\nB_mm = np.memmap(train_B_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\nM_mm = np.memmap(train_M_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n\n# ----------------------------\n# 2b) Read Stage6 policy to detect SHIFT_BAND_IDS (important!)\n# ----------------------------\nSHIFT_BAND_IDS = False\nPAD_BAND_ID = 0\npolicy_path = FIX_DIR / \"length_policy_config.json\"\nif policy_path.exists():\n    try:\n        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n            pol = json.load(f)\n        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n    except Exception:\n        pass\n\n# detect token mode\nSEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\nif SEQ_TOKEN_MODE is None:\n    if (\"mag\" in feat) and (\"mag_err_log\" in feat):\n        SEQ_TOKEN_MODE = \"mag\"\n    elif (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n        SEQ_TOKEN_MODE = \"asinh\"\n    else:\n        raise RuntimeError(f\"Cannot infer token_mode from features: {SEQ_FEATURE_NAMES}\")\n\nfor k in [\"snr_tanh\",\"detected\"]:\n    if k not in feat:\n        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n\nVAL_FEAT = \"mag\" if SEQ_TOKEN_MODE == \"mag\" else \"flux_asinh\"\nif VAL_FEAT not in feat:\n    raise RuntimeError(f\"Feature '{VAL_FEAT}' missing for token_mode={SEQ_TOKEN_MODE}.\")\n\n# ----------------------------\n# 3) Build RAW meta global features\n# ----------------------------\nBASE_G_COLS = [\"Z\",\"Z_err\",\"EBV\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\nfor c in BASE_G_COLS:\n    if c not in df_train_meta.columns:\n        df_train_meta[c] = 0.0\n\nG_meta = df_train_meta.iloc[pos_idx][BASE_G_COLS].copy()\nfor c in BASE_G_COLS:\n    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\nG_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n\nwith open(Path(LOG_DIR)/\"global_meta_cols.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"cols\": BASE_G_COLS}, f, indent=2)\n\n# ----------------------------\n# 3b) Sequence aggregate features (global + per-band) — BOOST\n# ----------------------------\nUSE_AGG_SEQ_FEATURES = True\nN_BANDS = 6\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1.0)\n\ndef build_agg_seq_features(X_mm, B_mm, M_mm, chunk=2048):\n    snr_i = feat[\"snr_tanh\"]\n    det_i = feat[\"detected\"]\n    val_i = feat[VAL_FEAT]\n\n    out_chunks = []\n    for s in range(0, N, chunk):\n        e = min(N, s + chunk)\n        Xc = np.asarray(X_mm[s:e])  # (B,L,F)\n        Bc = np.asarray(B_mm[s:e])  # (B,L)\n        Mc = np.asarray(M_mm[s:e])  # (B,L)\n\n        real = (Mc == 1)\n        tok_count = real.sum(axis=1).astype(np.float32)\n\n        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32)\n        det = (Xc[:, :, det_i] > 0.5).astype(np.float32)\n        val = Xc[:, :, val_i].astype(np.float32)\n\n        snr_r = snr * real\n        det_r = det * real\n\n        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n        max_abs_snr = np.where(tok_count > 0, (snr_r).max(axis=1), 0.0).astype(np.float32)\n\n        if SEQ_TOKEN_MODE == \"mag\":\n            val_r = np.where(real, val, np.nan)\n            mean_val = np.nan_to_num(np.nanmean(val_r, axis=1).astype(np.float32), nan=0.0)\n            std_val  = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32),  nan=0.0)\n            min_val  = np.nan_to_num(np.nanmin(val_r, axis=1).astype(np.float32),  nan=0.0)\n            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n        else:\n            aval = np.abs(val).astype(np.float32)\n            aval_r = aval * real\n            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n            val_r = np.where(real, val, np.nan)\n            std_val = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32), nan=0.0)\n            max_aval = np.where(tok_count > 0, (aval_r).max(axis=1), 0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n\n        per_band = []\n        for b in range(N_BANDS):\n            bm = (Bc == b) & real\n            cnt = bm.sum(axis=1).astype(np.float32)\n            detb = (det * bm).sum(axis=1).astype(np.float32)\n            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n\n            det_frac_b = _safe_div(detb, cnt)\n            mean_abs_snr_b = _safe_div(snrb, cnt)\n\n            if SEQ_TOKEN_MODE == \"mag\":\n                vb = np.where(bm, val, np.nan)\n                mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n            else:\n                ab = (np.abs(val).astype(np.float32) * bm).sum(axis=1).astype(np.float32)\n                mean_val_b = _safe_div(ab, cnt).astype(np.float32)\n\n            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n\n        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n\n        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n\n        out_chunks.append(agg)\n        del Xc, Bc, Mc\n        if (s // chunk) % 3 == 0:\n            gc.collect()\n\n    return np.concatenate(out_chunks, axis=0).astype(np.float32)\n\nif USE_AGG_SEQ_FEATURES:\n    print(\"[Stage 8] Building AGG sequence features (one-time)...\")\n    t0 = time.time()\n    G_seq_np = build_agg_seq_features(X_mm, B_mm, M_mm, chunk=2048)\n    print(f\"[Stage 8] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\nelse:\n    G_seq_np = np.zeros((N,0), dtype=np.float32)\n\nG_raw_np = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\ng_dim = int(G_raw_np.shape[1])\n\nwith open(Path(LOG_DIR)/\"global_feature_spec.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"meta_cols\": BASE_G_COLS,\n            \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n            \"token_mode\": SEQ_TOKEN_MODE,\n            \"val_feat\": VAL_FEAT,\n            \"agg_dim\": int(G_seq_np.shape[1]),\n            \"total_g_dim\": int(g_dim),\n            \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n            \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n        },\n        f,\n        indent=2,\n    )\n\n# ----------------------------\n# 4) Dataset / Loader (num_workers=0) + optional augmentation\n# ----------------------------\nAUG_TOKENDROP_P = 0.05     # 0.0 disable\nAUG_VALUE_NOISE = 0.01     # 0.0 disable\n\nclass MemmapSeqDataset(torch.utils.data.Dataset):\n    def __init__(self, idx, X_mm, B_mm, M_mm, G_scaled_np, y=None, train_mode=False):\n        self.idx = np.asarray(idx, dtype=np.int32)\n        self.X_mm = X_mm\n        self.B_mm = B_mm\n        self.M_mm = M_mm\n        self.G = G_scaled_np\n        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n        self.train_mode = bool(train_mode)\n        self.rng = np.random.default_rng(SEED + (777 if train_mode else 0))\n\n    def __len__(self):\n        return len(self.idx)\n\n    def __getitem__(self, i):\n        j = int(self.idx[i])\n\n        X = np.asarray(self.X_mm[j])  # (L,F) view-ish\n        B = np.asarray(self.B_mm[j])  # (L,)\n        M = np.asarray(self.M_mm[j])  # (L,)\n        G = np.asarray(self.G[j])     # (g_dim,)\n\n        # IMPORTANT: handle SHIFT_BAND_IDS from Stage 6\n        if SHIFT_BAND_IDS:\n            # real bands are 1..6, pad is 0. Convert real->0..5\n            real = (M == 1)\n            if real.any():\n                B = B.astype(np.int16, copy=False)\n                B2 = B.copy()\n                B2[real] = np.clip(B2[real] - 1, 0, N_BANDS - 1)\n                B2[~real] = 0\n                B = B2.astype(np.int8, copy=False)\n\n        if self.train_mode:\n            # token dropout: drop a small fraction of REAL tokens, keep at least 1\n            if AUG_TOKENDROP_P and AUG_TOKENDROP_P > 0:\n                real = (M == 1)\n                nreal = int(real.sum())\n                if nreal > 1:\n                    drop = (self.rng.random(M.shape[0]) < AUG_TOKENDROP_P) & real\n                    if int(drop.sum()) >= nreal:  # would drop all real tokens\n                        keep_pos = np.where(real)[0][int(self.rng.integers(0, nreal))]\n                        drop[keep_pos] = False\n                    if drop.any():\n                        M = M.copy()\n                        M[drop] = 0\n\n            # small value noise on real tokens\n            if AUG_VALUE_NOISE and AUG_VALUE_NOISE > 0:\n                vi = feat[VAL_FEAT]\n                real = (M == 1)\n                if real.any():\n                    X = X.copy()\n                    noise = self.rng.normal(0.0, AUG_VALUE_NOISE, size=int(real.sum())).astype(np.float32)\n                    X[real, vi] = (X[real, vi] + noise).astype(np.float32)\n\n        Xt = torch.from_numpy(X.astype(np.float32, copy=False))\n        Bt = torch.from_numpy(B.astype(np.int64, copy=False))\n        Mt = torch.from_numpy(M.astype(np.int64, copy=False))\n        Gt = torch.from_numpy(G.astype(np.float32, copy=False))\n\n        if self.y is None:\n            return Xt, Bt, Mt, Gt\n\n        yy = float(self.y[j])\n        return Xt, Bt, Mt, Gt, torch.tensor(yy, dtype=torch.float32)\n\ndef make_loader(ds, batch_size, shuffle, sampler=None):\n    return torch.utils.data.DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=(sampler is None and shuffle),\n        sampler=sampler,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n# ----------------------------\n# 5) Model — stronger pooling\n# ----------------------------\nclass MultibandEventTransformer(nn.Module):\n    def __init__(self, feat_dim, max_len, n_bands=6, d_model=160, n_heads=4, n_layers=3, ff_mult=2, dropout=0.12, g_dim=0):\n        super().__init__()\n        self.n_bands = n_bands\n        self.d_model = d_model\n        self.max_len = max_len\n\n        self.x_proj = nn.Sequential(\n            nn.Linear(feat_dim, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n        self.band_emb = nn.Embedding(n_bands, d_model)\n\n        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=int(d_model * ff_mult),\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        self.attn = nn.Linear(d_model, 1)\n        self.pool_ln = nn.LayerNorm(d_model)\n\n        g_out = max(32, d_model // 2)\n        self.g_proj = nn.Sequential(\n            nn.Linear(g_dim, g_out),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model + g_out, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def forward(self, X, band_id, mask, G):\n        X = X.to(torch.float32)\n        band_id = band_id.to(torch.long)\n        mask = mask.to(torch.long)\n\n        # clamp band to 0..n_bands-1\n        band_id = band_id.clamp(0, self.n_bands - 1)\n\n        pad_mask = (mask == 0)  # True=pad\n        all_pad = pad_mask.all(dim=1)\n        if all_pad.any():\n            pad_mask = pad_mask.clone()\n            pad_mask[all_pad, 0] = False\n\n        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        a = self.attn(h).squeeze(-1)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)\n        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n\n        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n        denom = valid.sum(dim=1).clamp_min(1.0)\n        pooled_mean = (h * valid).sum(dim=1) / denom\n\n        pooled = 0.6 * pooled_attn + 0.4 * pooled_mean\n        pooled = self.pool_ln(pooled)\n\n        g = self.g_proj(G.to(torch.float32))\n        z = torch.cat([pooled, g], dim=1)\n        return self.head(z).squeeze(-1)\n\n# ----------------------------\n# 6) Training config (CPU safe)\n# ----------------------------\nCFG = {\n    \"d_model\": 160,\n    \"n_heads\": 4,\n    \"n_layers\": 3,\n    \"ff_mult\": 2,\n    \"dropout\": 0.12,\n\n    \"batch_size\": 16,\n    \"grad_accum\": 2,\n\n    \"epochs\": 14,\n    \"lr\": 5e-4,\n    \"weight_decay\": 0.02,\n\n    \"patience\": 4,            # early stop by AUC\n    \"max_grad_norm\": 1.0,\n\n    # imbalance strategy: \"sampler\" | \"pos_weight\" | \"both\" | \"none\"\n    \"balance_mode\": \"sampler\",\n\n    \"label_smoothing\": 0.03,\n    \"scheduler\": \"onecycle\",\n}\n\n# auto soften for long seq\nif L >= 512:\n    CFG[\"d_model\"] = 128\n    CFG[\"n_heads\"] = 4\n    CFG[\"n_layers\"] = 2\n    CFG[\"batch_size\"] = 12\n    CFG[\"grad_accum\"] = 2\n    CFG[\"lr\"] = 4e-4\n\ncfg_path = Path(LOG_DIR) / \"train_cfg_stage8.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(CFG, f, indent=2)\n\npos_all = int((y == 1).sum())\nneg_all = int((y == 0).sum())\nprint(\"[Stage 8] TRAIN CONFIG (CPU)\")\nprint(f\"- N={N:,} | pos={pos_all:,} | neg={neg_all:,} | pos%={pos_all/max(N,1)*100:.6f}%\")\nprint(f\"- token_mode={SEQ_TOKEN_MODE} | val_feat={VAL_FEAT} | g_dim={g_dim} | use_agg_seq={USE_AGG_SEQ_FEATURES}\")\nprint(f\"- SHIFT_BAND_IDS(from stage6)={SHIFT_BAND_IDS} | PAD_BAND_ID(from stage6)={PAD_BAND_ID}\")\nprint(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\nprint(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\nprint(f\"- balance_mode={CFG['balance_mode']} | label_smoothing={CFG['label_smoothing']}\")\nprint(f\"- CKPT_DIR={CKPT_DIR}\")\nprint(f\"- OOF_DIR ={OOF_DIR}\")\nprint(f\"- LOG_DIR ={LOG_DIR}\")\n\n# ----------------------------\n# 7) Helpers\n# ----------------------------\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef f1_binary(y_true, y_pred01):\n    y_true = y_true.astype(np.int32)\n    y_pred01 = y_pred01.astype(np.int32)\n    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n    if tp == 0:\n        return 0.0\n    prec = tp / max(tp + fp, 1)\n    rec  = tp / max(tp + fn, 1)\n    if prec + rec == 0:\n        return 0.0\n    return float(2 * prec * rec / (prec + rec))\n\n@torch.no_grad()\ndef eval_model(model, loader, criterion):\n    model.eval()\n    losses, logits_all, y_all = [], [], []\n    for batch in loader:\n        Xb, Bb, Mb, Gb, yb = batch\n        Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n        logit = model(Xb, Bb, Mb, Gb)\n        loss = criterion(logit, yb)\n        losses.append(float(loss.item()))\n        logits_all.append(logit.detach().cpu().numpy())\n        y_all.append(yb.detach().cpu().numpy())\n    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n    probs = sigmoid_np(logits_all)\n    pred01 = (probs >= 0.5).astype(np.int8)\n    f1 = f1_binary(y_all, pred01)\n    auc = float(roc_auc_score(y_all, probs)) if (len(np.unique(y_all)) == 2) else float(\"nan\")\n    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1, auc\n\n# fold-wise scaler (NO leakage)\ndef fit_scaler_fold(G_raw_np, tr_idx):\n    X = G_raw_np[tr_idx]\n    mean = X.mean(axis=0).astype(np.float32)\n    std  = X.std(axis=0).astype(np.float32)\n    std  = np.where(std < 1e-6, 1.0, std).astype(np.float32)\n    return mean, std\n\ndef apply_scaler(G_raw_np, mean, std):\n    return ((G_raw_np - mean) / std).astype(np.float32)\n\n# ----------------------------\n# 8) CV Train\n# ----------------------------\noof_prob = np.zeros((N,), dtype=np.float32)\nfold_metrics = []\n\nall_idx = np.arange(N, dtype=np.int32)\nn_splits = int(globals()[\"n_splits\"])\n\nstart_time = time.time()\n\nfor fold_info in globals()[\"folds\"]:\n    fold = int(fold_info[\"fold\"])\n    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n\n    val_mask = np.zeros(N, dtype=bool)\n    val_mask[val_idx] = True\n    tr_idx = all_idx[~val_mask]\n\n    y_tr = y[tr_idx]\n    pos = int((y_tr == 1).sum())\n    neg = int((y_tr == 0).sum())\n    if pos == 0:\n        raise RuntimeError(f\"[Stage 8] Fold {fold}: no positives in training split.\")\n\n    # imbalance knobs\n    balance_mode = str(CFG.get(\"balance_mode\", \"sampler\")).lower()\n    use_sampler = balance_mode in (\"sampler\", \"both\")\n    use_posw    = balance_mode in (\"pos_weight\", \"both\")\n\n    pos_weight = float(neg / max(pos, 1))\n    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\n\n    # label smoothing\n    ls = float(CFG[\"label_smoothing\"])\n    def smooth(yb):\n        if ls <= 0:\n            return yb\n        return yb * (1.0 - ls) + 0.5 * ls\n\n    # criterion\n    if use_posw:\n        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n\n    print(f\"\\n[Stage 8] FOLD {fold}/{n_splits-1} | train={len(tr_idx):,} val={len(val_idx):,} \"\n          f\"| pos={pos:,} neg={neg:,} | pos_weight={pos_weight:.4f} | balance_mode={balance_mode}\")\n\n    # fold-wise scaler (NO leakage)\n    g_mean, g_std = fit_scaler_fold(G_raw_np, tr_idx)\n    G_fold_z = apply_scaler(G_raw_np, g_mean, g_std)\n\n    # datasets\n    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_fold_z, y=y, train_mode=True)\n    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_fold_z, y=y, train_mode=False)\n\n    # optional weighted sampler (train only)\n    sampler = None\n    if use_sampler:\n        w = np.ones((len(tr_idx),), dtype=np.float32)\n        ytr_local = y[tr_idx]\n        w[ytr_local == 1] = float(neg / max(pos, 1))\n        w_t = torch.from_numpy(w)\n        sampler = torch.utils.data.WeightedRandomSampler(weights=w_t, num_samples=len(tr_idx), replacement=True)\n\n    dl_tr = make_loader(ds_tr, batch_size=int(CFG[\"batch_size\"]), shuffle=True, sampler=sampler)\n    dl_va = make_loader(ds_va, batch_size=int(CFG[\"batch_size\"]), shuffle=False)\n\n    model = MultibandEventTransformer(\n        feat_dim=Fdim,\n        max_len=L,\n        n_bands=6,\n        d_model=int(CFG[\"d_model\"]),\n        n_heads=int(CFG[\"n_heads\"]),\n        n_layers=int(CFG[\"n_layers\"]),\n        ff_mult=int(CFG[\"ff_mult\"]),\n        dropout=float(CFG[\"dropout\"]),\n        g_dim=g_dim,\n    ).to(device)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=float(CFG[\"lr\"]), weight_decay=float(CFG[\"weight_decay\"]))\n\n    # scheduler (FIX: steps_per_epoch must match optimizer steps when using grad_accum)\n    scheduler = None\n    grad_accum = int(CFG[\"grad_accum\"])\n    if str(CFG.get(\"scheduler\",\"\")).lower() == \"onecycle\":\n        steps_per_epoch_opt = int(math.ceil(len(dl_tr) / max(grad_accum, 1)))\n        steps_per_epoch_opt = max(steps_per_epoch_opt, 1)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            opt,\n            max_lr=float(CFG[\"lr\"]),\n            epochs=int(CFG[\"epochs\"]),\n            steps_per_epoch=steps_per_epoch_opt,\n            pct_start=0.1,\n            anneal_strategy=\"cos\",\n            div_factor=10.0,\n            final_div_factor=50.0,\n        )\n\n    best_val_auc = -1e9\n    best_val_loss = float(\"inf\")\n    best_epoch = -1\n    best_probs = None\n    patience_left = int(CFG[\"patience\"])\n\n    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        total_loss = 0.0\n        n_batches = 0\n        accum = 0\n        opt_steps = 0\n\n        for batch in dl_tr:\n            Xb, Bb, Mb, Gb, yb = batch\n            Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n\n            yb_s = smooth(yb)\n\n            logit = model(Xb, Bb, Mb, Gb)\n            loss = criterion(logit, yb_s)\n\n            total_loss += float(loss.item())\n            n_batches += 1\n\n            (loss / float(grad_accum)).backward()\n            accum += 1\n\n            if accum == grad_accum:\n                if CFG[\"max_grad_norm\"] is not None:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n                opt.step()\n                opt.zero_grad(set_to_none=True)\n                opt_steps += 1\n                accum = 0\n                if scheduler is not None:\n                    scheduler.step()\n\n        # remainder step\n        if accum > 0:\n            if CFG[\"max_grad_norm\"] is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n            opt.step()\n            opt.zero_grad(set_to_none=True)\n            opt_steps += 1\n            if scheduler is not None:\n                scheduler.step()\n\n        train_loss = total_loss / max(n_batches, 1)\n\n        # validate (use NON-smoothed y)\n        val_loss, probs, y_val, f1_05, val_auc = eval_model(model, dl_va, criterion)\n\n        improved = (val_auc > best_val_auc + 1e-6) or (math.isnan(best_val_auc) and not math.isnan(val_auc))\n        if (not improved) and (abs(val_auc - best_val_auc) <= 1e-6) and (val_loss < best_val_loss - 1e-6):\n            improved = True\n\n        if improved:\n            best_val_auc = float(val_auc)\n            best_val_loss = float(val_loss)\n            best_epoch = int(epoch)\n            best_probs = probs.copy()\n\n            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n            torch.save(\n                {\n                    \"fold\": fold,\n                    \"epoch\": epoch,\n                    \"model_state\": model.state_dict(),\n                    \"cfg\": CFG,\n                    \"seq_feature_names\": SEQ_FEATURE_NAMES,\n                    \"max_len\": L,\n                    \"token_mode\": SEQ_TOKEN_MODE,\n                    \"val_feat\": VAL_FEAT,\n                    \"global_meta_cols\": BASE_G_COLS,\n                    \"use_agg_seq_features\": bool(USE_AGG_SEQ_FEATURES),\n                    \"global_scaler\": {\"mean\": g_mean, \"std\": g_std},\n                    \"pos_weight_train\": float(pos_weight),\n                    \"balance_mode\": balance_mode,\n                    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n                    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n                },\n                ckpt_path,\n            )\n            patience_left = int(CFG[\"patience\"])\n        else:\n            patience_left -= 1\n\n        lr_now = opt.param_groups[0][\"lr\"]\n        print(f\"  epoch {epoch:02d} | lr={lr_now:.2e} | opt_steps={opt_steps:4d} | \"\n              f\"train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | val_auc={val_auc:.5f} | f1@0.5={f1_05:.4f} | \"\n              f\"best_ep={best_epoch} | pat={patience_left}\")\n\n        if patience_left <= 0:\n            break\n\n    if best_probs is None:\n        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n\n    # fill OOF\n    oof_prob[val_idx] = best_probs.astype(np.float32)\n\n    pred01 = (best_probs >= 0.5).astype(np.int8)\n    best_f1_05 = f1_binary(y[val_idx], pred01)\n\n    fold_metrics.append({\n        \"fold\": fold,\n        \"val_size\": int(len(val_idx)),\n        \"best_epoch\": int(best_epoch),\n        \"best_val_auc\": float(best_val_auc),\n        \"best_val_loss\": float(best_val_loss),\n        \"f1_at_0p5\": float(best_f1_05),\n        \"pos_weight_train\": float(pos_weight),\n        \"g_dim\": int(g_dim),\n        \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n        \"balance_mode\": balance_mode,\n        \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n    })\n\n    del model, opt, ds_tr, ds_va, dl_tr, dl_va, G_fold_z\n    gc.collect()\n\nelapsed = time.time() - start_time\n\n# ----------------------------\n# 9) Save OOF artifacts + summary\n# ----------------------------\noof_path_npy = OOF_DIR / \"oof_prob.npy\"\nnp.save(oof_path_npy, oof_prob)\n\ndf_oof = pd.DataFrame({\"object_id\": train_ids, \"target\": y.astype(int), \"oof_prob\": oof_prob.astype(np.float32)})\noof_path_csv = OOF_DIR / \"oof_prob.csv\"\ndf_oof.to_csv(oof_path_csv, index=False)\n\nmetrics_path = OOF_DIR / \"fold_metrics.json\"\nwith open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed)}, f, indent=2)\n\noof_pred01 = (oof_prob >= 0.5).astype(np.int8)\noof_f1_05 = f1_binary(y, oof_pred01)\noof_auc = float(roc_auc_score(y, oof_prob)) if (len(np.unique(y)) == 2) else float(\"nan\")\n\nprint(\"\\n[Stage 8] CV TRAIN DONE\")\nprint(f\"- elapsed: {elapsed/60:.2f} min\")\nprint(f\"- OOF saved: {oof_path_npy}\")\nprint(f\"- OOF saved: {oof_path_csv}\")\nprint(f\"- fold metrics: {metrics_path}\")\nprint(f\"- OOF AUC (rough): {oof_auc:.5f}\")\nprint(f\"- OOF F1@0.5 (rough): {oof_f1_05:.4f}\")\n\nglobals().update({\n    \"oof_prob\": oof_prob,\n    \"OOF_PROB_PATH\": oof_path_npy,\n    \"OOF_CSV_PATH\": oof_path_csv,\n    \"FOLD_METRICS_PATH\": metrics_path,\n    \"TRAIN_CFG_PATH\": cfg_path,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:49:12.932239Z","iopub.execute_input":"2026-01-03T14:49:12.932566Z"}},"outputs":[{"name":"stdout","text":"[Stage 8] Building AGG sequence features (one-time)...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2308144241.py:258: RuntimeWarning: Mean of empty slice\n  mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n","output_type":"stream"},{"name":"stdout","text":"[Stage 8] AGG built: shape=(3043, 31) | time=0.2s\n[Stage 8] TRAIN CONFIG (CPU)\n- N=3,043 | pos=148 | neg=2,895 | pos%=4.863621%\n- token_mode=mag | val_feat=mag | g_dim=38 | use_agg_seq=True\n- SHIFT_BAND_IDS(from stage6)=False | PAD_BAND_ID(from stage6)=0\n- Model: d_model=160 heads=4 layers=3 dropout=0.12\n- Batch=16 grad_accum=2 epochs=14 lr=0.0005\n- balance_mode=sampler | label_smoothing=0.03\n- CKPT_DIR=/kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/checkpoints\n- OOF_DIR =/kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/oof\n- LOG_DIR =/kaggle/working/mallorn_run/run_20260103_134051_143e1c6a76/logs\n\n[Stage 8] FOLD 0/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=sampler\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n/tmp/ipykernel_55/2308144241.py:366: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n  Xt = torch.from_numpy(X.astype(np.float32, copy=False))\n","output_type":"stream"},{"name":"stdout","text":"  epoch 01 | lr=4.19e-04 | opt_steps=  77 | train_loss=0.68123 | val_loss=0.52205 | val_auc=0.68181 | f1@0.5=0.0615 | best_ep=1 | pat=4\n  epoch 02 | lr=4.97e-04 | opt_steps=  77 | train_loss=0.61258 | val_loss=0.73800 | val_auc=0.70150 | f1@0.5=0.1620 | best_ep=2 | pat=4\n  epoch 03 | lr=4.80e-04 | opt_steps=  77 | train_loss=0.54667 | val_loss=0.54998 | val_auc=0.75596 | f1@0.5=0.1756 | best_ep=3 | pat=4\n  epoch 04 | lr=4.49e-04 | opt_steps=  77 | train_loss=0.48130 | val_loss=0.41873 | val_auc=0.81911 | f1@0.5=0.2460 | best_ep=4 | pat=4\n  epoch 05 | lr=4.05e-04 | opt_steps=  77 | train_loss=0.43914 | val_loss=0.47571 | val_auc=0.83074 | f1@0.5=0.2212 | best_ep=5 | pat=4\n  epoch 06 | lr=3.52e-04 | opt_steps=  77 | train_loss=0.40878 | val_loss=0.33423 | val_auc=0.84231 | f1@0.5=0.2714 | best_ep=6 | pat=4\n  epoch 07 | lr=2.93e-04 | opt_steps=  77 | train_loss=0.39504 | val_loss=0.43668 | val_auc=0.84790 | f1@0.5=0.2434 | best_ep=7 | pat=4\n  epoch 08 | lr=2.31e-04 | opt_steps=  77 | train_loss=0.38102 | val_loss=0.41976 | val_auc=0.84347 | f1@0.5=0.2599 | best_ep=7 | pat=3\n  epoch 09 | lr=1.70e-04 | opt_steps=  77 | train_loss=0.36524 | val_loss=0.38303 | val_auc=0.84439 | f1@0.5=0.2857 | best_ep=7 | pat=2\n  epoch 10 | lr=1.14e-04 | opt_steps=  77 | train_loss=0.35977 | val_loss=0.43153 | val_auc=0.84767 | f1@0.5=0.2652 | best_ep=7 | pat=1\n  epoch 11 | lr=6.71e-05 | opt_steps=  77 | train_loss=0.36650 | val_loss=0.34635 | val_auc=0.84922 | f1@0.5=0.2667 | best_ep=11 | pat=4\n  epoch 12 | lr=3.10e-05 | opt_steps=  77 | train_loss=0.37017 | val_loss=0.41778 | val_auc=0.85066 | f1@0.5=0.2584 | best_ep=12 | pat=4\n  epoch 13 | lr=8.52e-06 | opt_steps=  77 | train_loss=0.37697 | val_loss=0.39467 | val_auc=0.85078 | f1@0.5=0.2805 | best_ep=13 | pat=4\n  epoch 14 | lr=1.00e-06 | opt_steps=  77 | train_loss=0.36985 | val_loss=0.39533 | val_auc=0.85107 | f1@0.5=0.2805 | best_ep=14 | pat=4\n\n[Stage 8] FOLD 1/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=sampler\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  epoch 01 | lr=4.19e-04 | opt_steps=  77 | train_loss=0.67701 | val_loss=0.65374 | val_auc=0.69522 | f1@0.5=0.1548 | best_ep=1 | pat=4\n  epoch 02 | lr=4.97e-04 | opt_steps=  77 | train_loss=0.59427 | val_loss=0.50821 | val_auc=0.69827 | f1@0.5=0.1545 | best_ep=2 | pat=4\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# OOF Prediction + Threshold Tuning","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v3.1 (ALIGN SUPER ROBUST + MULTI-METRIC + FAST SWEEP)\n#\n# Upgrade v3.1 vs v3:\n# - String-map df_train_meta.index -> row positions (anti dtype mismatch)\n# - Prefer oof_prob.csv (object_id + oof_prob) for safest alignment\n# - Clean oof_prob NaN/inf + clip [0,1]\n# - FAST threshold sweep via sorting + cumulative counts (vectorized)\n# - Best thresholds for: F1, Accuracy, Balanced Accuracy, MCC (+ Precision/Recall)\n# - Exports multiple BEST thresholds\n# ============================================================\n\nimport gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\"OOF_DIR\", \"df_train_meta\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n\nOOF_DIR = Path(OOF_DIR)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Helper: robust stringify id\n# ----------------------------\ndef _to_str_list(ids):\n    out = []\n    for x in ids:\n        if isinstance(x, (bytes, np.bytes_)):\n            out.append(x.decode(\"utf-8\", errors=\"ignore\").strip())\n        else:\n            out.append(str(x).strip())\n    return out\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\n# ----------------------------\n# Detect target column in df_train_meta\n# ----------------------------\ndef _detect_target_col(df):\n    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\nTARGET_COL = _detect_target_col(df_train_meta)\nif TARGET_COL is None:\n    raise RuntimeError(\n        \"Cannot detect target column in df_train_meta. \"\n        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n    )\n\n# ----------------------------\n# Load OOF (prefer CSV)\n# ----------------------------\ndef _load_oof():\n    pcsv = OOF_DIR / \"oof_prob.csv\"\n    if pcsv.exists():\n        df = pd.read_csv(pcsv)\n        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n            ids = df[\"object_id\"].astype(str).str.strip().tolist()\n            prob = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n            return ids, prob, \"csv(oof_prob.csv)\"\n\n    if \"oof_prob\" in globals():\n        prob = _as_1d_float32(globals()[\"oof_prob\"])\n        if isinstance(prob, np.ndarray) and prob.ndim != 0:\n            if \"train_ids_ordered\" in globals():\n                ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n                return ids, prob, \"globals(oof_prob + train_ids_ordered)\"\n            if len(prob) == len(df_train_meta):\n                ids = _to_str_list(df_train_meta.index.tolist())\n                return ids, prob, \"globals(oof_prob + df_train_meta.index)\"\n\n    pnpy = OOF_DIR / \"oof_prob.npy\"\n    if pnpy.exists():\n        prob = _as_1d_float32(np.load(pnpy, allow_pickle=False))\n        if \"train_ids_ordered\" in globals():\n            ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n            return ids, prob, \"npy(oof_prob.npy + train_ids_ordered)\"\n        if len(prob) == len(df_train_meta):\n            ids = _to_str_list(df_train_meta.index.tolist())\n            return ids, prob, \"npy(oof_prob.npy + df_train_meta.index)\"\n\n    raise FileNotFoundError(\"OOF prob not found (csv/globals/npy). Jalankan STAGE 8 dulu.\")\n\ntrain_ids, oof_prob, src = _load_oof()\n\nif not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n    raise TypeError(f\"Invalid oof_prob. Type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}\")\n\n# sanitize prob\noof_prob = np.nan_to_num(oof_prob, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32)\noof_prob = np.clip(oof_prob, 0.0, 1.0).astype(np.float32)\n\n# ----------------------------\n# SUPER ROBUST alignment: string-map df_train_meta.index -> positions\n# ----------------------------\nmeta_ids = _to_str_list(df_train_meta.index.tolist())\npos_map = pd.Series(np.arange(len(meta_ids), dtype=np.int32), index=pd.Index(meta_ids, dtype=\"object\"))\n\npos_idx = pos_map.reindex(train_ids).to_numpy()\nif np.isnan(pos_idx).any():\n    bad = [train_ids[i] for i in np.where(np.isnan(pos_idx))[0][:10]]\n    raise KeyError(f\"OOF ids not found in df_train_meta.index (string-mapped). ex={bad} | missing_n={int(np.isnan(pos_idx).sum())}\")\n\npos_idx = pos_idx.astype(np.int32)\n\n# load y aligned\ny_raw = pd.to_numeric(df_train_meta[TARGET_COL], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy()\ny = y_raw[pos_idx]\ny = (y > 0).astype(np.int8)\n\nif len(oof_prob) != len(y):\n    raise RuntimeError(f\"Length mismatch: oof_prob={len(oof_prob)} vs y={len(y)}\")\n\nuy = set(np.unique(y).tolist())\nif not uy.issubset({0, 1}):\n    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n\nN = int(len(y))\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\n\nprint(f\"[Stage 9] Loaded OOF from: {src}\")\nprint(f\"[Stage 9] N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}% | target_col={TARGET_COL}\")\n\n# ----------------------------\n# 1) Metrics helpers (vectorized-safe)\n# ----------------------------\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1e-12)\n\ndef _metrics_from_counts(tp, fp, fn, tn):\n    tp = tp.astype(np.float64)\n    fp = fp.astype(np.float64)\n    fn = fn.astype(np.float64)\n    tn = tn.astype(np.float64)\n\n    prec = _safe_div(tp, tp + fp)\n    rec  = _safe_div(tp, tp + fn)\n    f1   = _safe_div(2 * prec * rec, prec + rec)\n\n    acc  = _safe_div(tp + tn, tp + fp + fn + tn)\n\n    tpr  = _safe_div(tp, tp + fn)\n    tnr  = _safe_div(tn, tn + fp)\n    bacc = 0.5 * (tpr + tnr)\n\n    num = tp * tn - fp * fn\n    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n\n    return f1, prec, rec, acc, bacc, mcc\n\n# ----------------------------\n# 2) Threshold candidates (grid + quantiles + unique-prob sampling)\n# ----------------------------\ngrid = np.concatenate([\n    np.linspace(0.00, 0.10, 41),\n    np.linspace(0.10, 0.90, 161),\n    np.linspace(0.90, 1.00, 41),\n]).astype(np.float32)\n\nqs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\nquant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n\nuniq = np.unique(oof_prob)\nif len(uniq) > 6000:\n    take = np.linspace(0, len(uniq) - 1, 6000, dtype=int)\n    uniq = uniq[take].astype(np.float32)\n\nthr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr, uniq]), 0.0, 1.0)).astype(np.float32)\n\n# ----------------------------\n# 3) FAST sweep via sorting + cumulative counts\n#    Predict positive if oof_prob >= thr\n# ----------------------------\n# sort probabilities descending\nord_desc = np.argsort(-oof_prob)\np_sorted = oof_prob[ord_desc]\ny_sorted = y[ord_desc].astype(np.int8)\n\n# cumulative pos/neg for prefix k (k predicted positive)\npos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\nneg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\n\npos_total = int(pos_prefix[-1]) if N > 0 else 0\nneg_total = int(neg_prefix[-1]) if N > 0 else 0\n\n# for each thr, k = number of items with prob >= thr\n# since p_sorted is descending, find leftmost index where p_sorted < thr\n# k = searchsorted(-p_sorted, -thr, side=\"left\")\nk = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"left\").astype(np.int64)\n\ntp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\nfp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\nfn = (pos_total - tp).astype(np.int64)\ntn = (neg_total - fp).astype(np.int64)\n\nf1, prec, rec, acc, bacc, mmc = _metrics_from_counts(tp, fp, fn, tn)\npos_pred = k.astype(np.int64)\n\nthr_table = pd.DataFrame({\n    \"thr\": thr_candidates.astype(np.float32),\n    \"f1\": f1.astype(np.float32),\n    \"precision\": prec.astype(np.float32),\n    \"recall\": rec.astype(np.float32),\n    \"accuracy\": acc.astype(np.float32),\n    \"balanced_accuracy\": bacc.astype(np.float32),\n    \"mcc\": mmc.astype(np.float32),\n    \"tp\": tp.astype(np.int64),\n    \"fp\": fp.astype(np.int64),\n    \"fn\": fn.astype(np.int64),\n    \"tn\": tn.astype(np.int64),\n    \"pos_pred\": pos_pred.astype(np.int64),\n})\n\n# ----------------------------\n# 4) Pick best thresholds with tie-breakers\n# ----------------------------\ndef _pick_best(df, primary, tie_cols):\n    sort_cols = [primary] + tie_cols\n    asc = [False] * len(sort_cols)\n    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n\nbest_f1_row  = _pick_best(thr_table, \"f1\", [\"recall\", \"precision\", \"balanced_accuracy\"])\nbest_acc_row = _pick_best(thr_table, \"accuracy\", [\"balanced_accuracy\", \"f1\"])\nbest_bac_row = _pick_best(thr_table, \"balanced_accuracy\", [\"accuracy\", \"f1\"])\nbest_mcc_row = _pick_best(thr_table, \"mcc\", [\"f1\", \"balanced_accuracy\"])\n\ndef _row_to_dict(r):\n    return {\n        \"thr\": float(r[\"thr\"]),\n        \"f1\": float(r[\"f1\"]),\n        \"precision\": float(r[\"precision\"]),\n        \"recall\": float(r[\"recall\"]),\n        \"accuracy\": float(r[\"accuracy\"]),\n        \"balanced_accuracy\": float(r[\"balanced_accuracy\"]),\n        \"mcc\": float(r[\"mcc\"]),\n        \"tp\": int(r[\"tp\"]), \"fp\": int(r[\"fp\"]), \"fn\": int(r[\"fn\"]), \"tn\": int(r[\"tn\"]),\n        \"pos_pred\": int(r[\"pos_pred\"]),\n    }\n\ndef _eval_at(thr):\n    thr = float(thr)\n    k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"left\"))\n    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n    fn0 = int(pos_total - tp0)\n    tn0 = int(neg_total - fp0)\n\n    prec0 = tp0 / max(tp0 + fp0, 1)\n    rec0  = tp0 / max(tp0 + fn0, 1)\n    f10 = 0.0 if (tp0 == 0 or (prec0 + rec0) == 0) else (2 * prec0 * rec0 / (prec0 + rec0))\n    acc0 = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n\n    return {\n        \"thr\": thr,\n        \"f1\": float(f10),\n        \"precision\": float(prec0),\n        \"recall\": float(rec0),\n        \"accuracy\": float(acc0),\n        \"balanced_accuracy\": float(bacc0),\n        \"mcc\": float(mcc0),\n        \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0,\n        \"pos_pred\": int(k0),\n    }\n\nbase05 = _eval_at(0.5)\n\nBEST_THR_F1   = float(best_f1_row[\"thr\"])\nBEST_THR_ACC  = float(best_acc_row[\"thr\"])\nBEST_THR_BACC = float(best_bac_row[\"thr\"])\nBEST_THR_MCC  = float(best_mcc_row[\"thr\"])\n\nbest_f1_full  = _eval_at(BEST_THR_F1)\nbest_acc_full = _eval_at(BEST_THR_ACC)\nbest_bac_full = _eval_at(BEST_THR_BACC)\nbest_mcc_full = _eval_at(BEST_THR_MCC)\n\n# ----------------------------\n# 5) Save artifacts\n# ----------------------------\nout_json = OOF_DIR / \"threshold_tuning.json\"\nout_txt  = OOF_DIR / \"threshold_report.txt\"\nout_csv  = OOF_DIR / \"threshold_table_top500.csv\"\n\npayload = {\n    \"source\": src,\n    \"target_col\": TARGET_COL,\n    \"n_objects\": int(N),\n    \"pos\": int(pos),\n    \"neg\": int(neg),\n    \"baseline_thr_0p5\": base05,\n    \"best_thr_f1\": best_f1_full,\n    \"best_thr_accuracy\": best_acc_full,\n    \"best_thr_balanced_accuracy\": best_bac_full,\n    \"best_thr_mcc\": best_mcc_full,\n}\n\nwith open(out_json, \"w\", encoding=\"utf-8\") as f:\n    json.dump(payload, f, indent=2)\n\nthr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).head(500).to_csv(out_csv, index=False)\n\ntop_f1 = thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).head(10).reset_index(drop=True)\n\nlines = []\nlines.append(\"OOF Threshold Tuning Report (v3.1)\")\nlines.append(f\"- source={src}\")\nlines.append(f\"- target_col={TARGET_COL}\")\nlines.append(f\"- N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\nlines.append(\"\")\nlines.append(\"Baseline @ thr=0.5\")\nlines.append(f\"- F1={base05['f1']:.6f} | P={base05['precision']:.6f} | R={base05['recall']:.6f} | \"\n             f\"ACC={base05['accuracy']:.6f} | BACC={base05['balanced_accuracy']:.6f} | MCC={base05['mcc']:.6f}\")\nlines.append(f\"- tp={base05['tp']} fp={base05['fp']} fn={base05['fn']} tn={base05['tn']} | pos_pred={base05['pos_pred']}\")\nlines.append(\"\")\nlines.append(f\"BEST-F1   @ thr={best_f1_full['thr']:.6f} | F1={best_f1_full['f1']:.6f} | P={best_f1_full['precision']:.6f} | R={best_f1_full['recall']:.6f} | pos_pred={best_f1_full['pos_pred']}\")\nlines.append(f\"BEST-ACC  @ thr={best_acc_full['thr']:.6f} | ACC={best_acc_full['accuracy']:.6f} | BACC={best_acc_full['balanced_accuracy']:.6f} | F1={best_acc_full['f1']:.6f}\")\nlines.append(f\"BEST-BACC @ thr={best_bac_full['thr']:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f} | ACC={best_bac_full['accuracy']:.6f} | F1={best_bac_full['f1']:.6f}\")\nlines.append(f\"BEST-MCC  @ thr={best_mcc_full['thr']:.6f} | MCC={best_mcc_full['mcc']:.6f} | F1={best_mcc_full['f1']:.6f} | BACC={best_mcc_full['balanced_accuracy']:.6f}\")\nlines.append(\"\")\nlines.append(\"Top 10 by F1:\")\nfor i in range(len(top_f1)):\n    r = top_f1.iloc[i]\n    lines.append(f\"{i+1:02d}. thr={float(r['thr']):.6f} | f1={float(r['f1']):.6f} | P={float(r['precision']):.6f} | \"\n                 f\"R={float(r['recall']):.6f} | ACC={float(r['accuracy']):.6f} | BACC={float(r['balanced_accuracy']):.6f} | \"\n                 f\"MCC={float(r['mcc']):.6f} | pos_pred={int(r['pos_pred'])}\")\n\nwith open(out_txt, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\nprint(\"[Stage 9] DONE\")\nprint(f\"- Saved: {out_json}\")\nprint(f\"- Saved: {out_txt}\")\nprint(f\"- Saved: {out_csv}\")\nprint(f\"- BEST_THR_F1  ={BEST_THR_F1:.6f} | F1={best_f1_full['f1']:.6f} (P={best_f1_full['precision']:.6f} R={best_f1_full['recall']:.6f})\")\nprint(f\"- BEST_THR_ACC ={BEST_THR_ACC:.6f} | ACC={best_acc_full['accuracy']:.6f} BACC={best_acc_full['balanced_accuracy']:.6f} F1={best_acc_full['f1']:.6f}\")\nprint(f\"- BEST_THR_BACC={BEST_THR_BACC:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f} ACC={best_bac_full['accuracy']:.6f} F1={best_bac_full['f1']:.6f}\")\nprint(f\"- BEST_THR_MCC ={BEST_THR_MCC:.6f} | MCC={best_mcc_full['mcc']:.6f} F1={best_mcc_full['f1']:.6f} BACC={best_mcc_full['balanced_accuracy']:.6f}\")\n\nglobals().update({\n    \"train_ids_oof\": train_ids,\n    \"oof_prob\": oof_prob,\n    \"BEST_THR\": BEST_THR_F1,          # default tetap F1\n    \"BEST_THR_F1\": BEST_THR_F1,\n    \"BEST_THR_ACC\": BEST_THR_ACC,\n    \"BEST_THR_BACC\": BEST_THR_BACC,\n    \"BEST_THR_MCC\": BEST_THR_MCC,\n    \"thr_table\": thr_table,\n    \"THR_JSON_PATH\": out_json,\n    \"THR_REPORT_PATH\": out_txt,\n    \"THR_TABLE_CSV_PATH\": out_csv,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Inference (Fold Ensemble)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v3.2 (MATCH STAGE8 FORWARD + BUILD TEST AGG FEATS + LOGIT ENSEMBLE + ID ALIGN HARD)\n#\n# Fix utama vs v3.1 kamu:\n# - Forward model sama dengan STAGE 8 (x_proj includes GELU, pooling mix attn+mean)\n# - Build G_test raw = meta_cols + agg_seq_feats (dari memmap) persis STAGE 8\n# - Align df_test_meta.index pakai string-map (anti dtype mismatch)\n# - Apply fold scaler dari ckpt (NO leakage, consistent)\n# ============================================================\n\nimport os, gc, json, re, math, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True.*\")\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\"ART_DIR\",\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_test_meta\",\"CKPT_DIR\",\"n_splits\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n\n# Torch\ntry:\n    import torch\n    import torch.nn as nn\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\ndevice = torch.device(\"cpu\")\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# Thread guard (CPU)\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\nFIX_DIR = Path(FIX_DIR)\nART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\nCKPT_DIR = Path(CKPT_DIR)\n\nOUT_DIR = ART_DIR / \"preds\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# helper: normalize id robustly\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _load_ids_npy(path: Path):\n    arr = np.load(path, allow_pickle=False)\n    xs = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n    return [_norm_id(z) for z in xs]\n\n# ----------------------------\n# 1) Load TEST ordering (must match STAGE 6)\n# ----------------------------\ntest_ids_path = FIX_DIR / \"test_ids.npy\"\nif not test_ids_path.exists():\n    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n\ntest_ids = _load_ids_npy(test_ids_path)\nNTE = len(test_ids)\nif NTE <= 0:\n    raise RuntimeError(\"test_ids kosong (NTE=0). Pastikan STAGE 6 sukses membuat test_ids.npy.\")\n\n# Align df_test_meta.index via string-map (HARD)\ndf_test_meta = df_test_meta.copy(deep=False)\nmeta_ids = [_norm_id(z) for z in df_test_meta.index.tolist()]\ndf_test_meta.index = pd.Index(meta_ids, name=df_test_meta.index.name)\n\npos_map = pd.Series(np.arange(len(meta_ids), dtype=np.int32), index=pd.Index(meta_ids, dtype=\"object\"))\npos_idx = pos_map.reindex(test_ids).to_numpy()\nif np.isnan(pos_idx).any():\n    bad = [test_ids[i] for i in np.where(np.isnan(pos_idx))[0][:10]]\n    raise KeyError(f\"Some test_ids not found in df_test_meta.index (string-mapped). ex={bad} | missing_n={int(np.isnan(pos_idx).sum())}\")\npos_idx = pos_idx.astype(np.int32)\n\nif len(set(test_ids)) != len(test_ids):\n    s = pd.Series(test_ids)\n    dup = s[s.duplicated()].head(10).tolist()\n    raise ValueError(f\"Duplicate object_id in test_ids ordering (examples): {dup}\")\n\n# ----------------------------\n# 2) Open fixed-length TEST memmaps\n# ----------------------------\nSEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\nFdim = len(SEQ_FEATURE_NAMES)\nL = int(MAX_LEN)\n\ntest_X_path = FIX_DIR / \"test_X.dat\"\ntest_B_path = FIX_DIR / \"test_B.dat\"\ntest_M_path = FIX_DIR / \"test_M.dat\"\nfor p in [test_X_path, test_B_path, test_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\nXte = np.memmap(test_X_path, dtype=np.float32, mode=\"r\", shape=(NTE, L, Fdim))\nBte = np.memmap(test_B_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\nMte = np.memmap(test_M_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n\nfeat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n\n# detect token mode (same as STAGE 8)\nif (\"mag\" in feat) and (\"mag_err_log\" in feat):\n    SEQ_TOKEN_MODE = \"mag\"\n    VAL_FEAT = \"mag\"\nelif (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n    SEQ_TOKEN_MODE = \"asinh\"\n    VAL_FEAT = \"flux_asinh\"\nelse:\n    raise RuntimeError(f\"Cannot infer token_mode from SEQ_FEATURE_NAMES. Found={SEQ_FEATURE_NAMES}\")\n\nfor k in [\"snr_tanh\",\"detected\"]:\n    if k not in feat:\n        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\nif VAL_FEAT not in feat:\n    raise RuntimeError(f\"Feature '{VAL_FEAT}' missing for token_mode={SEQ_TOKEN_MODE}.\")\n\n# ----------------------------\n# 3) Checkpoints (fold_*.pt)\n# ----------------------------\nckpts = []\nfor f in range(int(n_splits)):\n    p = CKPT_DIR / f\"fold_{f}.pt\"\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n    ckpts.append(p)\n\n# ----------------------------\n# 4) Safe/compat checkpoint loader\n# ----------------------------\ndef torch_load_compat(path: Path):\n    try:\n        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n        if isinstance(obj, dict) and (\"model_state\" in obj or \"cfg\" in obj or \"global_scaler\" in obj):\n            return obj\n        return torch.load(path, map_location=\"cpu\", weights_only=False)\n    except TypeError:\n        return torch.load(path, map_location=\"cpu\")\n    except Exception:\n        return torch.load(path, map_location=\"cpu\", weights_only=False)\n\ndef extract_state_and_meta(ckpt_obj):\n    if isinstance(ckpt_obj, dict) and \"model_state\" in ckpt_obj and isinstance(ckpt_obj[\"model_state\"], dict):\n        return ckpt_obj[\"model_state\"], ckpt_obj\n    if isinstance(ckpt_obj, dict):\n        any_tensor = any(hasattr(v, \"shape\") for v in ckpt_obj.values())\n        if any_tensor:\n            return ckpt_obj, {}\n        return ckpt_obj, ckpt_obj\n    raise RuntimeError(f\"Unsupported ckpt object type: {type(ckpt_obj)}\")\n\n# ----------------------------\n# 5) Infer architecture from state_dict\n# ----------------------------\ndef infer_from_state(sd: dict):\n    keys = set(sd.keys())\n\n    if \"band_emb.weight\" not in sd:\n        raise RuntimeError(\"state_dict missing band_emb.weight.\")\n    n_bands = int(sd[\"band_emb.weight\"].shape[0])\n    d_model = int(sd[\"band_emb.weight\"].shape[1])\n\n    if \"pos_emb\" not in sd:\n        raise RuntimeError(\"state_dict missing pos_emb.\")\n    max_len_ckpt = int(sd[\"pos_emb\"].shape[1])\n\n    xproj_is_seq = (\"x_proj.0.weight\" in keys)\n    if xproj_is_seq:\n        feat_dim = int(sd[\"x_proj.0.weight\"].shape[1])\n    else:\n        if \"x_proj.weight\" not in sd:\n            raise RuntimeError(\"state_dict missing x_proj.weight or x_proj.0.weight.\")\n        feat_dim = int(sd[\"x_proj.weight\"].shape[1])\n\n    # g_proj\n    if \"g_proj.0.weight\" in sd:\n        g_dim = int(sd[\"g_proj.0.weight\"].shape[1])\n        g_hidden = int(sd[\"g_proj.0.weight\"].shape[0])\n    else:\n        g_dim = 0\n        g_hidden = max(32, d_model // 2)\n\n    # encoder layers\n    layer_ids = set()\n    for k in keys:\n        m = re.match(r\"encoder\\.layers\\.(\\d+)\\.\", k)\n        if m:\n            layer_ids.add(int(m.group(1)))\n    n_layers = (max(layer_ids) + 1) if layer_ids else 0\n    if n_layers <= 0:\n        raise RuntimeError(\"Cannot infer n_layers from state_dict (encoder.layers.* not found).\")\n\n    # dim_ff from linear1\n    k_lin1 = \"encoder.layers.0.linear1.weight\"\n    if k_lin1 in sd:\n        dim_ff = int(sd[k_lin1].shape[0])\n    else:\n        lin1_keys = [k for k in keys if k.endswith(\"linear1.weight\")]\n        if not lin1_keys:\n            raise RuntimeError(\"Cannot infer dim_feedforward (linear1.weight not found).\")\n        dim_ff = int(sd[sorted(lin1_keys)[0]].shape[0])\n\n    has_pool_ln = (\"pool_ln.weight\" in keys and \"pool_ln.bias\" in keys)\n\n    # head final idx\n    head_w_idx = []\n    for k in keys:\n        m = re.match(r\"head\\.(\\d+)\\.weight\", k)\n        if m:\n            head_w_idx.append(int(m.group(1)))\n    if not head_w_idx:\n        raise RuntimeError(\"Cannot infer head structure (head.*.weight not found).\")\n    head_final_idx = max(sorted(set(head_w_idx)))\n\n    return {\n        \"n_bands\": n_bands,\n        \"d_model\": d_model,\n        \"max_len_ckpt\": max_len_ckpt,\n        \"feat_dim\": feat_dim,\n        \"g_dim\": g_dim,\n        \"g_hidden\": g_hidden,\n        \"n_layers\": n_layers,\n        \"dim_ff\": dim_ff,\n        \"has_pool_ln\": has_pool_ln,\n        \"xproj_is_seq\": xproj_is_seq,\n        \"head_final_idx\": head_final_idx,\n    }\n\n# ----------------------------\n# 6) Build model that matches STAGE 8 forward\n# ----------------------------\nclass FlexMultibandEventTransformer(nn.Module):\n    def __init__(self, feat_dim, max_len, n_bands, d_model, n_heads, n_layers, dim_ff, dropout,\n                 g_dim, g_hidden, xproj_is_seq=True, has_pool_ln=True, head_final_idx=3):\n        super().__init__()\n        self.n_bands = int(n_bands)\n        self.max_len = int(max_len)\n        self.d_model = int(d_model)\n\n        # IMPORTANT: match STAGE 8 (Linear + GELU + Dropout)\n        if xproj_is_seq:\n            self.x_proj = nn.Sequential(\n                nn.Linear(int(feat_dim), int(d_model)),\n                nn.GELU(),\n                nn.Dropout(float(dropout)),\n            )\n        else:\n            # still include GELU+Dropout for forward match\n            self.x_proj = nn.Sequential(\n                nn.Linear(int(feat_dim), int(d_model)),\n                nn.GELU(),\n                nn.Dropout(float(dropout)),\n            )\n\n        self.band_emb = nn.Embedding(int(n_bands), int(d_model))\n\n        self.pos_emb = nn.Parameter(torch.zeros(1, int(max_len), int(d_model)))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=int(d_model),\n            nhead=int(n_heads),\n            dim_feedforward=int(dim_ff),\n            dropout=float(dropout),\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=int(n_layers))\n\n        self.attn = nn.Linear(int(d_model), 1)\n\n        self.has_pool_ln = bool(has_pool_ln)\n        if self.has_pool_ln:\n            self.pool_ln = nn.LayerNorm(int(d_model))\n\n        self.g_proj = nn.Sequential(\n            nn.Linear(int(g_dim), int(g_hidden)),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n        )\n\n        in_head = int(d_model + g_hidden)\n        if head_final_idx == 3:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.GELU(),\n                nn.Dropout(float(dropout)),\n                nn.Linear(int(d_model), 1),\n            )\n        elif head_final_idx == 2:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.GELU(),\n                nn.Linear(int(d_model), 1),\n            )\n        else:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.Linear(int(d_model), 1),\n            )\n\n    def forward(self, X, band_id, mask, G):\n        X = X.to(torch.float32)\n        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n        mask = mask.to(torch.long)\n\n        pad_mask = (mask == 0)  # True=pad\n        all_pad = pad_mask.all(dim=1)\n        if all_pad.any():\n            pad_mask = pad_mask.clone()\n            pad_mask[all_pad, 0] = False\n\n        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        # attn pooling\n        a = self.attn(h).squeeze(-1)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)\n        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n\n        # mean pooling on valid tokens\n        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n        denom = valid.sum(dim=1).clamp_min(1.0)\n        pooled_mean = (h * valid).sum(dim=1) / denom\n\n        pooled = 0.6 * pooled_attn + 0.4 * pooled_mean  # match STAGE 8\n        if self.has_pool_ln:\n            pooled = self.pool_ln(pooled)\n\n        g = self.g_proj(G.to(torch.float32))\n        z = torch.cat([pooled, g], dim=1)\n        return self.head(z).squeeze(-1)  # logit\n\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\n# ----------------------------\n# 7) Build TEST global features (meta + agg seq feats) — match STAGE 8\n# ----------------------------\nBASE_G_COLS_DEFAULT = [\"Z\",\"Z_err\",\"EBV\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\nN_BANDS = 6\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1.0)\n\ndef build_agg_seq_features_memmap(Xmm, Bmm, Mmm, chunk=512):\n    \"\"\"\n    Match STAGE 8 agg:\n      glob: [tok_count, det_frac, mean_abs_snr, max_abs_snr] (4)\n      val stats:\n        mag  : [mean_mag, std_mag, min_mag] (3)\n        asinh: [mean_abs_flux, std_flux, max_abs_flux] (3)\n      per-band (b=0..5): [cnt, det_frac_b, mean_abs_snr_b, mean_val_b] (4*6=24)\n    total agg_dim=31\n    \"\"\"\n    snr_i = feat[\"snr_tanh\"]\n    det_i = feat[\"detected\"]\n    val_i = feat[VAL_FEAT]\n\n    out = np.zeros((NTE, 31), dtype=np.float32)\n\n    for start in range(0, NTE, int(chunk)):\n        end = min(NTE, start + int(chunk))\n        Xc = np.asarray(Xmm[start:end])  # (B,L,F)\n        Bc = np.asarray(Bmm[start:end])  # (B,L)\n        Mc = np.asarray(Mmm[start:end])  # (B,L)\n\n        real = (Mc == 1)\n        tok_count = real.sum(axis=1).astype(np.float32)\n\n        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32)\n        det = (Xc[:, :, det_i] > 0.5).astype(np.float32)\n        val = Xc[:, :, val_i].astype(np.float32)\n\n        snr_r = snr * real\n        det_r = det * real\n\n        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n        max_abs_snr = np.where(tok_count > 0, (snr * real).max(axis=1), 0.0).astype(np.float32)\n\n        if SEQ_TOKEN_MODE == \"mag\":\n            val_r = np.where(real, val, np.nan)\n            mean_val = np.nanmean(val_r, axis=1).astype(np.float32)\n            std_val  = np.nanstd(val_r, axis=1).astype(np.float32)\n            min_val  = np.nanmin(val_r, axis=1).astype(np.float32)\n            mean_val = np.nan_to_num(mean_val, nan=0.0).astype(np.float32)\n            std_val  = np.nan_to_num(std_val,  nan=0.0).astype(np.float32)\n            min_val  = np.nan_to_num(min_val,  nan=0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1)\n        else:\n            aval = np.abs(val)\n            aval_r = aval * real\n            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count)\n            val_r = np.where(real, val, np.nan)\n            std_val = np.nanstd(val_r, axis=1).astype(np.float32)\n            max_aval = np.where(tok_count > 0, (aval * real).max(axis=1), 0.0).astype(np.float32)\n            std_val = np.nan_to_num(std_val, nan=0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_aval.astype(np.float32), std_val, max_aval], axis=1)\n\n        per_band = []\n        for b in range(N_BANDS):\n            bm = (Bc == b) & real\n            cnt = bm.sum(axis=1).astype(np.float32)\n\n            detb = (det * bm).sum(axis=1).astype(np.float32)\n            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n\n            det_frac_b = _safe_div(detb, cnt)\n            mean_abs_snr_b = _safe_div(snrb, cnt)\n\n            if SEQ_TOKEN_MODE == \"mag\":\n                val_b = np.where(bm, val, np.nan)\n                mean_val_b = np.nanmean(val_b, axis=1).astype(np.float32)\n                mean_val_b = np.nan_to_num(mean_val_b, nan=0.0).astype(np.float32)\n            else:\n                aval_b = np.abs(val) * bm\n                mean_val_b = _safe_div(aval_b.sum(axis=1).astype(np.float32), cnt)\n\n            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n\n        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n\n        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n        agg = np.concatenate([glob, global_val_feats.astype(np.float32), per_band], axis=1).astype(np.float32)\n\n        out[start:end] = agg\n\n        del Xc, Bc, Mc\n        if (start // int(chunk)) % 4 == 0:\n            gc.collect()\n\n    return out\n\nprint(f\"[Stage 10] Build TEST global features: token_mode={SEQ_TOKEN_MODE} | val_feat={VAL_FEAT}\")\n\n# meta part (always same order)\nBASE_G_COLS = BASE_G_COLS_DEFAULT\nfor c in BASE_G_COLS:\n    if c not in df_test_meta.columns:\n        df_test_meta[c] = 0.0\n\nG_meta = df_test_meta.iloc[pos_idx][BASE_G_COLS].copy()\nfor c in BASE_G_COLS:\n    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\nG_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n\n# agg part (same as STAGE 8) — compute ONCE\nprint(\"[Stage 10] Building AGG seq features for TEST (one-time)...\")\nt0 = time.time()\nG_seq_np = build_agg_seq_features_memmap(Xte, Bte, Mte, chunk=512)\nprint(f\"[Stage 10] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\n\n# raw global\nG_raw_default = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)  # (NTE, 38)\n\n# ----------------------------\n# 8) Inference per fold (logit ensemble)\n# ----------------------------\n@torch.no_grad()\ndef predict_logits_batchwise(model, Xmm, Bmm, Mmm, G_raw, mean=None, std=None, batch_size=64):\n    model.eval()\n    out = np.zeros((Xmm.shape[0],), dtype=np.float32)\n    N0 = int(Xmm.shape[0])\n    for i in range(0, N0, int(batch_size)):\n        j = min(N0, i + int(batch_size))\n        Xb = torch.from_numpy(np.asarray(Xmm[i:j]).astype(np.float32, copy=False))\n        Bb = torch.from_numpy(np.asarray(Bmm[i:j]).astype(np.int64, copy=False))\n        Mb = torch.from_numpy(np.asarray(Mmm[i:j]).astype(np.int64, copy=False))\n\n        Gb = G_raw[i:j]\n        if mean is not None and std is not None:\n            Gb = ((Gb - mean) / std).astype(np.float32, copy=False)\n        Gb = torch.from_numpy(Gb.astype(np.float32, copy=False))\n\n        logit = model(Xb.to(device), Bb.to(device), Mb.to(device), Gb.to(device))\n        out[i:j] = logit.detach().cpu().numpy().astype(np.float32, copy=False)\n    return out\n\nBATCH_SIZE = 64\ntest_logit_folds = np.zeros((NTE, int(n_splits)), dtype=np.float32)\n\nprint(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} | ensemble=mean_logits\")\n\narch_used = None\n\nfor fold, ckpt_path in enumerate(ckpts):\n    ckpt_obj = torch_load_compat(ckpt_path)\n    sd, meta = extract_state_and_meta(ckpt_obj)\n\n    arch = infer_from_state(sd)\n    if arch_used is None:\n        arch_used = dict(arch)\n\n    cfg = meta.get(\"cfg\", {}) if isinstance(meta, dict) else {}\n    dropout = float(cfg.get(\"dropout\", 0.0)) if isinstance(cfg, dict) else 0.0\n\n    # n_heads: use cfg if valid else pick divisor\n    n_heads = int(cfg.get(\"n_heads\", 0)) if isinstance(cfg, dict) else 0\n    if n_heads <= 0 or (arch[\"d_model\"] % n_heads != 0):\n        for h in [8, 4, 2, 1, 16, 32]:\n            if arch[\"d_model\"] % h == 0:\n                n_heads = h\n                break\n        if n_heads <= 0:\n            n_heads = 4\n\n    # HARD schema checks\n    if arch[\"feat_dim\"] != Fdim:\n        raise RuntimeError(\n            f\"Fold {fold}: feature_dim mismatch.\\n\"\n            f\"- ckpt expects feat_dim={arch['feat_dim']}\\n\"\n            f\"- memmap has Fdim={Fdim}\\n\"\n            \"Solusi: pastikan STAGE 6 feature list sama saat training ckpt dibuat.\"\n        )\n    if arch[\"max_len_ckpt\"] != L:\n        raise RuntimeError(\n            f\"Fold {fold}: max_len mismatch.\\n\"\n            f\"- ckpt max_len={arch['max_len_ckpt']}\\n\"\n            f\"- memmap MAX_LEN={L}\\n\"\n        )\n\n    # Decide G_raw to match ckpt g_dim\n    g_dim = int(arch[\"g_dim\"])\n    if g_dim <= 0:\n        G_raw = np.zeros((NTE, 0), dtype=np.float32)\n        g_mean = None\n        g_std = None\n    else:\n        # If ckpt expects 38 dims (7 meta + 31 agg), use default.\n        # Otherwise crop/pad default to match g_dim (best-effort).\n        if G_raw_default.shape[1] == g_dim:\n            G_raw = G_raw_default\n        elif G_raw_default.shape[1] > g_dim:\n            G_raw = G_raw_default[:, :g_dim].copy()\n        else:\n            pad = np.zeros((NTE, g_dim - G_raw_default.shape[1]), dtype=np.float32)\n            G_raw = np.concatenate([G_raw_default, pad], axis=1).astype(np.float32)\n\n        scaler = meta.get(\"global_scaler\", None) if isinstance(meta, dict) else None\n        if scaler is not None and isinstance(scaler, dict) and (\"mean\" in scaler) and (\"std\" in scaler):\n            g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32).reshape(-1)\n            g_std  = np.asarray(scaler[\"std\"],  dtype=np.float32).reshape(-1)\n            if g_mean.shape[0] != g_dim or g_std.shape[0] != g_dim:\n                g_mean = None; g_std = None\n            else:\n                g_std = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n        else:\n            g_mean = None; g_std = None\n\n    model = FlexMultibandEventTransformer(\n        feat_dim=arch[\"feat_dim\"],\n        max_len=arch[\"max_len_ckpt\"],\n        n_bands=arch[\"n_bands\"],\n        d_model=arch[\"d_model\"],\n        n_heads=n_heads,\n        n_layers=arch[\"n_layers\"],\n        dim_ff=arch[\"dim_ff\"],\n        dropout=dropout,\n        g_dim=g_dim,\n        g_hidden=arch[\"g_hidden\"],\n        xproj_is_seq=True,                 # match Stage 8 forward\n        has_pool_ln=arch[\"has_pool_ln\"],\n        head_final_idx=arch[\"head_final_idx\"],\n    ).to(device)\n\n    model.load_state_dict(sd, strict=True)\n\n    logits = predict_logits_batchwise(\n        model, Xte, Bte, Mte, G_raw, mean=g_mean, std=g_std, batch_size=BATCH_SIZE\n    )\n    test_logit_folds[:, fold] = logits\n\n    probs_tmp = sigmoid_np(logits)\n    print(f\"  fold {fold}: d_model={arch['d_model']} n_heads={n_heads} g_dim={g_dim} | \"\n          f\"logit_mean={float(logits.mean()):.6f} | prob_mean={float(probs_tmp.mean()):.6f} | prob_std={float(probs_tmp.std()):.6f}\")\n\n    del model, logits, probs_tmp\n    gc.collect()\n\n# ensemble on logits\ntest_logit_ens = test_logit_folds.mean(axis=1).astype(np.float32)\ntest_prob_folds = sigmoid_np(test_logit_folds).astype(np.float32)\ntest_prob_ens   = sigmoid_np(test_logit_ens).astype(np.float32)\n\n# ----------------------------\n# 9) Save artifacts\n# ----------------------------\nlogit_fold_path = OUT_DIR / \"test_logit_folds.npy\"\nlogit_ens_path  = OUT_DIR / \"test_logit_ens.npy\"\nprob_fold_path  = OUT_DIR / \"test_prob_folds.npy\"\nprob_ens_path   = OUT_DIR / \"test_prob_ens.npy\"\ncsv_path        = OUT_DIR / \"test_prob_ens.csv\"\ncfg_path        = OUT_DIR / \"test_infer_config.json\"\n\nnp.save(logit_fold_path, test_logit_folds)\nnp.save(logit_ens_path,  test_logit_ens)\nnp.save(prob_fold_path,  test_prob_folds)\nnp.save(prob_ens_path,   test_prob_ens)\n\npd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens}).to_csv(csv_path, index=False)\n\ninfer_cfg = {\n    \"seed\": int(SEED),\n    \"n_splits\": int(n_splits),\n    \"ensemble\": \"mean_logits_then_sigmoid\",\n    \"batch_size\": int(BATCH_SIZE),\n    \"max_len\": int(L),\n    \"feature_dim\": int(Fdim),\n    \"token_mode\": SEQ_TOKEN_MODE,\n    \"val_feat\": VAL_FEAT,\n    \"global_meta_cols\": BASE_G_COLS_DEFAULT,\n    \"global_agg_dim\": 31,\n    \"global_default_dim\": int(G_raw_default.shape[1]),\n    \"ckpt_dir\": str(CKPT_DIR),\n    \"ckpts\": [str(p) for p in ckpts],\n    \"arch_inferred_from_first_fold\": arch_used,\n    \"outputs\": {\n        \"test_logit_folds\": str(logit_fold_path),\n        \"test_logit_ens\": str(logit_ens_path),\n        \"test_prob_folds\": str(prob_fold_path),\n        \"test_prob_ens\": str(prob_ens_path),\n        \"test_prob_ens_csv\": str(csv_path),\n    }\n}\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(infer_cfg, f, indent=2)\n\nprint(\"\\n[Stage 10] DONE\")\nprint(f\"- Saved logits folds: {logit_fold_path}\")\nprint(f\"- Saved logits ens  : {logit_ens_path}\")\nprint(f\"- Saved probs folds : {prob_fold_path}\")\nprint(f\"- Saved probs ens   : {prob_ens_path}\")\nprint(f\"- Saved csv         : {csv_path}\")\nprint(f\"- Saved config      : {cfg_path}\")\nprint(f\"- ens prob mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | \"\n      f\"min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n\n# Export globals for submission stage\nglobals().update({\n    \"test_ids\": test_ids,\n    \"test_logit_folds\": test_logit_folds,\n    \"test_logit_ens\": test_logit_ens,\n    \"test_prob_folds\": test_prob_folds,\n    \"test_prob_ens\": test_prob_ens,\n    \"TEST_LOGIT_FOLDS_PATH\": logit_fold_path,\n    \"TEST_LOGIT_ENS_PATH\": logit_ens_path,\n    \"TEST_PROB_FOLDS_PATH\": prob_fold_path,\n    \"TEST_PROB_ENS_PATH\": prob_ens_path,\n    \"TEST_PROB_CSV_PATH\": csv_path,\n    \"TEST_INFER_CFG_PATH\": cfg_path,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evalution ","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ONE CELL — EVALUATION (Precision / Recall / F1) + Threshold Sweep (OOF)\n# REVISI FULL v3 — Robust target detect + robust align + handle dup ids + better thr candidates\n#\n# Input minimal:\n# - df_train_meta (index: object_id, kolom target: target/y/label/class/is_tde/binary_target)\n# - oof_prob (globals) ATAU file OOF_DIR/oof_prob.npy ATAU OOF_DIR/oof_prob.csv\n#\n# Output:\n# - Print ringkasan metrik\n# - Save: eval_report.txt + eval_threshold_table.csv + eval_summary.json\n# - Export globals: BEST_THR_F1, BEST_THR_F05, BEST_THR_F2, thr_table_eval\n# ============================================================\n\nimport gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require minimal\n# ----------------------------\nif \"df_train_meta\" not in globals():\n    raise RuntimeError(\"Missing df_train_meta. Jalankan stage meta dulu.\")\n\nART_DIR = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\nOOF_DIR = Path(globals().get(\"OOF_DIR\", ART_DIR / \"oof\"))\nOOF_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Utils: id normalize + robust 1D float32\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\ndef _sanitize_prob(p):\n    p = np.asarray(p, dtype=np.float32)\n    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n    p = np.clip(p, 0.0, 1.0)\n    return p.astype(np.float32)\n\n# ensure meta index normalized\ndf_train_meta = df_train_meta.copy(deep=False)\ndf_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\n\n# ----------------------------\n# 0b) Detect target column (robust)\n# ----------------------------\ndef _detect_target_col(df):\n    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\nTARGET_COL = _detect_target_col(df_train_meta)\nif TARGET_COL is None:\n    raise RuntimeError(\n        \"Cannot detect target column in df_train_meta. \"\n        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n    )\n\ndef _get_y_aligned(ids):\n    yy = pd.to_numeric(df_train_meta.loc[ids, TARGET_COL], errors=\"coerce\").fillna(0).to_numpy()\n    yy = (yy.astype(np.float32) > 0).astype(np.int8)\n    return yy\n\n# ----------------------------\n# 1) Load oof_prob (prefer csv for safest alignment)\n# ----------------------------\ndef load_oof():\n    pcsv = OOF_DIR / \"oof_prob.csv\"\n    if pcsv.exists():\n        df = pd.read_csv(pcsv)\n        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n            p = _sanitize_prob(_as_1d_float32(df[\"oof_prob\"].to_numpy()))\n            if len(p) != len(df):\n                raise RuntimeError(\"oof_prob.csv: length mismatch after parsing.\")\n            df = df[[\"object_id\"]].copy()\n            df[\"oof_prob\"] = p\n            return p, df, \"csv\"\n\n    if \"oof_prob\" in globals():\n        p = _as_1d_float32(globals()[\"oof_prob\"])\n        if isinstance(p, np.ndarray) and p.ndim != 0:\n            return _sanitize_prob(p), None, \"globals\"\n\n    pnpy = OOF_DIR / \"oof_prob.npy\"\n    if pnpy.exists():\n        p = _sanitize_prob(_as_1d_float32(np.load(pnpy, allow_pickle=False)))\n        return p, None, \"npy\"\n\n    raise FileNotFoundError(\"OOF prob tidak ditemukan (oof_prob.csv / globals oof_prob / oof_prob.npy).\")\n\noof_prob, df_oof_csv, oof_src = load_oof()\nif not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n    raise TypeError(f\"Invalid oof_prob (scalar/unsized). type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}\")\n\n# ----------------------------\n# 2) Align y (target) to oof order\n# ----------------------------\ntrain_ids = None\ny = None\n\nif df_oof_csv is not None:\n    # Handle duplicate ids: mean per object_id but keep first-seen order\n    ids_first = pd.unique(df_oof_csv[\"object_id\"].to_numpy())\n    if len(ids_first) != len(df_oof_csv):\n        df_mean = df_oof_csv.groupby(\"object_id\", as_index=True)[\"oof_prob\"].mean()\n        df_oof_csv = pd.DataFrame({\"object_id\": ids_first})\n        df_oof_csv[\"oof_prob\"] = df_mean.reindex(ids_first).to_numpy(dtype=np.float32)\n        oof_prob = _sanitize_prob(df_oof_csv[\"oof_prob\"].to_numpy())\n\n    train_ids = df_oof_csv[\"object_id\"].tolist()\n\n    # drop ids not in meta\n    ok = np.asarray([oid in df_train_meta.index for oid in train_ids], dtype=bool)\n    if not ok.all():\n        bad = [train_ids[i] for i in np.where(~ok)[0][:10]]\n        print(f\"[WARN] oof_prob.csv contains ids not in df_train_meta: missing_n={int((~ok).sum())} examples={bad}\")\n        df_oof_csv = df_oof_csv.loc[ok].reset_index(drop=True)\n        train_ids = df_oof_csv[\"object_id\"].tolist()\n        oof_prob = _sanitize_prob(df_oof_csv[\"oof_prob\"].to_numpy())\n\n    y = _get_y_aligned(train_ids)\n\nif y is None and (\"train_ids_ordered\" in globals()):\n    ids = [_norm_id(z) for z in list(globals()[\"train_ids_ordered\"])]\n    if len(ids) == len(oof_prob):\n        missing = [oid for oid in ids if oid not in df_train_meta.index]\n        if missing:\n            raise KeyError(f\"train_ids_ordered contains ids not in df_train_meta. ex={missing[:10]} missing_n={len(missing)}\")\n        train_ids = ids\n        y = _get_y_aligned(train_ids)\n\nif y is None:\n    if len(oof_prob) != len(df_train_meta):\n        raise RuntimeError(\n            f\"Tidak bisa align y. len(oof_prob)={len(oof_prob)} != len(df_train_meta)={len(df_train_meta)} \"\n            \"dan tidak ada oof_prob.csv (object_id) atau train_ids_ordered.\"\n        )\n    train_ids = df_train_meta.index.astype(str).tolist()\n    y = _get_y_aligned(train_ids)\n\nif len(y) != len(oof_prob):\n    raise RuntimeError(f\"Length mismatch: y={len(y)} vs oof_prob={len(oof_prob)}\")\n\nuy = set(np.unique(y).tolist())\nif not uy.issubset({0, 1}):\n    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n\nN = int(len(y))\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\n\nprint(f\"[Eval] OOF source={oof_src} | target_col={TARGET_COL}\")\nprint(f\"[Eval] N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}%\")\n\n# ----------------------------\n# 3) Metrics: P/R/F1 + Fbeta + AUC optional\n# ----------------------------\ndef prf_from_pred(y_true, y_pred01):\n    y_true = np.asarray(y_true, dtype=np.int32)\n    y_pred01 = np.asarray(y_pred01, dtype=np.int32)\n\n    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n    tn = int(((y_true == 0) & (y_pred01 == 0)).sum())\n\n    precision = tp / max(tp + fp, 1)\n    recall    = tp / max(tp + fn, 1)\n    f1 = 0.0 if (precision + recall) == 0 else (2.0 * precision * recall / (precision + recall))\n\n    return {\n        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n        \"precision\": float(precision),\n        \"recall\": float(recall),\n        \"f1\": float(f1),\n        \"pos_pred\": int(y_pred01.sum()),\n        \"acc\": float((tp + tn) / max(len(y_true), 1)),\n    }\n\ndef fbeta_from_pr(precision, recall, beta=1.0):\n    b2 = beta * beta\n    denom = (b2 * precision + recall)\n    if denom <= 0:\n        return 0.0\n    return float((1 + b2) * precision * recall / denom)\n\ndef eval_at_threshold(prob, y_true, thr):\n    pred = (prob >= float(thr)).astype(np.int8)\n    met = prf_from_pred(y_true, pred)\n    met[\"thr\"] = float(thr)\n    met[\"f0.5\"] = fbeta_from_pr(met[\"precision\"], met[\"recall\"], beta=0.5)\n    met[\"f2\"]   = fbeta_from_pr(met[\"precision\"], met[\"recall\"], beta=2.0)\n    return met\n\nroc_auc = None\npr_auc = None\ntry:\n    from sklearn.metrics import roc_auc_score, average_precision_score\n    if (y.max() == 1) and (y.min() == 0):\n        roc_auc = float(roc_auc_score(y, oof_prob))\n        pr_auc  = float(average_precision_score(y, oof_prob))\nexcept Exception:\n    pass\n\nbase = eval_at_threshold(oof_prob, y, 0.5)\n\n# ----------------------------\n# 4) Threshold candidates (grid + quantiles + sampled uniques)\n# ----------------------------\ngrid = np.concatenate([\n    np.linspace(0.00, 0.10, 41),\n    np.linspace(0.10, 0.90, 161),\n    np.linspace(0.90, 1.00, 41),\n]).astype(np.float32)\n\nqs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\nquant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n\nuniq = np.unique(oof_prob)\nif len(uniq) > 4000:\n    take = np.linspace(0, len(uniq)-1, 4000, dtype=int)\n    uniq = uniq[take].astype(np.float32)\n\nthr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr, uniq]), 0.0, 1.0)).astype(np.float32)\n\nrows = []\nbest_f1  = base.copy()\nbest_f05 = base.copy()\nbest_f2  = base.copy()\n\nfor thr in thr_candidates:\n    met = eval_at_threshold(oof_prob, y, float(thr))\n    rows.append([\n        met[\"thr\"], met[\"f1\"], met[\"f0.5\"], met[\"f2\"],\n        met[\"precision\"], met[\"recall\"], met[\"acc\"],\n        met[\"tp\"], met[\"fp\"], met[\"fn\"], met[\"tn\"], met[\"pos_pred\"]\n    ])\n\n    # best F1 tie-break: recall higher, then fp lower\n    if (met[\"f1\"] > best_f1[\"f1\"] + 1e-12) or (\n        abs(met[\"f1\"] - best_f1[\"f1\"]) <= 1e-12 and (met[\"recall\"] > best_f1[\"recall\"] + 1e-12)\n    ) or (\n        abs(met[\"f1\"] - best_f1[\"f1\"]) <= 1e-12 and abs(met[\"recall\"] - best_f1[\"recall\"]) <= 1e-12 and (met[\"fp\"] < best_f1[\"fp\"])\n    ):\n        best_f1 = met.copy()\n\n    # best F0.5 (precision-leaning)\n    if (met[\"f0.5\"] > best_f05.get(\"f0.5\", -1.0) + 1e-12):\n        best_f05 = met.copy()\n\n    # best F2 (recall-leaning)\n    if (met[\"f2\"] > best_f2.get(\"f2\", -1.0) + 1e-12):\n        best_f2 = met.copy()\n\nthr_table = pd.DataFrame(\n    rows,\n    columns=[\"thr\",\"f1\",\"f0.5\",\"f2\",\"precision\",\"recall\",\"acc\",\"tp\",\"fp\",\"fn\",\"tn\",\"pos_pred\"]\n).sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).reset_index(drop=True)\n\nBEST_THR_F1  = float(best_f1[\"thr\"])\nBEST_THR_F05 = float(best_f05[\"thr\"])\nBEST_THR_F2  = float(best_f2[\"thr\"])\n\n# ----------------------------\n# 5) Print report\n# ----------------------------\nprint(\"\\nEVALUATION (OOF) — Precision/Recall/F1\")\nif roc_auc is not None:\n    print(f\"- ROC-AUC={roc_auc:.6f} | PR-AUC={pr_auc:.6f}\")\nprint(\"\\nBaseline @ thr=0.5\")\nprint(f\"- F1={base['f1']:.6f} | P={base['precision']:.6f} | R={base['recall']:.6f} | ACC={base['acc']:.6f}\")\nprint(f\"  tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\")\n\nprint(f\"\\nBEST-F1  @ thr={BEST_THR_F1:.6f}\")\nprint(f\"- F1={best_f1['f1']:.6f} | P={best_f1['precision']:.6f} | R={best_f1['recall']:.6f} | ACC={best_f1['acc']:.6f}\")\nprint(f\"  tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\")\n\nprint(f\"\\nBEST-F0.5 @ thr={BEST_THR_F05:.6f} (precision-leaning)\")\nprint(f\"- F0.5={best_f05['f0.5']:.6f} | P={best_f05['precision']:.6f} | R={best_f05['recall']:.6f} | F1={best_f05['f1']:.6f}\")\n\nprint(f\"\\nBEST-F2   @ thr={BEST_THR_F2:.6f} (recall-leaning)\")\nprint(f\"- F2={best_f2['f2']:.6f} | P={best_f2['precision']:.6f} | R={best_f2['recall']:.6f} | F1={best_f2['f1']:.6f}\")\n\nprint(\"\\nTop 10 thresholds by F1:\")\nfor i in range(min(10, len(thr_table))):\n    r = thr_table.iloc[i]\n    print(f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | f0.5={r['f0.5']:.6f} | f2={r['f2']:.6f} | \"\n          f\"P={r['precision']:.6f} R={r['recall']:.6f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n\n# ----------------------------\n# 6) Save artifacts\n# ----------------------------\nout_txt  = OOF_DIR / \"eval_report.txt\"\nout_csv  = OOF_DIR / \"eval_threshold_table.csv\"\nout_json = OOF_DIR / \"eval_summary.json\"\n\nlines = []\nlines.append(\"OOF Evaluation Report (Precision/Recall/F1)\")\nlines.append(f\"source={oof_src} | target_col={TARGET_COL}\")\nlines.append(f\"N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.8f}%\")\nif roc_auc is not None:\n    lines.append(f\"ROC-AUC={roc_auc:.10f} | PR-AUC={pr_auc:.10f}\")\nlines.append(\"\")\nlines.append(\"Baseline @ thr=0.5\")\nlines.append(f\"F1={base['f1']:.10f} | P={base['precision']:.10f} | R={base['recall']:.10f} | ACC={base['acc']:.10f}\")\nlines.append(f\"tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\")\nlines.append(\"\")\nlines.append(f\"BEST-F1 @ thr={BEST_THR_F1:.10f}\")\nlines.append(f\"F1={best_f1['f1']:.10f} | P={best_f1['precision']:.10f} | R={best_f1['recall']:.10f} | ACC={best_f1['acc']:.10f}\")\nlines.append(f\"tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\")\nlines.append(\"\")\nlines.append(f\"BEST-F0.5 @ thr={BEST_THR_F05:.10f}\")\nlines.append(f\"F0.5={best_f05['f0.5']:.10f} | P={best_f05['precision']:.10f} | R={best_f05['recall']:.10f} | F1={best_f05['f1']:.10f}\")\nlines.append(\"\")\nlines.append(f\"BEST-F2 @ thr={BEST_THR_F2:.10f}\")\nlines.append(f\"F2={best_f2['f2']:.10f} | P={best_f2['precision']:.10f} | R={best_f2['recall']:.10f} | F1={best_f2['f1']:.10f}\")\nlines.append(\"\")\nlines.append(\"Top 10 thresholds by F1:\")\nfor i in range(min(10, len(thr_table))):\n    r = thr_table.iloc[i]\n    lines.append(f\"{i+1:02d}. thr={r['thr']:.10f} | f1={r['f1']:.10f} | f0.5={r['f0.5']:.10f} | f2={r['f2']:.10f} | \"\n                 f\"P={r['precision']:.10f} R={r['recall']:.10f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n\nwith open(out_txt, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\nthr_table.to_csv(out_csv, index=False)\n\npayload = {\n    \"source\": oof_src,\n    \"target_col\": TARGET_COL,\n    \"N\": N, \"pos\": pos, \"neg\": neg,\n    \"roc_auc\": roc_auc, \"pr_auc\": pr_auc,\n    \"baseline_thr_0p5\": base,\n    \"best_f1\": best_f1,\n    \"best_f0.5\": best_f05,\n    \"best_f2\": best_f2,\n    \"paths\": {\"report\": str(out_txt), \"table\": str(out_csv)}\n}\nwith open(out_json, \"w\", encoding=\"utf-8\") as f:\n    json.dump(payload, f, indent=2)\n\nprint(\"\\nSaved:\")\nprint(f\"- {out_txt}\")\nprint(f\"- {out_csv}\")\nprint(f\"- {out_json}\")\n\n# Export for next stages\nglobals().update({\n    \"BEST_THR_F1\": BEST_THR_F1,\n    \"BEST_THR_F05\": BEST_THR_F05,\n    \"BEST_THR_F2\": BEST_THR_F2,\n    \"thr_table_eval\": thr_table,\n    \"EVAL_REPORT_PATH\": out_txt,\n    \"EVAL_TABLE_PATH\": out_csv,\n    \"EVAL_SUMMARY_PATH\": out_json,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission Build","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 11 — Submission Build (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL v3\n#\n# Fix utama v3:\n# - Cari pred test sesuai STAGE 10 (ART_DIR/preds/test_prob_ens.csv) + baca dari TEST_INFER_CFG_PATH jika ada\n# - Fallback lebih lengkap (globals / cfg json / csv / npy)\n# - Strict align ke sample_submission order, prediction harus 0/1\n#\n# Output:\n# - /kaggle/working/submission.csv\n# - SUB_DIR/submission.csv (copy)\n# ============================================================\n\nimport gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 0 globals\n# ----------------------------\nfor need in [\"PATHS\", \"SUB_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n\nsample_path = Path(PATHS[\"SAMPLE_SUB\"])\nif not sample_path.exists():\n    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n\ndf_sub = pd.read_csv(sample_path)\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\ndef _sanitize_prob(p):\n    p = np.asarray(p, dtype=np.float32)\n    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n    p = np.clip(p, 0.0, 1.0)\n    return p.astype(np.float32)\n\ndef _load_ids_npy(path: Path):\n    arr = np.load(path, allow_pickle=False)\n    ids = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n    return [_norm_id(x) for x in ids]\n\ndef _try_load_stage10_cfg_csv():\n    \"\"\"\n    If STAGE 10 wrote config json, use it to find the exact csv path.\n    Returns Path or None.\n    \"\"\"\n    p = globals().get(\"TEST_INFER_CFG_PATH\", None)\n    if p is None:\n        return None\n    p = Path(p)\n    if not p.exists():\n        return None\n    try:\n        cfg = json.load(open(p, \"r\", encoding=\"utf-8\"))\n        out = cfg.get(\"outputs\", {})\n        csvp = out.get(\"test_prob_ens_csv\", None)\n        if csvp:\n            csvp = Path(csvp)\n            if csvp.exists():\n                return csvp\n    except Exception:\n        return None\n    return None\n\ndef _load_pred_df():\n    \"\"\"\n    Return df_pred with columns: object_id, prob\n    Priority:\n      A) globals: test_ids + test_prob_ens\n      B) stage10 config json -> outputs.test_prob_ens_csv\n      C) csv fallbacks (ART_DIR/preds/test_prob_ens.csv etc)\n      D) npy fallback: FIX_DIR/test_ids.npy + test_prob_ens.npy\n    \"\"\"\n    # ---- A) globals ----\n    if (\"test_prob_ens\" in globals()) and (globals()[\"test_prob_ens\"] is not None) and \\\n       (\"test_ids\" in globals()) and (globals()[\"test_ids\"] is not None):\n        ids = [_norm_id(x) for x in list(globals()[\"test_ids\"])]\n        prob = _sanitize_prob(_as_1d_float32(globals()[\"test_prob_ens\"]))\n        if isinstance(prob, np.ndarray) and prob.ndim != 0 and len(ids) == len(prob) and len(ids) > 0:\n            return pd.DataFrame({\"object_id\": ids, \"prob\": prob})\n\n    # ---- B) exact csv path from STAGE 10 config ----\n    cfg_csv = _try_load_stage10_cfg_csv()\n    if cfg_csv is not None and cfg_csv.exists():\n        df = pd.read_csv(cfg_csv)\n        if \"object_id\" in df.columns and (\"prob\" in df.columns or \"prediction\" in df.columns):\n            df = df.copy()\n            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n            colp = \"prob\" if \"prob\" in df.columns else \"prediction\"\n            prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n            if len(prob) != len(df):\n                raise RuntimeError(f\"CSV prob length mismatch: {cfg_csv}\")\n            return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob})\n\n    # ---- C) csv fallback (best if already aligned with object_id) ----\n    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n    preds_dir = art_dir / \"preds\"\n\n    cand_csv = []\n    if \"TEST_PROB_CSV_PATH\" in globals() and globals()[\"TEST_PROB_CSV_PATH\"] is not None:\n        cand_csv.append(Path(globals()[\"TEST_PROB_CSV_PATH\"]))\n    # STAGE 10 default\n    cand_csv.append(preds_dir / \"test_prob_ens.csv\")\n    # older / alternative\n    cand_csv.append(art_dir / \"test_prob_ens.csv\")\n\n    for p in cand_csv:\n        if p.exists():\n            df = pd.read_csv(p)\n            if \"object_id\" in df.columns and (\"prob\" in df.columns or \"prediction\" in df.columns):\n                df = df.copy()\n                df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n                colp = \"prob\" if \"prob\" in df.columns else \"prediction\"\n                prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n                if len(prob) != len(df):\n                    raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n                return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob})\n\n    # ---- D) npy fallback ----\n    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n    p_ids = fix_dir / \"test_ids.npy\"\n    if not p_ids.exists():\n        raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat fixed_seq/test_ids.npy atau STAGE 10 export test_ids.\")\n\n    ids = _load_ids_npy(p_ids)\n    if len(ids) == 0:\n        raise RuntimeError(\"test_ids.npy kosong.\")\n\n    cand_npy = []\n    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n        cand_npy.append(Path(globals()[\"TEST_PROB_ENS_PATH\"]))\n    cand_npy.append(preds_dir / \"test_prob_ens.npy\")\n    cand_npy.append(art_dir / \"test_prob_ens.npy\")\n\n    prob = None\n    for p in cand_npy:\n        if p.exists():\n            prob = _sanitize_prob(_as_1d_float32(np.load(p, allow_pickle=False)))\n            break\n    if prob is None:\n        raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n\n    if not isinstance(prob, np.ndarray) or prob.ndim == 0:\n        raise TypeError(f\"Invalid test_prob (scalar/unsized). type={type(prob)} ndim={getattr(prob,'ndim',None)}\")\n\n    if len(prob) != len(ids):\n        raise RuntimeError(f\"Length mismatch (NPY): test_prob={len(prob)} vs test_ids={len(ids)}\")\n\n    return pd.DataFrame({\"object_id\": ids, \"prob\": prob})\n\n# ----------------------------\n# 1) Load prediction df\n# ----------------------------\ndf_pred = _load_pred_df()\nif df_pred.empty:\n    raise RuntimeError(\"df_pred empty (unexpected).\")\n\ndf_pred[\"object_id\"] = df_pred[\"object_id\"].apply(_norm_id)\nif df_pred[\"object_id\"].duplicated().any():\n    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n\np = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\nif not np.isfinite(p).all():\n    bad = int((~np.isfinite(p)).sum())\n    raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\ndf_pred[\"prob\"] = _sanitize_prob(p)\n\n# ----------------------------\n# 2) Threshold selection (priority)\n# ----------------------------\nFORCE_THR = None  # set manual if you want, e.g. 0.37\nif FORCE_THR is not None:\n    thr = float(FORCE_THR)\nelif \"BEST_THR_F1\" in globals() and globals()[\"BEST_THR_F1\"] is not None:\n    thr = float(globals()[\"BEST_THR_F1\"])\nelif \"BEST_THR\" in globals() and globals()[\"BEST_THR\"] is not None:\n    thr = float(globals()[\"BEST_THR\"])\nelse:\n    thr = 0.5\nthr = float(np.clip(thr, 0.0, 1.0))\n\n# ----------------------------\n# 3) Align to sample_submission order + build BINARY prediction (0/1)\n# ----------------------------\ndf_sub = df_sub.copy()\ndf_sub[\"object_id\"] = df_sub[\"object_id\"].apply(_norm_id)\n\nif df_sub[\"object_id\"].duplicated().any():\n    dup = df_sub.loc[df_sub[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(f\"sample_submission has duplicate object_id (unexpected). examples={dup}\")\n\ndf_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n\nif df_out[\"prob\"].isna().any():\n    missing_n = int(df_out[\"prob\"].isna().sum())\n    miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(\n        f\"Some sample_submission object_id have no prediction: missing_n={missing_n}. Examples: {miss_ids}\\n\"\n        \"Biasanya karena mismatch id normalization atau pred df tidak lengkap.\"\n    )\n\ndf_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\ndf_out = df_out[[\"object_id\", \"prediction\"]]\n\nu = set(np.unique(df_out[\"prediction\"].to_numpy()).tolist())\nif not u.issubset({0, 1}):\n    raise RuntimeError(f\"submission prediction contains values outside {{0,1}}: {sorted(list(u))}\")\n\nif len(df_out) != len(df_sub):\n    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n\npos_pred = int(df_out[\"prediction\"].sum())\nprint(\"[Stage 11] SUBMISSION READY (BINARY 0/1)\")\nprint(f\"- threshold_used={thr:.6f}\")\nprint(f\"- rows={len(df_out):,} | pos_pred={pos_pred:,} ({pos_pred/max(len(df_out),1)*100:.6f}%)\")\n\n# ----------------------------\n# 4) Write files\n# ----------------------------\nSUB_DIR = Path(SUB_DIR)\nSUB_DIR.mkdir(parents=True, exist_ok=True)\n\nout_main = Path(\"/kaggle/working/submission.csv\")\nout_copy = SUB_DIR / \"submission.csv\"\n\ndf_out.to_csv(out_main, index=False)\ndf_out.to_csv(out_copy, index=False)\n\nprint(f\"- wrote: {out_main}\")\nprint(f\"- copy : {out_copy}\")\nprint(\"\\nPreview:\")\nprint(df_out.head(8).to_string(index=False))\n\nglobals().update({\n    \"SUBMISSION_PATH\": out_main,\n    \"SUBMISSION_COPY_PATH\": out_copy,\n    \"SUBMISSION_MODE\": \"binary\",\n    \"SUBMISSION_THRESHOLD\": thr,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}