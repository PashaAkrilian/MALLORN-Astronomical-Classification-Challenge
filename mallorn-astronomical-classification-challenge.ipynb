{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84b9e47",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-02T09:08:48.530814Z",
     "iopub.status.busy": "2026-01-02T09:08:48.530409Z",
     "iopub.status.idle": "2026-01-02T09:08:49.628716Z",
     "shell.execute_reply": "2026-01-02T09:08:49.627615Z"
    },
    "papermill": {
     "duration": 1.11116,
     "end_time": "2026-01-02T09:08:49.630691",
     "exception": false,
     "start_time": "2026-01-02T09:08:48.519531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mallorn-dataset/sample_submission.csv\n",
      "/kaggle/input/mallorn-dataset/test_log.csv\n",
      "/kaggle/input/mallorn-dataset/train_log.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/test_full_lightcurves.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791e199",
   "metadata": {
    "papermill": {
     "duration": 0.007439,
     "end_time": "2026-01-02T09:08:49.646194",
     "exception": false,
     "start_time": "2026-01-02T09:08:49.638755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Kaggle CPU Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97165a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:08:49.663325Z",
     "iopub.status.busy": "2026-01-02T09:08:49.662846Z",
     "iopub.status.idle": "2026-01-02T09:08:54.492164Z",
     "shell.execute_reply": "2026-01-02T09:08:54.491263Z"
    },
    "papermill": {
     "duration": 4.840413,
     "end_time": "2026-01-02T09:08:54.493829",
     "exception": false,
     "start_time": "2026-01-02T09:08:49.653416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV OK (Kaggle CPU)\n",
      "- Python: 3.12.12\n",
      "- Numpy:  2.0.2\n",
      "- Pandas: 2.2.2\n",
      "- Torch:  2.8.0+cu126 | CUDA available: False\n",
      "- USE_ASTROMER: True\n",
      "\n",
      "DATA OK\n",
      "- train_log: 3,043 objects | pos(TDE)=148 | neg=2,895 | pos%=4.86%\n",
      "- test_log:  7,135 objects\n",
      "- submission template rows: 7,135\n",
      "- splits detected: 20 folders (split_01..split_20)\n",
      "\n",
      "Saved env snapshot: /kaggle/working/mallorn_run/env_config.txt\n",
      "Saved env snapshot: /kaggle/working/mallorn_run/env_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 0 — Kaggle CPU Environment Setup (ONE CELL, SAFE + COHESIVE) — REVISI FULL\n",
    "# Fokus:\n",
    "# - Path sesuai dataset kamu: /kaggle/input/mallorn-dataset\n",
    "# - Hard guards (file + split + konsistensi id)\n",
    "# - Thread limits anti-freeze\n",
    "# - Siap untuk pipeline ASTROMER (tanpa install / tanpa load lightcurve besar)\n",
    "# ============================================================\n",
    "\n",
    "import os, sys, gc, json, random, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Quiet + deterministic\n",
    "# ----------------------------\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "SEED = 2025\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CPU thread limits (anti-freeze on Kaggle CPU)\n",
    "# ----------------------------\n",
    "# BLAS/OMP oversubscription sering bikin notebook lambat/hang\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# Siap ASTROMER (umumnya pakai TensorFlow/Keras di belakang)\n",
    "# (Tidak meng-import TF di sini; hanya set env agar aman saat stage ASTROMER nanti)\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"TF_NUM_INTRAOP_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"TF_NUM_INTEROP_THREADS\", \"1\")\n",
    "\n",
    "# Torch optional (boleh tetap ada; pipeline ASTROMER tidak wajib torch)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.set_num_threads(2)\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Paths (sesuai yang kamu tulis)\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\n",
    "\n",
    "PATHS = {\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n",
    "    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n",
    "    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n",
    "    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Working directories (writeable on Kaggle)\n",
    "# ----------------------------\n",
    "WORKDIR = Path(\"/kaggle/working\")\n",
    "RUN_DIR = WORKDIR / \"mallorn_run\"\n",
    "ART_DIR = RUN_DIR / \"artifacts\"\n",
    "CACHE_DIR = RUN_DIR / \"cache\"\n",
    "EMB_DIR = CACHE_DIR / \"embeddings\"     # untuk ASTROMER embedding (nanti)\n",
    "FEAT_DIR = CACHE_DIR / \"features\"      # untuk tabular final (nanti)\n",
    "OOF_DIR = RUN_DIR / \"oof\"\n",
    "SUB_DIR = RUN_DIR / \"submissions\"\n",
    "LOG_DIR = RUN_DIR / \"logs\"\n",
    "\n",
    "for d in [RUN_DIR, ART_DIR, CACHE_DIR, EMB_DIR, FEAT_DIR, OOF_DIR, SUB_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Flag pipeline (dipakai stage berikutnya)\n",
    "USE_ASTROMER = True\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Hard guards: files must exist\n",
    "# ----------------------------\n",
    "def _must_exist(p: Path, what: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n",
    "\n",
    "_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n",
    "_must_exist(PATHS[\"TRAIN_LOG\"], \"train_log.csv\")\n",
    "_must_exist(PATHS[\"TEST_LOG\"],  \"test_log.csv\")\n",
    "\n",
    "missing_splits = [s for s in PATHS[\"SPLITS\"] if not s.exists()]\n",
    "if missing_splits:\n",
    "    sample = \"\\n\".join(str(x) for x in missing_splits[:5])\n",
    "    raise FileNotFoundError(f\"Some split folders are missing (showing up to 5):\\n{sample}\")\n",
    "\n",
    "bad = []\n",
    "for sd in PATHS[\"SPLITS\"]:\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        bad.append((sd.name, tr.exists(), te.exists()))\n",
    "if bad:\n",
    "    msg = \"\\n\".join([f\"- {name}: train={tr_ok}, test={te_ok}\" for name, tr_ok, te_ok in bad[:10]])\n",
    "    raise FileNotFoundError(\n",
    "        \"Some split lightcurve files are missing (showing up to 10):\\n\"\n",
    "        f\"{msg}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Load small metadata only (safe on CPU)\n",
    "# ----------------------------\n",
    "# dtype ringan untuk kolom string supaya parsing stabil\n",
    "dtype_sub = {\"object_id\": \"string\"}\n",
    "df_sub = pd.read_csv(PATHS[\"SAMPLE_SUB\"], dtype=dtype_sub)\n",
    "\n",
    "# sample_submission harus punya header object_id,prediction\n",
    "df_sub.columns = [c.strip() for c in df_sub.columns]\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission columns must include object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "dtype_log = {\"object_id\": \"string\", \"split\": \"string\"}\n",
    "df_train_log = pd.read_csv(PATHS[\"TRAIN_LOG\"], dtype=dtype_log)\n",
    "df_test_log  = pd.read_csv(PATHS[\"TEST_LOG\"],  dtype=dtype_log)\n",
    "\n",
    "# rapikan nama kolom\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "df_train_log = _norm_cols(df_train_log)\n",
    "df_test_log  = _norm_cols(df_test_log)\n",
    "\n",
    "# Validasi minimal kolom wajib (sesuai deskripsi dataset kamu)\n",
    "need_train = {\"object_id\", \"EBV\", \"Z\", \"split\", \"target\"}\n",
    "need_test  = {\"object_id\", \"EBV\", \"Z\", \"split\"}  # Z_err opsional (bisa ada di test)\n",
    "missing_train = sorted(list(need_train - set(df_train_log.columns)))\n",
    "missing_test  = sorted(list(need_test - set(df_test_log.columns)))\n",
    "\n",
    "if missing_train:\n",
    "    raise ValueError(f\"train_log.csv missing required columns: {missing_train}\")\n",
    "if missing_test:\n",
    "    raise ValueError(f\"test_log.csv missing required columns: {missing_test}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Normalize split name -> \"split_XX\"\n",
    "# ----------------------------\n",
    "valid_split_names = {f\"split_{i:02d}\" for i in range(1, 21)}\n",
    "\n",
    "def _normalize_split(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # kasus: \"01\" / \"1\"\n",
    "    if s.isdigit():\n",
    "        i = int(s)\n",
    "        return f\"split_{i:02d}\"\n",
    "    # kasus: \"split_01\" / \"split-01\" / \"Split 01\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.startswith(\"split_\"):\n",
    "        tail = s2.split(\"split_\", 1)[1]\n",
    "        tail = tail.strip(\"_\")\n",
    "        if tail.isdigit():\n",
    "            i = int(tail)\n",
    "            return f\"split_{i:02d}\"\n",
    "        return s2\n",
    "    return s\n",
    "\n",
    "df_train_log[\"split\"] = df_train_log[\"split\"].map(_normalize_split)\n",
    "df_test_log[\"split\"]  = df_test_log[\"split\"].map(_normalize_split)\n",
    "\n",
    "bad_train_split = sorted(set(df_train_log[\"split\"]) - valid_split_names)\n",
    "bad_test_split  = sorted(set(df_test_log[\"split\"]) - valid_split_names)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Basic sanity: uniqueness + target binary\n",
    "# ----------------------------\n",
    "if df_train_log[\"object_id\"].duplicated().any():\n",
    "    dup_n = int(df_train_log[\"object_id\"].duplicated().sum())\n",
    "    raise ValueError(f\"train_log.csv has duplicated object_id rows: {dup_n} duplicates found.\")\n",
    "if df_test_log[\"object_id\"].duplicated().any():\n",
    "    dup_n = int(df_test_log[\"object_id\"].duplicated().sum())\n",
    "    raise ValueError(f\"test_log.csv has duplicated object_id rows: {dup_n} duplicates found.\")\n",
    "\n",
    "# target harus 0/1\n",
    "df_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\")\n",
    "if df_train_log[\"target\"].isna().any():\n",
    "    n_na = int(df_train_log[\"target\"].isna().sum())\n",
    "    raise ValueError(f\"train_log target has NaN after numeric coercion: {n_na} rows.\")\n",
    "uniq_t = set(pd.unique(df_train_log[\"target\"]).tolist())\n",
    "if not uniq_t.issubset({0, 1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Submission ↔ test_log consistency (HARUS match)\n",
    "# ----------------------------\n",
    "sub_ids = set(df_sub[\"object_id\"].astype(\"string\"))\n",
    "test_ids = set(df_test_log[\"object_id\"].astype(\"string\"))\n",
    "\n",
    "missing_in_test = sub_ids - test_ids\n",
    "missing_in_sub  = test_ids - sub_ids\n",
    "\n",
    "if missing_in_test:\n",
    "    sample = list(missing_in_test)[:5]\n",
    "    raise ValueError(f\"sample_submission has object_id not found in test_log (showing up to 5): {sample}\")\n",
    "if missing_in_sub:\n",
    "    sample = list(missing_in_sub)[:5]\n",
    "    raise ValueError(f\"test_log has object_id not present in sample_submission (showing up to 5): {sample}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Quick dataset summary (ringkas)\n",
    "# ----------------------------\n",
    "pos = int((df_train_log[\"target\"] == 1).sum())\n",
    "neg = int((df_train_log[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_log))\n",
    "\n",
    "print(\"ENV OK (Kaggle CPU)\")\n",
    "print(f\"- Python: {sys.version.split()[0]}\")\n",
    "print(f\"- Numpy:  {np.__version__}\")\n",
    "print(f\"- Pandas: {pd.__version__}\")\n",
    "if torch is not None:\n",
    "    print(f\"- Torch:  {torch.__version__} | CUDA available: {torch.cuda.is_available()}\")\n",
    "else:\n",
    "    print(\"- Torch:  not available\")\n",
    "print(f\"- USE_ASTROMER: {USE_ASTROMER}\")\n",
    "\n",
    "print(\"\\nDATA OK\")\n",
    "print(f\"- train_log: {len(df_train_log):,} objects | pos(TDE)={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.2f}%\")\n",
    "print(f\"- test_log:  {len(df_test_log):,} objects\")\n",
    "print(f\"- submission template rows: {len(df_sub):,}\")\n",
    "print(f\"- splits detected: {len(PATHS['SPLITS'])} folders (split_01..split_20)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Save config snapshot for reproducibility\n",
    "# ----------------------------\n",
    "cfg = {\n",
    "    \"SEED\": SEED,\n",
    "    \"DATA_ROOT\": str(DATA_ROOT),\n",
    "    \"WORKDIR\": str(WORKDIR),\n",
    "    \"USE_ASTROMER\": bool(USE_ASTROMER),\n",
    "    \"THREADS\": {k: os.environ.get(k, \"\") for k in [\n",
    "        \"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\n",
    "        \"TF_NUM_INTRAOP_THREADS\",\"TF_NUM_INTEROP_THREADS\"\n",
    "    ]},\n",
    "}\n",
    "cfg_path_txt = RUN_DIR / \"env_config.txt\"\n",
    "cfg_path_json = RUN_DIR / \"env_config.json\"\n",
    "\n",
    "with open(cfg_path_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    for k, v in cfg.items():\n",
    "        if k != \"THREADS\":\n",
    "            f.write(f\"{k}={v}\\n\")\n",
    "    f.write(\"THREADS:\\n\")\n",
    "    for k, v in cfg[\"THREADS\"].items():\n",
    "        f.write(f\"  {k}={v}\\n\")\n",
    "\n",
    "with open(cfg_path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved env snapshot: {cfg_path_txt}\")\n",
    "print(f\"Saved env snapshot: {cfg_path_json}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Export to globals (dipakai stage berikutnya)\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"SEED\": SEED,\n",
    "    \"USE_ASTROMER\": USE_ASTROMER,\n",
    "    \"PATHS\": PATHS,\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"CACHE_DIR\": CACHE_DIR,\n",
    "    \"EMB_DIR\": EMB_DIR,\n",
    "    \"FEAT_DIR\": FEAT_DIR,\n",
    "    \"OOF_DIR\": OOF_DIR,\n",
    "    \"SUB_DIR\": SUB_DIR,\n",
    "    \"LOG_DIR\": LOG_DIR,\n",
    "    \"df_sub\": df_sub,\n",
    "    \"df_train_log\": df_train_log,\n",
    "    \"df_test_log\": df_test_log,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45215b0",
   "metadata": {
    "papermill": {
     "duration": 0.007414,
     "end_time": "2026-01-02T09:08:54.509129",
     "exception": false,
     "start_time": "2026-01-02T09:08:54.501715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Verify Dataset Paths & Split Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75eaf61e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:08:54.526209Z",
     "iopub.status.busy": "2026-01-02T09:08:54.525898Z",
     "iopub.status.idle": "2026-01-02T09:08:55.379936Z",
     "shell.execute_reply": "2026-01-02T09:08:55.379102Z"
    },
    "papermill": {
     "duration": 0.865349,
     "end_time": "2026-01-02T09:08:55.382096",
     "exception": false,
     "start_time": "2026-01-02T09:08:54.516747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT DISCOVERY OK\n",
      "- DATA_ROOT: /kaggle/input/mallorn-dataset\n",
      "- Splits on disk: 20 (split_01..split_20)\n",
      "\n",
      "OBJECT COUNTS PER SPLIT (from logs)\n",
      "- split_01: train_objects=155 | test_objects=364\n",
      "- split_02: train_objects=170 | test_objects=414\n",
      "- split_03: train_objects=138 | test_objects=338\n",
      "- split_04: train_objects=145 | test_objects=332\n",
      "- split_05: train_objects=165 | test_objects=375\n",
      "- split_06: train_objects=155 | test_objects=374\n",
      "- split_07: train_objects=165 | test_objects=398\n",
      "- split_08: train_objects=162 | test_objects=387\n",
      "- split_09: train_objects=128 | test_objects=289\n",
      "- split_10: train_objects=144 | test_objects=331\n",
      "- split_11: train_objects=146 | test_objects=325\n",
      "- split_12: train_objects=155 | test_objects=353\n",
      "- split_13: train_objects=143 | test_objects=379\n",
      "- split_14: train_objects=154 | test_objects=351\n",
      "- split_15: train_objects=158 | test_objects=342\n",
      "- split_16: train_objects=155 | test_objects=354\n",
      "- split_17: train_objects=153 | test_objects=351\n",
      "- split_18: train_objects=152 | test_objects=345\n",
      "- split_19: train_objects=147 | test_objects=375\n",
      "- split_20: train_objects=153 | test_objects=358\n",
      "\n",
      "LIGHTCURVE FILE SIZES (MB)\n",
      "- split_01: train_full=     1.4 MB | test_full=     3.1 MB\n",
      "- split_02: train_full=     1.3 MB | test_full=     3.7 MB\n",
      "- split_03: train_full=     1.1 MB | test_full=     2.8 MB\n",
      "- split_04: train_full=     1.2 MB | test_full=     2.7 MB\n",
      "- split_05: train_full=     1.3 MB | test_full=     3.1 MB\n",
      "- split_06: train_full=     1.3 MB | test_full=     2.9 MB\n",
      "- split_07: train_full=     1.3 MB | test_full=     3.4 MB\n",
      "- split_08: train_full=     1.3 MB | test_full=     3.2 MB\n",
      "- split_09: train_full=     1.0 MB | test_full=     2.4 MB\n",
      "- split_10: train_full=     1.3 MB | test_full=     2.6 MB\n",
      "- split_11: train_full=     1.2 MB | test_full=     2.6 MB\n",
      "- split_12: train_full=     1.3 MB | test_full=     2.8 MB\n",
      "- split_13: train_full=     1.2 MB | test_full=     3.3 MB\n",
      "- split_14: train_full=     1.3 MB | test_full=     3.0 MB\n",
      "- split_15: train_full=     1.2 MB | test_full=     2.7 MB\n",
      "- split_16: train_full=     1.3 MB | test_full=     3.0 MB\n",
      "- split_17: train_full=     1.2 MB | test_full=     3.1 MB\n",
      "- split_18: train_full=     1.1 MB | test_full=     2.7 MB\n",
      "- split_19: train_full=     1.1 MB | test_full=     2.9 MB\n",
      "- split_20: train_full=     1.2 MB | test_full=     3.0 MB\n",
      "\n",
      "Stage 1 complete: splits ready for split-wise preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Verify Dataset Paths & Split Discovery (ONE CELL, CPU-SAFE) — REVISI FULL\n",
    "# - Uses globals from STAGE 0: PATHS, df_train_log, df_test_log\n",
    "# - DOES NOT load full lightcurves (header-only checks + tiny samples)\n",
    "# - Confirms:\n",
    "#   * split folders exist (split_01..split_20)\n",
    "#   * required lightcurve files exist per split\n",
    "#   * lightcurve schema matches expected columns\n",
    "#   * Filter values look sane (u,g,r,i,z,y) on small samples\n",
    "# - Summarizes:\n",
    "#   * object counts per split (from logs)\n",
    "#   * file sizes\n",
    "# - Exports: DATA_ROOT, SPLIT_DIRS, SPLIT_LIST\n",
    "# ============================================================\n",
    "\n",
    "import re, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"df_train_log\", \"df_test_log\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n",
    "\n",
    "DATA_ROOT = PATHS[\"DATA_ROOT\"]\n",
    "SPLIT_DIRS = {p.name: p for p in PATHS[\"SPLITS\"]}  # split_01..split_20 -> Path\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Canonical split names (keep consistent with STAGE 0)\n",
    "# ----------------------------\n",
    "VALID_SPLITS = {f\"split_{i:02d}\" for i in range(1, 21)}\n",
    "\n",
    "def normalize_split_name(x) -> str:\n",
    "    \"\"\"\n",
    "    Normalize split formats to canonical 'split_XX'.\n",
    "    Accepts: 'split_01', '01', '1', 'split1', 'Split 01', 'split-01', etc.\n",
    "    \"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    # digits only\n",
    "    if s2.isdigit():\n",
    "        k = int(s2)\n",
    "        return f\"split_{k:02d}\"\n",
    "    # split_XX\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    # splitXX\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    return s2\n",
    "\n",
    "def sizeof_mb(p: Path) -> float:\n",
    "    try:\n",
    "        return p.stat().st_size / (1024**2)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "# Expected lightcurve schema (as per dataset description)\n",
    "REQ_LC_COLS = {\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"}\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "\n",
    "def read_header_cols(p: Path):\n",
    "    \"\"\"Read header only (nrows=0) to validate schema with minimal IO.\"\"\"\n",
    "    df0 = pd.read_csv(p, nrows=0)\n",
    "    return [c.strip() for c in df0.columns]\n",
    "\n",
    "def sample_filter_values(p: Path, nrows: int = 200):\n",
    "    \"\"\"Read tiny sample of Filter only to sanity-check values.\"\"\"\n",
    "    df = pd.read_csv(p, usecols=[\"Filter\"], nrows=nrows)\n",
    "    vals = (\n",
    "        df[\"Filter\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    return sorted(set(vals))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Ensure split column in logs is canonical (idempotent)\n",
    "# ----------------------------\n",
    "for df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n",
    "    if \"split\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing 'split' column.\")\n",
    "    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n",
    "\n",
    "bad_train_split = sorted(set(df_train_log[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test_log[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Verify disk splits are complete and match expected 20 folders\n",
    "# ----------------------------\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "if disk_splits != VALID_SPLITS:\n",
    "    missing = sorted(list(VALID_SPLITS - disk_splits))\n",
    "    extra   = sorted(list(disk_splits - VALID_SPLITS))\n",
    "    msg = []\n",
    "    if missing:\n",
    "        msg.append(f\"Missing split folders: {missing[:10]}\")\n",
    "    if extra:\n",
    "        msg.append(f\"Unexpected split folders: {extra[:10]}\")\n",
    "    raise RuntimeError(\"Split folder set mismatch.\\n\" + \"\\n\".join(msg))\n",
    "\n",
    "# Verify logs reference only splits on disk (should pass if above checks pass)\n",
    "train_splits = set(df_train_log[\"split\"].unique())\n",
    "test_splits  = set(df_test_log[\"split\"].unique())\n",
    "bad_train = sorted([s for s in train_splits if s not in disk_splits])\n",
    "bad_test  = sorted([s for s in test_splits  if s not in disk_splits])\n",
    "if bad_train:\n",
    "    raise FileNotFoundError(f\"train_log references split(s) not found on disk: {bad_train[:10]}\")\n",
    "if bad_test:\n",
    "    raise FileNotFoundError(f\"test_log references split(s) not found on disk: {bad_test[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Verify required files per split exist\n",
    "# ----------------------------\n",
    "missing_files = []\n",
    "split_file_info = []  # (split, train_mb, test_mb)\n",
    "\n",
    "for split_name in sorted(disk_splits):\n",
    "    sd = SPLIT_DIRS[split_name]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if not tr.exists():\n",
    "        missing_files.append(str(tr))\n",
    "    if not te.exists():\n",
    "        missing_files.append(str(te))\n",
    "    split_file_info.append((split_name, sizeof_mb(tr), sizeof_mb(te)))\n",
    "\n",
    "if missing_files:\n",
    "    sample = \"\\n\".join(missing_files[:10])\n",
    "    raise FileNotFoundError(f\"Some lightcurve files missing (showing up to 10):\\n{sample}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Header-only schema check (very light)\n",
    "# ----------------------------\n",
    "col_issues = []\n",
    "for split_name in sorted(disk_splits):\n",
    "    sd = SPLIT_DIRS[split_name]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "\n",
    "    cols_tr = read_header_cols(tr)\n",
    "    cols_te = read_header_cols(te)\n",
    "\n",
    "    miss_tr = sorted(list(REQ_LC_COLS - set(cols_tr)))\n",
    "    miss_te = sorted(list(REQ_LC_COLS - set(cols_te)))\n",
    "\n",
    "    if miss_tr or miss_te:\n",
    "        col_issues.append((split_name, miss_tr, miss_te, cols_tr, cols_te))\n",
    "\n",
    "if col_issues:\n",
    "    s, miss_tr, miss_te, cols_tr, cols_te = col_issues[0]\n",
    "    raise ValueError(\n",
    "        \"Lightcurve column mismatch detected.\\n\"\n",
    "        f\"Example split: {s}\\n\"\n",
    "        f\"Missing in train_full_lightcurves.csv: {miss_tr}\\n\"\n",
    "        f\"Missing in test_full_lightcurves.csv : {miss_te}\\n\"\n",
    "        f\"Train columns: {cols_tr}\\n\"\n",
    "        f\"Test columns : {cols_te}\\n\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Tiny filter-value sanity check (small IO)\n",
    "# ----------------------------\n",
    "filter_issues = []\n",
    "for split_name in sorted(disk_splits):\n",
    "    sd = SPLIT_DIRS[split_name]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "\n",
    "    vals_tr = sample_filter_values(tr, nrows=200)\n",
    "    vals_te = sample_filter_values(te, nrows=200)\n",
    "\n",
    "    bad_tr = sorted([v for v in vals_tr if v not in ALLOWED_FILTERS and v != \"nan\"])\n",
    "    bad_te = sorted([v for v in vals_te if v not in ALLOWED_FILTERS and v != \"nan\"])\n",
    "\n",
    "    if bad_tr:\n",
    "        filter_issues.append((split_name, \"train\", bad_tr, vals_tr))\n",
    "    if bad_te:\n",
    "        filter_issues.append((split_name, \"test\", bad_te, vals_te))\n",
    "\n",
    "if filter_issues:\n",
    "    ex = filter_issues[0]\n",
    "    raise ValueError(\n",
    "        \"Unexpected Filter values detected (example):\\n\"\n",
    "        f\"split={ex[0]} file={ex[1]} bad={ex[2]} all_sampled={ex[3]}\\n\"\n",
    "        \"Fix by stripping/lowercasing Filter during preprocessing.\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Summaries (counts per split, file sizes)\n",
    "# ----------------------------\n",
    "train_counts = df_train_log[\"split\"].value_counts().to_dict()\n",
    "test_counts  = df_test_log[\"split\"].value_counts().to_dict()\n",
    "\n",
    "# Sanity: sums must match log lengths\n",
    "if int(sum(train_counts.values())) != int(len(df_train_log)):\n",
    "    raise RuntimeError(\"Train split counts do not sum to train_log length (unexpected).\")\n",
    "if int(sum(test_counts.values())) != int(len(df_test_log)):\n",
    "    raise RuntimeError(\"Test split counts do not sum to test_log length (unexpected).\")\n",
    "\n",
    "print(\"SPLIT DISCOVERY OK\")\n",
    "print(f\"- DATA_ROOT: {DATA_ROOT}\")\n",
    "print(f\"- Splits on disk: {len(disk_splits)} (split_01..split_20)\")\n",
    "\n",
    "print(\"\\nOBJECT COUNTS PER SPLIT (from logs)\")\n",
    "for s in sorted(disk_splits):\n",
    "    print(f\"- {s}: train_objects={train_counts.get(s,0):,} | test_objects={test_counts.get(s,0):,}\")\n",
    "\n",
    "print(\"\\nLIGHTCURVE FILE SIZES (MB)\")\n",
    "for s, mb_tr, mb_te in split_file_info:\n",
    "    print(f\"- {s}: train_full={mb_tr:8.1f} MB | test_full={mb_te:8.1f} MB\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Export split index for later stages (routing + loops)\n",
    "# ----------------------------\n",
    "SPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "globals().update({\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SPLIT_DIRS\": SPLIT_DIRS,\n",
    "    \"SPLIT_LIST\": SPLIT_LIST,\n",
    "})\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\nStage 1 complete: splits ready for split-wise preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5185f",
   "metadata": {
    "papermill": {
     "duration": 0.007637,
     "end_time": "2026-01-02T09:08:55.397740",
     "exception": false,
     "start_time": "2026-01-02T09:08:55.390103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and Validate Train/Test Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b15939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:08:55.415507Z",
     "iopub.status.busy": "2026-01-02T09:08:55.415155Z",
     "iopub.status.idle": "2026-01-02T09:08:55.776332Z",
     "shell.execute_reply": "2026-01-02T09:08:55.775527Z"
    },
    "papermill": {
     "duration": 0.372392,
     "end_time": "2026-01-02T09:08:55.778030",
     "exception": false,
     "start_time": "2026-01-02T09:08:55.405638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGS OK (clean + validated)\n",
      "- train objects: 3,043 | pos(TDE)=148 | neg=2,895 | pos%=4.864%\n",
      "- test objects : 7,135\n",
      "- saved train  : /kaggle/working/mallorn_run/artifacts/train_log_clean.parquet\n",
      "- saved test   : /kaggle/working/mallorn_run/artifacts/test_log_clean.parquet\n",
      "- saved stats  : /kaggle/working/mallorn_run/artifacts/split_stats.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Load and Validate Train/Test Logs (ONE CELL, CPU-SAFE) — REVISI FULL\n",
    "# - Ringan (tanpa load full lightcurves)\n",
    "# - Output:\n",
    "#   * df_train_meta, df_test_meta  (index=object_id, bersih & siap dipakai)\n",
    "#   * id2split_train, id2split_test (routing cepat ke split folder)\n",
    "#   * artifacts/train_log_clean.parquet (atau .csv fallback)\n",
    "#   * artifacts/test_log_clean.parquet  (atau .csv fallback)\n",
    "#   * artifacts/split_stats.csv\n",
    "# ============================================================\n",
    "\n",
    "import re, gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0/1 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"ART_DIR\", \"SPLIT_DIRS\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 & STAGE 1 dulu.\")\n",
    "\n",
    "TRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\n",
    "TEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "VALID_SPLITS = {f\"split_{i:02d}\" for i in range(1, 21)}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers (selaras dengan STAGE 0/1)\n",
    "# ----------------------------\n",
    "def normalize_split_name(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.isdigit():\n",
    "        k = int(s2)\n",
    "        return f\"split_{k:02d}\"\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    return s2\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _coerce_num(df: pd.DataFrame, col: str):\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Load logs (fresh read for consistency)\n",
    "# ----------------------------\n",
    "dtype_log = {\"object_id\": \"string\", \"split\": \"string\"}\n",
    "df_train = pd.read_csv(TRAIN_LOG_PATH, dtype=dtype_log)\n",
    "df_test  = pd.read_csv(TEST_LOG_PATH,  dtype=dtype_log)\n",
    "\n",
    "df_train = _norm_cols(df_train)\n",
    "df_test  = _norm_cols(df_test)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Required columns check\n",
    "# ----------------------------\n",
    "req_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\n",
    "req_train  = req_common | {\"target\"}\n",
    "req_test   = req_common\n",
    "\n",
    "miss_train = sorted(list(req_train - set(df_train.columns)))\n",
    "miss_test  = sorted(list(req_test  - set(df_test.columns)))\n",
    "\n",
    "if miss_train:\n",
    "    raise ValueError(f\"train_log.csv missing required columns: {miss_train} | found={list(df_train.columns)}\")\n",
    "if miss_test:\n",
    "    raise ValueError(f\"test_log.csv missing required columns: {miss_test} | found={list(df_test.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Basic cleaning (types + canonical split)\n",
    "# ----------------------------\n",
    "df_train[\"object_id\"] = df_train[\"object_id\"].astype(\"string\").str.strip()\n",
    "df_test[\"object_id\"]  = df_test[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "df_train[\"split\"] = df_train[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "df_test[\"split\"]  = df_test[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "\n",
    "# Split validity\n",
    "bad_train_split = sorted(set(df_train[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "bad_train_disk = sorted([s for s in set(df_train[\"split\"].unique()) if s not in disk_splits])\n",
    "bad_test_disk  = sorted([s for s in set(df_test[\"split\"].unique())  if s not in disk_splits])\n",
    "if bad_train_disk:\n",
    "    raise FileNotFoundError(f\"train_log references unknown split(s) not on disk: {bad_train_disk[:10]}\")\n",
    "if bad_test_disk:\n",
    "    raise FileNotFoundError(f\"test_log references unknown split(s) not on disk: {bad_test_disk[:10]}\")\n",
    "\n",
    "# Numeric coercion\n",
    "for c in [\"EBV\", \"Z\", \"Z_err\"]:\n",
    "    _coerce_num(df_train, c)\n",
    "    _coerce_num(df_test, c)\n",
    "\n",
    "# Ensure Z_err exists in both (schema consistency)\n",
    "if \"Z_err\" not in df_train.columns:\n",
    "    df_train[\"Z_err\"] = np.nan\n",
    "if \"Z_err\" not in df_test.columns:\n",
    "    df_test[\"Z_err\"] = np.nan\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Duplicates / overlap checks (hard fail)\n",
    "# ----------------------------\n",
    "if df_train[\"object_id\"].duplicated().any():\n",
    "    ex = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in train_log (examples): {ex}\")\n",
    "if df_test[\"object_id\"].duplicated().any():\n",
    "    ex = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in test_log (examples): {ex}\")\n",
    "\n",
    "overlap = set(df_train[\"object_id\"].tolist()) & set(df_test[\"object_id\"].tolist())\n",
    "if overlap:\n",
    "    ex = list(overlap)[:5]\n",
    "    raise ValueError(f\"object_id overlap between train_log and test_log (examples): {ex}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Target validation (train)\n",
    "# ----------------------------\n",
    "df_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    n_na = int(df_train[\"target\"].isna().sum())\n",
    "    raise ValueError(f\"train_log target has NaN after coercion: {n_na} rows.\")\n",
    "uniq_t = set(pd.unique(df_train[\"target\"]).tolist())\n",
    "if not uniq_t.issubset({0, 1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n",
    "df_train[\"target\"] = df_train[\"target\"].astype(np.int8)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Missing flags + fills (vectorized, CPU-safe)\n",
    "# ----------------------------\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"EBV_missing\"] = df[\"EBV\"].isna().astype(np.int8)\n",
    "    df[\"Z_missing\"]   = df[\"Z\"].isna().astype(np.int8)\n",
    "    df[\"Z_err_missing\"] = df[\"Z_err\"].isna().astype(np.int8)\n",
    "\n",
    "# EBV: fill NaN -> 0.0\n",
    "df_train[\"EBV\"] = df_train[\"EBV\"].fillna(0.0)\n",
    "df_test[\"EBV\"]  = df_test[\"EBV\"].fillna(0.0)\n",
    "\n",
    "# Z: fill NaN -> median per split (fallback global median)\n",
    "def fill_z_by_split(df: pd.DataFrame) -> pd.Series:\n",
    "    z = df[\"Z\"]\n",
    "    if not z.isna().any():\n",
    "        return z\n",
    "    z_fill = z.fillna(df.groupby(\"split\")[\"Z\"].transform(\"median\"))\n",
    "    global_med = float(z.median()) if z.notna().any() else 0.0\n",
    "    z_fill = z_fill.fillna(global_med)\n",
    "    return z_fill\n",
    "\n",
    "df_train[\"Z\"] = fill_z_by_split(df_train)\n",
    "df_test[\"Z\"]  = fill_z_by_split(df_test)\n",
    "\n",
    "# Z_err: fill NaN -> 0.0 (schema stability)\n",
    "df_train[\"Z_err\"] = df_train[\"Z_err\"].fillna(0.0)\n",
    "df_test[\"Z_err\"]  = df_test[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "# Domain flag (analysis-only; jangan dipakai sebagai fitur model)\n",
    "df_train[\"is_photoz\"] = np.int8(0)\n",
    "df_test[\"is_photoz\"]  = np.int8(1)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Build meta tables (index=object_id) + routing dicts\n",
    "# ----------------------------\n",
    "keep_train = [\n",
    "    \"object_id\",\"split\",\"EBV\",\"Z\",\"Z_err\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Z_err_missing\",\n",
    "    \"is_photoz\",\"target\"\n",
    "]\n",
    "keep_test = [\n",
    "    \"object_id\",\"split\",\"EBV\",\"Z\",\"Z_err\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Z_err_missing\",\n",
    "    \"is_photoz\"\n",
    "]\n",
    "\n",
    "# Optional: keep SpecType for analysis\n",
    "if \"SpecType\" in df_train.columns:\n",
    "    keep_train.append(\"SpecType\")\n",
    "\n",
    "df_train_meta = df_train[keep_train].copy()\n",
    "df_test_meta  = df_test[keep_test].copy()\n",
    "\n",
    "df_train_meta = df_train_meta.set_index(\"object_id\", drop=True).sort_index()\n",
    "df_test_meta  = df_test_meta.set_index(\"object_id\", drop=True).sort_index()\n",
    "\n",
    "id2split_train = df_train_meta[\"split\"].to_dict()\n",
    "id2split_test  = df_test_meta[\"split\"].to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Save cleaned logs (parquet preferred, csv fallback)\n",
    "# ----------------------------\n",
    "train_out_pq = Path(ART_DIR) / \"train_log_clean.parquet\"\n",
    "test_out_pq  = Path(ART_DIR) / \"test_log_clean.parquet\"\n",
    "train_out_csv = Path(ART_DIR) / \"train_log_clean.csv\"\n",
    "test_out_csv  = Path(ART_DIR) / \"test_log_clean.csv\"\n",
    "\n",
    "saved_train = None\n",
    "saved_test  = None\n",
    "try:\n",
    "    df_train_meta.to_parquet(train_out_pq, index=True)\n",
    "    df_test_meta.to_parquet(test_out_pq, index=True)\n",
    "    saved_train = str(train_out_pq)\n",
    "    saved_test  = str(test_out_pq)\n",
    "except Exception:\n",
    "    df_train_meta.to_csv(train_out_csv, index=True)\n",
    "    df_test_meta.to_csv(test_out_csv, index=True)\n",
    "    saved_train = str(train_out_csv)\n",
    "    saved_test  = str(test_out_csv)\n",
    "\n",
    "# Split stats (debug)\n",
    "split_stats = pd.DataFrame({\n",
    "    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(sorted(disk_splits)).fillna(0).astype(int),\n",
    "    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(sorted(disk_splits)).fillna(0).astype(int),\n",
    "})\n",
    "split_stats.index.name = \"split\"\n",
    "split_stats_path = Path(ART_DIR) / \"split_stats.csv\"\n",
    "split_stats.to_csv(split_stats_path)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Print summary\n",
    "# ----------------------------\n",
    "pos = int((df_train_meta[\"target\"] == 1).sum())\n",
    "neg = int((df_train_meta[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_meta))\n",
    "\n",
    "print(\"LOGS OK (clean + validated)\")\n",
    "print(f\"- train objects: {tot:,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.3f}%\")\n",
    "print(f\"- test objects : {len(df_test_meta):,}\")\n",
    "print(f\"- saved train  : {saved_train}\")\n",
    "print(f\"- saved test   : {saved_test}\")\n",
    "print(f\"- saved stats  : {split_stats_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Export globals for next stages\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"df_train_meta\": df_train_meta,\n",
    "    \"df_test_meta\": df_test_meta,\n",
    "    \"id2split_train\": id2split_train,\n",
    "    \"id2split_test\": id2split_test,\n",
    "    \"split_stats\": split_stats,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c66c70",
   "metadata": {
    "papermill": {
     "duration": 0.007935,
     "end_time": "2026-01-02T09:08:55.794216",
     "exception": false,
     "start_time": "2026-01-02T09:08:55.786281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lightcurve Loading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f13b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:08:55.812591Z",
     "iopub.status.busy": "2026-01-02T09:08:55.812218Z",
     "iopub.status.idle": "2026-01-02T09:08:56.581148Z",
     "shell.execute_reply": "2026-01-02T09:08:56.580257Z"
    },
    "papermill": {
     "duration": 0.780687,
     "end_time": "2026-01-02T09:08:56.582850",
     "exception": false,
     "start_time": "2026-01-02T09:08:55.802163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIGHTCURVE LOADING STRATEGY OK (split-wise + chunked)\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/split_file_manifest.csv\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/object_counts_by_split.csv\n",
      "- Ready for next stage: photometric preprocessing + ASTROMER input building (split-wise loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — Lightcurve Loading Strategy (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL\n",
    "# - Split-wise file mapping + chunked reader utilities (no full concat)\n",
    "# - Builds:\n",
    "#   * SPLIT_FILES: {split_XX: {\"train\": Path, \"test\": Path}}\n",
    "#   * train_ids_by_split / test_ids_by_split: routing object_ids per split\n",
    "#   * iter_lightcurve_chunks(): generator read_csv(chunksize=...)\n",
    "#   * load_object_lightcurve(): debug-safe per-object extraction (streaming, optional guard)\n",
    "# - Saves:\n",
    "#   * artifacts/split_file_manifest.csv\n",
    "#   * artifacts/object_counts_by_split.csv\n",
    "# ============================================================\n",
    "\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build split file mapping (train/test lightcurves)\n",
    "# ----------------------------\n",
    "SPLIT_FILES = {}\n",
    "for s in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[s]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n",
    "    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n",
    "\n",
    "# Save split file manifest\n",
    "manifest = []\n",
    "for s in SPLIT_LIST:\n",
    "    p_tr = SPLIT_FILES[s][\"train\"]\n",
    "    p_te = SPLIT_FILES[s][\"test\"]\n",
    "    manifest.append({\n",
    "        \"split\": s,\n",
    "        \"train_path\": str(p_tr),\n",
    "        \"test_path\": str(p_te),\n",
    "        \"train_mb\": float(p_tr.stat().st_size) / (1024**2),\n",
    "        \"test_mb\":  float(p_te.stat().st_size) / (1024**2),\n",
    "    })\n",
    "df_manifest = pd.DataFrame(manifest).sort_values(\"split\")\n",
    "manifest_path = Path(ART_DIR) / \"split_file_manifest.csv\"\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build object routing by split (FAST + correct)\n",
    "# ----------------------------\n",
    "train_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "test_ids_by_split  = {s: [] for s in SPLIT_LIST}\n",
    "\n",
    "# df_train_meta / df_test_meta index = object_id, and column \"split\" exists\n",
    "for oid, split_name in df_train_meta[\"split\"].items():\n",
    "    train_ids_by_split[str(split_name)].append(str(oid))\n",
    "for oid, split_name in df_test_meta[\"split\"].items():\n",
    "    test_ids_by_split[str(split_name)].append(str(oid))\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    \"split\": SPLIT_LIST,\n",
    "    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "})\n",
    "counts_path = Path(ART_DIR) / \"object_counts_by_split.csv\"\n",
    "df_counts.to_csv(counts_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Column normalization (canonical: object_id, mjd, flux, flux_err, filter)\n",
    "# ----------------------------\n",
    "REQ_LC_KEYS = [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "\n",
    "# Header config cache to avoid re-reading headers repeatedly\n",
    "_LC_CFG_CACHE = {}  # (split_name, which) -> dict(usecols=[...], dtype={...}, rename={...})\n",
    "\n",
    "def _build_lc_read_cfg(p: Path):\n",
    "    \"\"\"\n",
    "    Build a robust read config by:\n",
    "    - reading header only once\n",
    "    - matching required columns by stripped name (tolerant to whitespace)\n",
    "    - selecting best time column among known options\n",
    "    \"\"\"\n",
    "    header = pd.read_csv(p, nrows=0)\n",
    "    orig_cols = list(header.columns)\n",
    "\n",
    "    # map stripped -> original (first occurrence)\n",
    "    strip2orig = {}\n",
    "    for c in orig_cols:\n",
    "        cs = str(c).strip()\n",
    "        if cs not in strip2orig:\n",
    "            strip2orig[cs] = c\n",
    "\n",
    "    # choose time column\n",
    "    time_candidates = [\"Time (MJD)\", \"Time(MJD)\", \"Time\"]\n",
    "    time_col = None\n",
    "    for tc in time_candidates:\n",
    "        if tc in strip2orig:\n",
    "            time_col = strip2orig[tc]\n",
    "            break\n",
    "    if time_col is None:\n",
    "        raise ValueError(f\"Cannot find time column in {p}. Expected one of {time_candidates}. Found: {orig_cols[:20]}\")\n",
    "\n",
    "    # required columns (match by stripped name)\n",
    "    required = {\n",
    "        \"object_id\": strip2orig.get(\"object_id\", None),\n",
    "        \"mjd\": time_col,\n",
    "        \"flux\": strip2orig.get(\"Flux\", None),\n",
    "        \"flux_err\": strip2orig.get(\"Flux_err\", None),\n",
    "        \"filter\": strip2orig.get(\"Filter\", None),\n",
    "    }\n",
    "    missing = [k for k, v in required.items() if v is None]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required lightcurve columns in {p}: {missing}. Found: {orig_cols[:30]}\")\n",
    "\n",
    "    usecols = [required[\"object_id\"], required[\"mjd\"], required[\"flux\"], required[\"flux_err\"], required[\"filter\"]]\n",
    "    rename = {\n",
    "        required[\"object_id\"]: \"object_id\",\n",
    "        required[\"mjd\"]: \"mjd\",\n",
    "        required[\"flux\"]: \"flux\",\n",
    "        required[\"flux_err\"]: \"flux_err\",\n",
    "        required[\"filter\"]: \"filter\",\n",
    "    }\n",
    "\n",
    "    # dtypes: keep safe and small\n",
    "    dtypes = {\n",
    "        required[\"object_id\"]: \"string\",\n",
    "        required[\"filter\"]: \"string\",\n",
    "        required[\"flux\"]: \"float32\",\n",
    "        required[\"flux_err\"]: \"float32\",\n",
    "        required[\"mjd\"]: \"float32\",\n",
    "    }\n",
    "    return {\"usecols\": usecols, \"dtype\": dtypes, \"rename\": rename}\n",
    "\n",
    "def _normalize_lc_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize a raw chunk to canonical columns and clean string fields.\n",
    "    Output columns: object_id, mjd, flux, flux_err, filter\n",
    "    \"\"\"\n",
    "    df = df.rename(columns={c: str(c).strip() for c in df.columns})\n",
    "    # sometimes dtype coercion still yields float64; cast down\n",
    "    df = df.rename(columns={\n",
    "        \"Time (MJD)\": \"mjd\", \"Time(MJD)\": \"mjd\", \"Time\": \"mjd\",\n",
    "        \"Flux\": \"flux\", \"Flux_err\": \"flux_err\", \"Filter\": \"filter\",\n",
    "        \"object_id\": \"object_id\"\n",
    "    })\n",
    "    need = {\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"}\n",
    "    miss = sorted(list(need - set(df.columns)))\n",
    "    if miss:\n",
    "        raise ValueError(f\"Lightcurve chunk missing columns after rename: {miss}. Found: {list(df.columns)}\")\n",
    "\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "    df[\"filter\"] = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n",
    "\n",
    "    # numeric cast\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(\"float32\")\n",
    "    df[\"flux\"] = pd.to_numeric(df[\"flux\"], errors=\"coerce\").astype(\"float32\")\n",
    "    df[\"flux_err\"] = pd.to_numeric(df[\"flux_err\"], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    return df[REQ_LC_KEYS]\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Chunked readers (core strategy)\n",
    "# ----------------------------\n",
    "def iter_lightcurve_chunks(split_name: str, which: str, chunksize: int = 400_000):\n",
    "    \"\"\"\n",
    "    Stream read a split lightcurve CSV in chunks.\n",
    "    Yields normalized chunks with columns:\n",
    "      object_id, mjd, flux, flux_err, filter\n",
    "    \"\"\"\n",
    "    if split_name not in SPLIT_FILES:\n",
    "        raise KeyError(f\"Unknown split_name={split_name}.\")\n",
    "    if which not in (\"train\", \"test\"):\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    p = SPLIT_FILES[split_name][which]\n",
    "    key = (split_name, which)\n",
    "    if key not in _LC_CFG_CACHE:\n",
    "        _LC_CFG_CACHE[key] = _build_lc_read_cfg(p)\n",
    "\n",
    "    cfg = _LC_CFG_CACHE[key]\n",
    "    reader = pd.read_csv(\n",
    "        p,\n",
    "        usecols=cfg[\"usecols\"],\n",
    "        dtype=cfg[\"dtype\"],\n",
    "        chunksize=int(chunksize),\n",
    "    )\n",
    "    for chunk in reader:\n",
    "        chunk = chunk.rename(columns=cfg[\"rename\"])\n",
    "        yield _normalize_lc_chunk(chunk)\n",
    "\n",
    "def load_split_lightcurves(split_name: str, which: str, chunksize: int = 400_000):\n",
    "    \"\"\"\n",
    "    Convenience: load entire split file (NOT recommended for large files).\n",
    "    Use only for quick debugging.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=chunksize):\n",
    "        parts.append(ch)\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=REQ_LC_KEYS)\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "def load_object_lightcurve(object_id: str, which: str, chunksize: int = 400_000, sort_time: bool = True, max_chunks: int = None):\n",
    "    \"\"\"\n",
    "    Debug-safe per-object extraction by streaming the relevant split file.\n",
    "    WARNING: This scans the split CSV in chunks. Use only occasionally.\n",
    "    max_chunks: optional guard to stop after N chunks (useful to avoid accidental long scans).\n",
    "    \"\"\"\n",
    "    object_id = str(object_id).strip()\n",
    "    if which == \"train\":\n",
    "        if object_id not in df_train_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n",
    "        split_name = str(df_train_meta.loc[object_id, \"split\"])\n",
    "    elif which == \"test\":\n",
    "        if object_id not in df_test_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n",
    "        split_name = str(df_test_meta.loc[object_id, \"split\"])\n",
    "    else:\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    pieces = []\n",
    "    n_seen = 0\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=chunksize):\n",
    "        n_seen += 1\n",
    "        sub = ch[ch[\"object_id\"] == object_id]\n",
    "        if not sub.empty:\n",
    "            pieces.append(sub)\n",
    "        if max_chunks is not None and n_seen >= int(max_chunks):\n",
    "            break\n",
    "\n",
    "    if not pieces:\n",
    "        out = pd.DataFrame(columns=REQ_LC_KEYS)\n",
    "    else:\n",
    "        out = pd.concat(pieces, ignore_index=True)\n",
    "        if sort_time and len(out) > 1:\n",
    "            out = out.sort_values([\"mjd\", \"filter\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Quick smoke test (VERY light, no full scan)\n",
    "# ----------------------------\n",
    "# Tujuan: memastikan chunk reader bekerja + kolom benar + filter values wajar\n",
    "_smoke_splits = [\"split_01\", \"split_08\", \"split_17\"]\n",
    "for s in _smoke_splits:\n",
    "    if len(train_ids_by_split.get(s, [])) == 0 or len(test_ids_by_split.get(s, [])) == 0:\n",
    "        raise RuntimeError(f\"Split {s} has 0 objects in train/test log (unexpected).\")\n",
    "\n",
    "    # read one small chunk from train and test\n",
    "    ch_tr = next(iter_lightcurve_chunks(s, \"train\", chunksize=50_000))\n",
    "    ch_te = next(iter_lightcurve_chunks(s, \"test\",  chunksize=50_000))\n",
    "\n",
    "    # schema\n",
    "    if list(ch_tr.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Train chunk schema mismatch in {s}: {list(ch_tr.columns)}\")\n",
    "    if list(ch_te.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Test chunk schema mismatch in {s}: {list(ch_te.columns)}\")\n",
    "\n",
    "    # filter sanity on sample chunk\n",
    "    badf_tr = sorted(set(ch_tr[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n",
    "    badf_te = sorted(set(ch_te[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n",
    "    if badf_tr or badf_te:\n",
    "        raise ValueError(f\"Unexpected filter values in smoke chunk split={s}: train_bad={badf_tr} test_bad={badf_te}\")\n",
    "\n",
    "print(\"LIGHTCURVE LOADING STRATEGY OK (split-wise + chunked)\")\n",
    "print(f\"- Saved: {manifest_path}\")\n",
    "print(f\"- Saved: {counts_path}\")\n",
    "print(\"- Ready for next stage: photometric preprocessing + ASTROMER input building (split-wise loop).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Export globals for next stages\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"SPLIT_FILES\": SPLIT_FILES,\n",
    "    \"train_ids_by_split\": train_ids_by_split,\n",
    "    \"test_ids_by_split\": test_ids_by_split,\n",
    "    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n",
    "    \"load_object_lightcurve\": load_object_lightcurve,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce6bae",
   "metadata": {
    "papermill": {
     "duration": 0.008061,
     "end_time": "2026-01-02T09:08:56.599147",
     "exception": false,
     "start_time": "2026-01-02T09:08:56.591086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab88971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:08:56.617328Z",
     "iopub.status.busy": "2026-01-02T09:08:56.616985Z",
     "iopub.status.idle": "2026-01-02T09:09:02.932363Z",
     "shell.execute_reply": "2026-01-02T09:09:02.931597Z"
    },
    "papermill": {
     "duration": 6.326686,
     "end_time": "2026-01-02T09:09:02.933947",
     "exception": false,
     "start_time": "2026-01-02T09:08:56.607261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4] Building cleaned MAG lightcurve cache (split-wise) ...\n",
      "[Stage 4] split_01/train: parts=1 | rows=26,324 | det%=19.34% | neg_proxy%=38.95% | mag_range=[19.72, 25.96]\n",
      "[Stage 4] split_01/test: parts=1 | rows=59,235 | det%=23.02% | neg_proxy%=37.74% | mag_range=[19.61, 26.20]\n",
      "[Stage 4] split_02/train: parts=1 | rows=25,609 | det%=24.45% | neg_proxy%=34.02% | mag_range=[20.10, 26.04]\n",
      "[Stage 4] split_02/test: parts=1 | rows=71,229 | det%=21.69% | neg_proxy%=36.48% | mag_range=[18.77, 26.32]\n",
      "[Stage 4] split_03/train: parts=1 | rows=21,676 | det%=21.65% | neg_proxy%=36.82% | mag_range=[20.17, 26.23]\n",
      "[Stage 4] split_03/test: parts=1 | rows=53,751 | det%=21.90% | neg_proxy%=36.70% | mag_range=[19.61, 26.37]\n",
      "[Stage 4] split_04/train: parts=1 | rows=22,898 | det%=21.11% | neg_proxy%=38.36% | mag_range=[20.38, 26.16]\n",
      "[Stage 4] split_04/test: parts=1 | rows=51,408 | det%=21.70% | neg_proxy%=38.16% | mag_range=[19.66, 26.25]\n",
      "[Stage 4] split_05/train: parts=1 | rows=25,934 | det%=18.33% | neg_proxy%=39.19% | mag_range=[18.82, 26.33]\n",
      "[Stage 4] split_05/test: parts=1 | rows=61,179 | det%=18.21% | neg_proxy%=38.41% | mag_range=[19.09, 26.23]\n",
      "[Stage 4] split_06/train: parts=1 | rows=25,684 | det%=18.85% | neg_proxy%=38.07% | mag_range=[19.80, 26.15]\n",
      "[Stage 4] split_06/test: parts=1 | rows=57,620 | det%=19.94% | neg_proxy%=37.39% | mag_range=[19.28, 26.04]\n",
      "[Stage 4] split_07/train: parts=1 | rows=24,473 | det%=21.44% | neg_proxy%=36.28% | mag_range=[20.05, 26.32]\n",
      "[Stage 4] split_07/test: parts=1 | rows=65,101 | det%=19.10% | neg_proxy%=36.96% | mag_range=[19.97, 26.32]\n",
      "[Stage 4] split_08/train: parts=1 | rows=25,571 | det%=22.80% | neg_proxy%=39.54% | mag_range=[18.59, 26.30]\n",
      "[Stage 4] split_08/test: parts=1 | rows=61,498 | det%=24.50% | neg_proxy%=37.88% | mag_range=[17.18, 25.96]\n",
      "[Stage 4] split_09/train: parts=1 | rows=19,690 | det%=21.13% | neg_proxy%=36.18% | mag_range=[19.83, 26.31]\n",
      "[Stage 4] split_09/test: parts=1 | rows=47,239 | det%=22.70% | neg_proxy%=35.22% | mag_range=[18.88, 26.11]\n",
      "[Stage 4] split_10/train: parts=1 | rows=25,151 | det%=20.86% | neg_proxy%=40.38% | mag_range=[19.52, 26.21]\n",
      "[Stage 4] split_10/test: parts=1 | rows=51,056 | det%=21.21% | neg_proxy%=39.39% | mag_range=[16.62, 26.43]\n",
      "[Stage 4] split_11/train: parts=1 | rows=22,927 | det%=19.59% | neg_proxy%=39.52% | mag_range=[20.57, 26.42]\n",
      "[Stage 4] split_11/test: parts=1 | rows=49,723 | det%=20.17% | neg_proxy%=39.36% | mag_range=[20.31, 26.42]\n",
      "[Stage 4] split_12/train: parts=1 | rows=25,546 | det%=19.64% | neg_proxy%=40.10% | mag_range=[20.40, 26.32]\n",
      "[Stage 4] split_12/test: parts=1 | rows=54,499 | det%=19.29% | neg_proxy%=38.64% | mag_range=[19.98, 26.26]\n",
      "[Stage 4] split_13/train: parts=1 | rows=23,203 | det%=20.64% | neg_proxy%=37.92% | mag_range=[19.46, 26.05]\n",
      "[Stage 4] split_13/test: parts=1 | rows=63,653 | det%=19.56% | neg_proxy%=39.26% | mag_range=[18.03, 26.15]\n",
      "[Stage 4] split_14/train: parts=1 | rows=25,706 | det%=20.36% | neg_proxy%=35.93% | mag_range=[20.24, 26.40]\n",
      "[Stage 4] split_14/test: parts=1 | rows=58,643 | det%=17.91% | neg_proxy%=36.82% | mag_range=[19.51, 26.04]\n",
      "[Stage 4] split_15/train: parts=1 | rows=23,972 | det%=19.09% | neg_proxy%=38.02% | mag_range=[20.11, 26.04]\n",
      "[Stage 4] split_15/test: parts=1 | rows=52,943 | det%=20.03% | neg_proxy%=38.31% | mag_range=[19.91, 26.24]\n",
      "[Stage 4] split_16/train: parts=1 | rows=25,173 | det%=21.42% | neg_proxy%=36.92% | mag_range=[20.00, 26.26]\n",
      "[Stage 4] split_16/test: parts=1 | rows=58,192 | det%=20.12% | neg_proxy%=37.85% | mag_range=[19.57, 26.17]\n",
      "[Stage 4] split_17/train: parts=1 | rows=22,705 | det%=22.09% | neg_proxy%=35.75% | mag_range=[19.64, 26.17]\n",
      "[Stage 4] split_17/test: parts=1 | rows=59,482 | det%=19.59% | neg_proxy%=38.08% | mag_range=[19.96, 26.42]\n",
      "[Stage 4] split_18/train: parts=1 | rows=21,536 | det%=23.77% | neg_proxy%=36.63% | mag_range=[20.49, 26.05]\n",
      "[Stage 4] split_18/test: parts=1 | rows=53,887 | det%=23.88% | neg_proxy%=35.71% | mag_range=[20.60, 26.44]\n",
      "[Stage 4] split_19/train: parts=1 | rows=22,087 | det%=23.73% | neg_proxy%=36.38% | mag_range=[19.98, 26.31]\n",
      "[Stage 4] split_19/test: parts=1 | rows=56,355 | det%=24.17% | neg_proxy%=35.52% | mag_range=[19.93, 26.34]\n",
      "[Stage 4] split_20/train: parts=1 | rows=23,519 | det%=20.45% | neg_proxy%=37.99% | mag_range=[19.80, 25.93]\n",
      "[Stage 4] split_20/test: parts=1 | rows=58,432 | det%=19.65% | neg_proxy%=38.29% | mag_range=[19.87, 26.36]\n",
      "\n",
      "[Stage 4] Done.\n",
      "- Saved manifest: /kaggle/working/mallorn_run/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
      "- Saved summary : /kaggle/working/mallorn_run/artifacts/lc_clean_mag/lc_clean_mag_summary.csv\n",
      "- Saved config  : /kaggle/working/mallorn_run/artifacts/lc_clean_mag/photometric_config_mag.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Photometric Cleaning (De-extinction + NEGATIVE FLUX -> MAG SAFE) — REVISI FULL (ASTROMER-READY)\n",
    "# ONE CELL, Kaggle CPU-SAFE, split-wise + chunked\n",
    "#\n",
    "# Prasyarat:\n",
    "# - iter_lightcurve_chunks (STAGE 3)  -> yields: object_id,mjd,flux,flux_err,filter\n",
    "# - df_train_meta, df_test_meta (STAGE 2) with EBV\n",
    "# - ART_DIR, SPLIT_LIST (STAGE 0/1)\n",
    "#\n",
    "# Output (per observation, siap untuk ASTROMER input builder):\n",
    "# - lc_clean_mag/split_XX/{train|test}/part_*.parquet (atau .csv.gz fallback)\n",
    "#   columns:\n",
    "#     object_id (string), mjd(float32), band_id(int8), mag(float32), mag_err(float32),\n",
    "#     snr(float32), detected(int8)\n",
    "# - manifest + summary + config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings (CPU-safe defaults)\n",
    "# ----------------------------\n",
    "CHUNKSIZE   = 350_000   # bigger=faster but more RAM\n",
    "ERR_EPS     = 1e-6      # avoid div-by-zero\n",
    "SNR_DET     = 3.0       # detection threshold (proxy)\n",
    "DET_SIGMA   = 3.0       # for non-detection \"limit\" flux = DET_SIGMA * err\n",
    "\n",
    "# magnitude stability\n",
    "MIN_FLUX_POS_UJY   = 1e-6   # clamp minimum positive flux to avoid log10(0)\n",
    "MAG_MIN, MAG_MAX   = -10.0, 50.0\n",
    "MAGERR_FLOOR_DET   = 1e-3\n",
    "MAGERR_FLOOR_ND    = 0.75   # inflate uncertainty for non-detections (helps encoder + head)\n",
    "MAGERR_CAP         = 10.0\n",
    "\n",
    "WRITE_FORMAT = \"parquet\"     # parquet recommended; auto-fallback to csv.gz if parquet fails\n",
    "ONLY_SPLITS  = None          # None = process all 20 splits; else e.g. [\"split_01\",\"split_02\"]\n",
    "\n",
    "# Debug: keep flux_deext? (biasanya tidak perlu untuk ASTROMER, bikin file lebih besar)\n",
    "KEEP_FLUX_DEBUG = False\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Extinction coefficients (R_lambda)\n",
    "# NOTE: kalau kamu punya koefisien resmi dari notebook \"Using_the_Data\", ganti nilai di sini.\n",
    "# A_lambda = R_lambda * EBV\n",
    "# Flux de-extinction: flux_deext = flux * 10^(0.4 * A_lambda)\n",
    "# ----------------------------\n",
    "EXT_RLAMBDA = {\n",
    "    \"u\": 4.8,\n",
    "    \"g\": 3.6,\n",
    "    \"r\": 2.7,\n",
    "    \"i\": 2.1,\n",
    "    \"z\": 1.6,\n",
    "    \"y\": 1.3,\n",
    "}\n",
    "\n",
    "BAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\n",
    "ID2BAND = {v: k for k, v in BAND2ID.items()}\n",
    "ALLOWED_BANDS = set(BAND2ID.keys())\n",
    "\n",
    "# EBV mapping Series (index=object_id)\n",
    "EBV_TRAIN_SER = df_train_meta[\"EBV\"]\n",
    "EBV_TEST_SER  = df_test_meta[\"EBV\"]\n",
    "\n",
    "# AB magnitude zero-point for flux in microJansky (uJy):\n",
    "# mag_AB = 23.9 - 2.5*log10(flux_uJy)  (because 3631 Jy = 3.631e9 uJy)\n",
    "MAG_ZP = float(2.5 * np.log10(3631e6))  # ~23.899999...\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Core cleaning: chunk -> (de-extinct flux) -> mag/mag_err (negative-safe)\n",
    "# ----------------------------\n",
    "def clean_chunk_to_mag(ch: pd.DataFrame, ebv_ser: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input chunk columns: object_id,mjd,flux,flux_err,filter\n",
    "    Output columns:\n",
    "      object_id, mjd, band_id, mag, mag_err, snr, detected  (+optional flux_deext, err_deext)\n",
    "    \"\"\"\n",
    "    # base arrays\n",
    "    oid = ch[\"object_id\"].astype(\"string\").to_numpy()\n",
    "    mjd = ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    err  = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    err  = np.maximum(err, np.float32(ERR_EPS))\n",
    "\n",
    "    filt = ch[\"filter\"].astype(\"string\").to_numpy()\n",
    "    # normalize to lowercase single char\n",
    "    # (stage 3 already lowercased, but keep safe)\n",
    "    filt = np.char.lower(np.char.strip(filt.astype(str)))\n",
    "\n",
    "    # band_id (vectorized)\n",
    "    band_id = np.full(len(ch), -1, dtype=np.int8)\n",
    "    for b, bid in BAND2ID.items():\n",
    "        band_id[filt == b] = np.int8(bid)\n",
    "    if np.any(band_id < 0):\n",
    "        bad = sorted(set(filt[band_id < 0].tolist()))\n",
    "        raise ValueError(f\"Unknown filter values encountered (example up to 10): {bad[:10]}\")\n",
    "\n",
    "    # EBV lookup\n",
    "    ebv = ch[\"object_id\"].map(ebv_ser).fillna(0.0).to_numpy(dtype=np.float32)\n",
    "\n",
    "    # R_lambda lookup (vectorized)\n",
    "    rlam = np.zeros(len(ch), dtype=np.float32)\n",
    "    for b, rv in EXT_RLAMBDA.items():\n",
    "        rlam[filt == b] = np.float32(rv)\n",
    "\n",
    "    A = (rlam * ebv).astype(np.float32)  # A_lambda\n",
    "    mul = np.power(np.float32(10.0), (np.float32(0.4) * A)).astype(np.float32)\n",
    "\n",
    "    # de-extinct in flux domain\n",
    "    flux_deext = (flux * mul).astype(np.float32)\n",
    "    err_deext  = (err  * mul).astype(np.float32)\n",
    "\n",
    "    # SNR + detected\n",
    "    snr = (flux_deext / np.maximum(err_deext, np.float32(ERR_EPS))).astype(np.float32)\n",
    "    detected = (snr > np.float32(SNR_DET)).astype(np.int8)\n",
    "\n",
    "    # Negative-safe magnitude:\n",
    "    # - if detected: use measured de-extinct flux (clamped positive)\n",
    "    # - else: use detection-limit flux = DET_SIGMA * err_deext (clamped positive)\n",
    "    flux_detlim = (np.float32(DET_SIGMA) * err_deext).astype(np.float32)\n",
    "\n",
    "    flux_for_mag = np.where(\n",
    "        detected == 1,\n",
    "        np.maximum(flux_deext, np.float32(MIN_FLUX_POS_UJY)),\n",
    "        np.maximum(flux_detlim, np.float32(MIN_FLUX_POS_UJY)),\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # mag\n",
    "    mag = (np.float32(MAG_ZP) - np.float32(2.5) * np.log10(flux_for_mag)).astype(np.float32)\n",
    "    mag = np.clip(mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32)\n",
    "\n",
    "    # mag_err ~ (2.5/ln 10) * (err/flux_for_mag)\n",
    "    mag_err = (np.float32(1.0857362) * (err_deext / flux_for_mag)).astype(np.float32)\n",
    "    mag_err = np.clip(mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32)\n",
    "\n",
    "    # inflate uncertainty for non-detections (important)\n",
    "    if MAGERR_FLOOR_ND is not None and float(MAGERR_FLOOR_ND) > 0:\n",
    "        mag_err = np.where(\n",
    "            detected == 1,\n",
    "            mag_err,\n",
    "            np.maximum(mag_err, np.float32(MAGERR_FLOOR_ND))\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"object_id\": oid,\n",
    "        \"mjd\": mjd,\n",
    "        \"band_id\": band_id,\n",
    "        \"mag\": mag,\n",
    "        \"mag_err\": mag_err,\n",
    "        \"snr\": snr,\n",
    "        \"detected\": detected,\n",
    "    })\n",
    "\n",
    "    if KEEP_FLUX_DEBUG:\n",
    "        out[\"flux_deext\"] = flux_deext\n",
    "        out[\"err_deext\"]  = err_deext\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Writer (parquet preferred; fallback csv.gz)\n",
    "# ----------------------------\n",
    "def write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == \"parquet\":\n",
    "        try:\n",
    "            df.to_parquet(out_path, index=False)\n",
    "            return \"parquet\", out_path\n",
    "        except Exception as e:\n",
    "            alt = out_path.with_suffix(\".csv.gz\")\n",
    "            df.to_csv(alt, index=False, compression=\"gzip\")\n",
    "            return f\"csv.gz (fallback from parquet: {type(e).__name__})\", alt\n",
    "    elif fmt == \"csv.gz\":\n",
    "        alt = out_path.with_suffix(\".csv.gz\")\n",
    "        df.to_csv(alt, index=False, compression=\"gzip\")\n",
    "        return \"csv.gz\", alt\n",
    "    else:\n",
    "        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Process all splits split-wise (stream -> clean -> write parts)\n",
    "# ----------------------------\n",
    "splits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\n",
    "\n",
    "LC_CLEAN_DIR = Path(ART_DIR) / \"lc_clean_mag\"\n",
    "LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary_rows = []\n",
    "manifest_rows = []\n",
    "\n",
    "def process_split(split_name: str, which: str):\n",
    "    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n",
    "    out_dir = LC_CLEAN_DIR / split_name / which\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    part_idx = 0\n",
    "    n_rows_total = 0\n",
    "    n_flux_neg = 0\n",
    "    n_det = 0\n",
    "    n_finite_mag = 0\n",
    "    mag_min = np.inf\n",
    "    mag_max = -np.inf\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n",
    "        cleaned = clean_chunk_to_mag(ch, ebv_ser)\n",
    "\n",
    "        n_rows = int(len(cleaned))\n",
    "        n_rows_total += n_rows\n",
    "\n",
    "        # stats\n",
    "        if KEEP_FLUX_DEBUG and \"flux_deext\" in cleaned.columns:\n",
    "            n_flux_neg += int((cleaned[\"flux_deext\"].to_numpy(dtype=np.float32) < 0).sum())\n",
    "        else:\n",
    "            # if not keeping flux, approximate neg fraction using snr sign:\n",
    "            # (snr negative implies flux negative)\n",
    "            n_flux_neg += int((cleaned[\"snr\"].to_numpy(dtype=np.float32) < 0).sum())\n",
    "\n",
    "        det_arr = cleaned[\"detected\"].to_numpy(dtype=np.int8)\n",
    "        n_det += int(det_arr.sum())\n",
    "\n",
    "        mag_arr = cleaned[\"mag\"].to_numpy(dtype=np.float32)\n",
    "        fin = np.isfinite(mag_arr)\n",
    "        n_finite_mag += int(fin.sum())\n",
    "        if fin.any():\n",
    "            mag_min = float(min(mag_min, float(np.min(mag_arr[fin]))))\n",
    "            mag_max = float(max(mag_max, float(np.max(mag_arr[fin]))))\n",
    "\n",
    "        # write\n",
    "        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n",
    "        used_fmt, final_path = write_part(cleaned, out_path, WRITE_FORMAT)\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"which\": which,\n",
    "            \"part\": part_idx,\n",
    "            \"path\": str(final_path),\n",
    "            \"rows\": n_rows,\n",
    "            \"format\": used_fmt,\n",
    "        })\n",
    "\n",
    "        part_idx += 1\n",
    "        del cleaned, ch\n",
    "        if part_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"split\": split_name,\n",
    "        \"which\": which,\n",
    "        \"parts\": part_idx,\n",
    "        \"rows\": n_rows_total,\n",
    "        \"neg_flux_frac_proxy\": (n_flux_neg / max(n_rows_total, 1)),\n",
    "        \"det_frac_snr_gt_thr\": (n_det / max(n_rows_total, 1)),\n",
    "        \"finite_mag_frac\": (n_finite_mag / max(n_rows_total, 1)),\n",
    "        \"mag_min\": (mag_min if np.isfinite(mag_min) else np.nan),\n",
    "        \"mag_max\": (mag_max if np.isfinite(mag_max) else np.nan),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | \"\n",
    "        f\"det%={100*(n_det/max(n_rows_total,1)):.2f}% | \"\n",
    "        f\"neg_proxy%={100*(n_flux_neg/max(n_rows_total,1)):.2f}% | \"\n",
    "        f\"mag_range=[{(mag_min if np.isfinite(mag_min) else np.nan):.2f}, {(mag_max if np.isfinite(mag_max) else np.nan):.2f}]\"\n",
    "    )\n",
    "\n",
    "print(\"[Stage 4] Building cleaned MAG lightcurve cache (split-wise) ...\")\n",
    "for s in splits_to_use:\n",
    "    process_split(s, \"train\")\n",
    "    process_split(s, \"test\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save manifests + summary + config\n",
    "# ----------------------------\n",
    "df_manifest = pd.DataFrame(manifest_rows)\n",
    "df_summary  = pd.DataFrame(summary_rows)\n",
    "\n",
    "manifest_path = LC_CLEAN_DIR / \"lc_clean_mag_manifest.csv\"\n",
    "summary_path  = LC_CLEAN_DIR / \"lc_clean_mag_summary.csv\"\n",
    "\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "cfg_path = LC_CLEAN_DIR / \"photometric_config_mag.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "        \"SNR_DET\": SNR_DET,\n",
    "        \"DET_SIGMA\": DET_SIGMA,\n",
    "        \"ERR_EPS\": ERR_EPS,\n",
    "        \"MIN_FLUX_POS_UJY\": MIN_FLUX_POS_UJY,\n",
    "        \"MAG_ZP\": MAG_ZP,\n",
    "        \"MAG_MIN\": MAG_MIN,\n",
    "        \"MAG_MAX\": MAG_MAX,\n",
    "        \"MAGERR_FLOOR_DET\": MAGERR_FLOOR_DET,\n",
    "        \"MAGERR_FLOOR_ND\": MAGERR_FLOOR_ND,\n",
    "        \"MAGERR_CAP\": MAGERR_CAP,\n",
    "        \"CHUNKSIZE\": CHUNKSIZE,\n",
    "        \"WRITE_FORMAT\": WRITE_FORMAT,\n",
    "        \"ONLY_SPLITS\": splits_to_use,\n",
    "        \"KEEP_FLUX_DEBUG\": KEEP_FLUX_DEBUG,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 4] Done.\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved summary : {summary_path}\")\n",
    "print(f\"- Saved config  : {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Helper for next stages\n",
    "# ----------------------------\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    m = df_manifest[(df_manifest[\"split\"] == split_name) & (df_manifest[\"which\"] == which)].sort_values(\"part\")\n",
    "    return m[\"path\"].tolist()\n",
    "\n",
    "globals().update({\n",
    "    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "    \"BAND2ID\": BAND2ID,\n",
    "    \"ID2BAND\": ID2BAND,\n",
    "    \"MAG_ZP\": MAG_ZP,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"lc_clean_mag_manifest\": df_manifest,\n",
    "    \"lc_clean_mag_summary\": df_summary,\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aceb9c4",
   "metadata": {
    "papermill": {
     "duration": 0.009764,
     "end_time": "2026-01-02T09:09:02.952893",
     "exception": false,
     "start_time": "2026-01-02T09:09:02.943129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Tokenization (Event-based Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b88186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:09:02.972723Z",
     "iopub.status.busy": "2026-01-02T09:09:02.972386Z",
     "iopub.status.idle": "2026-01-02T09:09:19.005844Z",
     "shell.execute_reply": "2026-01-02T09:09:19.004958Z"
    },
    "papermill": {
     "duration": 16.04591,
     "end_time": "2026-01-02T09:09:19.007579",
     "exception": false,
     "start_time": "2026-01-02T09:09:02.961669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 5] Building sequences: split_01/train | expected_objects=155\n",
      "[Stage 5] OK: split_01/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_01/test | expected_objects=364\n",
      "[Stage 5] OK: split_01/test built_objects=364 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_02/train | expected_objects=170\n",
      "[Stage 5] OK: split_02/train built_objects=170 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_02/test | expected_objects=414\n",
      "[Stage 5] OK: split_02/test built_objects=414 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_03/train | expected_objects=138\n",
      "[Stage 5] OK: split_03/train built_objects=138 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_03/test | expected_objects=338\n",
      "[Stage 5] OK: split_03/test built_objects=338 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_04/train | expected_objects=145\n",
      "[Stage 5] OK: split_04/train built_objects=145 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_04/test | expected_objects=332\n",
      "[Stage 5] OK: split_04/test built_objects=332 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_05/train | expected_objects=165\n",
      "[Stage 5] OK: split_05/train built_objects=165 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_05/test | expected_objects=375\n",
      "[Stage 5] OK: split_05/test built_objects=375 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_06/train | expected_objects=155\n",
      "[Stage 5] OK: split_06/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_06/test | expected_objects=374\n",
      "[Stage 5] OK: split_06/test built_objects=374 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_07/train | expected_objects=165\n",
      "[Stage 5] OK: split_07/train built_objects=165 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_07/test | expected_objects=398\n",
      "[Stage 5] OK: split_07/test built_objects=398 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_08/train | expected_objects=162\n",
      "[Stage 5] OK: split_08/train built_objects=162 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_08/test | expected_objects=387\n",
      "[Stage 5] OK: split_08/test built_objects=387 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_09/train | expected_objects=128\n",
      "[Stage 5] OK: split_09/train built_objects=128 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_09/test | expected_objects=289\n",
      "[Stage 5] OK: split_09/test built_objects=289 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_10/train | expected_objects=144\n",
      "[Stage 5] OK: split_10/train built_objects=144 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_10/test | expected_objects=331\n",
      "[Stage 5] OK: split_10/test built_objects=331 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_11/train | expected_objects=146\n",
      "[Stage 5] OK: split_11/train built_objects=146 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_11/test | expected_objects=325\n",
      "[Stage 5] OK: split_11/test built_objects=325 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_12/train | expected_objects=155\n",
      "[Stage 5] OK: split_12/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_12/test | expected_objects=353\n",
      "[Stage 5] OK: split_12/test built_objects=353 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_13/train | expected_objects=143\n",
      "[Stage 5] OK: split_13/train built_objects=143 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_13/test | expected_objects=379\n",
      "[Stage 5] OK: split_13/test built_objects=379 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_14/train | expected_objects=154\n",
      "[Stage 5] OK: split_14/train built_objects=154 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_14/test | expected_objects=351\n",
      "[Stage 5] OK: split_14/test built_objects=351 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_15/train | expected_objects=158\n",
      "[Stage 5] OK: split_15/train built_objects=158 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_15/test | expected_objects=342\n",
      "[Stage 5] OK: split_15/test built_objects=342 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_16/train | expected_objects=155\n",
      "[Stage 5] OK: split_16/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_16/test | expected_objects=354\n",
      "[Stage 5] OK: split_16/test built_objects=354 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_17/train | expected_objects=153\n",
      "[Stage 5] OK: split_17/train built_objects=153 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_17/test | expected_objects=351\n",
      "[Stage 5] OK: split_17/test built_objects=351 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_18/train | expected_objects=152\n",
      "[Stage 5] OK: split_18/train built_objects=152 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_18/test | expected_objects=345\n",
      "[Stage 5] OK: split_18/test built_objects=345 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_19/train | expected_objects=147\n",
      "[Stage 5] OK: split_19/train built_objects=147 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_19/test | expected_objects=375\n",
      "[Stage 5] OK: split_19/test built_objects=375 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_20/train | expected_objects=153\n",
      "[Stage 5] OK: split_20/train built_objects=153 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_20/test | expected_objects=358\n",
      "[Stage 5] OK: split_20/test built_objects=358 | shards=1\n",
      "\n",
      "[Stage 5] DONE\n",
      "- token_mode : mag\n",
      "- features   : ['t_rel_log', 'dt_log', 'mag', 'mag_err_log', 'snr_tanh', 'detected']\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_manifest_train.csv (rows=3,043)\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_manifest_test.csv  (rows=7,135)\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_config.json\n",
      "\n",
      "[Stage 5] Smoke test object_id=Dornhoth_anwar_melethron\n",
      "- seq_len=184 | X_shape=(184, 6) | bands_unique=[0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL\n",
    "#\n",
    "# Tujuan:\n",
    "# - Mengubah cleaned lightcurve (STAGE 4) -> token sequence per object_id\n",
    "# - 1 observasi = 1 token\n",
    "# - Simpan shard .npz per split & (train/test) + manifest slice\n",
    "#\n",
    "# Kompatibilitas:\n",
    "# - Auto-detect schema STAGE 4:\n",
    "#   (A) MAG schema (revisi ASTROMER-ready):\n",
    "#       object_id,mjd,band_id,mag,mag_err,snr,detected\n",
    "#   (B) ASINH schema (versi lama):\n",
    "#       object_id,mjd,band_id,flux_asinh,err_log1p,snr,detected\n",
    "#\n",
    "# Input:\n",
    "# - LC_CLEAN_DIR, get_clean_parts (STAGE 4)\n",
    "# - df_train_meta, df_test_meta (STAGE 2)\n",
    "# - train_ids_by_split, test_ids_by_split (STAGE 3)\n",
    "# - SPLIT_LIST, ART_DIR\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/seq_tokens/split_XX/{train|test}/shard_*.npz\n",
    "# - artifacts/seq_tokens/seq_manifest_{train|test}.csv\n",
    "# - artifacts/seq_tokens/seq_config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"LC_CLEAN_DIR\", \"get_clean_parts\",\n",
    "             \"df_train_meta\", \"df_test_meta\",\n",
    "             \"train_ids_by_split\", \"test_ids_by_split\",\n",
    "             \"SPLIT_LIST\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 dulu.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings (CPU-safe)\n",
    "# ----------------------------\n",
    "ONLY_SPLITS = None                 # None = proses semua; atau [\"split_01\",\"split_02\"] untuk debug\n",
    "COMPRESS_NPZ = False               # True lebih kecil disk tapi jauh lebih lambat di CPU\n",
    "SHARD_MAX_OBJECTS = 1500           # jumlah object per shard file\n",
    "SNR_TANH_SCALE = 10.0              # snr_tanh = tanh(snr / scale)\n",
    "TIME_CLIP_MAX_DAYS = None          # None = no clip; atau mis. 2000.0\n",
    "DROP_BAD_TIME_ROWS = True          # drop rows with NaN/inf mjd\n",
    "FALLBACK_NUM_BUCKETS = 64          # dipakai jika fallback (hash bucket) diperlukan\n",
    "\n",
    "SEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Token mode auto-detect (ditentukan saat baca part pertama)\n",
    "TOKEN_MODE = None  # \"mag\" atau \"asinh\"\n",
    "\n",
    "# Feature spec akan di-set setelah mode terdeteksi\n",
    "FEATURE_NAMES = None\n",
    "FEATURE_DIM = None\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Robust readers for cleaned parts (parquet or csv.gz)\n",
    "# ----------------------------\n",
    "BASE_COLS = {\"object_id\",\"mjd\",\"band_id\",\"snr\",\"detected\"}\n",
    "MODE_COLS = {\n",
    "    \"mag\": {\"mag\",\"mag_err\"},\n",
    "    \"asinh\": {\"flux_asinh\",\"err_log1p\"},\n",
    "}\n",
    "\n",
    "def _read_clean_part(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Clean part missing: {p}\")\n",
    "\n",
    "    if p.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    elif p.name.endswith(\".csv.gz\"):\n",
    "        df = pd.read_csv(p, compression=\"gzip\")\n",
    "    else:\n",
    "        df = pd.read_csv(p)\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # detect mode once (global)\n",
    "    global TOKEN_MODE, FEATURE_NAMES, FEATURE_DIM\n",
    "\n",
    "    if TOKEN_MODE is None:\n",
    "        cols = set(df.columns)\n",
    "        if BASE_COLS.issubset(cols) and MODE_COLS[\"mag\"].issubset(cols):\n",
    "            TOKEN_MODE = \"mag\"\n",
    "            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"mag\", \"mag_err_log\", \"snr_tanh\", \"detected\"]\n",
    "        elif BASE_COLS.issubset(cols) and MODE_COLS[\"asinh\"].issubset(cols):\n",
    "            TOKEN_MODE = \"asinh\"\n",
    "            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Cannot detect cleaned schema.\\n\"\n",
    "                f\"Found columns: {list(df.columns)}\\n\"\n",
    "                \"Expected either:\\n\"\n",
    "                \"- MAG: object_id,mjd,band_id,mag,mag_err,snr,detected\\n\"\n",
    "                \"- ASINH: object_id,mjd,band_id,flux_asinh,err_log1p,snr,detected\"\n",
    "            )\n",
    "        FEATURE_DIM = len(FEATURE_NAMES)\n",
    "\n",
    "    # validate required cols for detected mode\n",
    "    req = set(BASE_COLS) | set(MODE_COLS[TOKEN_MODE])\n",
    "    missing = sorted(list(req - set(df.columns)))\n",
    "    if missing:\n",
    "        raise ValueError(f\"Clean part missing columns {missing}. Found: {list(df.columns)} | file={p}\")\n",
    "\n",
    "    # enforce dtypes lightly\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n",
    "\n",
    "    df[\"snr\"] = pd.to_numeric(df[\"snr\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n",
    "\n",
    "    if TOKEN_MODE == \"mag\":\n",
    "        df[\"mag\"] = pd.to_numeric(df[\"mag\"], errors=\"coerce\").astype(np.float32)\n",
    "        df[\"mag_err\"] = pd.to_numeric(df[\"mag_err\"], errors=\"coerce\").astype(np.float32)\n",
    "    else:\n",
    "        df[\"flux_asinh\"] = pd.to_numeric(df[\"flux_asinh\"], errors=\"coerce\").astype(np.float32)\n",
    "        df[\"err_log1p\"]  = pd.to_numeric(df[\"err_log1p\"], errors=\"coerce\").astype(np.float32)\n",
    "\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        df = df[np.isfinite(df[\"mjd\"].to_numpy())]\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build tokens for one object (sort by time inside object)\n",
    "# ----------------------------\n",
    "def build_object_tokens(df_obj: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X: (L, FEATURE_DIM) float32\n",
    "      B: (L,) int8 band_id\n",
    "    \"\"\"\n",
    "    if df_obj.empty:\n",
    "        return None, None\n",
    "\n",
    "    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n",
    "    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "    # sort by mjd, tie-break by band\n",
    "    order = np.lexsort((band, mjd))\n",
    "    mjd = mjd[order]\n",
    "    band = band[order]\n",
    "    snr  = snr[order]\n",
    "    det  = det[order]\n",
    "\n",
    "    # time features\n",
    "    t0 = mjd[0]\n",
    "    t_rel = mjd - t0\n",
    "    dt = np.empty_like(t_rel)\n",
    "    dt[0] = 0.0\n",
    "    if len(t_rel) > 1:\n",
    "        dt[1:] = np.maximum(mjd[1:] - mjd[:-1], 0.0)\n",
    "\n",
    "    if TIME_CLIP_MAX_DAYS is not None:\n",
    "        t_rel = np.clip(t_rel, 0.0, float(TIME_CLIP_MAX_DAYS))\n",
    "        dt    = np.clip(dt,    0.0, float(TIME_CLIP_MAX_DAYS))\n",
    "\n",
    "    t_rel_log = np.log1p(t_rel).astype(np.float32)\n",
    "    dt_log    = np.log1p(dt).astype(np.float32)\n",
    "\n",
    "    # snr -> tanh\n",
    "    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n",
    "\n",
    "    det_f = det.astype(np.float32)\n",
    "\n",
    "    # value channels (mode-specific)\n",
    "    if TOKEN_MODE == \"mag\":\n",
    "        mag = df_obj[\"mag\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        mag_err = df_obj[\"mag_err\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "\n",
    "        mag = np.nan_to_num(mag, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        mag_err = np.nan_to_num(mag_err, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        mag_err = np.maximum(mag_err, np.float32(0.0))\n",
    "        mag_err_log = np.log1p(mag_err).astype(np.float32)\n",
    "\n",
    "        X = np.stack([t_rel_log, dt_log, mag, mag_err_log, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "    else:\n",
    "        flux = df_obj[\"flux_asinh\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        elog = df_obj[\"err_log1p\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        flux = np.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        elog = np.nan_to_num(elog, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "        X = np.stack([t_rel_log, dt_log, flux, elog, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "\n",
    "    B = band.astype(np.int8)\n",
    "    return X, B\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Shard writer\n",
    "# ----------------------------\n",
    "def save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    obj_arr = np.asarray(object_ids, dtype=\"S\")  # bytes in npz\n",
    "    if COMPRESS_NPZ:\n",
    "        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "    else:\n",
    "        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Streaming builder (assumes contiguous object blocks in file order; auto-detect fallback)\n",
    "# ----------------------------\n",
    "def build_sequences_streaming(split_name: str, which: str, expected_ids: set, out_dir: Path):\n",
    "    parts = get_clean_parts(split_name, which)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"No cleaned parts for {split_name}/{which}. Pastikan STAGE 4 sukses.\")\n",
    "\n",
    "    manifest_rows = []\n",
    "    shard_idx = 0\n",
    "    batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n",
    "\n",
    "    cur_oid = None\n",
    "    cur_buf = []\n",
    "    seen_done = set()\n",
    "    fallback_needed = False\n",
    "\n",
    "    def flush_object(oid, buf_blocks):\n",
    "        nonlocal batch_obj_ids, batch_X_list, batch_B_list, batch_lengths\n",
    "        if oid is None or not buf_blocks:\n",
    "            return\n",
    "        if oid not in expected_ids:\n",
    "            return\n",
    "        df_obj = pd.concat(buf_blocks, ignore_index=True)\n",
    "        X, B = build_object_tokens(df_obj)\n",
    "        if X is None:\n",
    "            return\n",
    "        batch_obj_ids.append(oid)\n",
    "        batch_X_list.append(X)\n",
    "        batch_B_list.append(B)\n",
    "        batch_lengths.append(X.shape[0])\n",
    "\n",
    "    def flush_shard():\n",
    "        nonlocal shard_idx, batch_obj_ids, batch_X_list, batch_B_list, batch_lengths, manifest_rows\n",
    "        if not batch_obj_ids:\n",
    "            return\n",
    "        lengths = np.asarray(batch_lengths, dtype=np.int64)\n",
    "        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(lengths)\n",
    "        X_concat = np.concatenate(batch_X_list, axis=0).astype(np.float32)\n",
    "        B_concat = np.concatenate(batch_B_list, axis=0).astype(np.int8)\n",
    "\n",
    "        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n",
    "        save_shard(shard_path, batch_obj_ids, X_concat, B_concat, offsets)\n",
    "\n",
    "        for i, oid in enumerate(batch_obj_ids):\n",
    "            manifest_rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(shard_path),\n",
    "                \"start\": int(offsets[i]),\n",
    "                \"length\": int(lengths[i]),\n",
    "            })\n",
    "\n",
    "        shard_idx += 1\n",
    "        batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n",
    "        gc.collect()\n",
    "\n",
    "    for pi, p in enumerate(parts):\n",
    "        df = _read_clean_part(p)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        oids = df[\"object_id\"].to_numpy(dtype=object, copy=False)\n",
    "\n",
    "        # segment boundaries where object_id changes\n",
    "        change = np.empty(len(oids), dtype=bool)\n",
    "        change[0] = True\n",
    "        change[1:] = oids[1:] != oids[:-1]\n",
    "        seg_starts = np.flatnonzero(change)\n",
    "        seg_ends = np.append(seg_starts[1:], len(oids))\n",
    "\n",
    "        for s_idx, e_idx in zip(seg_starts, seg_ends):\n",
    "            oid = str(oids[s_idx])\n",
    "            block = df.iloc[s_idx:e_idx]\n",
    "\n",
    "            if (oid in seen_done) and (oid != cur_oid):\n",
    "                fallback_needed = True\n",
    "                break\n",
    "\n",
    "            if cur_oid is None:\n",
    "                cur_oid = oid\n",
    "                cur_buf = [block]\n",
    "            elif oid == cur_oid:\n",
    "                cur_buf.append(block)\n",
    "            else:\n",
    "                flush_object(cur_oid, cur_buf)\n",
    "                seen_done.add(cur_oid)\n",
    "\n",
    "                if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                    flush_shard()\n",
    "\n",
    "                cur_oid = oid\n",
    "                cur_buf = [block]\n",
    "\n",
    "        del df\n",
    "        if fallback_needed:\n",
    "            break\n",
    "        if (pi + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    if not fallback_needed:\n",
    "        flush_object(cur_oid, cur_buf)\n",
    "        if cur_oid is not None:\n",
    "            seen_done.add(cur_oid)\n",
    "        flush_shard()\n",
    "\n",
    "    built = len(seen_done.intersection(expected_ids))\n",
    "    return manifest_rows, built, fallback_needed\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Fallback: Hash-bucket builder (robust if not contiguous)\n",
    "# - Uses pyarrow if available (fast). If not, hard fail with clear message.\n",
    "# ----------------------------\n",
    "def build_sequences_fallback_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 64):\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Fallback bucketization membutuhkan pyarrow (biasanya ada di Kaggle). \"\n",
    "            \"Jika environment kamu tidak punya pyarrow, jalankan lagi di Kaggle atau pastikan parquet engine tersedia.\"\n",
    "        ) from e\n",
    "\n",
    "    parts = get_clean_parts(split_name, which)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"No cleaned parts for {split_name}/{which}.\")\n",
    "\n",
    "    tmp_dir = Path(ART_DIR) / \"tmp_buckets\" / split_name / which\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    writers = {}\n",
    "\n",
    "    def bucket_idx(series_objid: pd.Series) -> np.ndarray:\n",
    "        h = pd.util.hash_pandas_object(series_objid, index=False).to_numpy(dtype=np.uint64, copy=False)\n",
    "        return (h % np.uint64(num_buckets)).astype(np.int16)\n",
    "\n",
    "    # write bucket parquet incrementally\n",
    "    for p in parts:\n",
    "        df = _read_clean_part(p)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df = df[df[\"object_id\"].isin(expected_ids)]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        bidx = bucket_idx(df[\"object_id\"])\n",
    "        df[\"_b\"] = bidx\n",
    "\n",
    "        for b in np.unique(bidx):\n",
    "            sub = df[df[\"_b\"] == b].drop(columns=[\"_b\"])\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            file_path = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n",
    "            table = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "            if int(b) not in writers:\n",
    "                writers[int(b)] = pq.ParquetWriter(file_path, table.schema, compression=\"snappy\")\n",
    "            writers[int(b)].write_table(table)\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()\n",
    "\n",
    "    # process bucket files\n",
    "    manifest_rows = []\n",
    "    shard_idx = 0\n",
    "    batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n",
    "    built_ids = set()\n",
    "\n",
    "    def flush_shard_local():\n",
    "        nonlocal shard_idx, batch_obj_ids, batch_X_list, batch_B_list, batch_lengths, manifest_rows\n",
    "        if not batch_obj_ids:\n",
    "            return\n",
    "        lengths = np.asarray(batch_lengths, dtype=np.int64)\n",
    "        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(lengths)\n",
    "        X_concat = np.concatenate(batch_X_list, axis=0).astype(np.float32)\n",
    "        B_concat = np.concatenate(batch_B_list, axis=0).astype(np.int8)\n",
    "\n",
    "        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n",
    "        save_shard(shard_path, batch_obj_ids, X_concat, B_concat, offsets)\n",
    "\n",
    "        for i, oid in enumerate(batch_obj_ids):\n",
    "            manifest_rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(shard_path),\n",
    "                \"start\": int(offsets[i]),\n",
    "                \"length\": int(lengths[i]),\n",
    "            })\n",
    "\n",
    "        shard_idx += 1\n",
    "        batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n",
    "        gc.collect()\n",
    "\n",
    "    for bf in sorted(tmp_dir.glob(\"bucket_*.parquet\")):\n",
    "        dfb = pd.read_parquet(bf)\n",
    "        dfb.columns = [c.strip() for c in dfb.columns]\n",
    "        if dfb.empty:\n",
    "            bf.unlink(missing_ok=True)\n",
    "            continue\n",
    "\n",
    "        for oid, g in dfb.groupby(\"object_id\", sort=False):\n",
    "            oid = str(oid)\n",
    "            if oid in built_ids:\n",
    "                continue\n",
    "            X, B = build_object_tokens(g)\n",
    "            if X is None:\n",
    "                continue\n",
    "\n",
    "            batch_obj_ids.append(oid)\n",
    "            batch_X_list.append(X)\n",
    "            batch_B_list.append(B)\n",
    "            batch_lengths.append(X.shape[0])\n",
    "            built_ids.add(oid)\n",
    "\n",
    "            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                flush_shard_local()\n",
    "\n",
    "        bf.unlink(missing_ok=True)\n",
    "        del dfb\n",
    "        gc.collect()\n",
    "\n",
    "    flush_shard_local()\n",
    "\n",
    "    # cleanup tmp dir\n",
    "    try:\n",
    "        tmp_dir.rmdir()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return manifest_rows, len(built_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Run tokenization for all splits (train & test)\n",
    "# ----------------------------\n",
    "splits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\n",
    "\n",
    "all_manifest_train = []\n",
    "all_manifest_test  = []\n",
    "\n",
    "def expected_set_for(split_name: str, which: str) -> set:\n",
    "    return set(train_ids_by_split[split_name]) if which == \"train\" else set(test_ids_by_split[split_name])\n",
    "\n",
    "for split_name in splits_to_run:\n",
    "    for which in [\"train\", \"test\"]:\n",
    "        out_dir = SEQ_DIR / split_name / which\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        expected_ids = expected_set_for(split_name, which)\n",
    "        if len(expected_ids) == 0:\n",
    "            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}. Cek log/split mapping.\")\n",
    "\n",
    "        print(f\"\\n[Stage 5] Building sequences: {split_name}/{which} | expected_objects={len(expected_ids):,}\")\n",
    "\n",
    "        manifest_rows, built, fallback_needed = build_sequences_streaming(\n",
    "            split_name=split_name,\n",
    "            which=which,\n",
    "            expected_ids=expected_ids,\n",
    "            out_dir=out_dir\n",
    "        )\n",
    "\n",
    "        if fallback_needed or built != len(expected_ids):\n",
    "            print(f\"[Stage 5] Streaming not safe for {split_name}/{which} \"\n",
    "                  f\"(built={built:,} vs expected={len(expected_ids):,}, fallback_needed={fallback_needed}).\")\n",
    "            print(\"[Stage 5] Switching to robust bucket fallback (temporary buckets, then cleaned).\")\n",
    "\n",
    "            # clear partial outputs\n",
    "            for f in out_dir.glob(\"shard_*.npz\"):\n",
    "                try:\n",
    "                    f.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            manifest_rows, built2 = build_sequences_fallback_bucket(\n",
    "                split_name=split_name,\n",
    "                which=which,\n",
    "                expected_ids=expected_ids,\n",
    "                out_dir=out_dir,\n",
    "                num_buckets=FALLBACK_NUM_BUCKETS\n",
    "            )\n",
    "            if built2 != len(expected_ids):\n",
    "                raise RuntimeError(f\"Fallback mismatch for {split_name}/{which}: built={built2:,} expected={len(expected_ids):,}\")\n",
    "            built = built2\n",
    "\n",
    "        print(f\"[Stage 5] OK: {split_name}/{which} built_objects={built:,} | shards={len(list(out_dir.glob('shard_*.npz'))):,}\")\n",
    "\n",
    "        if which == \"train\":\n",
    "            all_manifest_train.extend(manifest_rows)\n",
    "        else:\n",
    "            all_manifest_test.extend(manifest_rows)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save manifests + config\n",
    "# ----------------------------\n",
    "df_m_train = pd.DataFrame(all_manifest_train).sort_values([\"split\",\"shard\",\"start\"]).reset_index(drop=True)\n",
    "df_m_test  = pd.DataFrame(all_manifest_test).sort_values([\"split\",\"shard\",\"start\"]).reset_index(drop=True)\n",
    "\n",
    "mtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\n",
    "mtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\n",
    "df_m_train.to_csv(mtrain_path, index=False)\n",
    "df_m_test.to_csv(mtest_path, index=False)\n",
    "\n",
    "cfg = {\n",
    "    \"token_mode\": TOKEN_MODE,\n",
    "    \"feature_names\": FEATURE_NAMES,\n",
    "    \"feature_dim\": int(FEATURE_DIM),\n",
    "    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n",
    "    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n",
    "    \"compress_npz\": bool(COMPRESS_NPZ),\n",
    "    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n",
    "    \"fallback_num_buckets\": int(FALLBACK_NUM_BUCKETS),\n",
    "}\n",
    "cfg_path = SEQ_DIR / \"seq_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 5] DONE\")\n",
    "print(f\"- token_mode : {TOKEN_MODE}\")\n",
    "print(f\"- features   : {FEATURE_NAMES}\")\n",
    "print(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\n",
    "print(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Smoke test: load one object sequence\n",
    "# ----------------------------\n",
    "def load_sequence(object_id: str, which: str):\n",
    "    object_id = str(object_id).strip()\n",
    "    m = df_m_train if which == \"train\" else df_m_test\n",
    "    row = m[m[\"object_id\"] == object_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n",
    "    r = row.iloc[0]\n",
    "    data = np.load(r[\"shard\"], allow_pickle=False)\n",
    "    start = int(r[\"start\"])\n",
    "    length = int(r[\"length\"])\n",
    "    X = data[\"x\"][start:start+length]\n",
    "    B = data[\"band\"][start:start+length]\n",
    "    return X, B\n",
    "\n",
    "_smoke_oid = str(df_train_meta.index[0])\n",
    "X_sm, B_sm = load_sequence(_smoke_oid, \"train\")\n",
    "print(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\n",
    "print(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n",
    "\n",
    "# Export globals for next stages\n",
    "globals().update({\n",
    "    \"SEQ_DIR\": SEQ_DIR,\n",
    "    \"seq_manifest_train\": df_m_train,\n",
    "    \"seq_manifest_test\": df_m_test,\n",
    "    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n",
    "    \"SEQ_FEATURE_DIM\": int(FEATURE_DIM),\n",
    "    \"SEQ_TOKEN_MODE\": TOKEN_MODE,\n",
    "    \"load_sequence\": load_sequence,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24454ea4",
   "metadata": {
    "papermill": {
     "duration": 0.010991,
     "end_time": "2026-01-02T09:09:19.030319",
     "exception": false,
     "start_time": "2026-01-02T09:09:19.019328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Length Policy (Padding, Truncation, Windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e155cd46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:09:19.053945Z",
     "iopub.status.busy": "2026-01-02T09:09:19.053591Z",
     "iopub.status.idle": "2026-01-02T09:09:20.562628Z",
     "shell.execute_reply": "2026-01-02T09:09:20.561856Z"
    },
    "papermill": {
     "duration": 1.523292,
     "end_time": "2026-01-02T09:09:20.564380",
     "exception": false,
     "start_time": "2026-01-02T09:09:19.041088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 6] Detected token_mode = mag | score_value_feat = mag\n",
      "\n",
      "TRAIN length stats\n",
      "- n_objects=3,043 | min=17 | p50=150 | p90=183 | p95=194 | p99=908 | max=1164\n",
      "\n",
      "TEST length stats\n",
      "- n_objects=7,135 | min=18 | p50=152 | p90=183 | p95=193 | p99=990 | max=1186\n",
      "\n",
      "[Stage 6] Chosen MAX_LEN = 256 (based on p95=194)\n",
      "\n",
      "[Stage 6] Building fixed-length cache (TRAIN)...\n",
      "[Stage 6] TRAIN filled: 3,043/3,043 | duplicates_skipped=0\n",
      "\n",
      "[Stage 6] Building fixed-length cache (TEST)...\n",
      "[Stage 6] TEST  filled: 7,135/7,135 | duplicates_skipped=0\n",
      "\n",
      "[Stage 6] Sanity samples (train):\n",
      "- idx=1360 oid=gwilwileth_adel_amloth orig_len=157 kept=157 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3020 oid=vin_araf_gwador orig_len=151 kept=151 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3025 oid=ylf_alph_mindon orig_len=167 kept=167 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] Sanity samples (test):\n",
      "- idx=3191 oid=rom_bellas_lebdas orig_len=142 kept=142 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7082 oid=nim_nestad_thor orig_len=161 kept=161 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7094 oid=rach_bellas_dol orig_len=156 kept=156 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] DONE\n",
      "- Saved fixed cache dir: /kaggle/working/mallorn_run/artifacts/fixed_seq\n",
      "- Saved config: /kaggle/working/mallorn_run/artifacts/fixed_seq/length_policy_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n",
    "# ONE CELL, Kaggle CPU-SAFE, nyambung dengan STAGE 0..5 — REVISI FULL (MAG/ASINH COMPAT)\n",
    "#\n",
    "# Upgrade utama vs versi kamu:\n",
    "# - Kompatibel dengan STAGE 5 (AUTO token_mode):\n",
    "#     * mode=\"asinh\":  feature ada flux_asinh, err_log1p\n",
    "#     * mode=\"mag\"  :  feature ada mag, mag_err_log\n",
    "# - Windowing score adaptif:\n",
    "#     * asinh: pakai |snr_tanh| + |flux_asinh| + detected\n",
    "#     * mag  : pakai |snr_tanh| + brightness_proxy(median_mag - mag) + detected\n",
    "# - Strict sanity: deteksi missing fill + duplikasi object_id saat isi memmap\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/fixed_seq/{train|test}_{X|B|M}.dat  (memmap)\n",
    "# - artifacts/fixed_seq/{train|test}_ids.npy\n",
    "# - artifacts/fixed_seq/train_y.npy\n",
    "# - artifacts/fixed_seq/{train|test}_origlen.npy, {train|test}_winstart.npy\n",
    "# - artifacts/fixed_seq/length_policy_config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, math, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"seq_manifest_train\", \"seq_manifest_test\", \"SEQ_FEATURE_NAMES\",\n",
    "             \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 -> 5 dulu.\")\n",
    "\n",
    "m_train = seq_manifest_train.copy()\n",
    "m_test  = seq_manifest_test.copy()\n",
    "\n",
    "SEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\n",
    "feat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Detect token_mode (MAG vs ASINH)\n",
    "# ----------------------------\n",
    "SEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    if (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"asinh\"\n",
    "    elif (\"mag\" in feat) and (\"mag_err_log\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"mag\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Cannot infer SEQ_TOKEN_MODE from SEQ_FEATURE_NAMES.\\n\"\n",
    "            f\"SEQ_FEATURE_NAMES={SEQ_FEATURE_NAMES}\\n\"\n",
    "            \"Expected either (flux_asinh, err_log1p) or (mag, mag_err_log).\"\n",
    "        )\n",
    "\n",
    "# Required common features\n",
    "REQ_COMMON = [\"t_rel_log\", \"dt_log\", \"snr_tanh\", \"detected\"]\n",
    "for k in REQ_COMMON:\n",
    "    if k not in feat:\n",
    "        raise ValueError(f\"SEQ_FEATURE_NAMES must include '{k}'. Found: {SEQ_FEATURE_NAMES}\")\n",
    "\n",
    "# Required mode-specific features (for scoring + sanity)\n",
    "if SEQ_TOKEN_MODE == \"asinh\":\n",
    "    if \"flux_asinh\" not in feat:\n",
    "        raise ValueError(f\"token_mode=asinh requires 'flux_asinh'. Found: {SEQ_FEATURE_NAMES}\")\n",
    "    SCORE_VALUE_FEAT = \"flux_asinh\"\n",
    "elif SEQ_TOKEN_MODE == \"mag\":\n",
    "    if \"mag\" not in feat:\n",
    "        raise ValueError(f\"token_mode=mag requires 'mag'. Found: {SEQ_FEATURE_NAMES}\")\n",
    "    SCORE_VALUE_FEAT = \"mag\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SEQ_TOKEN_MODE={SEQ_TOKEN_MODE}\")\n",
    "\n",
    "print(f\"[Stage 6] Detected token_mode = {SEQ_TOKEN_MODE} | score_value_feat = {SCORE_VALUE_FEAT}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Inspect length distribution -> choose MAX_LEN (CPU-friendly)\n",
    "# ----------------------------\n",
    "def describe_lengths(m: pd.DataFrame, name: str):\n",
    "    L = m[\"length\"].to_numpy(dtype=np.int32, copy=False)\n",
    "    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n",
    "    print(f\"\\n{name} length stats\")\n",
    "    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[7])} | p95={int(q[8])} | p99={int(q[10])} | max={int(q[-1])}\")\n",
    "    return q\n",
    "\n",
    "q_tr = describe_lengths(m_train, \"TRAIN\")\n",
    "q_te = describe_lengths(m_test,  \"TEST\")\n",
    "\n",
    "p95 = int(max(q_tr[8], q_te[8]))\n",
    "\n",
    "# CPU-safe caps:\n",
    "# - p95 <= 256 => 256\n",
    "# - <= 384 => 384\n",
    "# - else => 512\n",
    "if p95 <= 256:\n",
    "    MAX_LEN = 256\n",
    "elif p95 <= 384:\n",
    "    MAX_LEN = 384\n",
    "else:\n",
    "    MAX_LEN = 512\n",
    "\n",
    "FORCE_MAX_LEN = None  # e.g. 256\n",
    "if FORCE_MAX_LEN is not None:\n",
    "    MAX_LEN = int(FORCE_MAX_LEN)\n",
    "\n",
    "print(f\"\\n[Stage 6] Chosen MAX_LEN = {MAX_LEN} (based on p95={p95})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Windowing / truncation policy (adaptive)\n",
    "# ----------------------------\n",
    "# Score components:\n",
    "# - always: |snr_tanh| + detected\n",
    "# - mode-specific:\n",
    "#   asinh: + |flux_asinh|\n",
    "#   mag  : + brightness_proxy = relu(median_mag - mag)\n",
    "W_SNR = 1.0\n",
    "W_VAL = 0.35\n",
    "W_DET = 0.25\n",
    "\n",
    "def _brightness_proxy_from_mag(mag: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Higher = brighter peak. Use median-mag, clipped at 0.\"\"\"\n",
    "    mag = np.nan_to_num(mag, nan=np.float32(0.0), posinf=np.float32(0.0), neginf=np.float32(0.0)).astype(np.float32)\n",
    "    med = np.float32(np.median(mag)) if mag.size > 0 else np.float32(0.0)\n",
    "    br = (med - mag).astype(np.float32)\n",
    "    br = np.maximum(br, np.float32(0.0))\n",
    "    # compress dynamic range a bit (optional but stabilizes)\n",
    "    br = np.log1p(br).astype(np.float32)\n",
    "    return br\n",
    "\n",
    "def select_window(X: np.ndarray, max_len: int) -> tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Returns (start, end, center) for window selection.\n",
    "    X shape (L, F).\n",
    "    \"\"\"\n",
    "    L = int(X.shape[0])\n",
    "    if L <= max_len:\n",
    "        return 0, L, 0\n",
    "\n",
    "    snr = np.abs(X[:, feat[\"snr_tanh\"]]).astype(np.float32, copy=False)\n",
    "    det = X[:, feat[\"detected\"]].astype(np.float32, copy=False)\n",
    "\n",
    "    if SEQ_TOKEN_MODE == \"asinh\":\n",
    "        val = np.abs(X[:, feat[\"flux_asinh\"]]).astype(np.float32, copy=False)\n",
    "    else:\n",
    "        mag = X[:, feat[\"mag\"]].astype(np.float32, copy=False)\n",
    "        val = _brightness_proxy_from_mag(mag)\n",
    "\n",
    "    score = (np.float32(W_SNR) * snr) + (np.float32(W_VAL) * val) + (np.float32(W_DET) * det)\n",
    "\n",
    "    if (not np.isfinite(score).any()) or (float(np.nanmax(score)) <= 0.0):\n",
    "        center = L // 2\n",
    "    else:\n",
    "        center = int(np.nanargmax(score))\n",
    "\n",
    "    half = max_len // 2\n",
    "    start = max(0, center - half)\n",
    "    start = min(start, L - max_len)\n",
    "    end = start + max_len\n",
    "    return start, end, center\n",
    "\n",
    "def pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      Xp: (max_len, F) float32\n",
    "      Bp: (max_len,) int8\n",
    "      Mp: (max_len,) int8  (1=real token)\n",
    "      orig_len, win_start, win_end\n",
    "    \"\"\"\n",
    "    L = int(X.shape[0])\n",
    "    if L <= 0:\n",
    "        Xp = np.zeros((max_len, X.shape[1]), dtype=np.float32)\n",
    "        Bp = np.zeros((max_len,), dtype=np.int8)\n",
    "        Mp = np.zeros((max_len,), dtype=np.int8)\n",
    "        return Xp, Bp, Mp, 0, 0, 0\n",
    "\n",
    "    s, e, _ = select_window(X, max_len=max_len)\n",
    "    Xw = X[s:e]\n",
    "    Bw = B[s:e]\n",
    "    lw = int(Xw.shape[0])\n",
    "\n",
    "    Xp = np.zeros((max_len, X.shape[1]), dtype=np.float32)\n",
    "    Bp = np.zeros((max_len,), dtype=np.int8)\n",
    "    Mp = np.zeros((max_len,), dtype=np.int8)\n",
    "\n",
    "    Xp[:lw] = Xw.astype(np.float32, copy=False)\n",
    "    Bp[:lw] = Bw.astype(np.int8, copy=False)\n",
    "    Mp[:lw] = 1\n",
    "    return Xp, Bp, Mp, L, s, e\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Fixed cache builder (efficient: process per shard)\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(ART_DIR) / \"fixed_seq\"\n",
    "FIX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ordering\n",
    "train_ids = df_train_meta.index.to_list()\n",
    "y_train = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in df_sub.columns:\n",
    "    test_ids = df_sub[\"object_id\"].astype(str).str.strip().to_list()\n",
    "else:\n",
    "    test_ids = df_test_meta.index.to_list()\n",
    "\n",
    "train_row = {oid: i for i, oid in enumerate(train_ids)}\n",
    "test_row  = {oid: i for i, oid in enumerate(test_ids)}\n",
    "\n",
    "NTR = len(train_ids)\n",
    "NTE = len(test_ids)\n",
    "F = len(SEQ_FEATURE_NAMES)\n",
    "\n",
    "# memmap paths\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "test_X_path  = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path  = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path  = FIX_DIR / \"test_M.dat\"\n",
    "\n",
    "# metadata arrays\n",
    "train_len_path = FIX_DIR / \"train_origlen.npy\"\n",
    "train_win_path = FIX_DIR / \"train_winstart.npy\"\n",
    "test_len_path  = FIX_DIR / \"test_origlen.npy\"\n",
    "test_win_path  = FIX_DIR / \"test_winstart.npy\"\n",
    "\n",
    "# create memmaps\n",
    "Xtr = np.memmap(train_X_path, dtype=np.float32, mode=\"w+\", shape=(NTR, MAX_LEN, F))\n",
    "Btr = np.memmap(train_B_path, dtype=np.int8,   mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "Mtr = np.memmap(train_M_path, dtype=np.int8,   mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=np.float32, mode=\"w+\", shape=(NTE, MAX_LEN, F))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,   mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,   mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "\n",
    "origlen_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "winstart_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "origlen_te = np.zeros((NTE,), dtype=np.int32)\n",
    "winstart_te = np.zeros((NTE,), dtype=np.int32)\n",
    "\n",
    "filled_tr_mask = np.zeros((NTR,), dtype=np.uint8)\n",
    "filled_te_mask = np.zeros((NTE,), dtype=np.uint8)\n",
    "\n",
    "def process_manifest_into_memmap(m: pd.DataFrame, which: str):\n",
    "    \"\"\"\n",
    "    Read each shard once, then fill memmaps following the chosen ordering.\n",
    "    Adds strict checks for duplicates/missing.\n",
    "    \"\"\"\n",
    "    if which == \"train\":\n",
    "        row_map = train_row\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        origlen, winstart = origlen_tr, winstart_tr\n",
    "        filled_mask = filled_tr_mask\n",
    "        expected_n = NTR\n",
    "    else:\n",
    "        row_map = test_row\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        origlen, winstart = origlen_te, winstart_te\n",
    "        filled_mask = filled_te_mask\n",
    "        expected_n = NTE\n",
    "\n",
    "    filled = 0\n",
    "    dup = 0\n",
    "\n",
    "    for shard_path, g in m.groupby(\"shard\", sort=False):\n",
    "        shard_path = str(shard_path)\n",
    "        data = np.load(shard_path, allow_pickle=False)\n",
    "        x_all = data[\"x\"]\n",
    "        b_all = data[\"band\"]\n",
    "\n",
    "        for _, r in g.iterrows():\n",
    "            oid = str(r[\"object_id\"])\n",
    "            idx = row_map.get(oid, None)\n",
    "            if idx is None:\n",
    "                continue\n",
    "\n",
    "            if filled_mask[idx]:\n",
    "                dup += 1\n",
    "                continue\n",
    "\n",
    "            start = int(r[\"start\"])\n",
    "            length = int(r[\"length\"])\n",
    "            if length <= 0:\n",
    "                continue\n",
    "\n",
    "            X = x_all[start:start+length]\n",
    "            B = b_all[start:start+length]\n",
    "\n",
    "            Xp, Bp, Mp, L0, ws, _ = pad_to_fixed(X, B, max_len=MAX_LEN)\n",
    "\n",
    "            Xmm[idx, :, :] = Xp\n",
    "            Bmm[idx, :] = Bp\n",
    "            Mmm[idx, :] = Mp\n",
    "            origlen[idx] = int(L0)\n",
    "            winstart[idx] = int(ws)\n",
    "            filled_mask[idx] = 1\n",
    "            filled += 1\n",
    "\n",
    "        del data\n",
    "        if filled % 2000 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    return filled, dup, expected_n\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed-length cache (TRAIN)...\")\n",
    "filled_tr, dup_tr, exp_tr = process_manifest_into_memmap(m_train, \"train\")\n",
    "print(f\"[Stage 6] TRAIN filled: {filled_tr:,}/{exp_tr:,} | duplicates_skipped={dup_tr:,}\")\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed-length cache (TEST)...\")\n",
    "filled_te, dup_te, exp_te = process_manifest_into_memmap(m_test, \"test\")\n",
    "print(f\"[Stage 6] TEST  filled: {filled_te:,}/{exp_te:,} | duplicates_skipped={dup_te:,}\")\n",
    "\n",
    "# flush memmaps\n",
    "Xtr.flush(); Btr.flush(); Mtr.flush()\n",
    "Xte.flush(); Bte.flush(); Mte.flush()\n",
    "\n",
    "# save ids + y + meta\n",
    "np.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"train_y.npy\",   y_train)\n",
    "\n",
    "np.save(train_len_path, origlen_tr)\n",
    "np.save(train_win_path, winstart_tr)\n",
    "np.save(test_len_path,  origlen_te)\n",
    "np.save(test_win_path,  winstart_te)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Sanity checks (hard)\n",
    "# ----------------------------\n",
    "def _check_missing(which: str):\n",
    "    if which == \"train\":\n",
    "        miss = np.where(filled_tr_mask == 0)[0]\n",
    "        ids = train_ids\n",
    "    else:\n",
    "        miss = np.where(filled_te_mask == 0)[0]\n",
    "        ids = test_ids\n",
    "\n",
    "    if len(miss) > 0:\n",
    "        sample = [ids[i] for i in miss[:10]]\n",
    "        raise RuntimeError(\n",
    "            f\"[Stage 6] Missing filled rows for {which}: {len(miss):,} / {len(ids):,}. \"\n",
    "            f\"Examples: {sample}\"\n",
    "        )\n",
    "\n",
    "_check_missing(\"train\")\n",
    "_check_missing(\"test\")\n",
    "\n",
    "def sanity_samples(which: str, n_show: int = 3):\n",
    "    rng = np.random.default_rng(2025)\n",
    "    if which == \"train\":\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        ids = train_ids\n",
    "        ol = origlen_tr\n",
    "    else:\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        ids = test_ids\n",
    "        ol = origlen_te\n",
    "\n",
    "    idxs = rng.choice(len(ids), size=min(n_show, len(ids)), replace=False)\n",
    "    print(f\"\\n[Stage 6] Sanity samples ({which}):\")\n",
    "    for i in idxs:\n",
    "        kept = int(Mmm[i].sum())\n",
    "        bands = sorted(set(Bmm[i, :kept].tolist())) if kept > 0 else []\n",
    "        print(f\"- idx={i} oid={ids[i]} orig_len={int(ol[i])} kept={kept} | bands_unique={bands}\")\n",
    "\n",
    "sanity_samples(\"train\", 3)\n",
    "sanity_samples(\"test\", 3)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Save config\n",
    "# ----------------------------\n",
    "policy_cfg = {\n",
    "    \"token_mode\": SEQ_TOKEN_MODE,\n",
    "    \"max_len\": int(MAX_LEN),\n",
    "    \"feature_names\": list(SEQ_FEATURE_NAMES),\n",
    "    \"window_score\": {\n",
    "        \"snr_abs\": float(W_SNR),\n",
    "        \"value\": float(W_VAL),\n",
    "        \"detected\": float(W_DET),\n",
    "        \"value_feat\": SCORE_VALUE_FEAT,\n",
    "        \"value_policy\": (\"abs(flux_asinh)\" if SEQ_TOKEN_MODE == \"asinh\" else \"relu(median_mag - mag) -> log1p\"),\n",
    "    },\n",
    "    \"train_order\": \"df_train_meta.index\",\n",
    "    \"test_order\": \"df_sub.object_id\" if (\"df_sub\" in globals() and isinstance(df_sub, pd.DataFrame) and \"object_id\" in df_sub.columns) else \"df_test_meta.index\",\n",
    "    \"files\": {\n",
    "        \"train_X\": str(train_X_path),\n",
    "        \"train_B\": str(train_B_path),\n",
    "        \"train_M\": str(train_M_path),\n",
    "        \"train_y\": str(FIX_DIR / \"train_y.npy\"),\n",
    "        \"train_ids\": str(FIX_DIR / \"train_ids.npy\"),\n",
    "        \"train_origlen\": str(train_len_path),\n",
    "        \"train_winstart\": str(train_win_path),\n",
    "        \"test_X\": str(test_X_path),\n",
    "        \"test_B\": str(test_B_path),\n",
    "        \"test_M\": str(test_M_path),\n",
    "        \"test_ids\": str(FIX_DIR / \"test_ids.npy\"),\n",
    "        \"test_origlen\": str(test_len_path),\n",
    "        \"test_winstart\": str(test_win_path),\n",
    "    }\n",
    "}\n",
    "cfg_path = FIX_DIR / \"length_policy_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(policy_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 6] DONE\")\n",
    "print(f\"- Saved fixed cache dir: {FIX_DIR}\")\n",
    "print(f\"- Saved config: {cfg_path}\")\n",
    "\n",
    "# Export globals for training stage\n",
    "globals().update({\n",
    "    \"FIX_DIR\": FIX_DIR,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"FIX_TRAIN_X_PATH\": train_X_path,\n",
    "    \"FIX_TRAIN_B_PATH\": train_B_path,\n",
    "    \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "    \"FIX_TEST_X_PATH\": test_X_path,\n",
    "    \"FIX_TEST_B_PATH\": test_B_path,\n",
    "    \"FIX_TEST_M_PATH\": test_M_path,\n",
    "    \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "    \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "    \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "    \"FIX_POLICY_CFG_PATH\": cfg_path,\n",
    "    \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85508fef",
   "metadata": {
    "papermill": {
     "duration": 0.011081,
     "end_time": "2026-01-02T09:09:20.586850",
     "exception": false,
     "start_time": "2026-01-02T09:09:20.575769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Split (Object-Level, Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2dc05a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:09:20.610957Z",
     "iopub.status.busy": "2026-01-02T09:09:20.610596Z",
     "iopub.status.idle": "2026-01-02T09:09:22.353402Z",
     "shell.execute_reply": "2026-01-02T09:09:22.352548Z"
    },
    "papermill": {
     "duration": 1.757396,
     "end_time": "2026-01-02T09:09:22.355247",
     "exception": false,
     "start_time": "2026-01-02T09:09:20.597851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 7] seed=2025 | default_splits=5 | min_pos_per_fold=3 | group_by_split=False\n",
      "[Stage 7] n_splits=5 | N=3,043 | pos=148 | neg=2,895 | pos%=4.8636% | order_source=fixed_seq/train_ids.npy\n",
      "\n",
      "[Stage 7] CV split OK\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_folds.csv\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_folds.npz\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_config.json\n",
      "Order source: fixed_seq/train_ids.npy\n",
      "Total: N=3043 | pos=148 | neg=2895 | pos%=4.863621%\n",
      "Per-fold distribution:\n",
      "- fold 0: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 1: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 2: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 3: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n",
      "- fold 4: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL\n",
    "#\n",
    "# Tujuan:\n",
    "# - Buat split CV di level object_id (bukan per baris lightcurve)\n",
    "# - Konsisten dengan urutan TRAIN yang dipakai di STAGE 6 (fixed_seq/train_ids.npy)\n",
    "#\n",
    "# Upgrade:\n",
    "# - Prefer urutan dari FIX_DIR/train_ids.npy (kalau ada), robust decode bytes->str\n",
    "# - Pilihan CV:\n",
    "#     * StratifiedKFold (default)\n",
    "#     * StratifiedGroupKFold dengan group=\"split\" (opsional, lebih jujur jika ada perbedaan cadence antar split)\n",
    "# - n_splits adaptif + MIN_POS_PER_FOLD supaya tiap fold punya cukup TDE (lebih stabil untuk threshold/F1)\n",
    "# - Save train_idx + val_idx per fold (npz) untuk training cepat\n",
    "# - Validasi keras: missing ids, duplikasi, fold tanpa kelas\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/cv/cv_folds.csv\n",
    "# - artifacts/cv/cv_folds.npz   (train_idx_f + val_idx_f)\n",
    "# - artifacts/cv/cv_report.txt\n",
    "# - artifacts/cv/cv_config.json\n",
    "# - globals: fold_assign, folds, n_splits, CV_DIR\n",
    "# ============================================================\n",
    "\n",
    "import gc, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"df_train_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 2 dulu (Load and Validate Train/Test Logs).\")\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CV Settings (safe defaults for extreme imbalance)\n",
    "# ----------------------------\n",
    "DEFAULT_SPLITS = 5\n",
    "FORCE_N_SPLITS = None          # set int to force (misal 3), else None\n",
    "MIN_POS_PER_FOLD = 3           # rekomendasi: 3–10 (semakin besar semakin stabil, tapi butuh pos cukup)\n",
    "USE_GROUP_BY_SPLIT = False     # True => StratifiedGroupKFold (group = df_train_meta[\"split\"])\n",
    "\n",
    "print(f\"[Stage 7] seed={SEED} | default_splits={DEFAULT_SPLITS} | min_pos_per_fold={MIN_POS_PER_FOLD} | group_by_split={USE_GROUP_BY_SPLIT}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Determine train_ids ordering (prefer fixed cache from STAGE 6)\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "order_source = \"df_train_meta.index\"\n",
    "\n",
    "if \"FIX_DIR\" in globals():\n",
    "    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        raw = np.load(p, allow_pickle=False)\n",
    "        # robust decode (bytes -> str)\n",
    "        if raw.dtype.kind in (\"S\", \"O\"):\n",
    "            train_ids = [x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else str(x) for x in raw.tolist()]\n",
    "        else:\n",
    "            train_ids = raw.astype(str).tolist()\n",
    "        order_source = \"fixed_seq/train_ids.npy\"\n",
    "\n",
    "if train_ids is None:\n",
    "    train_ids = df_train_meta.index.astype(str).tolist()\n",
    "\n",
    "# uniqueness check\n",
    "if len(train_ids) != len(set(train_ids)):\n",
    "    s = pd.Series(train_ids)\n",
    "    dup = s[s.duplicated()].iloc[:10].tolist()\n",
    "    raise RuntimeError(f\"[Stage 7] train_ids has duplicates (examples): {dup}\")\n",
    "\n",
    "# ensure all ids exist in df_train_meta\n",
    "missing_in_meta = [oid for oid in train_ids if oid not in df_train_meta.index]\n",
    "if missing_in_meta:\n",
    "    raise RuntimeError(f\"[Stage 7] Some train_ids not found in df_train_meta (examples): {missing_in_meta[:10]}\")\n",
    "\n",
    "# y aligned to train_ids\n",
    "y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "N = len(train_ids)\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "if pos == 0 or neg == 0:\n",
    "    raise RuntimeError(f\"[Stage 7] Invalid class distribution: pos={pos}, neg={neg}. Cannot do stratified CV.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Choose n_splits safely\n",
    "# ----------------------------\n",
    "# Hard limits: each fold needs >=1 pos and >=1 neg\n",
    "max_splits_by_pos = pos\n",
    "max_splits_by_neg = neg\n",
    "\n",
    "# Stability limit: each fold ideally has >= MIN_POS_PER_FOLD positives\n",
    "max_splits_by_minpos = max(1, pos // max(int(MIN_POS_PER_FOLD), 1))\n",
    "\n",
    "n_splits = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg, max_splits_by_minpos)\n",
    "\n",
    "if FORCE_N_SPLITS is not None:\n",
    "    n_splits = int(FORCE_N_SPLITS)\n",
    "\n",
    "if n_splits < 2:\n",
    "    raise RuntimeError(\n",
    "        f\"[Stage 7] Too few samples for stratified CV. \"\n",
    "        f\"pos={pos}, neg={neg}, MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} => n_splits={n_splits}. \"\n",
    "        \"Turunkan MIN_POS_PER_FOLD atau pakai holdout.\"\n",
    "    )\n",
    "\n",
    "print(f\"[Stage 7] n_splits={n_splits} | N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.4f}% | order_source={order_source}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Build folds (SKLearn)\n",
    "# ----------------------------\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedGroupKFold\n",
    "    except Exception:\n",
    "        StratifiedGroupKFold = None\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn is not available in this environment.\") from e\n",
    "\n",
    "groups = None\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    if StratifiedGroupKFold is None:\n",
    "        raise RuntimeError(\"StratifiedGroupKFold not available in this sklearn version/environment.\")\n",
    "    groups = df_train_meta.loc[train_ids, \"split\"].astype(str).to_numpy()\n",
    "\n",
    "fold_assign = np.full(N, -1, dtype=np.int16)\n",
    "folds = []\n",
    "\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    splitter = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    split_iter = splitter.split(np.zeros(N), y, groups=groups)\n",
    "else:\n",
    "    splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    split_iter = splitter.split(np.zeros(N), y)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(split_iter):\n",
    "    fold_assign[val_idx] = fold\n",
    "    folds.append({\n",
    "        \"fold\": int(fold),\n",
    "        \"train_idx\": tr_idx.astype(np.int32),\n",
    "        \"val_idx\": val_idx.astype(np.int32),\n",
    "    })\n",
    "\n",
    "if (fold_assign < 0).any():\n",
    "    raise RuntimeError(\"[Stage 7] Fold assignment still has -1 (some objects not assigned).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Validate per-fold distribution (hard checks)\n",
    "# ----------------------------\n",
    "lines = []\n",
    "lines.append(f\"CV={('StratifiedGroupKFold(split)' if USE_GROUP_BY_SPLIT else 'StratifiedKFold')} n_splits={n_splits} seed={SEED}\")\n",
    "lines.append(f\"Order source: {order_source}\")\n",
    "lines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "lines.append(\"Per-fold distribution:\")\n",
    "\n",
    "ok = True\n",
    "min_pos_seen = 10**9\n",
    "for f in range(n_splits):\n",
    "    idx = np.where(fold_assign == f)[0]\n",
    "    yf = y[idx]\n",
    "    pf = int((yf == 1).sum())\n",
    "    nf = int((yf == 0).sum())\n",
    "    min_pos_seen = min(min_pos_seen, pf)\n",
    "    lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:9.6f}%\")\n",
    "    if pf == 0 or nf == 0:\n",
    "        ok = False\n",
    "\n",
    "if not ok:\n",
    "    raise RuntimeError(\n",
    "        \"[Stage 7] A fold has pos=0 or neg=0. \"\n",
    "        \"Reduce n_splits or disable group_by_split, then rebuild.\"\n",
    "    )\n",
    "\n",
    "if min_pos_seen < MIN_POS_PER_FOLD:\n",
    "    lines.append(f\"NOTE: min positives in a fold = {min_pos_seen} (< MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}). \"\n",
    "                 \"Training/threshold tuning may be noisy; consider smaller n_splits or lower MIN_POS_PER_FOLD.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save artifacts (csv + npz + report + config)\n",
    "# ----------------------------\n",
    "ART_DIR = Path(ART_DIR)\n",
    "CV_DIR = ART_DIR / \"cv\"\n",
    "CV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\n",
    "folds_csv = CV_DIR / \"cv_folds.csv\"\n",
    "df_folds.to_csv(folds_csv, index=False)\n",
    "\n",
    "# Save idx arrays per fold\n",
    "npz_path = CV_DIR / \"cv_folds.npz\"\n",
    "npz_kwargs = {}\n",
    "for f in range(n_splits):\n",
    "    npz_kwargs[f\"train_idx_{f}\"] = folds[f][\"train_idx\"]\n",
    "    npz_kwargs[f\"val_idx_{f}\"]   = folds[f][\"val_idx\"]\n",
    "np.savez(npz_path, **npz_kwargs)\n",
    "\n",
    "report_path = CV_DIR / \"cv_report.txt\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "cfg_path = CV_DIR / \"cv_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"seed\": SEED,\n",
    "            \"n_splits\": int(n_splits),\n",
    "            \"cv_type\": \"StratifiedGroupKFold(split)\" if USE_GROUP_BY_SPLIT else \"StratifiedKFold\",\n",
    "            \"min_pos_per_fold\": int(MIN_POS_PER_FOLD),\n",
    "            \"order_source\": order_source,\n",
    "            \"artifacts\": {\n",
    "                \"folds_csv\": str(folds_csv),\n",
    "                \"folds_npz\": str(npz_path),\n",
    "                \"report_txt\": str(report_path),\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n[Stage 7] CV split OK\")\n",
    "print(f\"- Saved: {folds_csv}\")\n",
    "print(f\"- Saved: {npz_path}\")\n",
    "print(f\"- Saved: {report_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "print(\"\\n\".join(lines[-(n_splits + 3):]))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Export globals for next stage\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"CV_DIR\": CV_DIR,\n",
    "    \"n_splits\": n_splits,\n",
    "    \"train_ids_ordered\": train_ids,\n",
    "    \"y_ordered\": y,\n",
    "    \"fold_assign\": fold_assign,\n",
    "    \"folds\": folds,\n",
    "    \"CV_FOLDS_CSV\": folds_csv,\n",
    "    \"CV_FOLDS_NPZ\": npz_path,\n",
    "    \"CV_CFG_PATH\": cfg_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024a06f",
   "metadata": {
    "papermill": {
     "duration": 0.012282,
     "end_time": "2026-01-02T09:09:22.378908",
     "exception": false,
     "start_time": "2026-01-02T09:09:22.366626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model (CPU-Safe Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5a08aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:09:22.403942Z",
     "iopub.status.busy": "2026-01-02T09:09:22.403607Z",
     "iopub.status.idle": "2026-01-02T09:37:39.664744Z",
     "shell.execute_reply": "2026-01-02T09:37:39.663878Z"
    },
    "papermill": {
     "duration": 1697.285561,
     "end_time": "2026-01-02T09:37:39.675876",
     "exception": false,
     "start_time": "2026-01-02T09:09:22.390315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8] TRAIN CONFIG (CPU)\n",
      "- N=3,043 | pos=148 | neg=2,895 | pos%=4.8636%\n",
      "- Model: d_model=128 heads=4 layers=2 dropout=0.1\n",
      "- Batch=16 grad_accum=2 epochs=10 lr=0.0003\n",
      "- CKPT_DIR=/kaggle/working/mallorn_run/checkpoints\n",
      "- OOF_DIR =/kaggle/working/mallorn_run/oof\n",
      "- LOG_DIR =/kaggle/working/mallorn_run/logs\n",
      "\n",
      "[Stage 8] FOLD 0/4 | train=2,434 val=609 | pos_weight=19.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17/1828498180.py:172: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  torch.from_numpy(X),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | train_loss=1.43790 | val_loss=1.35755 | f1@0.5=0.0000 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.46963 | val_loss=1.31184 | f1@0.5=0.0972 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.40327 | val_loss=1.34925 | f1@0.5=0.0963 | best_epoch=2 | patience_left=2\n",
      "  epoch 04 | train_loss=1.33664 | val_loss=1.31863 | f1@0.5=0.0000 | best_epoch=2 | patience_left=1\n",
      "  epoch 05 | train_loss=1.35236 | val_loss=1.40819 | f1@0.5=0.0000 | best_epoch=2 | patience_left=0\n",
      "\n",
      "[Stage 8] FOLD 1/4 | train=2,434 val=609 | pos_weight=19.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | train_loss=1.46357 | val_loss=1.61596 | f1@0.5=0.0990 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.43054 | val_loss=1.90663 | f1@0.5=0.0000 | best_epoch=1 | patience_left=2\n",
      "  epoch 03 | train_loss=1.41220 | val_loss=1.77561 | f1@0.5=0.0000 | best_epoch=1 | patience_left=1\n",
      "  epoch 04 | train_loss=1.37819 | val_loss=1.67626 | f1@0.5=0.1148 | best_epoch=1 | patience_left=0\n",
      "\n",
      "[Stage 8] FOLD 2/4 | train=2,434 val=609 | pos_weight=19.6271\n",
      "  epoch 01 | train_loss=1.75239 | val_loss=1.50390 | f1@0.5=0.0000 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.40910 | val_loss=1.34395 | f1@0.5=0.0000 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.42046 | val_loss=1.42483 | f1@0.5=0.0000 | best_epoch=2 | patience_left=2\n",
      "  epoch 04 | train_loss=1.33406 | val_loss=1.35694 | f1@0.5=0.0968 | best_epoch=2 | patience_left=1\n",
      "  epoch 05 | train_loss=1.41585 | val_loss=1.29759 | f1@0.5=0.1020 | best_epoch=5 | patience_left=3\n",
      "  epoch 06 | train_loss=1.35510 | val_loss=1.33570 | f1@0.5=0.0993 | best_epoch=5 | patience_left=2\n",
      "  epoch 07 | train_loss=1.31323 | val_loss=1.30734 | f1@0.5=0.0606 | best_epoch=5 | patience_left=1\n",
      "  epoch 08 | train_loss=1.32095 | val_loss=1.33246 | f1@0.5=0.0000 | best_epoch=5 | patience_left=0\n",
      "\n",
      "[Stage 8] FOLD 3/4 | train=2,435 val=608 | pos_weight=19.4622\n",
      "  epoch 01 | train_loss=1.40073 | val_loss=1.30352 | f1@0.5=0.0000 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.41265 | val_loss=1.29334 | f1@0.5=0.1071 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.36518 | val_loss=1.33319 | f1@0.5=0.0000 | best_epoch=2 | patience_left=2\n",
      "  epoch 04 | train_loss=1.35160 | val_loss=1.29358 | f1@0.5=0.0980 | best_epoch=2 | patience_left=1\n",
      "  epoch 05 | train_loss=1.33043 | val_loss=1.30272 | f1@0.5=0.0606 | best_epoch=2 | patience_left=0\n",
      "\n",
      "[Stage 8] FOLD 4/4 | train=2,435 val=608 | pos_weight=19.4622\n",
      "  epoch 01 | train_loss=1.55281 | val_loss=1.30603 | f1@0.5=0.0000 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.39697 | val_loss=1.30499 | f1@0.5=0.0948 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.35676 | val_loss=1.29271 | f1@0.5=0.1024 | best_epoch=3 | patience_left=3\n",
      "  epoch 04 | train_loss=1.32456 | val_loss=1.31189 | f1@0.5=0.0000 | best_epoch=3 | patience_left=2\n",
      "  epoch 05 | train_loss=1.38404 | val_loss=1.29410 | f1@0.5=0.1012 | best_epoch=3 | patience_left=1\n",
      "  epoch 06 | train_loss=1.34565 | val_loss=1.28830 | f1@0.5=0.0000 | best_epoch=6 | patience_left=3\n",
      "  epoch 07 | train_loss=1.36022 | val_loss=1.35087 | f1@0.5=0.0000 | best_epoch=6 | patience_left=2\n",
      "  epoch 08 | train_loss=1.32101 | val_loss=1.33408 | f1@0.5=0.0870 | best_epoch=6 | patience_left=1\n",
      "  epoch 09 | train_loss=1.32876 | val_loss=1.58536 | f1@0.5=0.0000 | best_epoch=6 | patience_left=0\n",
      "\n",
      "[Stage 8] CV TRAIN DONE\n",
      "- elapsed: 28.28 min\n",
      "- OOF saved: /kaggle/working/mallorn_run/oof/oof_prob.npy\n",
      "- OOF saved: /kaggle/working/mallorn_run/oof/oof_prob.csv\n",
      "- fold metrics: /kaggle/working/mallorn_run/oof/fold_metrics.json\n",
      "- OOF f1@0.5 (rough): 0.0992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 8 — Train Multiband Event Transformer (CPU-Safe) — REVISI FULL v2\n",
    "# Fixes (tetap):\n",
    "# - NO leakage: global feature scaler fit per fold (train_idx only) + disimpan di ckpt\n",
    "# - pos_weight dihitung per fold (train_idx only)\n",
    "# - grad_accum remainder di-handle (last step tidak dibuang)\n",
    "# - all-pad guard di attention pooling (hindari NaN)\n",
    "#\n",
    "# Fix baru (untuk error kamu):\n",
    "# - CKPT_DIR / OOF_DIR / LOG_DIR TIDAK wajib sudah ada di globals:\n",
    "#   otomatis dibuat dari RUN_DIR/ART_DIR atau default /kaggle/working/mallorn_run/*\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, math, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimum previous stages (lebih fleksibel)\n",
    "# ----------------------------\n",
    "need_min = [\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_train_meta\",\"n_splits\",\"folds\"]\n",
    "for k in need_min:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n",
    "\n",
    "# Ordering & labels (fallback aman)\n",
    "if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n",
    "    train_ids = list(globals()[\"train_ids_ordered\"])\n",
    "else:\n",
    "    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        train_ids = np.load(p, allow_pickle=False).astype(\"S\").astype(str).tolist()\n",
    "    else:\n",
    "        train_ids = df_train_meta.index.astype(str).tolist()\n",
    "\n",
    "if \"y_ordered\" in globals() and globals()[\"y_ordered\"] is not None and len(globals()[\"y_ordered\"]) == len(train_ids):\n",
    "    y = np.asarray(globals()[\"y_ordered\"], dtype=np.int8)\n",
    "else:\n",
    "    y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Ensure output dirs exist (FIX untuk CKPT_DIR missing)\n",
    "# ----------------------------\n",
    "# Base run dir\n",
    "if \"RUN_DIR\" in globals() and globals()[\"RUN_DIR\"] is not None:\n",
    "    RUN_DIR = Path(globals()[\"RUN_DIR\"])\n",
    "else:\n",
    "    # infer from ART_DIR if ada; else default\n",
    "    if \"ART_DIR\" in globals() and globals()[\"ART_DIR\"] is not None:\n",
    "        RUN_DIR = Path(globals()[\"ART_DIR\"]).parent\n",
    "    else:\n",
    "        RUN_DIR = Path(\"/kaggle/working/mallorn_run\")\n",
    "\n",
    "# Artifacts dir (kalau belum ada)\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", RUN_DIR / \"artifacts\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create dirs if missing from globals\n",
    "CKPT_DIR = Path(globals().get(\"CKPT_DIR\", RUN_DIR / \"checkpoints\"))\n",
    "OOF_DIR  = Path(globals().get(\"OOF_DIR\",  RUN_DIR / \"oof\"))\n",
    "LOG_DIR  = Path(globals().get(\"LOG_DIR\",  RUN_DIR / \"logs\"))\n",
    "\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export back so next stages see them\n",
    "globals().update({\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"CKPT_DIR\": CKPT_DIR,\n",
    "    \"OOF_DIR\": OOF_DIR,\n",
    "    \"LOG_DIR\": LOG_DIR,\n",
    "})\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Imports (torch) + CPU safety\n",
    "# ----------------------------\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# CPU thread guard (avoid oversubscription)\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open fixed-length memmaps (do NOT load into RAM)\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(globals()[\"FIX_DIR\"])\n",
    "N = len(train_ids)\n",
    "L = int(globals()[\"MAX_LEN\"])\n",
    "Fdim = len(globals()[\"SEQ_FEATURE_NAMES\"])\n",
    "\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "\n",
    "for p in [train_X_path, train_B_path, train_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "X_mm = np.memmap(train_X_path, dtype=np.float32, mode=\"r\", shape=(N, L, Fdim))\n",
    "B_mm = np.memmap(train_B_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "M_mm = np.memmap(train_M_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build RAW global features aligned to train_ids (NO scaling here)\n",
    "# ----------------------------\n",
    "G_COLS = [\"Z\", \"Z_err\", \"EBV\", \"Z_missing\", \"Z_err_missing\", \"EBV_missing\", \"is_photoz\"]\n",
    "for c in G_COLS:\n",
    "    if c not in df_train_meta.columns:\n",
    "        df_train_meta[c] = 0.0\n",
    "\n",
    "G_raw = df_train_meta.loc[train_ids, G_COLS].copy()\n",
    "for c in G_COLS:\n",
    "    G_raw[c] = pd.to_numeric(G_raw[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "G_raw_np = G_raw.to_numpy(dtype=np.float32, copy=False)  # (N, g_dim)\n",
    "g_dim = int(G_raw_np.shape[1])\n",
    "\n",
    "# save global columns list (optional)\n",
    "global_cols_path = Path(LOG_DIR) / \"global_feature_cols.json\"\n",
    "with open(global_cols_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"cols\": G_COLS}, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset / DataLoader (num_workers=0)\n",
    "# ----------------------------\n",
    "class MemmapSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, idx, X_mm, B_mm, M_mm, G_scaled_np, y=None):\n",
    "        self.idx = np.asarray(idx, dtype=np.int32)\n",
    "        self.X_mm = X_mm\n",
    "        self.B_mm = B_mm\n",
    "        self.M_mm = M_mm\n",
    "        self.G = G_scaled_np  # (N,g_dim)\n",
    "        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "        X = self.X_mm[j]  # (L,F) float32\n",
    "        B = self.B_mm[j]  # (L,) int8\n",
    "        M = self.M_mm[j]  # (L,) int8\n",
    "        G = self.G[j]     # (g_dim,) float32\n",
    "        if self.y is None:\n",
    "            return (\n",
    "                torch.from_numpy(X),\n",
    "                torch.from_numpy(B.astype(np.int64, copy=False)),\n",
    "                torch.from_numpy(M.astype(np.int64, copy=False)),\n",
    "                torch.from_numpy(G),\n",
    "            )\n",
    "        yy = self.y[j]\n",
    "        return (\n",
    "            torch.from_numpy(X),\n",
    "            torch.from_numpy(B.astype(np.int64, copy=False)),\n",
    "            torch.from_numpy(M.astype(np.int64, copy=False)),\n",
    "            torch.from_numpy(G),\n",
    "            torch.tensor(float(yy), dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Model (match inference stage) + all-pad guard\n",
    "# ----------------------------\n",
    "class MultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, max_len, n_bands=6, d_model=128, n_heads=4, n_layers=2, ff_mult=2, dropout=0.10, g_dim=7):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.x_proj = nn.Linear(feat_dim, d_model)\n",
    "        self.band_emb = nn.Embedding(n_bands, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * ff_mult),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(g_dim, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model + (d_model // 2), d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        pad_mask = (mask == 0)  # True = pad\n",
    "        # ALL-PAD GUARD: kalau 1 sequence full pad, buka token 0\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        return self.head(z).squeeze(-1)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Training hyperparams (CPU-safe)\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    \"d_model\": 128,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 2,\n",
    "    \"ff_mult\": 2,\n",
    "    \"dropout\": 0.10,\n",
    "    \"batch_size\": 16,\n",
    "    \"grad_accum\": 2,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"patience\": 3,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "}\n",
    "\n",
    "# auto CPU soften if sequence long\n",
    "if L >= 512:\n",
    "    CFG[\"d_model\"] = 96\n",
    "    CFG[\"n_heads\"] = 4\n",
    "    CFG[\"n_layers\"] = 2\n",
    "    CFG[\"batch_size\"] = 12\n",
    "    CFG[\"grad_accum\"] = 2\n",
    "\n",
    "cfg_path = Path(LOG_DIR) / \"train_cfg.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "\n",
    "pos_all = int((y == 1).sum())\n",
    "neg_all = int((y == 0).sum())\n",
    "print(\"[Stage 8] TRAIN CONFIG (CPU)\")\n",
    "print(f\"- N={N:,} | pos={pos_all:,} | neg={neg_all:,} | pos%={pos_all/max(N,1)*100:.4f}%\")\n",
    "print(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\n",
    "print(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\n",
    "print(f\"- CKPT_DIR={CKPT_DIR}\")\n",
    "print(f\"- OOF_DIR ={OOF_DIR}\")\n",
    "print(f\"- LOG_DIR ={LOG_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Metrics helpers\n",
    "# ----------------------------\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def f1_binary(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return float(2 * prec * rec / (prec + rec))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses, logits_all, y_all = [], [], []\n",
    "    for batch in loader:\n",
    "        Xb, Bb, Mb, Gb, yb = batch\n",
    "        Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n",
    "        logit = model(Xb, Bb, Mb, Gb)\n",
    "        loss = criterion(logit, yb)\n",
    "        losses.append(float(loss.item()))\n",
    "        logits_all.append(logit.detach().cpu().numpy())\n",
    "        y_all.append(yb.detach().cpu().numpy())\n",
    "    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n",
    "    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n",
    "    probs = sigmoid_np(logits_all)\n",
    "    pred01 = (probs >= 0.5).astype(np.int8)\n",
    "    f1 = f1_binary(y_all, pred01)\n",
    "    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1\n",
    "\n",
    "# ----------------------------\n",
    "# 8) CV Train (fold-wise scaler + pos_weight)\n",
    "# ----------------------------\n",
    "oof_prob = np.zeros((N,), dtype=np.float32)\n",
    "fold_metrics = []\n",
    "\n",
    "all_idx = np.arange(N, dtype=np.int32)\n",
    "n_splits = int(globals()[\"n_splits\"])\n",
    "\n",
    "def fit_scaler_fold(G_raw_np, tr_idx):\n",
    "    X = G_raw_np[tr_idx]\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    std  = X.std(axis=0).astype(np.float32)\n",
    "    std  = np.where(std < 1e-6, 1.0, std).astype(np.float32)\n",
    "    return mean, std\n",
    "\n",
    "def apply_scaler(G_raw_np, mean, std):\n",
    "    return ((G_raw_np - mean) / std).astype(np.float32)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for fold_info in globals()[\"folds\"]:\n",
    "    fold = int(fold_info[\"fold\"])\n",
    "    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n",
    "\n",
    "    val_mask = np.zeros(N, dtype=bool)\n",
    "    val_mask[val_idx] = True\n",
    "    tr_idx = all_idx[~val_mask]\n",
    "\n",
    "    # fold-wise pos_weight\n",
    "    y_tr = y[tr_idx]\n",
    "    pos = int((y_tr == 1).sum())\n",
    "    neg = int((y_tr == 0).sum())\n",
    "    if pos == 0:\n",
    "        raise RuntimeError(f\"[Stage 8] Fold {fold}: no positives in training split.\")\n",
    "    pos_weight = float(neg / max(pos, 1))\n",
    "    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "\n",
    "    print(f\"\\n[Stage 8] FOLD {fold}/{n_splits-1} | train={len(tr_idx):,} val={len(val_idx):,} | pos_weight={pos_weight:.4f}\")\n",
    "\n",
    "    # fold-wise scaler (NO leakage)\n",
    "    g_mean, g_std = fit_scaler_fold(G_raw_np, tr_idx)\n",
    "    G_fold_z = apply_scaler(G_raw_np, g_mean, g_std)  # (N,g_dim) kecil\n",
    "\n",
    "    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_fold_z, y=y)\n",
    "    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_fold_z, y=y)\n",
    "    dl_tr = make_loader(ds_tr, batch_size=CFG[\"batch_size\"], shuffle=True)\n",
    "    dl_va = make_loader(ds_va, batch_size=CFG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model = MultibandEventTransformer(\n",
    "        feat_dim=Fdim,\n",
    "        max_len=L,\n",
    "        n_bands=6,\n",
    "        d_model=CFG[\"d_model\"],\n",
    "        n_heads=CFG[\"n_heads\"],\n",
    "        n_layers=CFG[\"n_layers\"],\n",
    "        ff_mult=CFG[\"ff_mult\"],\n",
    "        dropout=CFG[\"dropout\"],\n",
    "        g_dim=g_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_probs = None\n",
    "    patience_left = int(CFG[\"patience\"])\n",
    "\n",
    "    grad_accum = int(CFG[\"grad_accum\"])\n",
    "\n",
    "    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss_true = 0.0\n",
    "        n_batches = 0\n",
    "        accum = 0\n",
    "\n",
    "        for batch in dl_tr:\n",
    "            Xb, Bb, Mb, Gb, yb = batch\n",
    "            Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n",
    "\n",
    "            logit = model(Xb, Bb, Mb, Gb)\n",
    "            loss = criterion(logit, yb)\n",
    "\n",
    "            # accumulate true loss for logging\n",
    "            total_loss_true += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "            # scale for grad accumulation\n",
    "            (loss / float(grad_accum)).backward()\n",
    "            accum += 1\n",
    "\n",
    "            if accum == grad_accum:\n",
    "                if CFG[\"max_grad_norm\"] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                accum = 0\n",
    "\n",
    "        # remainder step (IMPORTANT)\n",
    "        if accum > 0:\n",
    "            if CFG[\"max_grad_norm\"] is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        train_loss = total_loss_true / max(n_batches, 1)\n",
    "\n",
    "        # validate\n",
    "        val_loss, probs, y_val, f1_05 = eval_model(model, dl_va, criterion)\n",
    "\n",
    "        improved = val_loss < (best_val_loss - 1e-6)\n",
    "        if improved:\n",
    "            best_val_loss = float(val_loss)\n",
    "            best_epoch = int(epoch)\n",
    "            best_probs = probs.copy()\n",
    "\n",
    "            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"fold\": fold,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"cfg\": CFG,\n",
    "                    \"seq_feature_names\": list(globals()[\"SEQ_FEATURE_NAMES\"]),\n",
    "                    \"max_len\": L,\n",
    "                    \"global_cols\": G_COLS,\n",
    "                    \"global_scaler\": {\"mean\": g_mean, \"std\": g_std},  # fold-wise scaler\n",
    "                    \"pos_weight\": pos_weight,\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "            patience_left = int(CFG[\"patience\"])\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "\n",
    "        print(f\"  epoch {epoch:02d} | train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | f1@0.5={f1_05:.4f} | best_epoch={best_epoch} | patience_left={patience_left}\")\n",
    "\n",
    "        if patience_left <= 0:\n",
    "            break\n",
    "\n",
    "    if best_probs is None:\n",
    "        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n",
    "\n",
    "    # fill OOF\n",
    "    oof_prob[val_idx] = best_probs.astype(np.float32)\n",
    "\n",
    "    pred01 = (best_probs >= 0.5).astype(np.int8)\n",
    "    best_f1_05 = f1_binary(y[val_idx], pred01)\n",
    "\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold,\n",
    "        \"val_size\": int(len(val_idx)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"f1_at_0p5\": float(best_f1_05),\n",
    "        \"pos_weight\": float(pos_weight),\n",
    "    })\n",
    "\n",
    "    del model, opt, ds_tr, ds_va, dl_tr, dl_va, G_fold_z\n",
    "    gc.collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Save OOF artifacts\n",
    "# ----------------------------\n",
    "oof_path_npy = OOF_DIR / \"oof_prob.npy\"\n",
    "np.save(oof_path_npy, oof_prob)\n",
    "\n",
    "df_oof = pd.DataFrame({\"object_id\": train_ids, \"target\": y.astype(int), \"oof_prob\": oof_prob.astype(np.float32)})\n",
    "oof_path_csv = OOF_DIR / \"oof_prob.csv\"\n",
    "df_oof.to_csv(oof_path_csv, index=False)\n",
    "\n",
    "metrics_path = OOF_DIR / \"fold_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed)}, f, indent=2)\n",
    "\n",
    "oof_pred01 = (oof_prob >= 0.5).astype(np.int8)\n",
    "oof_f1_05 = f1_binary(y, oof_pred01)\n",
    "\n",
    "print(\"\\n[Stage 8] CV TRAIN DONE\")\n",
    "print(f\"- elapsed: {elapsed/60:.2f} min\")\n",
    "print(f\"- OOF saved: {oof_path_npy}\")\n",
    "print(f\"- OOF saved: {oof_path_csv}\")\n",
    "print(f\"- fold metrics: {metrics_path}\")\n",
    "print(f\"- OOF f1@0.5 (rough): {oof_f1_05:.4f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"oof_prob\": oof_prob,\n",
    "    \"OOF_PROB_PATH\": oof_path_npy,\n",
    "    \"OOF_CSV_PATH\": oof_path_csv,\n",
    "    \"FOLD_METRICS_PATH\": metrics_path,\n",
    "    \"GLOBAL_COLS_PATH\": global_cols_path,\n",
    "    \"TRAIN_CFG_PATH\": cfg_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e14b9",
   "metadata": {
    "papermill": {
     "duration": 0.012676,
     "end_time": "2026-01-02T09:37:39.701108",
     "exception": false,
     "start_time": "2026-01-02T09:37:39.688432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OOF Prediction + Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb63357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:37:39.728098Z",
     "iopub.status.busy": "2026-01-02T09:37:39.727763Z",
     "iopub.status.idle": "2026-01-02T09:37:39.993926Z",
     "shell.execute_reply": "2026-01-02T09:37:39.993028Z"
    },
    "papermill": {
     "duration": 0.282221,
     "end_time": "2026-01-02T09:37:39.995746",
     "exception": false,
     "start_time": "2026-01-02T09:37:39.713525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9] Threshold tuning DONE\n",
      "- Best threshold: 0.510000\n",
      "- Best F1:        0.101343  (P=0.055705 R=0.560811)\n",
      "- Baseline F1@0.5:0.099182  (P=0.053650 R=0.655405)\n",
      "- Saved: /kaggle/working/mallorn_run/oof/threshold_tuning.json\n",
      "- Saved: /kaggle/working/mallorn_run/oof/threshold_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/oof/threshold_table_top200.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL v2.1\n",
    "#\n",
    "# Fix:\n",
    "# - Robust oof_prob 1D (anti \"len() of unsized object\")\n",
    "# - Robust train_ids_ordered bytes->str\n",
    "# - Clean oof_prob NaN/inf + clip to [0,1] before quantile/sweep\n",
    "# - Clear error if object_id alignment missing in df_train_meta\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"OOF_DIR\", \"df_train_meta\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n",
    "\n",
    "OOF_DIR = Path(OOF_DIR)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: robust stringify id\n",
    "# ----------------------------\n",
    "def _to_str_list(ids):\n",
    "    out = []\n",
    "    for x in ids:\n",
    "        if isinstance(x, (bytes, np.bytes_)):\n",
    "            out.append(x.decode(\"utf-8\", errors=\"ignore\").strip())\n",
    "        else:\n",
    "            out.append(str(x).strip())\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: robust load oof_prob as 1D float32\n",
    "# ----------------------------\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    # handle object dtype that may hold an array inside (0-d object)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a  # caller decides\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _load_oof_prob():\n",
    "    # 1) try from globals\n",
    "    if \"oof_prob\" in globals():\n",
    "        op = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "            return op\n",
    "\n",
    "    # 2) try from npy\n",
    "    p = OOF_DIR / \"oof_prob.npy\"\n",
    "    if p.exists():\n",
    "        op = _as_1d_float32(np.load(p, allow_pickle=False))\n",
    "        if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "            return op\n",
    "\n",
    "    # 3) try from csv (as last resort)\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if \"oof_prob\" in df.columns:\n",
    "            op = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n",
    "            if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "                return op\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob not found (globals/npy/csv). Jalankan STAGE 8 (training) dulu.\")\n",
    "\n",
    "# Load OOF probabilities (guaranteed 1D or raise)\n",
    "oof_prob = _load_oof_prob()\n",
    "\n",
    "# Final guard: if still scalar, fail clearly\n",
    "if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "    raise TypeError(\n",
    "        f\"Invalid oof_prob (unsized/scalar). Type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}. \"\n",
    "        \"Pastikan STAGE 8 menyimpan oof_prob sebagai array 1D.\"\n",
    "    )\n",
    "\n",
    "# sanitize oof_prob (avoid NaN/inf breaking quantile/sweep)\n",
    "oof_prob = np.nan_to_num(oof_prob, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32)\n",
    "oof_prob = np.clip(oof_prob, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# Align y ordering:\n",
    "# A) train_ids_ordered if valid length\n",
    "# B) fallback via oof_prob.csv (most robust)\n",
    "# C) last fallback df_train_meta order (requires same length)\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "y = None\n",
    "\n",
    "# Case A\n",
    "if \"train_ids_ordered\" in globals():\n",
    "    _ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n",
    "    if len(_ids) == len(oof_prob):\n",
    "        # validate existence in df_train_meta index\n",
    "        missing = [oid for oid in _ids if oid not in df_train_meta.index]\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"train_ids_ordered contains ids not in df_train_meta (examples): {missing[:5]} | missing_n={len(missing)}\"\n",
    "            )\n",
    "        train_ids = _ids\n",
    "        y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Case B\n",
    "if y is None:\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df_oof = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df_oof.columns) and (\"oof_prob\" in df_oof.columns):\n",
    "            df_oof[\"object_id\"] = df_oof[\"object_id\"].astype(str).str.strip()\n",
    "            ids_csv = df_oof[\"object_id\"].tolist()\n",
    "\n",
    "            # validate existence\n",
    "            missing = [oid for oid in ids_csv if oid not in df_train_meta.index]\n",
    "            if missing:\n",
    "                raise KeyError(\n",
    "                    f\"oof_prob.csv contains object_id not in df_train_meta (examples): {missing[:5]} | missing_n={len(missing)}\"\n",
    "                )\n",
    "\n",
    "            # override oof_prob with csv order to guarantee alignment\n",
    "            oof_prob = _as_1d_float32(df_oof[\"oof_prob\"].to_numpy())\n",
    "            if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "                raise TypeError(\"oof_prob from csv became scalar (unexpected).\")\n",
    "            oof_prob = np.nan_to_num(oof_prob.astype(np.float32), nan=0.0, posinf=1.0, neginf=0.0)\n",
    "            oof_prob = np.clip(oof_prob, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "            train_ids = ids_csv\n",
    "            y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Case C\n",
    "if y is None:\n",
    "    if len(oof_prob) != len(df_train_meta):\n",
    "        raise RuntimeError(\n",
    "            f\"Cannot align y: len(oof_prob)={len(oof_prob)} != len(df_train_meta)={len(df_train_meta)} \"\n",
    "            \"and train_ids_ordered not usable and oof_prob.csv not available.\"\n",
    "        )\n",
    "    train_ids = df_train_meta.index.astype(str).tolist()\n",
    "    y = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Final length check\n",
    "if len(oof_prob) != len(y):\n",
    "    raise RuntimeError(f\"Length mismatch: oof_prob={len(oof_prob)} vs y={len(y)}\")\n",
    "\n",
    "# y sanity\n",
    "uy = set(np.unique(y).tolist())\n",
    "if not uy.issubset({0, 1}):\n",
    "    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Metrics\n",
    "# ----------------------------\n",
    "def f1_binary(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return float(2 * prec * rec / (prec + rec))\n",
    "\n",
    "def precision_recall(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    return float(prec), float(rec), tp, fp, fn\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold sweep\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.01, 0.10, 19),\n",
    "    np.linspace(0.10, 0.90, 81),\n",
    "    np.linspace(0.90, 0.99, 19),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.01, 0.99, 99, dtype=np.float32)\n",
    "# quantile on cleaned probs\n",
    "quant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n",
    "\n",
    "thr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr]), 0.0, 1.0))\n",
    "\n",
    "best = {\"thr\": 0.5, \"f1\": -1.0, \"prec\": 0.0, \"rec\": 0.0, \"tp\": 0, \"fp\": 0, \"fn\": 0, \"pos_pred\": 0}\n",
    "rows = []\n",
    "\n",
    "for thr in thr_candidates:\n",
    "    pred = (oof_prob >= thr).astype(np.int8)\n",
    "    f1 = f1_binary(y, pred)\n",
    "    prec, rec, tp, fp, fn = precision_recall(y, pred)\n",
    "    pos_pred = int(pred.sum())\n",
    "    rows.append((float(thr), float(f1), float(prec), float(rec), int(tp), int(fp), int(fn), pos_pred))\n",
    "\n",
    "    if (f1 > best[\"f1\"] + 1e-12) or (\n",
    "        abs(f1 - best[\"f1\"]) <= 1e-12 and (rec > best[\"rec\"] + 1e-12)\n",
    "    ) or (\n",
    "        abs(f1 - best[\"f1\"]) <= 1e-12 and abs(rec - best[\"rec\"]) <= 1e-12 and (fp < best[\"fp\"])\n",
    "    ):\n",
    "        best.update({\"thr\": float(thr), \"f1\": float(f1), \"prec\": float(prec), \"rec\": float(rec),\n",
    "                     \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn), \"pos_pred\": int(pos_pred)})\n",
    "\n",
    "thr_table = pd.DataFrame(rows, columns=[\"thr\",\"f1\",\"precision\",\"recall\",\"tp\",\"fp\",\"fn\",\"pos_pred\"])\n",
    "thr_table = thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "BEST_THR = float(best[\"thr\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Baseline 0.5\n",
    "# ----------------------------\n",
    "pred05 = (oof_prob >= 0.5).astype(np.int8)\n",
    "f1_05 = f1_binary(y, pred05)\n",
    "prec_05, rec_05, tp_05, fp_05, fn_05 = precision_recall(y, pred05)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Save report\n",
    "# ----------------------------\n",
    "out_json = OOF_DIR / \"threshold_tuning.json\"\n",
    "out_txt  = OOF_DIR / \"threshold_report.txt\"\n",
    "out_csv  = OOF_DIR / \"threshold_table_top200.csv\"\n",
    "\n",
    "payload = {\n",
    "    \"best_threshold\": BEST_THR,\n",
    "    \"best_f1\": best[\"f1\"],\n",
    "    \"best_precision\": best[\"prec\"],\n",
    "    \"best_recall\": best[\"rec\"],\n",
    "    \"best_counts\": {\"tp\": best[\"tp\"], \"fp\": best[\"fp\"], \"fn\": best[\"fn\"], \"pos_pred\": best[\"pos_pred\"]},\n",
    "    \"baseline_thr_0p5\": {\"f1\": f1_05, \"precision\": prec_05, \"recall\": rec_05, \"tp\": tp_05, \"fp\": fp_05, \"fn\": fn_05, \"pos_pred\": int(pred05.sum())},\n",
    "    \"n_objects\": int(len(y)),\n",
    "    \"pos\": int((y == 1).sum()),\n",
    "    \"neg\": int((y == 0).sum()),\n",
    "}\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Threshold Tuning Report\")\n",
    "lines.append(f\"- N={payload['n_objects']} | pos={payload['pos']} | neg={payload['neg']} | pos%={payload['pos']/max(payload['n_objects'],1)*100:.4f}%\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"- F1={f1_05:.6f} | P={prec_05:.6f} | R={rec_05:.6f} | tp={tp_05} fp={fp_05} fn={fn_05} | pos_pred={int(pred05.sum())}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST @ thr={BEST_THR:.6f}\")\n",
    "lines.append(f\"- F1={best['f1']:.6f} | P={best['prec']:.6f} | R={best['rec']:.6f} | tp={best['tp']} fp={best['fp']} fn={best['fn']} | pos_pred={best['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 thresholds by (F1, recall, precision):\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    lines.append(\n",
    "        f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | P={r['precision']:.6f} | R={r['recall']:.6f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\"\n",
    "    )\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "thr_table.head(200).to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"[Stage 9] Threshold tuning DONE\")\n",
    "print(f\"- Best threshold: {BEST_THR:.6f}\")\n",
    "print(f\"- Best F1:        {best['f1']:.6f}  (P={best['prec']:.6f} R={best['rec']:.6f})\")\n",
    "print(f\"- Baseline F1@0.5:{f1_05:.6f}  (P={prec_05:.6f} R={rec_05:.6f})\")\n",
    "print(f\"- Saved: {out_json}\")\n",
    "print(f\"- Saved: {out_txt}\")\n",
    "print(f\"- Saved: {out_csv}\")\n",
    "\n",
    "globals().update({\n",
    "    \"BEST_THR\": BEST_THR,\n",
    "    \"thr_table\": thr_table,\n",
    "    \"THR_JSON_PATH\": out_json,\n",
    "    \"THR_REPORT_PATH\": out_txt,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727bcdc5",
   "metadata": {
    "papermill": {
     "duration": 0.013052,
     "end_time": "2026-01-02T09:37:40.022043",
     "exception": false,
     "start_time": "2026-01-02T09:37:40.008991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Inference (Fold Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667e30db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:37:40.050101Z",
     "iopub.status.busy": "2026-01-02T09:37:40.049773Z",
     "iopub.status.idle": "2026-01-02T09:40:48.815068Z",
     "shell.execute_reply": "2026-01-02T09:40:48.814285Z"
    },
    "papermill": {
     "duration": 188.781943,
     "end_time": "2026-01-02T09:40:48.816856",
     "exception": false,
     "start_time": "2026-01-02T09:37:40.034913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 10] Test inference: N_test=7,135 | folds=5 | batch=64 (CPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fold 0: prob_mean=0.533540 | prob_std=0.033738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fold 1: prob_mean=0.536049 | prob_std=0.021311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fold 2: prob_mean=0.502968 | prob_std=0.050674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fold 3: prob_mean=0.480641 | prob_std=0.026030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fold 4: prob_mean=0.437383 | prob_std=0.049572\n",
      "\n",
      "[Stage 10] DONE\n",
      "- Saved fold probs: /kaggle/working/mallorn_run/artifacts/test_prob_fold.npy\n",
      "- Saved ens probs : /kaggle/working/mallorn_run/artifacts/test_prob_ens.npy\n",
      "- Saved csv       : /kaggle/working/mallorn_run/artifacts/test_prob_ens.csv\n",
      "- ens mean=0.498116 | std=0.034568 | min=0.197329 | max=0.536540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL, Kaggle CPU-SAFE)\n",
    "#\n",
    "# REVISI FULL v2.3:\n",
    "# - NO dependency on global_scaler.json (supports STAGE 8 NO-leakage fold-wise scaler in ckpt)\n",
    "# - Per-fold standardization for test globals using ckpt[\"global_scaler\"]\n",
    "# - Robust decode test_ids.npy (anti \"b'...'\" ids) + auto-normalize df_test_meta.index if needed\n",
    "# - torch.load kompatibel PyTorch 2.6 weights_only default\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\n",
    "    \"ART_DIR\",\n",
    "    \"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\n",
    "    \"df_test_meta\",\n",
    "    \"CKPT_DIR\",\n",
    "    \"n_splits\",\n",
    "]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n",
    "\n",
    "# Torch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Thread guard\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FIX_DIR = Path(FIX_DIR)\n",
    "ART_DIR = Path(ART_DIR)\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR = Path(CKPT_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# helper: normalize id\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    # decode bytes early\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    # fix \"b'...'\" / 'b\"...\"'\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _load_ids_npy(path: Path):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    # arr could be bytes dtype, unicode, or object (bytes inside)\n",
    "    ids = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n",
    "    return [_norm_id(x) for x in ids]\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load TEST ordering (must match STAGE 6)\n",
    "# ----------------------------\n",
    "test_ids_path = FIX_DIR / \"test_ids.npy\"\n",
    "if not test_ids_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "test_ids = _load_ids_npy(test_ids_path)\n",
    "NTE = len(test_ids)\n",
    "if NTE <= 0:\n",
    "    raise RuntimeError(\"test_ids kosong (NTE=0). Pastikan STAGE 6 sukses membuat test_ids.npy.\")\n",
    "\n",
    "# Optional: normalize df_test_meta.index if mismatch\n",
    "missing_ids = [oid for oid in test_ids[:2000] if oid not in df_test_meta.index]  # quick probe\n",
    "if missing_ids:\n",
    "    # normalize full index (shallow copy, data not duplicated)\n",
    "    df_test_meta = df_test_meta.copy(deep=False)\n",
    "    df_test_meta.index = pd.Index([_norm_id(x) for x in df_test_meta.index], name=df_test_meta.index.name)\n",
    "\n",
    "# Final guard: ids must exist\n",
    "missing_ids = [oid for oid in test_ids if oid not in df_test_meta.index]\n",
    "if missing_ids:\n",
    "    raise KeyError(\n",
    "        f\"Some test_ids not found in df_test_meta.index (examples): {missing_ids[:10]} | missing_n={len(missing_ids)}.\\n\"\n",
    "        f\"- Cek apakah df_test_meta.index benar-benar object_id.\\n\"\n",
    "        f\"- Cek apakah test_ids.npy masih kebaca sebagai b'...'.\"\n",
    "    )\n",
    "\n",
    "# Duplicates guard\n",
    "if len(set(test_ids)) != len(test_ids):\n",
    "    # duplicates in ordering is dangerous\n",
    "    s = pd.Series(test_ids)\n",
    "    dup = s[s.duplicated()].head(10).tolist()\n",
    "    raise ValueError(f\"Duplicate object_id in test_ids ordering (examples): {dup}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open fixed-length TEST memmaps\n",
    "# ----------------------------\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "L = int(MAX_LEN)\n",
    "\n",
    "test_X_path = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path = FIX_DIR / \"test_M.dat\"\n",
    "\n",
    "for p in [test_X_path, test_B_path, test_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=np.float32, mode=\"r\", shape=(NTE, L, Fdim))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset/Loader for inference\n",
    "# ----------------------------\n",
    "class TestMemmapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xmm, Bmm, Mmm, G_np_z):\n",
    "        self.Xmm = Xmm\n",
    "        self.Bmm = Bmm\n",
    "        self.Mmm = Mmm\n",
    "        self.G = G_np_z\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.Xmm.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Note: read-only memmap is fine; no in-place writes are done.\n",
    "        X = self.Xmm[i]\n",
    "        B = self.Bmm[i].astype(np.int64, copy=False)\n",
    "        M = self.Mmm[i].astype(np.int64, copy=False)\n",
    "        G = self.G[i]\n",
    "        return (\n",
    "            torch.from_numpy(X),\n",
    "            torch.from_numpy(B),\n",
    "            torch.from_numpy(M),\n",
    "            torch.from_numpy(G),\n",
    "        )\n",
    "\n",
    "def make_loader(ds, batch_size=64):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Model definition (must match STAGE 8)\n",
    "# ----------------------------\n",
    "class MultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, n_bands=6, d_model=128, n_heads=4, n_layers=2, ff_mult=2, dropout=0.10, g_dim=7, max_len=512):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.x_proj = nn.Linear(feat_dim, d_model)\n",
    "        self.band_emb = nn.Embedding(n_bands, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * ff_mult),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(g_dim, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model + (d_model // 2), d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        pad_mask = (mask == 0)  # True for pad\n",
    "\n",
    "        # ALL-PAD GUARD (match your STAGE 8 revised safety)\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        logit = self.head(z).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs(model, loader):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for Xb, Bb, Mb, Gb in loader:\n",
    "        Xb = Xb.to(device)\n",
    "        Bb = Bb.to(device)\n",
    "        Mb = Mb.to(device)\n",
    "        Gb = Gb.to(device)\n",
    "        logit = model(Xb, Bb, Mb, Gb)\n",
    "        outs.append(logit.detach().cpu().numpy())\n",
    "    logits = np.concatenate(outs, axis=0) if outs else np.zeros((0,), dtype=np.float32)\n",
    "    return sigmoid_np(logits).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Safe/compat checkpoint loader (PyTorch 2.6 weights_only default)\n",
    "# ----------------------------\n",
    "def torch_load_compat(path: Path):\n",
    "    \"\"\"\n",
    "    Prefer loading full dict (needed for fold-wise scaler).\n",
    "    - Try weights_only=True; if result not a full ckpt dict, fallback.\n",
    "    - Fallback to weights_only=False (trusted ckpt from your own run).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "        # If it's just a state_dict (no metadata), fallback\n",
    "        if isinstance(obj, dict) and (\"model_state\" in obj or \"cfg\" in obj or \"global_scaler\" in obj):\n",
    "            return obj\n",
    "        # Often weights_only gives OrderedDict state_dict; fallback\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        # older torch without weights_only arg\n",
    "        return torch.load(path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# checkpoints list\n",
    "ckpts = []\n",
    "for f in range(int(n_splits)):\n",
    "    p = CKPT_DIR / f\"fold_{f}.pt\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n",
    "    ckpts.append(p)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Optional fallback scaler file (only used if ckpt missing scaler)\n",
    "# ----------------------------\n",
    "fallback_scaler = None\n",
    "fallback_scaler_path = Path(globals().get(\"GLOBAL_SCALER_PATH\", \"\")) if \"GLOBAL_SCALER_PATH\" in globals() else None\n",
    "if fallback_scaler_path and str(fallback_scaler_path) and fallback_scaler_path.exists():\n",
    "    try:\n",
    "        with open(fallback_scaler_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            fallback_scaler = json.load(f)\n",
    "    except Exception:\n",
    "        fallback_scaler = None\n",
    "else:\n",
    "    # try default logs path if user still has it\n",
    "    log_dir = Path(globals().get(\"LOG_DIR\", \"/kaggle/working/mallorn_run/logs\"))\n",
    "    p2 = log_dir / \"global_scaler.json\"\n",
    "    if p2.exists():\n",
    "        try:\n",
    "            with open(p2, \"r\", encoding=\"utf-8\") as f:\n",
    "                fallback_scaler = json.load(f)\n",
    "        except Exception:\n",
    "            fallback_scaler = None\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Run inference per fold (fold-wise scaler) -> ensemble mean\n",
    "# ----------------------------\n",
    "BATCH_SIZE = 64\n",
    "test_prob_folds = np.zeros((NTE, int(n_splits)), dtype=np.float32)\n",
    "\n",
    "print(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} (CPU)\")\n",
    "\n",
    "for fold, ckpt_path in enumerate(ckpts):\n",
    "    ckpt = torch_load_compat(ckpt_path)\n",
    "\n",
    "    # Extract model_state robustly\n",
    "    if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "        model_state = ckpt[\"model_state\"]\n",
    "    elif isinstance(ckpt, dict):\n",
    "        # might be state_dict directly\n",
    "        model_state = ckpt\n",
    "        ckpt = {}  # no metadata\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected checkpoint content type: {type(ckpt)}\")\n",
    "\n",
    "    cfg = ckpt.get(\"cfg\", {}) if isinstance(ckpt, dict) else {}\n",
    "    d_model  = int(cfg.get(\"d_model\", 128))\n",
    "    n_heads  = int(cfg.get(\"n_heads\", 4))\n",
    "    n_layers = int(cfg.get(\"n_layers\", 2))\n",
    "    ff_mult  = int(cfg.get(\"ff_mult\", 2))\n",
    "    dropout  = float(cfg.get(\"dropout\", 0.10))\n",
    "\n",
    "    # ----- fold-wise global scaler + cols -----\n",
    "    G_COLS = ckpt.get(\"global_cols\", None) if isinstance(ckpt, dict) else None\n",
    "    if G_COLS is None:\n",
    "        # fallback default cols (should match your STAGE 8)\n",
    "        G_COLS = [\"Z\", \"Z_err\", \"EBV\", \"Z_missing\", \"Z_err_missing\", \"EBV_missing\", \"is_photoz\"]\n",
    "    G_COLS = list(G_COLS)\n",
    "\n",
    "    # Ensure columns exist in df_test_meta\n",
    "    for c in G_COLS:\n",
    "        if c not in df_test_meta.columns:\n",
    "            df_test_meta[c] = 0.0\n",
    "\n",
    "    G_raw = df_test_meta.loc[test_ids, G_COLS].copy()\n",
    "    for c in G_COLS:\n",
    "        G_raw[c] = pd.to_numeric(G_raw[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "    G_np = G_raw.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    scaler = ckpt.get(\"global_scaler\", None) if isinstance(ckpt, dict) else None\n",
    "    if scaler is None and fallback_scaler is not None:\n",
    "        # only if ckpt does not store fold scaler (older pipeline)\n",
    "        # must match columns count\n",
    "        if (\"mean\" in fallback_scaler) and (\"std\" in fallback_scaler) and (\"cols\" in fallback_scaler):\n",
    "            if list(fallback_scaler[\"cols\"]) == G_COLS:\n",
    "                scaler = {\"mean\": fallback_scaler[\"mean\"], \"std\": fallback_scaler[\"std\"]}\n",
    "\n",
    "    if scaler is not None:\n",
    "        g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32)\n",
    "        g_std  = np.asarray(scaler[\"std\"], dtype=np.float32)\n",
    "        g_std  = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n",
    "        if g_mean.shape[0] != G_np.shape[1] or g_std.shape[0] != G_np.shape[1]:\n",
    "            # mismatch -> no scaling\n",
    "            G_np_z = G_np.astype(np.float32, copy=False)\n",
    "        else:\n",
    "            G_np_z = ((G_np - g_mean) / g_std).astype(np.float32)\n",
    "    else:\n",
    "        # last resort: no scaling\n",
    "        G_np_z = G_np.astype(np.float32, copy=False)\n",
    "\n",
    "    # Dataset/Loader (fold-specific G_np_z)\n",
    "    ds_test = TestMemmapDataset(Xte, Bte, Mte, G_np_z)\n",
    "    dl_test = make_loader(ds_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Build model + load state\n",
    "    model = MultibandEventTransformer(\n",
    "        feat_dim=Fdim,\n",
    "        n_bands=6,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        ff_mult=ff_mult,\n",
    "        dropout=dropout,\n",
    "        g_dim=G_np_z.shape[1],\n",
    "        max_len=L,\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(model_state, strict=True)\n",
    "\n",
    "    probs = predict_probs(model, dl_test)\n",
    "    if len(probs) != NTE:\n",
    "        raise RuntimeError(f\"Fold {fold}: probs length mismatch {len(probs)} vs {NTE}\")\n",
    "\n",
    "    test_prob_folds[:, fold] = probs\n",
    "    print(f\"  fold {fold}: prob_mean={float(probs.mean()):.6f} | prob_std={float(probs.std()):.6f}\")\n",
    "\n",
    "    del model, probs, ds_test, dl_test, G_raw, G_np\n",
    "    gc.collect()\n",
    "\n",
    "# Ensemble mean\n",
    "test_prob_ens = test_prob_folds.mean(axis=1).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save artifacts\n",
    "# ----------------------------\n",
    "fold_path = ART_DIR / \"test_prob_fold.npy\"\n",
    "ens_path  = ART_DIR / \"test_prob_ens.npy\"\n",
    "csv_path  = ART_DIR / \"test_prob_ens.csv\"\n",
    "\n",
    "np.save(fold_path, test_prob_folds)\n",
    "np.save(ens_path, test_prob_ens)\n",
    "\n",
    "df_pred = pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens})\n",
    "df_pred.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"\\n[Stage 10] DONE\")\n",
    "print(f\"- Saved fold probs: {fold_path}\")\n",
    "print(f\"- Saved ens probs : {ens_path}\")\n",
    "print(f\"- Saved csv       : {csv_path}\")\n",
    "print(f\"- ens mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n",
    "\n",
    "# Export globals for submission\n",
    "globals().update({\n",
    "    \"test_ids\": test_ids,\n",
    "    \"test_prob_folds\": test_prob_folds,\n",
    "    \"test_prob_ens\": test_prob_ens,\n",
    "    \"TEST_PROB_FOLD_PATH\": fold_path,\n",
    "    \"TEST_PROB_ENS_PATH\": ens_path,\n",
    "    \"TEST_PROB_CSV_PATH\": csv_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ecacb9",
   "metadata": {
    "papermill": {
     "duration": 0.013727,
     "end_time": "2026-01-02T09:40:48.844274",
     "exception": false,
     "start_time": "2026-01-02T09:40:48.830547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "391e75da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:40:48.873384Z",
     "iopub.status.busy": "2026-01-02T09:40:48.873052Z",
     "iopub.status.idle": "2026-01-02T09:40:49.121557Z",
     "shell.execute_reply": "2026-01-02T09:40:49.120690Z"
    },
    "papermill": {
     "duration": 0.26564,
     "end_time": "2026-01-02T09:40:49.123368",
     "exception": false,
     "start_time": "2026-01-02T09:40:48.857728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION (OOF) — F1 score metric\n",
      "- N=3,043 | pos=148 | neg=2,895 | pos%=4.8636%\n",
      "\n",
      "Baseline @ thr=0.5\n",
      "- F1=0.099182 | P=0.053650 | R=0.655405 | tp=97 fp=1711 fn=51 | pos_pred=1808\n",
      "\n",
      "BEST @ thr=0.510000\n",
      "- F1=0.101343 | P=0.055705 | R=0.560811 | tp=83 fp=1407 fn=65 | pos_pred=1490\n",
      "\n",
      "Top 10 thresholds:\n",
      "01. thr=0.510000 | f1=0.101343 | P=0.055705 | R=0.560811 | tp=83 fp=1407 fn=65 | pos_pred=1490\n",
      "02. thr=0.509971 | f1=0.101281 | P=0.055667 | R=0.560811 | tp=83 fp=1408 fn=65 | pos_pred=1491\n",
      "03. thr=0.496765 | f1=0.101229 | P=0.054584 | R=0.695946 | tp=103 fp=1784 fn=45 | pos_pred=1887\n",
      "04. thr=0.512106 | f1=0.100775 | P=0.055714 | R=0.527027 | tp=78 fp=1322 fn=70 | pos_pred=1400\n",
      "05. thr=0.510584 | f1=0.100684 | P=0.055441 | R=0.547297 | tp=81 fp=1380 fn=67 | pos_pred=1461\n",
      "06. thr=0.511362 | f1=0.100127 | P=0.055245 | R=0.533784 | tp=79 fp=1351 fn=69 | pos_pred=1430\n",
      "07. thr=0.539344 | f1=0.100106 | P=0.059418 | R=0.317568 | tp=47 fp=744 fn=101 | pos_pred=791\n",
      "08. thr=0.498222 | f1=0.099800 | P=0.053879 | R=0.675676 | tp=100 fp=1756 fn=48 | pos_pred=1856\n",
      "09. thr=0.495721 | f1=0.099758 | P=0.053730 | R=0.695946 | tp=103 fp=1814 fn=45 | pos_pred=1917\n",
      "10. thr=0.459195 | f1=0.099514 | P=0.052673 | R=0.898649 | tp=133 fp=2392 fn=15 | pos_pred=2525\n",
      "\n",
      "Saved:\n",
      "- /kaggle/working/mallorn_run/oof/eval_report.txt\n",
      "- /kaggle/working/mallorn_run/oof/eval_threshold_table.csv\n",
      "- /kaggle/working/mallorn_run/oof/eval_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ONE CELL — EVALUATION (Precision / Recall / F1) + Threshold Sweep (OOF)\n",
    "# Sesuai materi:\n",
    "#   Precision = TP / (TP + FP)\n",
    "#   Recall    = TP / (TP + FN)\n",
    "#   F1        = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "#\n",
    "# Input (minimal):\n",
    "# - df_train_meta (kolom: target)\n",
    "# - oof_prob (globals) ATAU file OOF_DIR/oof_prob.npy atau OOF_DIR/oof_prob.csv\n",
    "#\n",
    "# Output:\n",
    "# - Print ringkasan metrik\n",
    "# - (opsional) simpan: eval_report.txt + eval_threshold_table.csv\n",
    "# ============================================================\n",
    "\n",
    "import gc, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "if \"df_train_meta\" not in globals():\n",
    "    raise RuntimeError(\"Missing df_train_meta. Jalankan STAGE 2 dulu.\")\n",
    "\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "OOF_DIR = Path(globals().get(\"OOF_DIR\", ART_DIR / \"oof\"))\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Robust loader for oof_prob (1D float32)\n",
    "# ----------------------------\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def load_oof_prob():\n",
    "    # globals\n",
    "    if \"oof_prob\" in globals():\n",
    "        op = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "            return op, None  # no csv df\n",
    "    # npy\n",
    "    p = OOF_DIR / \"oof_prob.npy\"\n",
    "    if p.exists():\n",
    "        op = _as_1d_float32(np.load(p, allow_pickle=False))\n",
    "        if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "            return op, None\n",
    "    # csv\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if \"oof_prob\" in df.columns:\n",
    "            op = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n",
    "            if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "                return op, df\n",
    "    raise FileNotFoundError(\"OOF probability tidak ditemukan (globals / oof_prob.npy / oof_prob.csv). Jalankan STAGE 8 dulu.\")\n",
    "\n",
    "oof_prob, df_oof_csv = load_oof_prob()\n",
    "\n",
    "if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "    raise TypeError(f\"Invalid oof_prob (scalar/unsized). type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Align y (target) ke urutan oof_prob\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "y = None\n",
    "\n",
    "# Paling robust: pakai oof_prob.csv kalau ada object_id\n",
    "if df_oof_csv is not None and (\"object_id\" in df_oof_csv.columns):\n",
    "    df_oof_csv[\"object_id\"] = df_oof_csv[\"object_id\"].astype(str).str.strip()\n",
    "    train_ids = df_oof_csv[\"object_id\"].tolist()\n",
    "    # override oof_prob agar pasti align dengan object_id pada csv\n",
    "    oof_prob = _as_1d_float32(df_oof_csv[\"oof_prob\"].to_numpy())\n",
    "    y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Kalau tidak ada CSV, coba train_ids_ordered (STAGE 7/8)\n",
    "if y is None and (\"train_ids_ordered\" in globals()):\n",
    "    _ids = list(globals()[\"train_ids_ordered\"])\n",
    "    if len(_ids) == len(oof_prob):\n",
    "        train_ids = _ids\n",
    "        y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Last fallback: df_train_meta order harus match length\n",
    "if y is None:\n",
    "    if len(oof_prob) != len(df_train_meta):\n",
    "        raise RuntimeError(\n",
    "            f\"Tidak bisa align y. len(oof_prob)={len(oof_prob)} != len(df_train_meta)={len(df_train_meta)} \"\n",
    "            \"dan tidak ada oof_prob.csv yang menyertakan object_id.\"\n",
    "        )\n",
    "    train_ids = df_train_meta.index.astype(str).tolist()\n",
    "    y = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "if len(y) != len(oof_prob):\n",
    "    raise RuntimeError(f\"Length mismatch: y={len(y)} vs oof_prob={len(oof_prob)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Metrics sesuai materi (TP/FP/FN -> P/R/F1)\n",
    "# ----------------------------\n",
    "def prf_from_pred(y_true, y_pred01):\n",
    "    y_true = np.asarray(y_true, dtype=np.int32)\n",
    "    y_pred01 = np.asarray(y_pred01, dtype=np.int32)\n",
    "\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "\n",
    "    precision = tp / max(tp + fp, 1)\n",
    "    recall    = tp / max(tp + fn, 1)\n",
    "    f1 = 0.0 if (precision + recall) == 0 else (2.0 * precision * recall / (precision + recall))\n",
    "\n",
    "    return {\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"pos_pred\": int(y_pred01.sum()),\n",
    "    }\n",
    "\n",
    "def eval_at_threshold(prob, y_true, thr):\n",
    "    pred = (prob >= float(thr)).astype(np.int8)\n",
    "    return prf_from_pred(y_true, pred)\n",
    "\n",
    "# Baseline thr=0.5\n",
    "base = eval_at_threshold(oof_prob, y, 0.5)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Threshold sweep (opsional tapi sangat berguna untuk F1)\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.01, 0.10, 19),\n",
    "    np.linspace(0.10, 0.90, 81),\n",
    "    np.linspace(0.90, 0.99, 19),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.01, 0.99, 99, dtype=np.float32)\n",
    "quant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n",
    "thr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr]), 0.0, 1.0))\n",
    "\n",
    "best = {\"thr\": 0.5, **base}\n",
    "rows = []\n",
    "\n",
    "for thr in thr_candidates:\n",
    "    met = eval_at_threshold(oof_prob, y, thr)\n",
    "    rows.append([float(thr), met[\"f1\"], met[\"precision\"], met[\"recall\"], met[\"tp\"], met[\"fp\"], met[\"fn\"], met[\"pos_pred\"]])\n",
    "\n",
    "    # pilih terbaik: F1 tertinggi, tie-break: recall lebih besar, lalu fp lebih kecil\n",
    "    if (met[\"f1\"] > best[\"f1\"] + 1e-12) or (\n",
    "        abs(met[\"f1\"] - best[\"f1\"]) <= 1e-12 and (met[\"recall\"] > best[\"recall\"] + 1e-12)\n",
    "    ) or (\n",
    "        abs(met[\"f1\"] - best[\"f1\"]) <= 1e-12 and abs(met[\"recall\"] - best[\"recall\"]) <= 1e-12 and (met[\"fp\"] < best[\"fp\"])\n",
    "    ):\n",
    "        best = {\"thr\": float(thr), **met}\n",
    "\n",
    "thr_table = pd.DataFrame(rows, columns=[\"thr\",\"f1\",\"precision\",\"recall\",\"tp\",\"fp\",\"fn\",\"pos_pred\"])\n",
    "thr_table = thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "BEST_THR = float(best[\"thr\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Print report\n",
    "# ----------------------------\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "N = int(len(y))\n",
    "\n",
    "print(\"EVALUATION (OOF) — F1 score metric\")\n",
    "print(f\"- N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.4f}%\\n\")\n",
    "\n",
    "print(\"Baseline @ thr=0.5\")\n",
    "print(f\"- F1={base['f1']:.6f} | P={base['precision']:.6f} | R={base['recall']:.6f} | tp={base['tp']} fp={base['fp']} fn={base['fn']} | pos_pred={base['pos_pred']}\\n\")\n",
    "\n",
    "print(f\"BEST @ thr={BEST_THR:.6f}\")\n",
    "print(f\"- F1={best['f1']:.6f} | P={best['precision']:.6f} | R={best['recall']:.6f} | tp={best['tp']} fp={best['fp']} fn={best['fn']} | pos_pred={best['pos_pred']}\\n\")\n",
    "\n",
    "print(\"Top 10 thresholds:\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    print(f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | P={r['precision']:.6f} | R={r['recall']:.6f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save artifacts (report + table)\n",
    "# ----------------------------\n",
    "out_txt = OOF_DIR / \"eval_report.txt\"\n",
    "out_csv = OOF_DIR / \"eval_threshold_table.csv\"\n",
    "out_json = OOF_DIR / \"eval_summary.json\"\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Evaluation Report (Precision/Recall/F1)\")\n",
    "lines.append(f\"N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"F1={base['f1']:.8f} | P={base['precision']:.8f} | R={base['recall']:.8f} | tp={base['tp']} fp={base['fp']} fn={base['fn']} | pos_pred={base['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST @ thr={BEST_THR:.8f}\")\n",
    "lines.append(f\"F1={best['f1']:.8f} | P={best['precision']:.8f} | R={best['recall']:.8f} | tp={best['tp']} fp={best['fp']} fn={best['fn']} | pos_pred={best['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 thresholds:\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    lines.append(f\"{i+1:02d}. thr={r['thr']:.8f} | f1={r['f1']:.8f} | P={r['precision']:.8f} | R={r['recall']:.8f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "thr_table.to_csv(out_csv, index=False)\n",
    "\n",
    "payload = {\n",
    "    \"N\": N, \"pos\": pos, \"neg\": neg,\n",
    "    \"baseline_thr_0p5\": base,\n",
    "    \"best\": {\"thr\": BEST_THR, **best},\n",
    "}\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved:\")\n",
    "print(f\"- {out_txt}\")\n",
    "print(f\"- {out_csv}\")\n",
    "print(f\"- {out_json}\")\n",
    "\n",
    "# Export for next stages\n",
    "globals().update({\"BEST_THR\": BEST_THR, \"thr_table\": thr_table})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab044682",
   "metadata": {
    "papermill": {
     "duration": 0.013402,
     "end_time": "2026-01-02T09:40:49.150681",
     "exception": false,
     "start_time": "2026-01-02T09:40:49.137279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b93901f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T09:40:49.180116Z",
     "iopub.status.busy": "2026-01-02T09:40:49.179767Z",
     "iopub.status.idle": "2026-01-02T09:40:49.507302Z",
     "shell.execute_reply": "2026-01-02T09:40:49.506420Z"
    },
    "papermill": {
     "duration": 0.344832,
     "end_time": "2026-01-02T09:40:49.509159",
     "exception": false,
     "start_time": "2026-01-02T09:40:49.164327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11] SUBMISSION READY (BINARY 0/1)\n",
      "- threshold_used=0.510000\n",
      "- wrote: /kaggle/working/submission.csv\n",
      "- copy : /kaggle/working/mallorn_run/submissions/submission.csv\n",
      "- debug proba (optional): /kaggle/working/mallorn_run/submissions/submission_proba.csv\n",
      "- rows: 7,135\n",
      "\n",
      "Preview:\n",
      "                   object_id  prediction\n",
      "    Eluwaith_Mithrim_nothrim           0\n",
      "          Eru_heledir_archam           0\n",
      "           Gonhir_anann_fuin           0\n",
      "Gwathuirim_haradrim_tegilbor           1\n",
      "            achas_minai_maen           1\n",
      "               adab_fae_gath           0\n",
      "             adel_draug_gaur           1\n",
      "     aderthad_cuil_galadhrim           1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 11 — Submission Build (ONE CELL, Kaggle CPU-SAFE)\n",
    "#\n",
    "# REVISI: output SESUAI instruksi kompetisi -> prediction HARUS 0/1 (BINARY)\n",
    "# - header: object_id,prediction\n",
    "# - file utama: /kaggle/working/submission.csv\n",
    "# - tetap robust loader untuk test_ids + test_prob_ens\n",
    "# ============================================================\n",
    "\n",
    "import gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"SUB_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n",
    "\n",
    "sample_path = Path(PATHS[\"SAMPLE_SUB\"])\n",
    "if not sample_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n",
    "\n",
    "df_sub = pd.read_csv(sample_path)\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: robust loaders\n",
    "# ----------------------------\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _load_test_prob():\n",
    "    # 1) globals\n",
    "    if \"test_prob_ens\" in globals() and globals()[\"test_prob_ens\"] is not None:\n",
    "        tp = _as_1d_float32(globals()[\"test_prob_ens\"])\n",
    "        if isinstance(tp, np.ndarray) and tp.ndim != 0:\n",
    "            return tp\n",
    "\n",
    "    # 2) artifact path var\n",
    "    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n",
    "        p = Path(globals()[\"TEST_PROB_ENS_PATH\"])\n",
    "        if p.exists():\n",
    "            tp = _as_1d_float32(np.load(p, allow_pickle=False))\n",
    "            if tp.ndim != 0:\n",
    "                return tp\n",
    "\n",
    "    # 3) default artifact location\n",
    "    p = Path(globals().get(\"ART_DIR\", \"/kaggle/working\")) / \"test_prob_ens.npy\"\n",
    "    if p.exists():\n",
    "        tp = _as_1d_float32(np.load(p, allow_pickle=False))\n",
    "        if tp.ndim != 0:\n",
    "            return tp\n",
    "\n",
    "    # 4) csv fallback\n",
    "    p = Path(globals().get(\"ART_DIR\", \"/kaggle/working\")) / \"test_prob_ens.csv\"\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        if \"prob\" in df.columns:\n",
    "            tp = _as_1d_float32(df[\"prob\"].to_numpy())\n",
    "            if tp.ndim != 0:\n",
    "                return tp\n",
    "\n",
    "    raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n",
    "\n",
    "def _load_test_ids():\n",
    "    # 1) globals (must be non-None and iterable)\n",
    "    if \"test_ids\" in globals() and globals()[\"test_ids\"] is not None:\n",
    "        try:\n",
    "            ids = list(globals()[\"test_ids\"])\n",
    "            if len(ids) > 0:\n",
    "                return [str(x).strip() for x in ids]\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "    # 2) fixed_seq cache\n",
    "    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n",
    "    pids = fix_dir / \"test_ids.npy\"\n",
    "    if pids.exists():\n",
    "        ids = np.load(pids, allow_pickle=False).astype(\"S\").astype(str).tolist()\n",
    "        return [str(x).strip() for x in ids]\n",
    "\n",
    "    # 3) from test_prob_ens.csv (if exists)\n",
    "    p = Path(globals().get(\"ART_DIR\", \"/kaggle/working\")) / \"test_prob_ens.csv\"\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        if \"object_id\" in df.columns:\n",
    "            return df[\"object_id\"].astype(str).str.strip().tolist()\n",
    "\n",
    "    raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat test_ids.npy atau STAGE 10 export test_ids.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load predictions + ids\n",
    "# ----------------------------\n",
    "test_prob = _load_test_prob()\n",
    "test_ids  = _load_test_ids()\n",
    "\n",
    "if not isinstance(test_prob, np.ndarray) or test_prob.ndim == 0:\n",
    "    raise TypeError(f\"Invalid test_prob (scalar/unsized). type={type(test_prob)} ndim={getattr(test_prob,'ndim',None)}\")\n",
    "\n",
    "if len(test_prob) != len(test_ids):\n",
    "    raise RuntimeError(f\"Length mismatch: test_prob={len(test_prob)} vs test_ids={len(test_ids)}\")\n",
    "\n",
    "# Threshold (use BEST_THR if available; else 0.5)\n",
    "thr = float(globals().get(\"BEST_THR\", 0.5))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build mapping object_id -> prob + strict checks\n",
    "# ----------------------------\n",
    "df_pred = pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob.astype(np.float32, copy=False)})\n",
    "\n",
    "df_pred[\"object_id\"] = df_pred[\"object_id\"].astype(str).str.strip()\n",
    "if df_pred[\"object_id\"].duplicated().any():\n",
    "    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:5].tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n",
    "\n",
    "p = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "if not np.isfinite(p).all():\n",
    "    bad = int((~np.isfinite(p)).sum())\n",
    "    raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\n",
    "df_pred[\"prob\"] = np.clip(p, 0.0, 1.0)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Align to sample_submission order + BUILD BINARY PREDICTION (0/1)\n",
    "# ----------------------------\n",
    "df_sub[\"object_id\"] = df_sub[\"object_id\"].astype(str).str.strip()\n",
    "\n",
    "df_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if df_out[\"prob\"].isna().any():\n",
    "    missing_n = int(df_out[\"prob\"].isna().sum())\n",
    "    miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:5].tolist()\n",
    "    raise ValueError(\n",
    "        f\"Some sample_submission object_id have no prediction: {missing_n} missing. \"\n",
    "        f\"Examples: {miss_ids}\"\n",
    "    )\n",
    "\n",
    "# REQUIRED BY COMPETITION: binary 0/1\n",
    "df_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\n",
    "df_out = df_out[[\"object_id\", \"prediction\"]]\n",
    "\n",
    "# strict format checks\n",
    "if df_out[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(\"submission has duplicate object_id (unexpected).\")\n",
    "if len(df_out) != len(df_sub):\n",
    "    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n",
    "if not set(np.unique(df_out[\"prediction\"].to_numpy())).issubset({0, 1}):\n",
    "    raise RuntimeError(\"submission prediction contains values outside {0,1} (unexpected).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Write submission files\n",
    "# ----------------------------\n",
    "SUB_DIR = Path(SUB_DIR)\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_main  = Path(\"/kaggle/working/submission.csv\")\n",
    "out_copy  = SUB_DIR / \"submission.csv\"\n",
    "out_proba = SUB_DIR / \"submission_proba.csv\"  # debug only\n",
    "\n",
    "df_out.to_csv(out_main, index=False)\n",
    "df_out.to_csv(out_copy, index=False)\n",
    "\n",
    "# optional debug proba (NOT for Kaggle submit)\n",
    "df_dbg = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n",
    "df_dbg = df_dbg.rename(columns={\"prob\": \"prediction\"})\n",
    "df_dbg.to_csv(out_proba, index=False)\n",
    "\n",
    "print(\"[Stage 11] SUBMISSION READY (BINARY 0/1)\")\n",
    "print(f\"- threshold_used={thr:.6f}\")\n",
    "print(f\"- wrote: {out_main}\")\n",
    "print(f\"- copy : {out_copy}\")\n",
    "print(f\"- debug proba (optional): {out_proba}\")\n",
    "print(f\"- rows: {len(df_out):,}\")\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(df_out.head(8).to_string(index=False))\n",
    "\n",
    "globals().update({\n",
    "    \"SUBMISSION_PATH\": out_main,\n",
    "    \"SUBMISSION_COPY_PATH\": out_copy,\n",
    "    \"SUBMISSION_MODE\": \"binary\",\n",
    "    \"SUBMISSION_THRESHOLD\": thr,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8925232,
     "sourceId": 14010596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1927.23778,
   "end_time": "2026-01-02T09:40:52.567611",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-02T09:08:45.329831",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
