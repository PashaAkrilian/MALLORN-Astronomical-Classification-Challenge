{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14010596,"sourceType":"datasetVersion","datasetId":8925232}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Kaggle CPU Environment Setup","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Kaggle CPU Environment Setup (ONE CELL) — REVISI FULL v7.0\n# Fokus v7.0 (upgrade maksimal sesuai saran):\n# - Auto-discover DATA_ROOT lebih robust (cek 20 split)\n# - Validasi sample_submission vs test_log (set & count) + duplicate guard\n# - Mapping object_id -> split + meta flags (EBV/Z/Z_err missing)\n# - FAIL-FAST lightcurve validation (mode: \"sample\" / \"full\") per split\n# - Coverage summary per split (n_obj, pos%, EBV/Z stats) + lightcurve sample stats\n# - Simpan run_diagnostics.json (health report) untuk deteksi silent-fallback\n# - Resume-friendly (config hash + folder versioned)\n# ============================================================\n\nimport os, sys, gc, json, time, random, hashlib, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# Repro\n# ----------------------------\nSEED = 2025\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\n# ----------------------------\n# CPU thread limits (anti-freeze)\n# ----------------------------\nTHREADS = 2\nfor k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n    os.environ.setdefault(k, str(THREADS))\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\ntry:\n    import torch\n    torch.manual_seed(SEED)\n    torch.set_num_threads(THREADS)\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    torch = None\n\n# ----------------------------\n# Helpers\n# ----------------------------\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\ndef _must_exist(p: Path, what: str):\n    if not p.exists():\n        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef _normalize_split(x):\n    if pd.isna(x):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    if s.isdigit():\n        return f\"split_{int(s):02d}\"\n    s = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s.startswith(\"split_\"):\n        tail = s.split(\"split_\", 1)[1].strip(\"_\")\n        if tail.isdigit():\n            return f\"split_{int(tail):02d}\"\n    return s\n\ndef _discover_data_root(default_root: Path) -> Path:\n    \"\"\"\n    Cari folder di /kaggle/input yang punya:\n    - train_log.csv, test_log.csv, sample_submission.csv\n    - split_01..split_20 (minimal split_01 dan split_20)\n    \"\"\"\n    if default_root.exists():\n        return default_root\n    base = Path(\"/kaggle/input\")\n    if not base.exists():\n        return default_root  # nanti akan error di _must_exist\n    candidates = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"train_log.csv\").exists() and (d / \"test_log.csv\").exists() and (d / \"sample_submission.csv\").exists():\n            # cek split minimal\n            has_01 = (d / \"split_01\").exists() or (d / \"split_1\").exists()\n            has_20 = (d / \"split_20\").exists() or (d / \"split_20\").exists()\n            if has_01 and has_20:\n                candidates.append(d)\n    if len(candidates) == 1:\n        return candidates[0]\n    if len(candidates) > 1:\n        # pilih yang punya split_01 paling jelas & nama stabil\n        candidates = sorted(candidates, key=lambda x: (not (x / \"split_01\").exists(), x.name))\n        return candidates[0]\n    return default_root\n\ndef _hash_cfg(d: dict) -> str:\n    s = json.dumps(d, sort_keys=True)\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:10]\n\ndef _safe_float(x, default=0.0):\n    try:\n        v = float(x)\n        if np.isfinite(v):\n            return v\n    except Exception:\n        pass\n    return float(default)\n\ndef _sample_ids_per_split(df_log: pd.DataFrame, split_name: str, n: int, seed: int) -> list:\n    s = df_log.loc[df_log[\"split\"] == split_name, \"object_id\"].astype(str)\n    if len(s) == 0:\n        return []\n    if len(s) <= n:\n        return s.tolist()\n    return s.sample(n=n, random_state=seed).tolist()\n\ndef _scan_lightcurve_for_ids(csv_path: Path, target_ids: set, chunk_rows: int = 200_000):\n    \"\"\"\n    Scan streaming untuk memastikan object_id target benar-benar muncul di file lightcurve.\n    Kembalikan:\n      found_ids, obs_count_by_id, band_mask_by_id\n    \"\"\"\n    found = set()\n    obs_count = {oid: 0 for oid in target_ids}\n    band_mask = {oid: 0 for oid in target_ids}  # bitmask u,g,r,i,z,y (0..5)\n\n    band_to_bit = {\"u\": 1<<0, \"g\": 1<<1, \"r\": 1<<2, \"i\": 1<<3, \"z\": 1<<4, \"y\": 1<<5}\n\n    usecols = [\"object_id\", \"Filter\"]\n    # dtype \"string\" untuk object_id supaya konsisten; Filter bisa category/string\n    it = pd.read_csv(\n        csv_path,\n        usecols=usecols,\n        dtype={\"object_id\": \"string\"},\n        chunksize=chunk_rows,\n        **SAFE_READ_KW\n    )\n\n    for chunk in it:\n        chunk = _norm_cols(chunk)\n        if \"object_id\" not in chunk.columns or \"Filter\" not in chunk.columns:\n            raise ValueError(f\"[BAD LIGHTCURVE COLUMNS] {csv_path.name}: {list(chunk.columns)}\")\n\n        # normalize Filter\n        f = chunk[\"Filter\"].astype(\"string\").str.strip().str.lower()\n        oid = chunk[\"object_id\"].astype(\"string\").str.strip()\n\n        mask = oid.isin(target_ids)\n        if not mask.any():\n            continue\n\n        sub = pd.DataFrame({\"object_id\": oid[mask].astype(str).values, \"Filter\": f[mask].astype(str).values})\n        # update\n        for o, flt in zip(sub[\"object_id\"].values, sub[\"Filter\"].values):\n            found.add(o)\n            obs_count[o] = obs_count.get(o, 0) + 1\n            band_mask[o] = band_mask.get(o, 0) | band_to_bit.get(flt, 0)\n\n        if len(found) == len(target_ids):\n            break\n\n    return found, obs_count, band_mask\n\ndef _full_scan_lightcurve_object_ids(csv_path: Path, chunk_rows: int = 200_000):\n    \"\"\"\n    Full scan (lebih berat): kumpulkan semua object_id unik yang muncul di file.\n    Dipakai hanya jika CFG['STAGE0_LC_VALIDATE_MODE'] == 'full'\n    \"\"\"\n    found = set()\n    it = pd.read_csv(\n        csv_path,\n        usecols=[\"object_id\"],\n        dtype={\"object_id\": \"string\"},\n        chunksize=chunk_rows,\n        **SAFE_READ_KW\n    )\n    for chunk in it:\n        chunk = _norm_cols(chunk)\n        ids = chunk[\"object_id\"].astype(\"string\").str.strip()\n        found.update(ids.dropna().astype(str).unique().tolist())\n    return found\n\n# ----------------------------\n# PATHS (auto-discovery)\n# ----------------------------\nDEFAULT_DATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\nDATA_ROOT = _discover_data_root(DEFAULT_DATA_ROOT)\n\nPATHS = {\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n}\n\n# ----------------------------\n# WORKDIR (versioned run)\n# ----------------------------\nWORKDIR = Path(\"/kaggle/working\")\nBASE_RUN_DIR = WORKDIR / \"mallorn_run\"\nBASE_RUN_DIR.mkdir(parents=True, exist_ok=True)\n\nCFG = {\n    # Pipeline toggles\n    \"USE_GBDT\": True,\n    \"USE_DEEP_LITE\": False,      # CNN binned (CPU-friendly) - optional\n    \"USE_HYBRID_BLEND\": False,   # blend GBDT + DEEP_LITE - optional\n    \"USE_THRESHOLD_TUNING\": True,\n\n    # Feature settings\n    \"USE_ASINH_FLUX\": True,\n    \"SNR_CLIP\": 30.0,\n    \"SNR_DET_THR\": 3.0,\n    \"SNR_STRONG_THR\": 5.0,\n    \"MIN_FLUXERR\": 1e-6,\n\n    # Streaming\n    \"CHUNK_ROWS\": 200_000,\n\n    # CV\n    \"N_FOLDS\": 5,\n    \"CV_STRATIFY\": True,\n    \"CV_USE_SPLIT_COL\": True,    # group=split anti leakage\n\n    # Deep-lite (binned)\n    \"BINS\": 48,\n    \"DEEP_EPOCHS\": 8,\n    \"DEEP_BS\": 128,\n\n    # STAGE0 diagnostics / fail-fast\n    # mode: \"off\" | \"sample\" | \"full\"\n    # - sample: scan lightcurves sampai menemukan sample ids per split (lebih cepat)\n    # - full  : full scan untuk memastikan semua object_id di log benar-benar ada di file (lebih berat)\n    \"STAGE0_LC_VALIDATE_MODE\": \"sample\",\n    \"STAGE0_LC_SAMPLE_PER_SPLIT\": 40,   # per split per train/test (mode sample)\n    \"STAGE0_FAIL_FAST_MISSING_RATE\": 0.01,  # kalau missing_rate > 1% => stop\n}\n\nCFG_HASH = _hash_cfg(CFG)\nRUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\nRUN_DIR = BASE_RUN_DIR / f\"run_{RUN_TAG}_{CFG_HASH}\"\n\nART_DIR   = RUN_DIR / \"artifacts\"\nCACHE_DIR = RUN_DIR / \"cache\"\nOOF_DIR   = RUN_DIR / \"oof\"\nSUB_DIR   = RUN_DIR / \"submissions\"\nLOG_DIR   = RUN_DIR / \"logs\"\nFEAT_DIR  = CACHE_DIR / \"features\"\nSEQ_DIR   = CACHE_DIR / \"seq\"\n\nfor d in [RUN_DIR, ART_DIR, CACHE_DIR, OOF_DIR, SUB_DIR, LOG_DIR, FEAT_DIR, SEQ_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Validate existence (files + split folders)\n# ----------------------------\n_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n_must_exist(PATHS[\"TRAIN_LOG\"],  \"train_log.csv\")\n_must_exist(PATHS[\"TEST_LOG\"],   \"test_log.csv\")\n\nfor sd in PATHS[\"SPLITS\"]:\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    _must_exist(tr, f\"{sd.name}/train_full_lightcurves.csv\")\n    _must_exist(te, f\"{sd.name}/test_full_lightcurves.csv\")\n\n# ----------------------------\n# Load logs + sample\n# ----------------------------\ndf_sub = _norm_cols(pd.read_csv(PATHS[\"SAMPLE_SUB\"], dtype={\"object_id\": \"string\"}, **SAFE_READ_KW))\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission must have object_id,prediction. Found: {list(df_sub.columns)}\")\n\n# sample_submission duplicate check\nif df_sub[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in sample_submission: {int(df_sub['object_id'].duplicated().sum())}\")\n\ndf_train_log = _norm_cols(pd.read_csv(PATHS[\"TRAIN_LOG\"], dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\ndf_test_log  = _norm_cols(pd.read_csv(PATHS[\"TEST_LOG\"],  dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n\nneed_train = {\"object_id\",\"EBV\",\"Z\",\"split\",\"target\"}\nneed_test  = {\"object_id\",\"EBV\",\"Z\",\"split\"}\nmissing_train = sorted(list(need_train - set(df_train_log.columns)))\nmissing_test  = sorted(list(need_test - set(df_test_log.columns)))\nif missing_train:\n    raise ValueError(f\"train_log missing: {missing_train}\")\nif missing_test:\n    raise ValueError(f\"test_log missing: {missing_test}\")\n\n# Normalize split\ndf_train_log[\"split\"] = df_train_log[\"split\"].map(_normalize_split)\ndf_test_log[\"split\"]  = df_test_log[\"split\"].map(_normalize_split)\n\nvalid_splits = {f\"split_{i:02d}\" for i in range(1, 21)}\nbad_tr = sorted(set(df_train_log[\"split\"]) - valid_splits)\nbad_te = sorted(set(df_test_log[\"split\"]) - valid_splits)\nif bad_tr:\n    raise ValueError(f\"Invalid split in train_log (examples): {bad_tr[:10]}\")\nif bad_te:\n    raise ValueError(f\"Invalid split in test_log  (examples): {bad_te[:10]}\")\n\n# Numeric coercion + robust missing handling (EBV/Z)\nfor col in [\"EBV\", \"Z\"]:\n    df_train_log[col] = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n    df_test_log[col]  = pd.to_numeric(df_test_log[col],  errors=\"coerce\")\n\n# Missing flags\ndf_train_log[\"EBV_missing\"] = df_train_log[\"EBV\"].isna().astype(\"int8\")\ndf_train_log[\"Z_missing\"]   = df_train_log[\"Z\"].isna().astype(\"int8\")\ndf_test_log[\"EBV_missing\"]  = df_test_log[\"EBV\"].isna().astype(\"int8\")\ndf_test_log[\"Z_missing\"]    = df_test_log[\"Z\"].isna().astype(\"int8\")\n\n# Impute using TRAIN medians (avoid test leakage)\nebv_med = float(df_train_log[\"EBV\"].median(skipna=True)) if df_train_log[\"EBV\"].notna().any() else 0.0\nz_med   = float(df_train_log[\"Z\"].median(skipna=True))   if df_train_log[\"Z\"].notna().any()   else 0.0\n\ndf_train_log[\"EBV\"] = df_train_log[\"EBV\"].fillna(ebv_med)\ndf_train_log[\"Z\"]   = df_train_log[\"Z\"].fillna(z_med)\ndf_test_log[\"EBV\"]  = df_test_log[\"EBV\"].fillna(ebv_med)\ndf_test_log[\"Z\"]    = df_test_log[\"Z\"].fillna(z_med)\n\n# target handling\ndf_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\")\nif df_train_log[\"target\"].isna().any():\n    raise ValueError(f\"train_log target NaN after coercion: {int(df_train_log['target'].isna().sum())}\")\nu = set(pd.unique(df_train_log[\"target\"]).tolist())\nif not u.issubset({0, 1}):\n    raise ValueError(f\"train_log target must be 0/1. Found: {sorted(list(u))}\")\n\n# Z_err handling (test may have; train typically not)\nif \"Z_err\" not in df_test_log.columns:\n    df_test_log[\"Z_err\"] = np.nan\ndf_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\ndf_test_log[\"has_zerr\"] = (~df_test_log[\"Z_err\"].isna()).astype(\"int8\")\ndf_test_log[\"Z_err\"] = df_test_log[\"Z_err\"].fillna(0.0)\n\nif \"Z_err\" not in df_train_log.columns:\n    df_train_log[\"Z_err\"] = 0.0\ndf_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\").fillna(0.0)\ndf_train_log[\"has_zerr\"] = np.zeros(len(df_train_log), dtype=np.int8)\n\n# Uniqueness in logs\nif df_train_log[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in train_log: {int(df_train_log['object_id'].duplicated().sum())}\")\nif df_test_log[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in test_log:  {int(df_test_log['object_id'].duplicated().sum())}\")\n\n# sample_submission alignment check (set & count)\nsub_ids  = df_sub[\"object_id\"].astype(\"string\").str.strip()\ntest_ids = df_test_log[\"object_id\"].astype(\"string\").str.strip()\n\nif len(sub_ids) != len(test_ids):\n    raise ValueError(f\"Row mismatch: sample_submission={len(sub_ids)} vs test_log={len(test_ids)}\")\n\ns_sub = set(sub_ids.tolist())\ns_tst = set(test_ids.tolist())\nif s_sub != s_tst:\n    missing_in_test = list(s_sub - s_tst)[:5]\n    missing_in_sub  = list(s_tst - s_sub)[:5]\n    raise ValueError(\n        \"sample_submission and test_log object_id set mismatch.\\n\"\n        f\"- sample not in test_log (up to5): {missing_in_test}\\n\"\n        f\"- test_log not in sample (up to5): {missing_in_sub}\"\n    )\n\nSUB_ORDER = sub_ids.tolist()\n\n# ----------------------------\n# Build mappings (resume friendly)\n# ----------------------------\nOID2SPLIT_TRAIN = dict(zip(df_train_log[\"object_id\"].astype(str), df_train_log[\"split\"].astype(str)))\nOID2SPLIT_TEST  = dict(zip(df_test_log[\"object_id\"].astype(str),  df_test_log[\"split\"].astype(str)))\n\n# ----------------------------\n# Health report: per split meta summary\n# ----------------------------\nsplit_rows = []\nfor sp in sorted(valid_splits):\n    tr = df_train_log[df_train_log[\"split\"] == sp]\n    te = df_test_log[df_test_log[\"split\"] == sp]\n    pos = int((tr[\"target\"] == 1).sum())\n    tot = int(len(tr))\n    split_rows.append({\n        \"split\": sp,\n        \"train_n\": tot,\n        \"train_pos\": pos,\n        \"train_pos_pct\": (pos / max(tot, 1)) * 100.0,\n        \"test_n\": int(len(te)),\n        \"train_Z_med\": _safe_float(tr[\"Z\"].median(), 0.0) if tot else 0.0,\n        \"train_EBV_med\": _safe_float(tr[\"EBV\"].median(), 0.0) if tot else 0.0,\n    })\ndf_split_summary = pd.DataFrame(split_rows).sort_values(\"split\").reset_index(drop=True)\n\n# ----------------------------\n# FAIL-FAST Lightcurve validation (off/sample/full)\n# ----------------------------\nlc_mode = str(CFG.get(\"STAGE0_LC_VALIDATE_MODE\", \"sample\")).lower().strip()\nlc_sample_n = int(CFG.get(\"STAGE0_LC_SAMPLE_PER_SPLIT\", 40))\nchunk_rows = int(CFG.get(\"CHUNK_ROWS\", 200_000))\nfail_rate = float(CFG.get(\"STAGE0_FAIL_FAST_MISSING_RATE\", 0.01))\n\nlc_diag = {\n    \"mode\": lc_mode,\n    \"sample_per_split\": lc_sample_n,\n    \"chunk_rows\": chunk_rows,\n    \"missing_train_ids\": [],\n    \"missing_test_ids\": [],\n    \"missing_train_count\": 0,\n    \"missing_test_count\": 0,\n    \"missing_train_rate\": 0.0,\n    \"missing_test_rate\": 0.0,\n    \"sample_obs_stats\": {},   # per split, per train/test\n    \"sample_band_coverage\": {} # per split\n}\n\nif lc_mode not in [\"off\", \"sample\", \"full\"]:\n    raise ValueError(\"CFG['STAGE0_LC_VALIDATE_MODE'] must be one of: off/sample/full\")\n\nif lc_mode != \"off\":\n    print(\"\\n[STAGE0] Lightcurve validation:\", lc_mode)\n\n    missing_train = []\n    missing_test = []\n    sample_obs_stats = {}\n    sample_band_cov = {}\n\n    for sp in sorted(valid_splits):\n        split_dir = DATA_ROOT / sp\n        tr_path = split_dir / \"train_full_lightcurves.csv\"\n        te_path = split_dir / \"test_full_lightcurves.csv\"\n\n        # ids in log\n        tr_ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(str).tolist()\n        te_ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(str).tolist()\n\n        if lc_mode == \"full\":\n            # Full scan: collect unique ids from lightcurve file\n            found_tr = _full_scan_lightcurve_object_ids(tr_path, chunk_rows=chunk_rows) if len(tr_ids) else set()\n            found_te = _full_scan_lightcurve_object_ids(te_path, chunk_rows=chunk_rows) if len(te_ids) else set()\n\n            miss_tr = sorted(list(set(tr_ids) - found_tr))\n            miss_te = sorted(list(set(te_ids) - found_te))\n\n            if miss_tr:\n                missing_train.extend([(sp, x) for x in miss_tr[:50]])  # cap\n            if miss_te:\n                missing_test.extend([(sp, x) for x in miss_te[:50]])   # cap\n\n            # stats (full mode tidak hitung obs per id karena mahal; cukup count)\n            sample_obs_stats[sp] = {\n                \"full_mode\": True,\n                \"train_missing\": len(miss_tr),\n                \"test_missing\": len(miss_te),\n            }\n            sample_band_cov[sp] = {\"full_mode\": True}\n\n        else:\n            # Sample mode: sample IDs per split, lalu scan sampai semua sample ditemukan\n            tr_s = _sample_ids_per_split(df_train_log, sp, lc_sample_n, SEED)\n            te_s = _sample_ids_per_split(df_test_log,  sp, lc_sample_n, SEED + 1)\n\n            band_cov_bits = 0\n            # train sample scan\n            if len(tr_s):\n                found, obs_count, band_mask = _scan_lightcurve_for_ids(tr_path, set(tr_s), chunk_rows=chunk_rows)\n                miss = [x for x in tr_s if x not in found]\n                missing_train.extend([(sp, x) for x in miss])\n                # obs stats (sample)\n                counts = [obs_count.get(x, 0) for x in tr_s]\n                bandbits = [band_mask.get(x, 0) for x in tr_s]\n                band_cov_bits |= int(np.bitwise_or.reduce(bandbits)) if len(bandbits) else 0\n\n                sample_obs_stats.setdefault(sp, {})\n                sample_obs_stats[sp].update({\n                    \"train_sample_n\": len(tr_s),\n                    \"train_sample_missing\": len(miss),\n                    \"train_sample_obs_min\": int(np.min(counts)) if len(counts) else 0,\n                    \"train_sample_obs_med\": float(np.median(counts)) if len(counts) else 0.0,\n                    \"train_sample_obs_p95\": float(np.percentile(counts, 95)) if len(counts) else 0.0,\n                })\n            # test sample scan\n            if len(te_s):\n                found, obs_count, band_mask = _scan_lightcurve_for_ids(te_path, set(te_s), chunk_rows=chunk_rows)\n                miss = [x for x in te_s if x not in found]\n                missing_test.extend([(sp, x) for x in miss)\n                ]\n                counts = [obs_count.get(x, 0) for x in te_s]\n                bandbits = [band_mask.get(x, 0) for x in te_s]\n                band_cov_bits |= int(np.bitwise_or.reduce(bandbits)) if len(bandbits) else 0\n\n                sample_obs_stats.setdefault(sp, {})\n                sample_obs_stats[sp].update({\n                    \"test_sample_n\": len(te_s),\n                    \"test_sample_missing\": len(miss),\n                    \"test_sample_obs_min\": int(np.min(counts)) if len(counts) else 0,\n                    \"test_sample_obs_med\": float(np.median(counts)) if len(counts) else 0.0,\n                    \"test_sample_obs_p95\": float(np.percentile(counts, 95)) if len(counts) else 0.0,\n                })\n\n            # decode band coverage bits\n            bit_to_band = [(1<<0,\"u\"),(1<<1,\"g\"),(1<<2,\"r\"),(1<<3,\"i\"),(1<<4,\"z\"),(1<<5,\"y\")]\n            bands_present = [b for bit,b in bit_to_band if (band_cov_bits & bit)]\n            sample_band_cov[sp] = {\n                \"bands_present_in_sample\": bands_present,\n                \"bands_present_count\": len(bands_present),\n            }\n\n    # summarize missing\n    lc_diag[\"missing_train_ids\"] = missing_train[:200]\n    lc_diag[\"missing_test_ids\"] = missing_test[:200]\n    lc_diag[\"missing_train_count\"] = len(missing_train)\n    lc_diag[\"missing_test_count\"] = len(missing_test)\n\n    # Missing rates (sample mode rate is over sampled ids; full mode is true)\n    if lc_mode == \"full\":\n        # in full mode, missing lists capped; we can't compute true missing rate from capped list\n        # so we recompute exact counts by re-checking: expensive; skip and rely on per-split stats\n        lc_diag[\"missing_train_rate\"] = None\n        lc_diag[\"missing_test_rate\"]  = None\n    else:\n        # sample totals\n        total_train_sample = sum(v.get(\"train_sample_n\", 0) for v in sample_obs_stats.values())\n        total_test_sample  = sum(v.get(\"test_sample_n\", 0)  for v in sample_obs_stats.values())\n        lc_diag[\"missing_train_rate\"] = (len(missing_train) / max(total_train_sample, 1))\n        lc_diag[\"missing_test_rate\"]  = (len(missing_test)  / max(total_test_sample, 1))\n\n    lc_diag[\"sample_obs_stats\"] = sample_obs_stats\n    lc_diag[\"sample_band_coverage\"] = sample_band_cov\n\n    # FAIL-FAST (sample mode)\n    if lc_mode == \"sample\":\n        if lc_diag[\"missing_test_rate\"] > fail_rate or lc_diag[\"missing_train_rate\"] > fail_rate:\n            raise RuntimeError(\n                \"[FAIL-FAST] Lightcurve validation indicates missing object_id in lightcurve files.\\n\"\n                f\"- missing_train_rate={lc_diag['missing_train_rate']:.3%}\\n\"\n                f\"- missing_test_rate ={lc_diag['missing_test_rate']:.3%}\\n\"\n                \"Periksa split routing / file path / object_id normalization.\"\n            )\n        if len(missing_test) > 0 or len(missing_train) > 0:\n            # lebih ketat: idealnya 0; tapi biarkan lewat jika sangat kecil dan user mau lanjut\n            print(\"[WARN] Ada sample object_id yang tidak ditemukan di lightcurve file.\")\n            print(\"       Contoh missing_test:\", missing_test[:5])\n            print(\"       Contoh missing_train:\", missing_train[:5])\n\n# ----------------------------\n# Basic counts\n# ----------------------------\npos = int((df_train_log[\"target\"] == 1).sum())\nneg = int((df_train_log[\"target\"] == 0).sum())\ntot = int(len(df_train_log))\n\nprint(\"ENV OK (Kaggle CPU)\")\nprint(f\"- DATA_ROOT: {DATA_ROOT}\")\nprint(f\"- Python: {sys.version.split()[0]}\")\nprint(f\"- Numpy:  {np.__version__}\")\nprint(f\"- Pandas: {pd.__version__}\")\nif torch is not None:\n    print(f\"- Torch:  {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\nelse:\n    print(\"- Torch:  not available\")\n\nprint(\"\\nDATA OK\")\nprint(f\"- train_log objects: {tot:,} | pos={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.2f}%\")\nprint(f\"- test_log objects:  {len(df_test_log):,}\")\nprint(f\"- sample_submission: {len(df_sub):,}\")\nprint(f\"- splits: {len(PATHS['SPLITS'])} folders (01..20)\")\nprint(f\"- RUN_DIR: {RUN_DIR}\")\n\nprint(\"\\nSPLIT SUMMARY (meta)\")\ndisplay(df_split_summary.head(10))\n\n# ----------------------------\n# Save diagnostics snapshot (health report)\n# ----------------------------\ndiag = {\n    \"SEED\": SEED,\n    \"THREADS\": THREADS,\n    \"CFG\": CFG,\n    \"CFG_HASH\": CFG_HASH,\n    \"RUN_DIR\": str(RUN_DIR),\n    \"DATA_ROOT\": str(DATA_ROOT),\n    \"counts\": {\n        \"train_objects\": int(len(df_train_log)),\n        \"train_pos\": int(pos),\n        \"train_neg\": int(neg),\n        \"test_objects\": int(len(df_test_log)),\n        \"sample_rows\": int(len(df_sub)),\n    },\n    \"split_meta_summary\": df_split_summary.to_dict(orient=\"records\"),\n    \"lightcurve_validation\": lc_diag,\n}\n\nwith open(RUN_DIR / \"config_stage0.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"SEED\": SEED, \"THREADS\": THREADS, \"CFG\": CFG, \"CFG_HASH\": CFG_HASH,\n               \"RUN_DIR\": str(RUN_DIR), \"DATA_ROOT\": str(DATA_ROOT)}, f, indent=2)\n\nwith open(RUN_DIR / \"run_diagnostics.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(diag, f, indent=2)\n\n# ----------------------------\n# Export globals\n# ----------------------------\nglobals().update({\n    \"SEED\": SEED, \"THREADS\": THREADS, \"CFG\": CFG, \"CFG_HASH\": CFG_HASH,\n    \"PATHS\": PATHS, \"DATA_ROOT\": DATA_ROOT, \"RUN_DIR\": RUN_DIR,\n    \"ART_DIR\": ART_DIR, \"CACHE_DIR\": CACHE_DIR, \"OOF_DIR\": OOF_DIR,\n    \"SUB_DIR\": SUB_DIR, \"LOG_DIR\": LOG_DIR, \"FEAT_DIR\": FEAT_DIR, \"SEQ_DIR\": SEQ_DIR,\n    \"df_sub\": df_sub, \"df_train_log\": df_train_log, \"df_test_log\": df_test_log,\n    \"OID2SPLIT_TRAIN\": OID2SPLIT_TRAIN, \"OID2SPLIT_TEST\": OID2SPLIT_TEST,\n    \"SUB_ORDER\": SUB_ORDER,\n    \"df_split_summary\": df_split_summary,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Verify Dataset Paths & Split Discovery","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — Split Routing + Lightcurve Micro-Profiling (ONE CELL, CPU-SAFE)\n# REVISI FULL v4.0 (FAIL-FAST + COVERAGE + SNR STATS + MODE SAMPLE/FULL)\n#\n# Upgrade v4.0 (sesuai saran):\n# - Mode validasi lightcurve: off / sample / full (default ikut CFG STAGE0 jika ada)\n# - ID presence scan ADAPTIVE (kalau miss, naikkan chunks sampai cap keras)\n# - FAIL-FAST pakai missing rate agregat (bukan cuma per-file severe miss)\n# - Micro profiling tambah: SNR stats + snr>3/5 fraction + ferr zero/neg guard\n# - Ringkas band coverage & negative flux (signal penting untuk AGN vs TDE)\n# - Output tambahan: logs/lc_id_presence_warnings.csv (contoh missing sample id)\n#\n# Output:\n# - logs/split_routing.csv\n# - logs/lc_sample_stats.csv\n# - logs/lc_id_presence_warnings.csv\n# - logs/stage1_summary.json\n#\n# Catatan:\n# - Stage ini tidak meningkatkan akurasi langsung, tapi mencegah \"empty sequence\"\n#   & memastikan routing + schema aman, yang biasanya jadi penyebab F1 nyangkut.\n# ============================================================\n\nimport re, gc, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require STAGE 0 globals\n# ----------------------------\nneed0 = [\"PATHS\", \"df_train_log\", \"df_test_log\"]\nfor need in need0:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n\nDATA_ROOT = Path(PATHS[\"DATA_ROOT\"])\n\n# deterministic split list (ikuti PATHS[\"SPLITS\"] agar deterministik)\nif \"SPLITS\" not in PATHS or not isinstance(PATHS[\"SPLITS\"], (list, tuple)) or len(PATHS[\"SPLITS\"]) == 0:\n    raise RuntimeError(\"PATHS['SPLITS'] tidak valid. Pastikan STAGE 0 sukses.\")\nSPLIT_LIST = [Path(p).name for p in PATHS[\"SPLITS\"]]\nSPLIT_LIST = [s for s in SPLIT_LIST if s.startswith(\"split_\")]\nSPLIT_LIST = sorted(SPLIT_LIST)  # split_01..split_20\n\nVALID_SPLITS = set([f\"split_{i:02d}\" for i in range(1, 21)])\nif set(SPLIT_LIST) != VALID_SPLITS:\n    raise RuntimeError(\n        \"SPLIT_LIST mismatch dengan expected split_01..split_20.\\n\"\n        f\"Found (first 10): {sorted(list(set(SPLIT_LIST)))[:10]}\"\n    )\n\nSPLIT_DIRS = {Path(p).name: Path(p) for p in PATHS[\"SPLITS\"]}\n\nRUN_DIR = Path(globals().get(\"RUN_DIR\", \"/kaggle/working/mallorn_run\"))\nLOG_DIR = Path(globals().get(\"LOG_DIR\", RUN_DIR / \"logs\"))\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nCFG_LOCAL = globals().get(\"CFG\", {}) or {}\nSEED = int(globals().get(\"SEED\", 2025))\n\n# ----------------------------\n# 1) Safe read config (konsisten dengan STAGE 0)\n# ----------------------------\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\n# sampling knobs (CPU-safe)\nHEAD_ROWS = int(CFG_LOCAL.get(\"LC_HEAD_ROWS\", 4000))              # lebih aman (tetap kecil)\nSAMPLE_ID_PER_SPLIT = int(CFG_LOCAL.get(\"SAMPLE_ID_PER_SPLIT\", 12))\nCHUNK_ROWS = int(CFG_LOCAL.get(\"CHUNK_ROWS\", 200_000))\n\n# scan cap (adaptive)\nMAX_CHUNKS_PER_FILE = int(CFG_LOCAL.get(\"MAX_CHUNKS_PER_FILE\", 6))       # cap awal\nMAX_CHUNKS_HARD = int(CFG_LOCAL.get(\"MAX_CHUNKS_HARD\", 30))              # cap keras adaptive\n\n# numeric policy thresholds\nMAX_TIME_NA_FRAC = float(CFG_LOCAL.get(\"MAX_TIME_NA_FRAC\", 0.02))\nMAX_FERR_NA_FRAC = float(CFG_LOCAL.get(\"MAX_FERR_NA_FRAC\", 0.02))\nMIN_SAMPLE_ROWS  = int(CFG_LOCAL.get(\"MIN_SAMPLE_ROWS\", 200))            # lebih ketat\n\n# ID miss handling\nID_MISS_FAIL_FRAC = float(CFG_LOCAL.get(\"ID_MISS_FAIL_FRAC\", 0.80))       # per-file severe fail\nFAIL_FAST_MISSING_RATE = float(CFG_LOCAL.get(\"STAGE1_FAIL_FAST_MISSING_RATE\", 0.01))  # agregat\n\n# Stage1 validate mode\nLC_VALIDATE_MODE = str(CFG_LOCAL.get(\"STAGE1_LC_VALIDATE_MODE\",\n                                     CFG_LOCAL.get(\"STAGE0_LC_VALIDATE_MODE\", \"sample\"))).lower().strip()\nif LC_VALIDATE_MODE not in [\"off\", \"sample\", \"full\"]:\n    raise ValueError(\"CFG['STAGE1_LC_VALIDATE_MODE'] must be one of: off/sample/full\")\n\n# SNR policy (untuk micro-profiling)\nSNR_CLIP = float(CFG_LOCAL.get(\"SNR_CLIP\", 30.0))\nSNR_DET_THR = float(CFG_LOCAL.get(\"SNR_DET_THR\", 3.0))\nSNR_STRONG_THR = float(CFG_LOCAL.get(\"SNR_STRONG_THR\", 5.0))\nMIN_FLUXERR = float(CFG_LOCAL.get(\"MIN_FLUXERR\", 1e-6))\n\n# ----------------------------\n# 2) Helpers\n# ----------------------------\nREQ_LC_COLS = [\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"]\nREQ_LC_COLS_SET = set([c.strip() for c in REQ_LC_COLS])\nALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef normalize_split_name(x) -> str:\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s2.isdigit():\n        return f\"split_{int(s2):02d}\"\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    return s2\n\ndef sizeof_mb(p: Path) -> float:\n    try:\n        return p.stat().st_size / (1024**2)\n    except Exception:\n        return float(\"nan\")\n\n# cache header map agar tidak baca header berkali-kali\n_HEADER_CACHE = {}\n\ndef _get_usecols(csv_path: Path, required_trimmed: set):\n    key = str(csv_path)\n    if key in _HEADER_CACHE:\n        cols0, trim2orig = _HEADER_CACHE[key]\n    else:\n        df0 = pd.read_csv(csv_path, nrows=0, **SAFE_READ_KW)\n        cols0 = list(df0.columns)\n        trim2orig = {}\n        for c in cols0:\n            ct = str(c).strip()\n            if ct not in trim2orig:\n                trim2orig[ct] = c\n        _HEADER_CACHE[key] = (cols0, trim2orig)\n\n    missing = sorted(list(required_trimmed - set(trim2orig.keys())))\n    if missing:\n        found_trim = sorted(list(trim2orig.keys()))\n        raise ValueError(\n            f\"[LC SCHEMA] {csv_path} missing required columns (trim-aware): {missing}\\n\"\n            f\"Found columns (trimmed, first 50): {found_trim[:50]}\"\n        )\n    usecols = [trim2orig[c] for c in required_trimmed]\n    return usecols\n\ndef _read_sample_df(p: Path, nrows: int):\n    usecols = _get_usecols(p, REQ_LC_COLS_SET)\n    dfh = pd.read_csv(p, usecols=usecols, nrows=nrows, **SAFE_READ_KW)\n    dfh = _norm_cols(dfh)\n    return dfh\n\ndef _numeric_and_filter_stats(dfh: pd.DataFrame):\n    out = {\"n_sample\": int(len(dfh))}\n    if len(dfh) == 0:\n        # extreme\n        out.update({k: np.nan for k in [\n            \"time_na_frac\",\"flux_na_frac\",\"ferr_na_frac\",\n            \"time_min\",\"time_max\",\"time_span\",\n            \"flux_neg_frac\",\"flux_p01\",\"flux_p50\",\"flux_p99\",\n            \"ferr_min\",\"ferr_p50\",\"ferr_p99\",\n            \"snr_abs_p50\",\"snr_abs_p95\",\"snr_gt3_frac\",\"snr_gt5_frac\",\n            \"ferr_zero_frac\"\n        ]})\n        out.update({\"filter_bad\": \"\", \"filter_sample\": \"\"})\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            out[f\"frac_{b}\"] = 0.0\n        out[\"ferr_neg_any\"] = 0\n        return out\n\n    # Filter values\n    filt = dfh[\"Filter\"].astype(\"string\").str.strip().str.lower()\n    filt = filt[~filt.isna()]\n    uniq = sorted(set(filt.tolist()))\n    bad = sorted([v for v in uniq if v not in ALLOWED_FILTERS])\n    out[\"filter_bad\"] = \",\".join(bad[:10]) if bad else \"\"\n    out[\"filter_sample\"] = \",\".join(uniq[:10]) if uniq else \"\"\n\n    # Band coverage from sample\n    if len(filt) > 0:\n        vc = filt.value_counts()\n        denom = float(vc.sum())\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            out[f\"frac_{b}\"] = float(vc.get(b, 0) / denom)\n    else:\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            out[f\"frac_{b}\"] = 0.0\n\n    # Numeric coercion\n    t = pd.to_numeric(dfh[\"Time (MJD)\"], errors=\"coerce\")\n    f = pd.to_numeric(dfh[\"Flux\"], errors=\"coerce\")\n    e = pd.to_numeric(dfh[\"Flux_err\"], errors=\"coerce\")\n\n    out[\"time_na_frac\"] = float(t.isna().mean())\n    out[\"flux_na_frac\"] = float(f.isna().mean())\n    out[\"ferr_na_frac\"] = float(e.isna().mean())\n\n    if (~t.isna()).any():\n        out[\"time_min\"] = float(t.min())\n        out[\"time_max\"] = float(t.max())\n        out[\"time_span\"] = float(t.max() - t.min())\n    else:\n        out[\"time_min\"] = np.nan\n        out[\"time_max\"] = np.nan\n        out[\"time_span\"] = np.nan\n\n    if (~f.isna()).any():\n        fd = f.dropna()\n        out[\"flux_neg_frac\"] = float((fd < 0).mean())\n        out[\"flux_p01\"] = float(np.quantile(fd, 0.01))\n        out[\"flux_p50\"] = float(np.quantile(fd, 0.50))\n        out[\"flux_p99\"] = float(np.quantile(fd, 0.99))\n    else:\n        out[\"flux_neg_frac\"] = np.nan\n        out[\"flux_p01\"] = np.nan\n        out[\"flux_p50\"] = np.nan\n        out[\"flux_p99\"] = np.nan\n\n    if (~e.isna()).any():\n        ed = e.dropna()\n        out[\"ferr_min\"] = float(ed.min())\n        out[\"ferr_p50\"] = float(np.quantile(ed, 0.50))\n        out[\"ferr_p99\"] = float(np.quantile(ed, 0.99))\n        out[\"ferr_neg_any\"] = int((ed < 0).any())\n        out[\"ferr_zero_frac\"] = float((ed <= 0).mean())\n    else:\n        out[\"ferr_min\"] = np.nan\n        out[\"ferr_p50\"] = np.nan\n        out[\"ferr_p99\"] = np.nan\n        out[\"ferr_neg_any\"] = 0\n        out[\"ferr_zero_frac\"] = np.nan\n\n    # SNR stats (micro) - robust terhadap flux negatif\n    # snr = flux / max(err, MIN_FLUXERR)\n    if (~f.isna()).any() and (~e.isna()).any():\n        ff = f.to_numpy()\n        ee = e.to_numpy()\n        m = np.isfinite(ff) & np.isfinite(ee)\n        if m.any():\n            ee = np.maximum(ee[m], MIN_FLUXERR)\n            snr = ff[m] / ee\n            snr = np.clip(snr, -SNR_CLIP, SNR_CLIP)\n            snr_abs = np.abs(snr)\n            out[\"snr_abs_p50\"] = float(np.quantile(snr_abs, 0.50))\n            out[\"snr_abs_p95\"] = float(np.quantile(snr_abs, 0.95))\n            out[\"snr_gt3_frac\"] = float((snr_abs >= SNR_DET_THR).mean())\n            out[\"snr_gt5_frac\"] = float((snr_abs >= SNR_STRONG_THR).mean())\n        else:\n            out[\"snr_abs_p50\"] = np.nan\n            out[\"snr_abs_p95\"] = np.nan\n            out[\"snr_gt3_frac\"] = np.nan\n            out[\"snr_gt5_frac\"] = np.nan\n    else:\n        out[\"snr_abs_p50\"] = np.nan\n        out[\"snr_abs_p95\"] = np.nan\n        out[\"snr_gt3_frac\"] = np.nan\n        out[\"snr_gt5_frac\"] = np.nan\n\n    return out\n\ndef _sample_id_presence_adaptive(csv_path: Path, want_ids: set, chunk_rows: int,\n                                 max_chunks_init: int, max_chunks_hard: int):\n    \"\"\"\n    Limited scan memastikan beberapa object_id dari log benar-benar muncul di file.\n    Adaptive: kalau masih missing, naikkan jumlah chunks sampai cap keras.\n    Scan hanya kolom object_id (murah).\n    \"\"\"\n    if not want_ids:\n        return 0, set(), 0, max_chunks_init\n\n    usecols = _get_usecols(csv_path, {\"object_id\"})\n    remaining = set(want_ids)\n    found = set()\n    total_chunks_read = 0\n    used_cap = max_chunks_init\n\n    cap = max_chunks_init\n    while True:\n        # scan up to 'cap' chunks (or remaining file)\n        nread = 0\n        for i, chunk in enumerate(pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_rows, **SAFE_READ_KW)):\n            if i < total_chunks_read:\n                continue  # skip previously scanned chunks\n            nread += 1\n            total_chunks_read += 1\n            chunk = _norm_cols(chunk)\n            ids = set(chunk[\"object_id\"].astype(\"string\").dropna().str.strip().astype(str).tolist())\n            hit = remaining & ids\n            if hit:\n                found |= hit\n                remaining -= hit\n            if not remaining:\n                break\n            if nread >= cap:\n                break\n\n        used_cap = cap\n        if not remaining:\n            break\n        if total_chunks_read >= max_chunks_hard:\n            break\n        # escalate\n        cap = min(cap * 2, max_chunks_hard)\n\n    return len(found), remaining, total_chunks_read, used_cap\n\ndef _full_scan_lightcurve_object_ids(csv_path: Path, chunk_rows: int):\n    \"\"\"Full scan (mahal): kumpulkan semua object_id unik yang muncul di file.\"\"\"\n    found = set()\n    usecols = _get_usecols(csv_path, {\"object_id\"})\n    for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_rows, **SAFE_READ_KW):\n        chunk = _norm_cols(chunk)\n        ids = chunk[\"object_id\"].astype(\"string\").dropna().str.strip().astype(str).unique().tolist()\n        found.update(ids)\n    return found\n\n# ----------------------------\n# 3) Normalize split col in logs (idempotent)\n# ----------------------------\nfor df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n    if \"split\" not in df.columns:\n        raise ValueError(f\"{name} missing 'split' column.\")\n    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n\nbad_train_split = sorted(set(df_train_log[\"split\"].unique()) - VALID_SPLITS)\nbad_test_split  = sorted(set(df_test_log[\"split\"].unique())  - VALID_SPLITS)\nif bad_train_split:\n    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\nif bad_test_split:\n    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n\n# ----------------------------\n# 4) Verify disk splits set + required files exist\n# ----------------------------\ndisk_splits = set(SPLIT_DIRS.keys())\nmissing_dirs = sorted(list(VALID_SPLITS - disk_splits))\nextra_dirs   = sorted(list(disk_splits - VALID_SPLITS))\nif missing_dirs or extra_dirs:\n    msg = []\n    if missing_dirs: msg.append(f\"Missing split folders: {missing_dirs[:10]}\")\n    if extra_dirs:   msg.append(f\"Unexpected split folders: {extra_dirs[:10]}\")\n    raise RuntimeError(\"Split folder set mismatch.\\n\" + \"\\n\".join(msg))\n\nmissing_files = []\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    for kind in [\"train\", \"test\"]:\n        p = sd / f\"{kind}_full_lightcurves.csv\"\n        if not p.exists():\n            missing_files.append(str(p))\nif missing_files:\n    raise FileNotFoundError(\"Some lightcurve files missing (showing up to 10):\\n\" + \"\\n\".join(missing_files[:10]))\n\n# ----------------------------\n# 5) Build routing manifest\n# ----------------------------\ntrain_counts = df_train_log[\"split\"].value_counts().to_dict()\ntest_counts  = df_test_log[\"split\"].value_counts().to_dict()\n\nrouting_rows = []\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    routing_rows.append({\n        \"split\": sp,\n        \"train_csv\": str(tr),\n        \"test_csv\": str(te),\n        \"train_mb\": sizeof_mb(tr),\n        \"test_mb\": sizeof_mb(te),\n        \"n_train_objects_log\": int(train_counts.get(sp, 0)),\n        \"n_test_objects_log\":  int(test_counts.get(sp, 0)),\n    })\n\ndf_routing = pd.DataFrame(routing_rows)\nrouting_path = LOG_DIR / \"split_routing.csv\"\ndf_routing.to_csv(routing_path, index=False)\n\n# ----------------------------\n# 6) Sample profiling + ID crosscheck\n# ----------------------------\nstats_rows = []\nid_warn_rows = []\nwarn_flux_na_files = 0\n\nt0 = time.time()\n\n# for aggregate missing rate\nagg_id_total = 0\nagg_id_missing = 0\n\nprint(f\"[STAGE1] LC_VALIDATE_MODE={LC_VALIDATE_MODE} | HEAD_ROWS={HEAD_ROWS} | SAMPLE_ID_PER_SPLIT={SAMPLE_ID_PER_SPLIT}\")\n\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    for kind in [\"train\", \"test\"]:\n        p = sd / f\"{kind}_full_lightcurves.csv\"\n\n        # read sample once (trim-aware)\n        dfh = _read_sample_df(p, nrows=HEAD_ROWS)\n\n        # minimum sanity: sample should not be empty\n        if len(dfh) < MIN_SAMPLE_ROWS:\n            raise ValueError(f\"[LC SAMPLE] Too few rows sampled from {p} (n={len(dfh)}). Possible read issue.\")\n\n        # compute stats\n        st = _numeric_and_filter_stats(dfh)\n\n        # Filter sanity\n        if st.get(\"filter_bad\", \"\"):\n            raise ValueError(f\"[LC FILTER] Unexpected Filter values in {p}: {st['filter_bad']} (sample={st.get('filter_sample','')})\")\n\n        # numeric policy\n        if st.get(\"time_na_frac\", 0.0) > MAX_TIME_NA_FRAC:\n            raise ValueError(f\"[LC NUM] Time(MJD) NaN too high in sample: {p} frac={st['time_na_frac']:.4f}\")\n        if st.get(\"ferr_na_frac\", 0.0) > MAX_FERR_NA_FRAC:\n            raise ValueError(f\"[LC NUM] Flux_err NaN too high in sample: {p} frac={st['ferr_na_frac']:.4f}\")\n        if int(st.get(\"ferr_neg_any\", 0)) == 1:\n            raise ValueError(f\"[LC NUM] Negative Flux_err detected in sample of {p} (should be >=0).\")\n\n        if st.get(\"flux_na_frac\", 0.0) > 0:\n            warn_flux_na_files += 1\n\n        # ID crosscheck (optional)\n        id_k = 0\n        id_found = 0\n        id_missing = 0\n        id_scan_chunks = 0\n        id_scan_cap_used = 0\n        miss_ids_list = []\n\n        if LC_VALIDATE_MODE in [\"sample\", \"full\"]:\n            if kind == \"train\":\n                ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(\"string\").dropna().str.strip()\n            else:\n                ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(\"string\").dropna().str.strip()\n\n            if LC_VALIDATE_MODE == \"full\":\n                # Full scan: true coverage (mahal)\n                found_all = _full_scan_lightcurve_object_ids(p, chunk_rows=CHUNK_ROWS)\n                miss = sorted(list(set(ids.astype(str).tolist()) - found_all))\n                id_k = int(len(ids))\n                id_missing = int(len(miss))\n                id_found = id_k - id_missing\n                id_scan_chunks = None\n                id_scan_cap_used = None\n                miss_ids_list = miss[:20]\n\n            else:\n                # Sample scan: cheap, adaptive\n                id_k = int(min(SAMPLE_ID_PER_SPLIT, len(ids)))\n                want = set(ids.sample(n=id_k, random_state=SEED + (0 if kind==\"train\" else 7)).astype(str).tolist()) if id_k > 0 else set()\n\n                found_n, missing_ids, chunks_read, cap_used = _sample_id_presence_adaptive(\n                    p, want, CHUNK_ROWS, MAX_CHUNKS_PER_FILE, MAX_CHUNKS_HARD\n                )\n\n                id_found = int(found_n)\n                id_missing = int(len(missing_ids))\n                id_scan_chunks = int(chunks_read)\n                id_scan_cap_used = int(cap_used)\n                miss_ids_list = list(missing_ids)[:10]\n\n                # severe fail guard (per file)\n                miss_frac = (id_missing / max(id_k, 1)) if id_k else 0.0\n                agg_id_total += id_k\n                agg_id_missing += id_missing\n\n                if id_k and miss_frac >= ID_MISS_FAIL_FRAC:\n                    raise ValueError(\n                        f\"[LC ID] Severe mismatch within adaptive scan: {p} missing {id_missing}/{id_k} \"\n                        f\"(chunks_read={chunks_read}, hard_cap={MAX_CHUNKS_HARD}). Example missing: {miss_ids_list[:3]}\"\n                    )\n\n                if id_k and id_missing > 0:\n                    id_warn_rows.append({\n                        \"split\": sp, \"kind\": kind, \"file\": str(p),\n                        \"k\": id_k, \"missing\": id_missing,\n                        \"chunks_read\": chunks_read, \"cap_used\": cap_used,\n                        \"example_missing\": \",\".join(miss_ids_list[:5]),\n                    })\n\n        # compose row\n        row = {\n            \"split\": sp,\n            \"kind\": kind,\n            \"file\": str(p),\n            \"file_mb\": sizeof_mb(p),\n            \"n_sample\": st.get(\"n_sample\", 0),\n            \"time_na_frac\": st.get(\"time_na_frac\", np.nan),\n            \"flux_na_frac\": st.get(\"flux_na_frac\", np.nan),\n            \"ferr_na_frac\": st.get(\"ferr_na_frac\", np.nan),\n            \"time_min\": st.get(\"time_min\", np.nan),\n            \"time_max\": st.get(\"time_max\", np.nan),\n            \"time_span\": st.get(\"time_span\", np.nan),\n            \"flux_neg_frac\": st.get(\"flux_neg_frac\", np.nan),\n            \"flux_p01\": st.get(\"flux_p01\", np.nan),\n            \"flux_p50\": st.get(\"flux_p50\", np.nan),\n            \"flux_p99\": st.get(\"flux_p99\", np.nan),\n            \"ferr_min\": st.get(\"ferr_min\", np.nan),\n            \"ferr_p50\": st.get(\"ferr_p50\", np.nan),\n            \"ferr_p99\": st.get(\"ferr_p99\", np.nan),\n            \"ferr_zero_frac\": st.get(\"ferr_zero_frac\", np.nan),\n            \"snr_abs_p50\": st.get(\"snr_abs_p50\", np.nan),\n            \"snr_abs_p95\": st.get(\"snr_abs_p95\", np.nan),\n            \"snr_gt3_frac\": st.get(\"snr_gt3_frac\", np.nan),\n            \"snr_gt5_frac\": st.get(\"snr_gt5_frac\", np.nan),\n            \"filter_sample\": st.get(\"filter_sample\", \"\"),\n            \"id_check_k\": int(id_k),\n            \"id_found\": int(id_found),\n            \"id_missing\": int(id_missing),\n            \"id_scan_chunks\": id_scan_chunks,\n            \"id_scan_cap_used\": id_scan_cap_used,\n        }\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            row[f\"frac_{b}\"] = st.get(f\"frac_{b}\", 0.0)\n\n        stats_rows.append(row)\n\n# Save stats\ndf_lc_stats = pd.DataFrame(stats_rows)\nlc_stats_path = LOG_DIR / \"lc_sample_stats.csv\"\ndf_lc_stats.to_csv(lc_stats_path, index=False)\n\ndf_id_warn = pd.DataFrame(id_warn_rows)\nid_warn_path = LOG_DIR / \"lc_id_presence_warnings.csv\"\ndf_id_warn.to_csv(id_warn_path, index=False)\n\n# ----------------------------\n# 7) FAIL-FAST aggregate (sample mode)\n# ----------------------------\nagg_missing_rate = (agg_id_missing / max(agg_id_total, 1)) if (LC_VALIDATE_MODE == \"sample\") else None\nif LC_VALIDATE_MODE == \"sample\":\n    if agg_missing_rate > FAIL_FAST_MISSING_RATE:\n        raise RuntimeError(\n            \"[FAIL-FAST] Aggregate sample-ID missing rate terlalu tinggi.\\n\"\n            f\"- agg_missing_rate={agg_missing_rate:.3%} (missing={agg_id_missing}, total_sample={agg_id_total})\\n\"\n            f\"- threshold={FAIL_FAST_MISSING_RATE:.3%}\\n\"\n            \"Ini indikasi routing/split/object_id normalization bermasalah.\"\n        )\n\n# ----------------------------\n# 8) Summary prints + JSON summary\n# ----------------------------\nelapsed = time.time() - t0\n\nworst_id_missing = (\n    df_lc_stats.sort_values([\"id_missing\", \"id_check_k\"], ascending=[False, False])\n    .head(10)[[\"split\",\"kind\",\"id_missing\",\"id_check_k\",\"id_scan_chunks\",\"id_scan_cap_used\",\"file_mb\"]]\n)\nworst_flux_neg = (\n    df_lc_stats.sort_values(\"flux_neg_frac\", ascending=False)\n    .head(10)[[\"split\",\"kind\",\"flux_neg_frac\",\"snr_gt3_frac\",\"file_mb\"]]\n)\nworst_snr = (\n    df_lc_stats.sort_values(\"snr_gt3_frac\", ascending=False)\n    .head(10)[[\"split\",\"kind\",\"snr_gt3_frac\",\"snr_gt5_frac\",\"snr_abs_p95\",\"file_mb\"]]\n)\n\nsummary = {\n    \"stage\": \"stage1\",\n    \"data_root\": str(DATA_ROOT),\n    \"log_dir\": str(LOG_DIR),\n    \"lc_validate_mode\": LC_VALIDATE_MODE,\n    \"head_rows\": HEAD_ROWS,\n    \"sample_id_per_split\": SAMPLE_ID_PER_SPLIT,\n    \"chunk_rows\": CHUNK_ROWS,\n    \"max_chunks_per_file_init\": MAX_CHUNKS_PER_FILE,\n    \"max_chunks_hard\": MAX_CHUNKS_HARD,\n    \"thresholds\": {\n        \"MAX_TIME_NA_FRAC\": MAX_TIME_NA_FRAC,\n        \"MAX_FERR_NA_FRAC\": MAX_FERR_NA_FRAC,\n        \"ID_MISS_FAIL_FRAC\": ID_MISS_FAIL_FRAC,\n        \"FAIL_FAST_MISSING_RATE\": FAIL_FAST_MISSING_RATE,\n        \"MIN_SAMPLE_ROWS\": MIN_SAMPLE_ROWS\n    },\n    \"aggregate_id_missing\": {\n        \"total_sample_ids\": int(agg_id_total),\n        \"missing_sample_ids\": int(agg_id_missing),\n        \"missing_rate\": float(agg_missing_rate) if agg_missing_rate is not None else None\n    },\n    \"warn_flux_na_files\": int(warn_flux_na_files),\n    \"routing_csv\": str(routing_path),\n    \"lc_sample_stats_csv\": str(lc_stats_path),\n    \"lc_id_presence_warnings_csv\": str(id_warn_path),\n    \"elapsed_sec\": float(elapsed),\n}\n\nsummary_path = LOG_DIR / \"stage1_summary.json\"\nwith open(summary_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"STAGE 1 OK — SPLIT ROUTING READY\")\nprint(f\"- routing saved: {routing_path}\")\nprint(f\"- lc sample stats saved: {lc_stats_path}\")\nprint(f\"- id warnings saved: {id_warn_path}\")\nprint(f\"- summary json saved: {summary_path}\")\nprint(f\"- elapsed: {elapsed/60:.2f} min | warn_flux_na_files={warn_flux_na_files}\")\n\nprint(\"\\nOBJECT COUNTS (from logs)\")\nfor sp in SPLIT_LIST:\n    print(f\"- {sp}: train={int(train_counts.get(sp,0)):,} | test={int(test_counts.get(sp,0)):,}\")\n\nprint(\"\\nTOP ISSUES (ID missing in sample/adaptive scan)\")\nprint(worst_id_missing.to_string(index=False))\n\nprint(\"\\nTOP PATTERN (highest negative flux fraction in sample head)\")\nprint(worst_flux_neg.to_string(index=False))\n\nprint(\"\\nTOP PATTERN (highest high-SNR fraction in sample head)\")\nprint(worst_snr.to_string(index=False))\n\n# ----------------------------\n# 9) Export to globals (dipakai stage berikutnya)\n# ----------------------------\nglobals().update({\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SPLIT_DIRS\": SPLIT_DIRS,\n    \"SPLIT_LIST\": SPLIT_LIST,\n    \"df_split_routing\": df_routing,\n    \"df_lc_sample_stats\": df_lc_stats,\n    \"df_lc_id_presence_warnings\": df_id_warn,\n    \"STAGE1_SUMMARY_PATH\": summary_path,\n})\n\ngc.collect()\nprint(\"\\nStage 1 complete: splits verified + routing/stats exported.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and Validate Train/Test Logs","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — Clean Meta Logs + CV Fold Assignment (ONE CELL, CPU-SAFE)\n# REVISI FULL v5.0 (sesuai saran: group-by-split CV stabil + meta lebih kaya + fail-fast fold)\n#\n# Upgrade v5.0:\n# - CV split-aware yang benar: split->fold dengan MULTI-RESTART greedy (maksimalkan balance pos_rate & count)\n# - FAIL-FAST: pastikan setiap fold punya positive (kalau tidak, auto-restart lebih banyak)\n# - Anti leakage tetap: EBV/Z clip pakai TRAIN; Z_err clip pakai TEST rows yang memang punya zerr (has_zerr==1)\n# - Z fill: split-median TRAIN lalu global median TRAIN (robust)\n# - Feature meta tambahan: inv_1pz, zerr_rel, log1pZ/log1pZerr, split_id\n# - Artifacts lengkap + stage2_summary.json\n# ============================================================\n\nimport re, gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 0/1 globals\n# ----------------------------\nfor need in [\"PATHS\", \"ART_DIR\", \"SPLIT_DIRS\", \"CFG\", \"SEED\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 & STAGE 1 dulu.\")\n\nTRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\nTEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n\nART_DIR = Path(ART_DIR)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nSEED = int(SEED)\nN_FOLDS = int(CFG.get(\"N_FOLDS\", 5))\nCV_USE_SPLIT_COL = bool(CFG.get(\"CV_USE_SPLIT_COL\", True))\n\n# deterministic split list: ikuti PATHS[\"SPLITS\"] (sinkron STAGE 0/1)\nSPLIT_LIST = [Path(p).name for p in PATHS[\"SPLITS\"]]\nSPLIT_LIST = sorted([s for s in SPLIT_LIST if s.startswith(\"split_\")])\nVALID_SPLITS = set(SPLIT_LIST)\n\ndisk_splits = set(SPLIT_DIRS.keys())\nif disk_splits != VALID_SPLITS:\n    miss = sorted(list(VALID_SPLITS - disk_splits))\n    extra = sorted(list(disk_splits - VALID_SPLITS))\n    raise RuntimeError(\n        f\"SPLIT_DIRS mismatch. missing={miss[:5]} extra={extra[:5]} \"\n        f\"(jalankan ulang STAGE 1)\"\n    )\n\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\n# fold assignment tuning\nFOLD_QUOTA = int(CFG.get(\"SPLIT_PER_FOLD_QUOTA\", int(np.ceil(len(SPLIT_LIST)/max(N_FOLDS,1)))))  # default 4 for 20/5\nRESTARTS = int(CFG.get(\"SPLIT_ASSIGN_RESTARTS\", 256))  # multi-restart untuk balance\nRESTARTS_HARD = int(CFG.get(\"SPLIT_ASSIGN_RESTARTS_HARD\", 1024))  # kalau fold pos ada yang 0, escalate\nLAMBDA_COUNT = float(CFG.get(\"FOLD_BALANCE_LAMBDA_COUNT\", 0.20))\nLAMBDA_QUOTA = float(CFG.get(\"FOLD_BALANCE_LAMBDA_QUOTA\", 0.05))\nPENALTY_ZERO_POS = float(CFG.get(\"FOLD_BALANCE_PENALTY_ZERO_POS\", 2.0))\n\n# clipping quantiles\nQLO = float(CFG.get(\"META_QLO\", 0.001))\nQHI = float(CFG.get(\"META_QHI\", 0.999))\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\ndef normalize_split_name(x) -> str:\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s2.isdigit():\n        return f\"split_{int(s2):02d}\"\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    return s2\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef _coerce_float32(df: pd.DataFrame, col: str):\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\")\n\ndef _safe_clip(series: pd.Series, lo: float, hi: float) -> pd.Series:\n    return series.clip(lower=np.float32(lo), upper=np.float32(hi)).astype(\"float32\")\n\ndef _qclip_bounds(arr: np.ndarray, qlo=0.001, qhi=0.999, default=(0.0, 0.0)):\n    x = np.asarray(arr, dtype=float)\n    x = x[np.isfinite(x)]\n    if len(x) == 0:\n        return float(default[0]), float(default[1])\n    lo, hi = np.quantile(x, [qlo, qhi])\n    return float(lo), float(hi)\n\ndef _load_or_use_global(global_name: str, path: Path) -> pd.DataFrame:\n    if global_name in globals() and isinstance(globals()[global_name], pd.DataFrame):\n        return _norm_cols(globals()[global_name].copy())\n    return _norm_cols(pd.read_csv(path, dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n\ndef _fill_z(df: pd.DataFrame, split_med: dict, gmed: float):\n    z = df[\"Z\"].copy()\n    if z.isna().any():\n        z = z.fillna(df[\"split\"].map(split_med))\n        z = z.fillna(np.float32(gmed))\n    return z.astype(\"float32\")\n\ndef _assign_splits_to_folds_greedy_multi(sp_stat: pd.DataFrame, n_folds: int, quota: int, seed: int,\n                                        lambda_count: float, lambda_quota: float, penalty_zero_pos: float,\n                                        restarts: int):\n    \"\"\"\n    Multi-restart greedy split->fold assignment to balance:\n    - fold pos_rate vs global pos_rate\n    - fold sample count vs target count\n    - quota usage\n    Also penalize folds with zero positives (important untuk F1).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    global_pos_rate = float(sp_stat[\"pos\"].sum() / max(sp_stat[\"n\"].sum(), 1))\n    target_fold_n = float(sp_stat[\"n\"].sum() / max(n_folds, 1))\n\n    # Pre-order by \"difficulty\": high pos then high n\n    base_order = sp_stat.sort_values([\"pos\",\"n\"], ascending=False).reset_index(drop=True)\n\n    best = None  # (score, split2fold, fold_n, fold_pos, fold_k)\n\n    for t in range(max(restarts, 1)):\n        # tie-randomization: shuffle within small blocks to explore assignments\n        order = base_order.copy()\n        # random jitter to break ties deterministically via rng\n        jitter = rng.normal(0, 1e-6, size=len(order))\n        order[\"_j\"] = jitter\n        order = order.sort_values([\"pos\",\"n\",\"_j\"], ascending=[False,False,True]).drop(columns=[\"_j\"]).reset_index(drop=True)\n\n        fold_n = np.zeros(n_folds, dtype=float)\n        fold_pos = np.zeros(n_folds, dtype=float)\n        fold_k = np.zeros(n_folds, dtype=int)\n        split2fold = {}\n\n        for _, r in order.iterrows():\n            sp = r[\"split\"]; n = float(r[\"n\"]); p = float(r[\"pos\"])\n            cand = np.where(fold_k < quota)[0]\n            if len(cand) == 0:\n                cand = np.arange(n_folds)\n\n            scores = []\n            for f in cand:\n                n2 = fold_n[f] + n\n                p2 = fold_pos[f] + p\n                pr2 = (p2 / n2) if n2 > 0 else global_pos_rate\n                score = abs(pr2 - global_pos_rate) \\\n                        + lambda_count * abs(n2 - target_fold_n) / max(target_fold_n, 1.0) \\\n                        + lambda_quota * (fold_k[f] / max(quota, 1))\n                scores.append(score)\n\n            scores = np.asarray(scores, dtype=float)\n            best_idx = np.where(scores == scores.min())[0]\n            choose = int(cand[int(rng.choice(best_idx))]) if len(best_idx) > 1 else int(cand[int(best_idx[0])])\n\n            split2fold[sp] = choose\n            fold_n[choose] += n\n            fold_pos[choose] += p\n            fold_k[choose] += 1\n\n        # final score: sum imbalance + penalty for zero-pos fold\n        fold_pr = np.divide(fold_pos, np.maximum(fold_n, 1e-9))\n        score = float(np.sum(np.abs(fold_pr - global_pos_rate))) + float(lambda_count * np.std(fold_n) / max(target_fold_n, 1.0))\n        zero_pos = int(np.sum(fold_pos == 0))\n        score += penalty_zero_pos * zero_pos\n\n        cand_pack = (score, split2fold, fold_n.copy(), fold_pos.copy(), fold_k.copy(), zero_pos)\n\n        if best is None or cand_pack[0] < best[0]:\n            best = cand_pack\n\n        # early exit if perfect: no zero-pos & good enough\n        if best is not None and best[5] == 0 and best[0] < 0.01:\n            break\n\n    return best  # score, split2fold, fold_n, fold_pos, fold_k, zero_pos\n\n# ----------------------------\n# 2) Load logs\n# ----------------------------\ndf_train = _load_or_use_global(\"df_train_log\", TRAIN_LOG_PATH)\ndf_test  = _load_or_use_global(\"df_test_log\",  TEST_LOG_PATH)\n\n# ----------------------------\n# 3) Required columns check\n# ----------------------------\nreq_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\nreq_train  = req_common | {\"target\"}\nreq_test   = req_common\n\nmiss_train = sorted(list(req_train - set(df_train.columns)))\nmiss_test  = sorted(list(req_test  - set(df_test.columns)))\nif miss_train:\n    raise ValueError(f\"train_log missing columns: {miss_train} | found={list(df_train.columns)}\")\nif miss_test:\n    raise ValueError(f\"test_log missing columns: {miss_test} | found={list(df_test.columns)}\")\n\n# ----------------------------\n# 4) Basic cleaning\n# ----------------------------\ndf_train[\"object_id\"] = df_train[\"object_id\"].astype(\"string\").str.strip()\ndf_test[\"object_id\"]  = df_test[\"object_id\"].astype(\"string\").str.strip()\n\ndf_train[\"split\"] = df_train[\"split\"].astype(\"string\").map(normalize_split_name)\ndf_test[\"split\"]  = df_test[\"split\"].astype(\"string\").map(normalize_split_name)\n\nbad_train_split = sorted(set(df_train[\"split\"].unique()) - VALID_SPLITS)\nbad_test_split  = sorted(set(df_test[\"split\"].unique())  - VALID_SPLITS)\nif bad_train_split:\n    raise ValueError(f\"train_log invalid split values: {bad_train_split[:10]}\")\nif bad_test_split:\n    raise ValueError(f\"test_log invalid split values: {bad_test_split[:10]}\")\n\n# ----------------------------\n# 5) Ensure Z_err exists + numeric coercion\n# ----------------------------\nif \"Z_err\" not in df_train.columns:\n    df_train[\"Z_err\"] = np.nan\nif \"Z_err\" not in df_test.columns:\n    df_test[\"Z_err\"] = np.nan\n\n# has_zerr BEFORE fill (ini yang dipakai untuk \"is_photoz\" agar tidak sekadar train/test flag)\ndf_train[\"has_zerr\"] = (~pd.to_numeric(df_train[\"Z_err\"], errors=\"coerce\").isna()).astype(\"int8\")\ndf_test[\"has_zerr\"]  = (~pd.to_numeric(df_test[\"Z_err\"],  errors=\"coerce\").isna()).astype(\"int8\")\n\nfor c in [\"EBV\",\"Z\",\"Z_err\"]:\n    _coerce_float32(df_train, c)\n    _coerce_float32(df_test, c)\n\n# ----------------------------\n# 6) Duplicate / overlap checks\n# ----------------------------\nif df_train[\"object_id\"].duplicated().any():\n    ex = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n    raise ValueError(f\"Duplicated object_id in train_log (examples): {ex}\")\nif df_test[\"object_id\"].duplicated().any():\n    ex = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n    raise ValueError(f\"Duplicated object_id in test_log (examples): {ex}\")\n\noverlap = set(df_train[\"object_id\"].tolist()) & set(df_test[\"object_id\"].tolist())\nif overlap:\n    raise ValueError(f\"object_id overlap train vs test (examples): {list(overlap)[:5]}\")\n\n# ----------------------------\n# 7) Target validation\n# ----------------------------\ndf_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\nif df_train[\"target\"].isna().any():\n    raise ValueError(f\"train_log target has NaN after coercion: {int(df_train['target'].isna().sum())} rows.\")\nuniq_t = set(pd.unique(df_train[\"target\"]).tolist())\nif not uniq_t.issubset({0,1}):\n    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\ndf_train[\"target\"] = df_train[\"target\"].astype(\"int8\")\n\n# ----------------------------\n# 8) Missing flags + fills (ANTI-LEAKAGE)\n# ----------------------------\nfor df in [df_train, df_test]:\n    df[\"EBV_missing\"]  = df[\"EBV\"].isna().astype(\"int8\")\n    df[\"Z_missing\"]    = df[\"Z\"].isna().astype(\"int8\")\n    df[\"Zerr_missing\"] = df[\"Z_err\"].isna().astype(\"int8\")\n\n# EBV fill: TRAIN median\nebv_med = float(np.nanmedian(df_train[\"EBV\"].values.astype(float))) if np.isfinite(df_train[\"EBV\"].values.astype(float)).any() else 0.0\ndf_train[\"EBV\"] = df_train[\"EBV\"].fillna(np.float32(ebv_med)).astype(\"float32\")\ndf_test[\"EBV\"]  = df_test[\"EBV\"].fillna(np.float32(ebv_med)).astype(\"float32\")\n\n# Z fill: TRAIN split-median -> TRAIN global median\ntrain_split_med = df_train.groupby(\"split\")[\"Z\"].median().to_dict()\ntrain_gmed = float(np.nanmedian(df_train[\"Z\"].values.astype(float))) if np.isfinite(df_train[\"Z\"].values.astype(float)).any() else 0.0\ndf_train[\"Z\"] = _fill_z(df_train, train_split_med, train_gmed)\ndf_test[\"Z\"]  = _fill_z(df_test,  train_split_med, train_gmed)\n\n# Z_err fill: NaN -> 0 (tetap simpan has_zerr untuk signal)\ndf_train[\"Z_err\"] = df_train[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\ndf_test[\"Z_err\"]  = df_test[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n\n# is_photoz: pakai has_zerr (bukan train/test constant)\ndf_train[\"is_photoz\"] = df_train[\"has_zerr\"].astype(\"int8\")\ndf_test[\"is_photoz\"]  = df_test[\"has_zerr\"].astype(\"int8\")\n\n# ----------------------------\n# 9) Clipping + derived meta features\n# ----------------------------\n# EBV/Z clip pakai TRAIN (anti leakage)\nEBV_LO, EBV_HI = _qclip_bounds(df_train[\"EBV\"].values, QLO, QHI)\nZ_LO,   Z_HI   = _qclip_bounds(df_train[\"Z\"].values,   QLO, QHI)\n\ndf_train[\"EBV_clip\"] = _safe_clip(df_train[\"EBV\"], EBV_LO, EBV_HI)\ndf_test[\"EBV_clip\"]  = _safe_clip(df_test[\"EBV\"],  EBV_LO, EBV_HI)\n\ndf_train[\"Z_clip\"] = _safe_clip(df_train[\"Z\"], Z_LO, Z_HI)\ndf_test[\"Z_clip\"]  = _safe_clip(df_test[\"Z\"],  Z_LO, Z_HI)\n\n# Z_err clip: pakai TEST quantiles hanya pada rows yang memang punya zerr (has_zerr==1)\n# (unsupervised, tanpa label; aman untuk stabilisasi feature)\nZE_LO = 0.0\nzerr_pool = df_test.loc[df_test[\"has_zerr\"] == 1, \"Z_err\"].values\n_, ZE_HI = _qclip_bounds(zerr_pool, QLO, QHI, default=(0.0, 0.0))\nZE_HI = max(float(ZE_HI), 0.0)\n\ndf_train[\"Zerr_clip\"] = _safe_clip(df_train[\"Z_err\"], ZE_LO, ZE_HI)\ndf_test[\"Zerr_clip\"]  = _safe_clip(df_test[\"Z_err\"],  ZE_LO, ZE_HI)\n\n# Derived\ndf_train[\"log1pZ\"] = np.log1p(df_train[\"Z_clip\"]).astype(\"float32\")\ndf_test[\"log1pZ\"]  = np.log1p(df_test[\"Z_clip\"]).astype(\"float32\")\n\ndf_train[\"log1pZerr\"] = np.log1p(df_train[\"Zerr_clip\"]).astype(\"float32\")\ndf_test[\"log1pZerr\"]  = np.log1p(df_test[\"Zerr_clip\"]).astype(\"float32\")\n\neps = np.float32(1e-6)\ndf_train[\"zerr_rel\"] = (df_train[\"Zerr_clip\"] / (df_train[\"Z_clip\"] + eps)).astype(\"float32\")\ndf_test[\"zerr_rel\"]  = (df_test[\"Zerr_clip\"]  / (df_test[\"Z_clip\"]  + eps)).astype(\"float32\")\n\ndf_train[\"inv_1pz\"] = (1.0 / (1.0 + df_train[\"Z_clip\"])).astype(\"float32\")\ndf_test[\"inv_1pz\"]  = (1.0 / (1.0 + df_test[\"Z_clip\"])).astype(\"float32\")\n\nsplit2id = {f\"split_{i:02d}\": i for i in range(1, 21)}\ndf_train[\"split_id\"] = df_train[\"split\"].map(split2id).astype(\"int16\")\ndf_test[\"split_id\"]  = df_test[\"split\"].map(split2id).astype(\"int16\")\n\n# ----------------------------\n# 10) Fold assignment\n# ----------------------------\ndf_train[\"fold\"] = -1\n\nif CV_USE_SPLIT_COL:\n    sp_stat = (\n        df_train.groupby(\"split\")[\"target\"]\n        .agg([\"count\",\"sum\"])\n        .rename(columns={\"count\":\"n\",\"sum\":\"pos\"})\n        .reindex(SPLIT_LIST)\n        .fillna(0)\n        .astype({\"n\":int,\"pos\":int})\n        .reset_index()\n    )\n    sp_stat[\"neg\"] = sp_stat[\"n\"] - sp_stat[\"pos\"]\n    sp_stat[\"pos_rate\"] = sp_stat[\"pos\"] / sp_stat[\"n\"].clip(lower=1)\n\n    # First pass (restarts)\n    best = _assign_splits_to_folds_greedy_multi(\n        sp_stat, n_folds=N_FOLDS, quota=FOLD_QUOTA, seed=SEED,\n        lambda_count=LAMBDA_COUNT, lambda_quota=LAMBDA_QUOTA,\n        penalty_zero_pos=PENALTY_ZERO_POS, restarts=RESTARTS\n    )\n\n    # If still has zero-pos fold, escalate restarts (still deterministic via SEED)\n    if best is None:\n        raise RuntimeError(\"split->fold assignment failed unexpectedly.\")\n    if best[5] > 0 and RESTARTS_HARD > RESTARTS:\n        best2 = _assign_splits_to_folds_greedy_multi(\n            sp_stat, n_folds=N_FOLDS, quota=FOLD_QUOTA, seed=SEED + 999,\n            lambda_count=LAMBDA_COUNT, lambda_quota=LAMBDA_QUOTA,\n            penalty_zero_pos=PENALTY_ZERO_POS, restarts=RESTARTS_HARD\n        )\n        if best2 is not None and best2[0] <= best[0]:\n            best = best2\n\n    score, split2fold, fold_n, fold_pos, fold_k, zero_pos = best\n\n    df_train[\"fold\"] = df_train[\"split\"].map(split2fold).astype(\"int16\")\n\n    # sanity: must use 0..K-1\n    uniq_folds = sorted(df_train[\"fold\"].unique().tolist())\n    if uniq_folds != list(range(N_FOLDS)):\n        print(f\"[WARN] split->fold tidak memakai semua fold. uniq_folds={uniq_folds}. Fallback ke StratifiedKFold object-level.\")\n        CV_USE_SPLIT_COL = False\n    else:\n        with open(ART_DIR / \"split2fold.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump({k:int(v) for k,v in split2fold.items()}, f)\n\nif not CV_USE_SPLIT_COL:\n    # Fallback (lebih rawan shift/leakage split, tapi tetap jalan)\n    try:\n        from sklearn.model_selection import StratifiedKFold\n        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n        y = df_train[\"target\"].to_numpy()\n        idx = np.arange(len(df_train))\n        for fold_id, (_, va_idx) in enumerate(skf.split(idx, y)):\n            df_train.iloc[va_idx, df_train.columns.get_loc(\"fold\")] = fold_id\n        df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n    except Exception as e:\n        print(f\"[WARN] StratifiedKFold unavailable ({type(e).__name__}). Using round-robin fallback.\")\n        df_train[\"fold\"] = -1\n        rng = np.random.default_rng(SEED)\n        pos_idx = df_train.index[df_train[\"target\"] == 1].to_numpy()\n        neg_idx = df_train.index[df_train[\"target\"] == 0].to_numpy()\n        rng.shuffle(pos_idx); rng.shuffle(neg_idx)\n        for j, ii in enumerate(pos_idx):\n            df_train.at[ii, \"fold\"] = int(j % N_FOLDS)\n        for j, ii in enumerate(neg_idx):\n            df_train.at[ii, \"fold\"] = int(j % N_FOLDS)\n        df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n\nif (df_train[\"fold\"] < 0).any():\n    n_bad = int((df_train[\"fold\"] < 0).sum())\n    raise RuntimeError(f\"Fold assignment gagal: ada {n_bad} baris fold=-1\")\n\n# FAIL-FAST: tiap fold harus punya positive (ideal)\nfold_tab = (\n    df_train.groupby(\"fold\")[\"target\"]\n    .agg([\"count\",\"sum\"])\n    .rename(columns={\"sum\":\"pos\"})\n    .reindex(range(N_FOLDS)).fillna(0)\n)\nif (fold_tab[\"pos\"] == 0).any():\n    bad_folds = fold_tab.index[fold_tab[\"pos\"] == 0].tolist()\n    print(\"[WARN] Ada fold tanpa positive (TDE). Ini bisa merusak threshold tuning / F1.\")\n    print(\"       bad_folds:\", bad_folds)\n    # tidak raise agar pipeline tetap jalan, tapi kamu sebaiknya naikkan RESTARTS / cek distribusi per split.\n\n# ----------------------------\n# 11) Build meta tables (index=object_id)\n# ----------------------------\nkeep_train = [\n    \"object_id\",\"split\",\"split_id\",\n    \"EBV\",\"EBV_clip\",\n    \"Z\",\"Z_clip\",\"log1pZ\",\"inv_1pz\",\n    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_rel\",\n    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n    \"fold\",\"target\"\n]\nkeep_test = [\n    \"object_id\",\"split\",\"split_id\",\n    \"EBV\",\"EBV_clip\",\n    \"Z\",\"Z_clip\",\"log1pZ\",\"inv_1pz\",\n    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_rel\",\n    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\"\n]\n\nif \"SpecType\" in df_train.columns:\n    keep_train.append(\"SpecType\")\n\ndf_train_meta = df_train[keep_train].copy().set_index(\"object_id\", drop=True).sort_index()\ndf_test_meta  = df_test[keep_test].copy().set_index(\"object_id\", drop=True).sort_index()\n\nif not df_train_meta.index.is_unique:\n    raise RuntimeError(\"df_train_meta index (object_id) not unique after processing.\")\nif not df_test_meta.index.is_unique:\n    raise RuntimeError(\"df_test_meta index (object_id) not unique after processing.\")\n\nid2split_train = df_train_meta[\"split\"].to_dict()\nid2split_test  = df_test_meta[\"split\"].to_dict()\n\n# ----------------------------\n# 12) Save artifacts\n# ----------------------------\ntrain_pq = ART_DIR / \"train_meta.parquet\"\ntest_pq  = ART_DIR / \"test_meta.parquet\"\ntrain_csv = ART_DIR / \"train_meta.csv\"\ntest_csv  = ART_DIR / \"test_meta.csv\"\n\ntry:\n    df_train_meta.to_parquet(train_pq, index=True)\n    df_test_meta.to_parquet(test_pq, index=True)\n    saved_train, saved_test = str(train_pq), str(test_pq)\nexcept Exception:\n    df_train_meta.to_csv(train_csv, index=True)\n    df_test_meta.to_csv(test_csv, index=True)\n    saved_train, saved_test = str(train_csv), str(test_csv)\n\nsplit_stats = pd.DataFrame({\n    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n})\nsplit_stats.index.name = \"split\"\npos_by_split = df_train_meta.groupby(\"split\")[\"target\"].sum().reindex(SPLIT_LIST).fillna(0).astype(int)\nsplit_stats[\"train_pos\"] = pos_by_split.values\nsplit_stats[\"train_pos_rate\"] = (split_stats[\"train_pos\"] / split_stats[\"train_objects\"].clip(lower=1)).astype(\"float32\")\n\nsplit_stats_path = ART_DIR / \"split_stats.csv\"\nsplit_stats.to_csv(split_stats_path)\n\nfold_path = ART_DIR / \"train_folds.csv\"\ndf_train_meta.reset_index()[[\"object_id\",\"split\",\"fold\",\"target\"]].to_csv(fold_path, index=False)\n\nwith open(ART_DIR / \"id2split_train.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(id2split_train, f)\nwith open(ART_DIR / \"id2split_test.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(id2split_test, f)\n\n# stage2 summary\npos = int((df_train_meta[\"target\"] == 1).sum())\nneg = int((df_train_meta[\"target\"] == 0).sum())\ntot = int(len(df_train_meta))\npos_rate = pos / max(tot, 1)\nscale_pos_weight = float(neg / max(pos, 1))\n\nstage2_summary = {\n    \"stage\": \"stage2\",\n    \"N_FOLDS\": int(N_FOLDS),\n    \"CV_USE_SPLIT_COL_USED\": bool(CV_USE_SPLIT_COL),\n    \"counts\": {\n        \"train\": int(tot),\n        \"pos\": int(pos),\n        \"neg\": int(neg),\n        \"pos_rate\": float(pos_rate),\n        \"test\": int(len(df_test_meta))\n    },\n    \"clip_ranges\": {\n        \"EBV_train\": [float(EBV_LO), float(EBV_HI)],\n        \"Z_train\": [float(Z_LO), float(Z_HI)],\n        \"Zerr_test\": [float(ZE_LO), float(ZE_HI)]\n    },\n    \"scale_pos_weight\": float(scale_pos_weight),\n    \"artifacts\": {\n        \"train_meta\": saved_train,\n        \"test_meta\": saved_test,\n        \"split_stats\": str(split_stats_path),\n        \"train_folds\": str(fold_path),\n        \"id2split_train\": str(ART_DIR / \"id2split_train.json\"),\n        \"id2split_test\": str(ART_DIR / \"id2split_test.json\"),\n        \"split2fold\": str(ART_DIR / \"split2fold.json\") if (ART_DIR / \"split2fold.json\").exists() else None\n    }\n}\nwith open(ART_DIR / \"stage2_summary.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(stage2_summary, f, indent=2)\n\n# ----------------------------\n# 13) Print summary\n# ----------------------------\nprint(\"STAGE 2 OK — META READY (clean + folds)\")\nprint(f\"- CV_USE_SPLIT_COL_USED: {CV_USE_SPLIT_COL} | N_FOLDS={N_FOLDS} | split_quota={FOLD_QUOTA}\")\nprint(f\"- train objects: {tot:,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={pos_rate*100:.3f}%\")\nprint(f\"- test objects : {len(df_test_meta):,}\")\nprint(f\"- saved train  : {saved_train}\")\nprint(f\"- saved test   : {saved_test}\")\nprint(f\"- saved stats  : {split_stats_path}\")\nprint(f\"- saved folds  : {fold_path}\")\nprint(f\"- scale_pos_weight (neg/pos): {scale_pos_weight:.3f}\")\n\nprint(\"\\nCLIP RANGES\")\nprint(f\"- EBV clip (train): [{EBV_LO:.6f}, {EBV_HI:.6f}]\")\nprint(f\"- Z   clip (train): [{Z_LO:.6f}, {Z_HI:.6f}]\")\nprint(f\"- Zerr clip (test ): [{ZE_LO:.6f}, {ZE_HI:.6f}]\")\n\nfold_tab2 = fold_tab.copy()\nfold_tab2[\"pos_rate\"] = fold_tab2[\"pos\"] / fold_tab2[\"count\"].clip(lower=1)\nprint(\"\\nFOLD BALANCE (count/pos/pos_rate) — MUST SHOW 0..K-1\")\nprint(fold_tab2.to_string())\n\n# ----------------------------\n# 14) Export globals\n# ----------------------------\nglobals().update({\n    \"df_train_meta\": df_train_meta,\n    \"df_test_meta\": df_test_meta,\n    \"id2split_train\": id2split_train,\n    \"id2split_test\": id2split_test,\n    \"split_stats\": split_stats,\n    \"split2id\": split2id,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"CV_USE_SPLIT_COL_USED\": CV_USE_SPLIT_COL,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Lightcurve Loading Strategy","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — Robust Lightcurve Loader Utilities (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v4.0 (BATCH LOADER + NPZ CACHE + STRICT flux_err + SMOKE TEST LEBIH BERMAKNA)\n#\n# Upgrade v4.0:\n# - Tambah loader batch per-split: load_many_object_lightcurves() (hemat IO besar)\n# - Tambah cache per-object ke .npz: load_object_lightcurve_cached()\n# - Normalisasi lebih ketat: drop flux_err NaN/<=0 (atau clamp ke MIN_FLUXERR)\n# - Filter encoding opsional (int8) untuk hemat memory + sort cepat\n# - Smoke test: cek schema + coba temukan beberapa object (limited scan)\n#\n# Output globals:\n# - SPLIT_FILES, train_ids_by_split, test_ids_by_split\n# - iter_lightcurve_chunks, load_object_lightcurve\n# - load_many_object_lightcurves, load_object_lightcurve_cached\n# - get_lc_cache_path, LC_CACHE_DIR\n# - REQ_LC_KEYS, ALLOWED_FILTERS, FILTER_ORDER\n# ============================================================\n\nimport gc, re, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed_prev = [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"CFG\", \"SEED\"]\nfor need in need_prev:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n\nART_DIR = Path(ART_DIR)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nSEED = int(SEED)\n\n# Optional CACHE_DIR (dari STAGE 0). Kalau tidak ada, fallback ke ART_DIR/cache\nCACHE_DIR = Path(globals().get(\"CACHE_DIR\", ART_DIR / \"cache\"))\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\n\nMIN_FLUXERR = float(CFG.get(\"MIN_FLUXERR\", 1e-6))\nCHUNK_ROWS_DEFAULT = int(CFG.get(\"CHUNK_ROWS\", 400_000))\n\n# konsisten dengan STAGE 0/2\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\nREQ_LC_KEYS = [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]\nALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\nALLOWED_FILTERS_TUP = (\"u\", \"g\", \"r\", \"i\", \"z\", \"y\")\nFILTER_ORDER = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\nFILTER2ID = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\nID2FILTER = {v:k for k,v in FILTER2ID.items()}\n\n# Cache configs\n_LC_CFG_CACHE = {}  # (split_name, which) -> cfg dict\n_LC_OBJ_CACHE = {}  # in-memory small cache: (which, object_id) -> df\nLC_CACHE_DIR = CACHE_DIR / \"lightcurves_npz\"\nLC_CACHE_DIR.mkdir(parents=True, exist_ok=True)\nMAX_MEM_CACHE = int(CFG.get(\"LC_MEM_CACHE_MAX\", 64))  # max objects in RAM cache\n\n# ----------------------------\n# 1) Build split file mapping (train/test lightcurves)\n# ----------------------------\nSPLIT_FILES = {}\nfor s in SPLIT_LIST:\n    sd = Path(SPLIT_DIRS[s])\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    if (not tr.exists()) or (not te.exists()):\n        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n\n# Save split file manifest\nmanifest = []\nfor s in SPLIT_LIST:\n    p_tr = SPLIT_FILES[s][\"train\"]\n    p_te = SPLIT_FILES[s][\"test\"]\n    manifest.append({\n        \"split\": s,\n        \"train_path\": str(p_tr),\n        \"test_path\": str(p_te),\n        \"train_mb\": float(p_tr.stat().st_size) / (1024**2),\n        \"test_mb\":  float(p_te.stat().st_size) / (1024**2),\n    })\ndf_manifest = pd.DataFrame(manifest).sort_values(\"split\")\nmanifest_path = ART_DIR / \"split_file_manifest.csv\"\ndf_manifest.to_csv(manifest_path, index=False)\n\n# ----------------------------\n# 2) Build object routing by split\n# ----------------------------\ntrain_ids_by_split = {s: [] for s in SPLIT_LIST}\ntest_ids_by_split  = {s: [] for s in SPLIT_LIST}\n\ntr_groups = df_train_meta.groupby(\"split\").groups  # split -> Index(labels=object_id)\nte_groups = df_test_meta.groupby(\"split\").groups\n\nfor sp, idx in tr_groups.items():\n    if sp in train_ids_by_split:\n        train_ids_by_split[sp] = pd.Index(idx).astype(str).tolist()\n\nfor sp, idx in te_groups.items():\n    if sp in test_ids_by_split:\n        test_ids_by_split[sp] = pd.Index(idx).astype(str).tolist()\n\n# sanity totals\nif sum(len(v) for v in train_ids_by_split.values()) != len(df_train_meta):\n    raise RuntimeError(\"Routing train_ids_by_split mismatch total vs df_train_meta length.\")\nif sum(len(v) for v in test_ids_by_split.values()) != len(df_test_meta):\n    raise RuntimeError(\"Routing test_ids_by_split mismatch total vs df_test_meta length.\")\n\ndf_counts = pd.DataFrame({\n    \"split\": SPLIT_LIST,\n    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n})\ncounts_path = ART_DIR / \"object_counts_by_split.csv\"\ndf_counts.to_csv(counts_path, index=False)\n\n# ----------------------------\n# 3) Robust header mapping -> canonical columns\n# ----------------------------\ndef _canon_col(x: str) -> str:\n    s = str(x).strip().lower()\n    s = s.replace(\"\\ufeff\", \"\")\n    s = re.sub(r\"\\s+\", \"\", s)\n    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n    s = s.replace(\"-\", \"_\")\n    return s\n\ndef _build_lc_read_cfg(p: Path):\n    h = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n    orig_cols = list(h.columns)\n\n    c2o = {}\n    for c in orig_cols:\n        k = _canon_col(c)\n        if k not in c2o:\n            c2o[k] = c\n\n    obj_col = c2o.get(\"object_id\", None)\n\n    time_col = None\n    for k in [\"time_mjd\", \"timemjd\", \"mjd\", \"time\"]:\n        if k in c2o:\n            time_col = c2o[k]\n            break\n\n    flux_col = c2o.get(\"flux\", None)\n\n    ferr_col = None\n    for k in [\"flux_err\", \"fluxerr\", \"fluxerror\"]:\n        if k in c2o:\n            ferr_col = c2o[k]\n            break\n\n    filt_col = c2o.get(\"filter\", None)\n\n    missing = []\n    if obj_col is None:  missing.append(\"object_id\")\n    if time_col is None: missing.append(\"Time (MJD)\")\n    if flux_col is None: missing.append(\"Flux\")\n    if ferr_col is None: missing.append(\"Flux_err\")\n    if filt_col is None: missing.append(\"Filter\")\n    if missing:\n        raise ValueError(\n            f\"Missing required lightcurve columns in {p.name}: {missing}. \"\n            f\"Header sample: {orig_cols[:20]}\"\n        )\n\n    usecols = [obj_col, time_col, flux_col, ferr_col, filt_col]\n    rename = {obj_col:\"object_id\", time_col:\"mjd\", flux_col:\"flux\", ferr_col:\"flux_err\", filt_col:\"filter\"}\n\n    # enforce string only for id/filter (numeric we'll coerce later)\n    dtypes = {obj_col:\"string\", filt_col:\"string\"}\n\n    return {\"usecols\": usecols, \"dtype\": dtypes, \"rename\": rename}\n\ndef _normalize_lc_chunk(\n    df: pd.DataFrame,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n    encode_filter: bool = False,\n):\n    \"\"\"\n    Normalisasi chunk:\n    - object_id trimmed\n    - filter lower + valid-only\n    - mjd/flux/flux_err numeric float32\n    - flux_err: clamp ke MIN_FLUXERR (dan optional drop NaN/<=0)\n    \"\"\"\n    df = df[[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"]].copy()\n\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n    df[\"filter\"] = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n\n    # invalid filters -> NA\n    df.loc[~df[\"filter\"].isin(ALLOWED_FILTERS_TUP), \"filter\"] = pd.NA\n\n    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(\"float32\")\n    df[\"flux\"] = pd.to_numeric(df[\"flux\"], errors=\"coerce\").astype(\"float32\")\n    df[\"flux_err\"] = pd.to_numeric(df[\"flux_err\"], errors=\"coerce\").astype(\"float32\")\n\n    # Guard flux_err\n    fe = df[\"flux_err\"]\n    if drop_bad_fluxerr:\n        df = df[fe.notna()]\n        fe = df[\"flux_err\"]\n        df = df[fe > 0]\n        fe = df[\"flux_err\"]\n\n    if MIN_FLUXERR > 0:\n        fe = df[\"flux_err\"]\n        df.loc[fe.notna() & (fe < MIN_FLUXERR), \"flux_err\"] = np.float32(MIN_FLUXERR)\n\n    # Drop empty id\n    df = df[df[\"object_id\"].notna() & (df[\"object_id\"] != \"\")]\n\n    if drop_bad_filter:\n        df = df[df[\"filter\"].notna()]\n    if drop_bad_mjd:\n        df = df[df[\"mjd\"].notna()]\n\n    df = df[REQ_LC_KEYS]\n\n    if encode_filter:\n        # add filter_id int8 (hemat memory/sort cepat)\n        df = df.copy()\n        df[\"filter_id\"] = df[\"filter\"].map(FILTER2ID).astype(\"int8\")\n\n    return df\n\n# ----------------------------\n# 4) Chunked readers\n# ----------------------------\ndef iter_lightcurve_chunks(\n    split_name: str,\n    which: str,\n    chunksize: int = None,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n    encode_filter: bool = False,\n):\n    split_name = str(split_name).strip()\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Unknown split_name={split_name}. Known={list(SPLIT_FILES.keys())[:5]}..\")\n    if which not in (\"train\", \"test\"):\n        raise ValueError(\"which must be 'train' or 'test'\")\n\n    if chunksize is None:\n        chunksize = CHUNK_ROWS_DEFAULT\n\n    p = SPLIT_FILES[split_name][which]\n    key = (split_name, which)\n    if key not in _LC_CFG_CACHE:\n        _LC_CFG_CACHE[key] = _build_lc_read_cfg(p)\n    cfg = _LC_CFG_CACHE[key]\n\n    reader = pd.read_csv(\n        p,\n        usecols=cfg[\"usecols\"],\n        dtype=cfg[\"dtype\"],\n        chunksize=int(chunksize),\n        **SAFE_READ_KW\n    )\n    for chunk in reader:\n        chunk = chunk.rename(columns=cfg[\"rename\"])\n        yield _normalize_lc_chunk(\n            chunk,\n            drop_bad_filter=drop_bad_filter,\n            drop_bad_mjd=drop_bad_mjd,\n            drop_bad_fluxerr=drop_bad_fluxerr,\n            encode_filter=encode_filter,\n        )\n\ndef load_object_lightcurve(\n    object_id: str,\n    which: str,\n    chunksize: int = None,\n    sort_time: bool = True,\n    max_chunks: int = None,\n    stop_after_found_block: bool = True,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n):\n    \"\"\"\n    Per-object loader (lebih mahal). Prefer pakai load_many_object_lightcurves untuk batch.\n    \"\"\"\n    object_id = str(object_id).strip()\n\n    if which == \"train\":\n        if object_id not in df_train_meta.index:\n            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n        split_name = str(df_train_meta.loc[object_id, \"split\"]).strip()\n    elif which == \"test\":\n        if object_id not in df_test_meta.index:\n            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n        split_name = str(df_test_meta.loc[object_id, \"split\"]).strip()\n    else:\n        raise ValueError(\"which must be 'train' or 'test'\")\n\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Routing split not found in SPLIT_FILES: split={split_name} object_id={object_id}\")\n\n    if chunksize is None:\n        chunksize = CHUNK_ROWS_DEFAULT\n\n    pieces = []\n    seen = 0\n    found_any = False\n    last_hit = False\n\n    for ch in iter_lightcurve_chunks(\n        split_name, which, chunksize=chunksize,\n        drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd, drop_bad_fluxerr=drop_bad_fluxerr,\n        encode_filter=False\n    ):\n        seen += 1\n        sub = ch[ch[\"object_id\"] == object_id]\n        hit = (len(sub) > 0)\n        if hit:\n            pieces.append(sub)\n            found_any = True\n\n        # fast-stop (jika file di-block per object_id)\n        if stop_after_found_block and found_any and last_hit and (not hit):\n            break\n        last_hit = hit\n\n        if max_chunks is not None and seen >= int(max_chunks):\n            break\n\n    if not pieces:\n        out = pd.DataFrame(columns=REQ_LC_KEYS)\n    else:\n        out = pd.concat(pieces, ignore_index=True)\n        if sort_time and len(out) > 1:\n            # sort by time then band\n            out[\"filter_ord\"] = out[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n            out = (\n                out.sort_values([\"mjd\", \"filter_ord\"], kind=\"mergesort\")\n                   .drop(columns=[\"filter_ord\"])\n                   .reset_index(drop=True)\n            )\n\n    return out\n\n# ----------------------------\n# 5) Batch loader (HEMAT IO)\n# ----------------------------\ndef load_many_object_lightcurves(\n    split_name: str,\n    which: str,\n    object_ids,\n    chunksize: int = None,\n    max_chunks: int = None,\n    sort_time: bool = True,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n):\n    \"\"\"\n    Batch load banyak object_id dalam 1 pass file split.\n    Return: dict {object_id: DataFrame}\n    \"\"\"\n    split_name = str(split_name).strip()\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Unknown split_name={split_name}\")\n    if which not in (\"train\",\"test\"):\n        raise ValueError(\"which must be train/test\")\n\n    if chunksize is None:\n        chunksize = CHUNK_ROWS_DEFAULT\n\n    # set untuk membership cepat\n    oid_set = set([str(x).strip() for x in object_ids if str(x).strip() != \"\"])\n    if len(oid_set) == 0:\n        return {}\n\n    out = {oid: [] for oid in oid_set}\n\n    seen = 0\n    for ch in iter_lightcurve_chunks(\n        split_name, which, chunksize=chunksize,\n        drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd, drop_bad_fluxerr=drop_bad_fluxerr,\n        encode_filter=False\n    ):\n        seen += 1\n        m = ch[\"object_id\"].isin(oid_set)\n        if m.any():\n            sub = ch.loc[m]\n            # group within chunk\n            for oid, g in sub.groupby(\"object_id\", sort=False):\n                out[str(oid)].append(g)\n\n        if max_chunks is not None and seen >= int(max_chunks):\n            break\n\n    # concat + sort\n    final = {}\n    for oid, parts in out.items():\n        if not parts:\n            continue\n        df = pd.concat(parts, ignore_index=True)\n        if sort_time and len(df) > 1:\n            df[\"filter_ord\"] = df[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n            df = (\n                df.sort_values([\"mjd\",\"filter_ord\"], kind=\"mergesort\")\n                  .drop(columns=[\"filter_ord\"])\n                  .reset_index(drop=True)\n            )\n        final[oid] = df\n\n    return final\n\n# ----------------------------\n# 6) NPZ cache utilities (sangat membantu untuk Stage 4/5)\n# ----------------------------\ndef get_lc_cache_path(object_id: str, which: str) -> Path:\n    object_id = str(object_id).strip()\n    which = str(which).strip()\n    return LC_CACHE_DIR / f\"{which}__{object_id}.npz\"\n\ndef _mem_cache_put(key, value):\n    _LC_OBJ_CACHE[key] = value\n    if len(_LC_OBJ_CACHE) > MAX_MEM_CACHE:\n        # pop oldest inserted (py3.7+ dict preserves order)\n        k0 = next(iter(_LC_OBJ_CACHE.keys()))\n        _LC_OBJ_CACHE.pop(k0, None)\n\ndef load_object_lightcurve_cached(\n    object_id: str,\n    which: str,\n    chunksize: int = None,\n    sort_time: bool = True,\n    max_chunks: int = None,\n    stop_after_found_block: bool = True,\n    use_npz_cache: bool = True,\n    refresh_cache: bool = False,\n):\n    \"\"\"\n    Return DataFrame (REQ_LC_KEYS).\n    Cache strategy:\n    - RAM cache (small)\n    - NPZ cache on disk (optional)\n    \"\"\"\n    object_id = str(object_id).strip()\n    which = str(which).strip()\n\n    mem_key = (which, object_id)\n    if mem_key in _LC_OBJ_CACHE and (not refresh_cache):\n        return _LC_OBJ_CACHE[mem_key].copy()\n\n    npz_path = get_lc_cache_path(object_id, which)\n    if use_npz_cache and npz_path.exists() and (not refresh_cache):\n        z = np.load(npz_path, allow_pickle=False)\n        mjd = z[\"mjd\"].astype(\"float32\")\n        flux = z[\"flux\"].astype(\"float32\")\n        ferr = z[\"flux_err\"].astype(\"float32\")\n        filt_id = z[\"filter_id\"].astype(\"int8\")\n        filt = np.array([ID2FILTER[int(i)] for i in filt_id], dtype=object)\n\n        df = pd.DataFrame({\n            \"object_id\": object_id,\n            \"mjd\": mjd,\n            \"flux\": flux,\n            \"flux_err\": ferr,\n            \"filter\": filt,\n        })\n        if sort_time and len(df) > 1:\n            df[\"filter_ord\"] = df[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n            df = df.sort_values([\"mjd\",\"filter_ord\"], kind=\"mergesort\").drop(columns=[\"filter_ord\"]).reset_index(drop=True)\n        _mem_cache_put(mem_key, df)\n        return df.copy()\n\n    # fallback to raw load\n    df = load_object_lightcurve(\n        object_id, which,\n        chunksize=chunksize,\n        sort_time=sort_time,\n        max_chunks=max_chunks,\n        stop_after_found_block=stop_after_found_block,\n        drop_bad_filter=True, drop_bad_mjd=True, drop_bad_fluxerr=True\n    )\n\n    if use_npz_cache:\n        try:\n            if len(df) > 0:\n                filt_id = df[\"filter\"].map(FILTER2ID).astype(\"int8\").to_numpy()\n                np.savez_compressed(\n                    npz_path,\n                    mjd=df[\"mjd\"].to_numpy(dtype=\"float32\"),\n                    flux=df[\"flux\"].to_numpy(dtype=\"float32\"),\n                    flux_err=df[\"flux_err\"].to_numpy(dtype=\"float32\"),\n                    filter_id=filt_id\n                )\n        except Exception:\n            pass\n\n    _mem_cache_put(mem_key, df)\n    return df.copy()\n\n# ----------------------------\n# 7) Smoke test (schema + coba menemukan object nyata)\n# ----------------------------\n# pilih sampai 3 split yang benar-benar punya object train & test\ncandidate_splits = []\nfor s in SPLIT_LIST:\n    if len(train_ids_by_split.get(s, [])) > 0 and len(test_ids_by_split.get(s, [])) > 0:\n        candidate_splits.append(s)\n    if len(candidate_splits) >= 3:\n        break\nif len(candidate_splits) == 0:\n    raise RuntimeError(\"Tidak ada split yang punya train & test objects (unexpected). Cek STAGE 2.\")\n\nrng = np.random.default_rng(SEED)\nSMOKE_MAX_CHUNKS = int(CFG.get(\"SMOKE_MAX_CHUNKS\", 10))\nSMOKE_CHUNK = int(CFG.get(\"SMOKE_CHUNK_ROWS\", 80_000))\nSMOKE_N_IDS = int(CFG.get(\"SMOKE_N_IDS_PER_SPLIT\", 2))\n\nfor s in candidate_splits:\n    # schema check\n    ch_tr = next(iter_lightcurve_chunks(s, \"train\", chunksize=SMOKE_CHUNK))\n    ch_te = next(iter_lightcurve_chunks(s, \"test\",  chunksize=SMOKE_CHUNK))\n\n    if list(ch_tr.columns) != REQ_LC_KEYS:\n        raise RuntimeError(f\"Train chunk schema mismatch in {s}: {list(ch_tr.columns)}\")\n    if list(ch_te.columns) != REQ_LC_KEYS:\n        raise RuntimeError(f\"Test chunk schema mismatch in {s}: {list(ch_te.columns)}\")\n\n    badf_tr = sorted(set(ch_tr[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n    badf_te = sorted(set(ch_te[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n    if badf_tr or badf_te:\n        raise ValueError(f\"Unexpected filter values in smoke chunk split={s}: train_bad={badf_tr} test_bad={badf_te}\")\n\n    # try find few objects (limited scan, warn only)\n    tr_ids = train_ids_by_split[s]\n    te_ids = test_ids_by_split[s]\n    pick_tr = [tr_ids[i] for i in rng.integers(0, len(tr_ids), size=min(SMOKE_N_IDS, len(tr_ids)))]\n    pick_te = [te_ids[i] for i in rng.integers(0, len(te_ids), size=min(SMOKE_N_IDS, len(te_ids)))]\n\n    # batch load limited chunks (murah)\n    got_tr = load_many_object_lightcurves(s, \"train\", pick_tr, chunksize=SMOKE_CHUNK, max_chunks=SMOKE_MAX_CHUNKS, sort_time=False)\n    got_te = load_many_object_lightcurves(s, \"test\",  pick_te, chunksize=SMOKE_CHUNK, max_chunks=SMOKE_MAX_CHUNKS, sort_time=False)\n\n    miss_tr = [x for x in pick_tr if x not in got_tr]\n    miss_te = [x for x in pick_te if x not in got_te]\n\n    if miss_tr:\n        print(f\"[WARN][SMOKE] split={s} train not found within first {SMOKE_MAX_CHUNKS} chunks: {miss_tr[:2]}\")\n    if miss_te:\n        print(f\"[WARN][SMOKE] split={s} test not found within first {SMOKE_MAX_CHUNKS} chunks: {miss_te[:2]}\")\n\nprint(\"STAGE 3 OK — LIGHTCURVE LOADING UTILITIES READY\")\nprint(f\"- Saved manifest: {manifest_path}\")\nprint(f\"- Saved counts  : {counts_path}\")\nprint(f\"- LC_CACHE_DIR  : {LC_CACHE_DIR}\")\nprint(f\"- Smoke splits  : {candidate_splits}\")\n\nglobals().update({\n    \"SPLIT_FILES\": SPLIT_FILES,\n    \"train_ids_by_split\": train_ids_by_split,\n    \"test_ids_by_split\": test_ids_by_split,\n    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n    \"load_object_lightcurve\": load_object_lightcurve,\n    \"load_many_object_lightcurves\": load_many_object_lightcurves,\n    \"load_object_lightcurve_cached\": load_object_lightcurve_cached,\n    \"get_lc_cache_path\": get_lc_cache_path,\n    \"LC_CACHE_DIR\": LC_CACHE_DIR,\n    \"REQ_LC_KEYS\": REQ_LC_KEYS,\n    \"ALLOWED_FILTERS\": ALLOWED_FILTERS,\n    \"FILTER_ORDER\": FILTER_ORDER,\n    \"FILTER2ID\": FILTER2ID,\n    \"ID2FILTER\": ID2FILTER,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — Photometric Cleaning (FORCE OVERWRITE) — REVISI FULL v6.4\n# - DustValues (Rubin r_x) for ugrizy\n# - Output MAG + ASINH (preserve negative flux info)\n# ============================================================\n\nimport gc, json, warnings, time, shutil\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n\nART_DIR = Path(ART_DIR)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 1) Settings\n# ----------------------------\nCHUNKSIZE   = 350_000\nERR_EPS     = 1e-6\n\nSNR_DET     = 3.0     # boleh coba 2.0 juga, tapi mulai dari 3.0 dulu\nDET_SIGMA   = 3.0\n\nMIN_FLUX_POS_UJY   = 1e-6\nMAG_MIN, MAG_MAX   = -10.0, 50.0\nMAGERR_FLOOR_DET   = 1e-3\nMAGERR_FLOOR_ND    = 0.75\nMAGERR_CAP         = 10.0\n\nWRITE_FORMAT = \"parquet\"   # \"parquet\" or \"csv.gz\"\nONLY_SPLITS  = None        # e.g. [\"split_01\"]\nKEEP_FLUX_DEBUG = False\nDROP_BAD_TIME_ROWS = True\n\nREBUILD_MODE = \"wipe_all\"  # \"wipe_all\" | \"wipe_parts_only\"\n\n# ----------------------------\n# 2) Extinction coefficients (Rubin DustValues().r_x)\n#    r_x * EBV = A_x  (extinction in mag)\n# ----------------------------\n# Sumber nilai (contoh print DustValues().r_x):\n# u: 4.757217815396922\n# g: 3.6605664439892625\n# r: 2.70136780871597\n# i: 2.0536599130965882\n# z: 1.5900964472616756\n# y: 1.3077049588254708\n\nEXT_RLAMBDA = {\n    \"u\": 4.757217815396922,\n    \"g\": 3.6605664439892625,\n    \"r\": 2.70136780871597,\n    \"i\": 2.0536599130965882,\n    \"z\": 1.5900964472616756,\n    \"y\": 1.3077049588254708,\n}\n\nBAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\nID2BAND = {v: k for k, v in BAND2ID.items()}\n\n# pakai EBV_clip jika ada (lebih stabil), fallback EBV\nEBV_TRAIN_SER = df_train_meta[\"EBV_clip\"] if \"EBV_clip\" in df_train_meta.columns else df_train_meta[\"EBV\"]\nEBV_TEST_SER  = df_test_meta[\"EBV_clip\"]  if \"EBV_clip\"  in df_test_meta.columns  else df_test_meta[\"EBV\"]\n\nMAG_ZP = float(2.5 * np.log10(3631e6))  # ~23.9 (uJy)\n\n# ----------------------------\n# 3) Output root + WIPE (with safety guard)\n# ----------------------------\nLC_CLEAN_DIR = ART_DIR / \"lc_clean_mag\"\n\nart_abs = ART_DIR.resolve()\nlc_abs  = LC_CLEAN_DIR.resolve()\n\ntry:\n    ok_rel = lc_abs.is_relative_to(art_abs)\nexcept AttributeError:\n    ok_rel = str(lc_abs).startswith(str(art_abs) + \"/\") or str(lc_abs).startswith(str(art_abs) + \"\\\\\")\n\nif not ok_rel:\n    raise RuntimeError(f\"Safety guard failed: LC_CLEAN_DIR bukan turunan ART_DIR.\\nART_DIR={art_abs}\\nLC_CLEAN_DIR={lc_abs}\")\n\nif REBUILD_MODE == \"wipe_all\":\n    if LC_CLEAN_DIR.exists():\n        shutil.rmtree(LC_CLEAN_DIR, ignore_errors=True)\n    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\nelif REBUILD_MODE == \"wipe_parts_only\":\n    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\nelse:\n    raise ValueError(\"REBUILD_MODE must be 'wipe_all' or 'wipe_parts_only'\")\n\n# ----------------------------\n# 4) Atomic writer\n# ----------------------------\ndef _atomic_write_parquet(df: pd.DataFrame, out_path: Path):\n    tmp = out_path.with_name(out_path.stem + \".tmp\" + out_path.suffix)\n    try:\n        df.to_parquet(tmp, index=False)\n        tmp.replace(out_path)\n    finally:\n        if tmp.exists() and (not out_path.exists()):\n            try: tmp.unlink()\n            except Exception: pass\n\ndef _atomic_write_csv_gz(df: pd.DataFrame, out_path: Path):\n    final_path = out_path.with_suffix(\".csv.gz\")\n    tmp = final_path.with_name(final_path.stem + \".tmp\" + \"\".join(final_path.suffixes))\n    try:\n        df.to_csv(tmp, index=False, compression=\"gzip\")\n        tmp.replace(final_path)\n    finally:\n        if tmp.exists() and (not final_path.exists()):\n            try: tmp.unlink()\n            except Exception: pass\n    return final_path\n\ndef write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    if fmt == \"parquet\":\n        try:\n            _atomic_write_parquet(df, out_path)\n            return \"parquet\", out_path\n        except Exception as e:\n            alt = _atomic_write_csv_gz(df, out_path)\n            return f\"csv.gz (fallback from parquet: {type(e).__name__})\", alt\n    elif fmt == \"csv.gz\":\n        alt = _atomic_write_csv_gz(df, out_path)\n        return \"csv.gz\", alt\n    else:\n        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n\n# ----------------------------\n# 5) Core cleaning -> MAG + ASINH\n# ----------------------------\ndef clean_chunk_to_phot(ch: pd.DataFrame, ebv_ser: pd.Series):\n    oid_ser  = ch[\"object_id\"].astype(\"string\").str.strip()\n    filt_ser = ch[\"filter\"].astype(\"string\").str.strip().str.lower()\n\n    mjd  = ch[\"mjd\"].to_numpy(copy=False).astype(np.float32, copy=False)\n    flux = ch[\"flux\"].to_numpy(copy=False).astype(np.float32, copy=False)\n    err  = ch[\"flux_err\"].to_numpy(copy=False).astype(np.float32, copy=False)\n\n    err = np.nan_to_num(err, nan=np.float32(ERR_EPS), posinf=np.float32(ERR_EPS), neginf=np.float32(ERR_EPS))\n    err = np.maximum(err, np.float32(ERR_EPS))\n\n    flux = flux.astype(np.float32, copy=False)\n    flux[~np.isfinite(flux)] = np.float32(np.nan)\n\n    band_id_ser = filt_ser.map(BAND2ID).fillna(-1).astype(\"int16\")\n    band_id = band_id_ser.to_numpy(copy=False).astype(np.int16, copy=False)\n    if np.any(band_id < 0):\n        bad = filt_ser[band_id_ser < 0].value_counts().head(10).index.tolist()\n        raise ValueError(f\"Unknown/invalid filter values encountered (top examples): {bad}\")\n    band_id = band_id.astype(np.int8, copy=False)\n\n    ebv = oid_ser.map(ebv_ser).fillna(0.0).to_numpy(copy=False).astype(np.float32, copy=False)\n    ebv[~np.isfinite(ebv)] = np.float32(0.0)\n\n    rlam = filt_ser.map(EXT_RLAMBDA).fillna(0.0).to_numpy(copy=False).astype(np.float32, copy=False)\n\n    A = (rlam * ebv).astype(np.float32, copy=False)\n    mul = np.power(np.float32(10.0), (np.float32(0.4) * A)).astype(np.float32, copy=False)\n\n    flux_deext = (flux * mul).astype(np.float32, copy=False)\n    err_deext  = (err  * mul).astype(np.float32, copy=False)\n\n    okf = np.isfinite(flux_deext)\n    snr = np.zeros_like(err_deext, dtype=np.float32)\n    snr[okf] = (flux_deext[okf] / np.maximum(err_deext[okf], np.float32(ERR_EPS))).astype(np.float32, copy=False)\n\n    detected = (snr > np.float32(SNR_DET)).astype(np.int8, copy=False)\n\n    nan_flux_rows = int((~okf).sum())\n    if nan_flux_rows:\n        detected[~okf] = np.int8(0)\n        snr[~okf] = np.float32(0.0)\n        flux_deext[~okf] = np.float32(0.0)\n\n    # -------- MAG branch (seperti kamu, tapi pakai deext)\n    flux_detlim = (np.float32(DET_SIGMA) * err_deext).astype(np.float32, copy=False)\n    flux_for_mag = np.where(\n        detected == 1,\n        np.maximum(flux_deext, np.float32(MIN_FLUX_POS_UJY)),\n        np.maximum(flux_detlim, np.float32(MIN_FLUX_POS_UJY)),\n    ).astype(np.float32, copy=False)\n\n    mag = (np.float32(MAG_ZP) - np.float32(2.5) * np.log10(flux_for_mag)).astype(np.float32, copy=False)\n    mag = np.clip(mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32, copy=False)\n\n    mag_err = (np.float32(1.0857362) * (err_deext / flux_for_mag)).astype(np.float32, copy=False)\n    mag_err = np.clip(mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32, copy=False)\n\n    if MAGERR_FLOOR_ND is not None and float(MAGERR_FLOOR_ND) > 0:\n        mag_err = np.where(\n            detected == 1,\n            mag_err,\n            np.maximum(mag_err, np.float32(MAGERR_FLOOR_ND))\n        ).astype(np.float32, copy=False)\n\n    # -------- ASINH branch (preserve sign; stable)\n    snr_safe = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n    flux_asinh = np.arcsinh(snr_safe).astype(np.float32)   # compressed SNR with sign\n    err_log1p  = np.log1p(np.maximum(err_deext, np.float32(ERR_EPS))).astype(np.float32)\n\n    out = pd.DataFrame({\n        \"object_id\": pd.array(oid_ser.to_numpy(copy=False), dtype=\"string\"),\n        \"mjd\": mjd,\n        \"band_id\": band_id,\n        \"mag\": mag,\n        \"mag_err\": mag_err,\n        \"flux_asinh\": flux_asinh,\n        \"err_log1p\": err_log1p,\n        \"snr\": snr_safe,\n        \"detected\": detected,\n    })\n\n    dropped_time = 0\n    if DROP_BAD_TIME_ROWS:\n        t = out[\"mjd\"].to_numpy(copy=False).astype(np.float32, copy=False)\n        keep = np.isfinite(t)\n        dropped_time = int((~keep).sum())\n        if dropped_time:\n            out = out[keep]\n\n    if KEEP_FLUX_DEBUG:\n        out[\"flux_deext\"] = pd.Series(flux_deext, dtype=\"float32\")\n        out[\"err_deext\"]  = pd.Series(err_deext, dtype=\"float32\")\n\n    return out, dropped_time, nan_flux_rows\n\n# ----------------------------\n# 6) Process split-wise\n# ----------------------------\nsplits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else list(SPLIT_LIST)\nsummary_rows, manifest_rows = [], []\n\ndef _wipe_parts_dir(out_dir: Path):\n    if out_dir.exists():\n        for pat in [\"part_*.parquet\", \"part_*.csv.gz\", \"*.tmp\", \"*.tmp.parquet\", \"*.tmp.csv.gz\"]:\n            for f in out_dir.glob(pat):\n                try: f.unlink()\n                except Exception: pass\n\ndef process_split(split_name: str, which: str):\n    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n    out_dir = LC_CLEAN_DIR / split_name / which\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    if REBUILD_MODE == \"wipe_parts_only\":\n        _wipe_parts_dir(out_dir)\n\n    t0 = time.time()\n    part_idx = 0\n    n_rows_total = 0\n    n_det = 0\n    dropped_time_total = 0\n    nan_flux_total = 0\n    mag_min = np.inf\n    mag_max = -np.inf\n\n    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n        cleaned, dropped_time, nan_flux = clean_chunk_to_phot(ch, ebv_ser)\n\n        dropped_time_total += int(dropped_time)\n        nan_flux_total += int(nan_flux)\n\n        n_rows = int(len(cleaned))\n        n_rows_total += n_rows\n        n_det += int(cleaned[\"detected\"].to_numpy(copy=False).astype(np.int8).sum())\n\n        mag_arr = cleaned[\"mag\"].to_numpy(copy=False).astype(np.float32, copy=False)\n        fin = np.isfinite(mag_arr)\n        if fin.any():\n            mag_min = float(min(mag_min, float(np.min(mag_arr[fin]))))\n            mag_max = float(max(mag_max, float(np.max(mag_arr[fin]))))\n\n        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n        used_fmt, final_path = write_part(cleaned, out_path, WRITE_FORMAT)\n\n        manifest_rows.append({\n            \"split\": split_name,\n            \"which\": which,\n            \"part\": int(part_idx),\n            \"path\": str(final_path),\n            \"rows\": int(n_rows),\n            \"format\": str(used_fmt),\n        })\n\n        part_idx += 1\n        del cleaned, ch\n        if part_idx % 10 == 0:\n            gc.collect()\n\n    dt = time.time() - t0\n    summary_rows.append({\n        \"split\": split_name,\n        \"which\": which,\n        \"parts\": int(part_idx),\n        \"rows\": int(n_rows_total),\n        \"det_frac_snr_gt_thr\": float(n_det / max(n_rows_total, 1)),\n        \"mag_min\": (mag_min if np.isfinite(mag_min) else np.nan),\n        \"mag_max\": (mag_max if np.isfinite(mag_max) else np.nan),\n        \"dropped_time_rows\": int(dropped_time_total),\n        \"nan_flux_rows\": int(nan_flux_total),\n        \"sec\": float(dt),\n    })\n\n    print(\n        f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | \"\n        f\"det%={100*(n_det/max(n_rows_total,1)):.2f}% | \"\n        f\"nan_flux={nan_flux_total:,} | drop_time={dropped_time_total:,} | \"\n        f\"mag_range=[{(mag_min if np.isfinite(mag_min) else np.nan):.2f}, {(mag_max if np.isfinite(mag_max) else np.nan):.2f}] | \"\n        f\"time={dt:.1f}s\"\n    )\n\nprint(f\"[Stage 4] REBUILD_MODE={REBUILD_MODE} | Writing to: {LC_CLEAN_DIR}\")\nfor s in splits_to_use:\n    process_split(s, \"train\")\n    process_split(s, \"test\")\n\ndf_parts_manifest = pd.DataFrame(manifest_rows)\ndf_summary  = pd.DataFrame(summary_rows)\n\nmanifest_path = LC_CLEAN_DIR / \"lc_clean_mag_manifest.csv\"\nsummary_path  = LC_CLEAN_DIR / \"lc_clean_mag_summary.csv\"\ndf_parts_manifest.to_csv(manifest_path, index=False)\ndf_summary.to_csv(summary_path, index=False)\n\ncfg_path = LC_CLEAN_DIR / \"photometric_config_mag.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\n        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n        \"SNR_DET\": float(SNR_DET),\n        \"DET_SIGMA\": float(DET_SIGMA),\n        \"ERR_EPS\": float(ERR_EPS),\n        \"MAG_ZP\": float(MAG_ZP),\n        \"MAG_MIN\": float(MAG_MIN),\n        \"MAG_MAX\": float(MAG_MAX),\n        \"CHUNKSIZE\": int(CHUNKSIZE),\n        \"WRITE_FORMAT\": str(WRITE_FORMAT),\n        \"ONLY_SPLITS\": list(splits_to_use),\n        \"KEEP_FLUX_DEBUG\": bool(KEEP_FLUX_DEBUG),\n        \"DROP_BAD_TIME_ROWS\": bool(DROP_BAD_TIME_ROWS),\n        \"REBUILD_MODE\": str(REBUILD_MODE),\n        \"SCHEMA\": \"mag+asinh\",\n    }, f, indent=2)\n\nprint(\"\\n[Stage 4] Done.\")\nprint(f\"- LC_CLEAN_DIR  : {LC_CLEAN_DIR}\")\nprint(f\"- Saved manifest: {manifest_path}\")\nprint(f\"- Saved summary : {summary_path}\")\nprint(f\"- Saved config  : {cfg_path}\")\n\ndef get_clean_parts(split_name: str, which: str):\n    m = df_parts_manifest[(df_parts_manifest[\"split\"] == split_name) & (df_parts_manifest[\"which\"] == which)].sort_values(\"part\")\n    return m[\"path\"].astype(str).tolist()\n\nglobals().update({\n    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n    \"BAND2ID\": BAND2ID,\n    \"ID2BAND\": ID2BAND,\n    \"MAG_ZP\": MAG_ZP,\n    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n    \"lc_clean_mag_manifest\": df_parts_manifest,\n    \"lc_clean_mag_summary\": df_summary,\n    \"get_clean_parts\": get_clean_parts,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sequence Tokenization (Event-based Tokens)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL)\n# REVISI FULL v5.4 (FORCE ASINH + FEATURESET v2 + META OPTIONAL)\n# ============================================================\n\nimport gc, json, warnings, time, shutil\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nfor need in [\"ART_DIR\", \"df_train_meta\", \"df_test_meta\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 0 + STAGE 2 dulu (ART_DIR + meta).\")\n\nART_DIR = Path(ART_DIR)\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\ndef _safe_string_series(s: pd.Series) -> pd.Series:\n    try:\n        return s.astype(\"string\").str.strip()\n    except Exception:\n        return s.astype(str).str.strip()\n\ndef _find_stage4_manifest(art_dir: Path):\n    cand = art_dir / \"lc_clean_mag\" / \"lc_clean_mag_manifest.csv\"\n    if cand.exists():\n        return cand\n    root = Path(\"/kaggle/working/mallorn_run\")\n    if not root.exists():\n        return None\n    cands = list(root.glob(\"run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\"))\n    if not cands:\n        cands = list(root.glob(\"run_*/**/lc_clean_mag_manifest.csv\"))\n    if not cands:\n        return None\n    cands = sorted(cands, key=lambda p: p.stat().st_mtime, reverse=True)\n    return cands[0]\n\ndef _sync_dirs_from_manifest(manifest_csv: Path):\n    lc_clean_dir = manifest_csv.parent\n    art_dir_new  = lc_clean_dir.parent\n    run_dir_new  = art_dir_new.parent\n    return run_dir_new, art_dir_new, lc_clean_dir\n\ndef _read_meta_file(art_dir_synced: Path, which: str) -> pd.DataFrame:\n    pq = art_dir_synced / f\"{which}_meta.parquet\"\n    csv = art_dir_synced / f\"{which}_meta.csv\"\n    if pq.exists():\n        df = pd.read_parquet(pq)\n    elif csv.exists():\n        df = pd.read_csv(csv)\n    else:\n        return None\n    if isinstance(df.index, pd.RangeIndex) and (\"object_id\" in df.columns):\n        df = df.set_index(\"object_id\", drop=True)\n    elif (\"object_id\" in df.columns) and (df.index.name != \"object_id\"):\n        df = df.set_index(\"object_id\", drop=True)\n    if isinstance(df.index, pd.RangeIndex):\n        for c in [\"Unnamed: 0\", \"index\"]:\n            if c in df.columns:\n                df = df.set_index(c, drop=True)\n                break\n    df.index = df.index.astype(\"string\")\n    return df\n\ndef _load_meta_if_needed(art_dir_synced: Path):\n    global df_train_meta, df_test_meta\n    cand_train = _read_meta_file(art_dir_synced, \"train\")\n    cand_test  = _read_meta_file(art_dir_synced, \"test\")\n    if cand_train is None or cand_test is None:\n        return False, \"meta file not found in synced ART_DIR; keep in-memory\"\n    try:\n        if (len(df_train_meta) != len(cand_train)) or (len(df_test_meta) != len(cand_test)):\n            df_train_meta = cand_train\n            df_test_meta  = cand_test\n            return True, \"reloaded meta due to size mismatch\"\n        sample_ids = df_train_meta.index[:5].astype(str).tolist()\n        if not all((sid in cand_train.index) for sid in sample_ids):\n            df_train_meta = cand_train\n            df_test_meta  = cand_test\n            return True, \"reloaded meta due to id mismatch\"\n        return False, \"meta already consistent\"\n    except Exception as e:\n        return False, f\"meta reload skipped ({type(e).__name__}: {e})\"\n\n# ----------------------------\n# 2) Locate STAGE 4 output\n# ----------------------------\nmanifest_csv = _find_stage4_manifest(ART_DIR)\nif manifest_csv is None:\n    root = Path(\"/kaggle/working/mallorn_run\")\n    runs = sorted([p.name for p in root.glob(\"run_*\") if p.is_dir()])[-15:] if root.exists() else []\n    raise RuntimeError(\n        \"Output STAGE 4 (lc_clean_mag_manifest.csv) tidak ditemukan.\\n\"\n        f\"- ART_DIR saat ini: {ART_DIR}\\n\"\n        f\"- Expected: {ART_DIR/'lc_clean_mag'}\\n\"\n        f\"- Runs available (last 15): {runs}\\n\"\n        \"Solusi: pastikan STAGE 4 selesai dan menulis artifacts/lc_clean_mag.\"\n    )\n\nRUN_DIR, ART_DIR, LC_CLEAN_DIR = _sync_dirs_from_manifest(manifest_csv)\n\nprint(\"STAGE 5 ROUTING SYNC OK\")\nprint(f\"- RUN_DIR      : {RUN_DIR}\")\nprint(f\"- ART_DIR      : {ART_DIR}\")\nprint(f\"- LC_CLEAN_DIR : {LC_CLEAN_DIR}\")\nprint(f\"- manifest_csv : {manifest_csv}\")\n\nreloaded, msg = _load_meta_if_needed(ART_DIR)\nprint(f\"- meta_sync    : {msg}\")\n\n# ----------------------------\n# 3) Load & validate Stage4 manifest\n# ----------------------------\n_df_clean_manifest = pd.read_csv(manifest_csv)\n_df_clean_manifest.columns = [c.strip() for c in _df_clean_manifest.columns]\n\nneed_cols = {\"split\", \"which\", \"part\", \"path\"}\nmiss = sorted(list(need_cols - set(_df_clean_manifest.columns)))\nif miss:\n    raise RuntimeError(f\"Manifest STAGE 4 missing columns: {miss} | cols={list(_df_clean_manifest.columns)}\")\n\npaths = _df_clean_manifest[\"path\"].astype(str).tolist()\nmissing_paths = [p for p in paths if not Path(p).exists()]\nif missing_paths:\n    raise RuntimeError(\n        \"Ada file part STAGE 4 yang hilang.\\n\"\n        f\"Missing count={len(missing_paths)} | contoh={missing_paths[:10]}\\n\"\n        \"Solusi: rerun STAGE 4 (wipe_all) untuk regenerasi cache.\"\n    )\n\ndef get_clean_parts(split_name: str, which: str):\n    m = _df_clean_manifest[(_df_clean_manifest[\"split\"] == split_name) & (_df_clean_manifest[\"which\"] == which)]\n    if m.empty:\n        return []\n    return m.sort_values(\"part\")[\"path\"].astype(str).tolist()\n\n# ----------------------------\n# 4) Recover SPLIT_LIST + routing ids\n# ----------------------------\nSPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\nsplits_in_manifest = sorted(set(_df_clean_manifest[\"split\"].astype(str).tolist()))\nSPLITS_TO_CONSIDER = [s for s in SPLIT_LIST if s in splits_in_manifest]\n\ntrain_ids_by_split = {s: [] for s in SPLIT_LIST}\nfor oid, sp in df_train_meta[\"split\"].astype(str).items():\n    if sp in train_ids_by_split:\n        train_ids_by_split[sp].append(str(oid))\n\ntest_ids_by_split = {s: [] for s in SPLIT_LIST}\nfor oid, sp in df_test_meta[\"split\"].astype(str).items():\n    if sp in test_ids_by_split:\n        test_ids_by_split[sp].append(str(oid))\n\n# ----------------------------\n# 5) Settings (recommended defaults)\n# ----------------------------\nONLY_SPLITS = None\nREBUILD_MODE = \"wipe_all\"          # \"wipe_all\" or \"reuse_if_exists\"\n\nCOMPRESS_NPZ = False\nSHARD_MAX_OBJECTS = 1500\n\nSNR_TANH_SCALE = 10.0\nTIME_CLIP_MAX_DAYS = None\nDROP_BAD_TIME_ROWS = True\n\nL_MAX = int(CFG.get(\"L_MAX\", 256)) if \"CFG\" in globals() else 256\nTRUNC_POLICY = str(CFG.get(\"TRUNC_POLICY\", \"smart\")) if \"CFG\" in globals() else \"smart\"\nKEEP_DET_FRAC = float(CFG.get(\"KEEP_DET_FRAC\", 0.70)) if \"CFG\" in globals() else 0.70\nKEEP_EDGE = True\nUSE_RESTFRAME_TIME = bool(CFG.get(\"USE_RESTFRAME_TIME\", True)) if \"CFG\" in globals() else True\n\n# speed: naikkan buckets biar file per bucket lebih kecil (biasanya lebih stabil RAM)\nNUM_BUCKETS = int(CFG.get(\"SEQ_NUM_BUCKETS\", 256)) if \"CFG\" in globals() else 256\n\n# NEW: mode & feature set\nTOKEN_MODE_FORCE = str(CFG.get(\"TOKEN_MODE_FORCE\", \"asinh\")) if \"CFG\" in globals() else \"asinh\"\nFEATURE_SET = str(CFG.get(\"SEQ_FEATURE_SET\", \"v2\")) if \"CFG\" in globals() else \"v2\"\nADD_OBJ_META_PER_TOKEN = bool(CFG.get(\"SEQ_ADD_META_PER_TOKEN\", True)) if \"CFG\" in globals() else True\n\n# meta cols (ambil jika tersedia)\nOBJ_META_COLS = [\"EBV_clip\", \"log1pZ\", \"zerr_rel\", \"is_photoz\"]\n\nSEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\nSEQ_DIR.mkdir(parents=True, exist_ok=True)\n\nTOKEN_MODE = None\nFEATURE_NAMES = None\nFEATURE_DIM = None\n_prev_cfg = SEQ_DIR / \"seq_config.json\"\nif _prev_cfg.exists():\n    try:\n        _c = json.loads(_prev_cfg.read_text())\n        TOKEN_MODE = _c.get(\"token_mode\", None)\n        FEATURE_NAMES = _c.get(\"feature_names\", None)\n        FEATURE_DIM = _c.get(\"feature_dim\", None)\n    except Exception:\n        pass\n\nBASE_COLS = {\"object_id\", \"mjd\", \"band_id\", \"snr\", \"detected\"}\n\n# Schema STAGE4 v6.4: mag+asinh always exists\nMODE_COLS = {\n    \"mag\": {\"mag\", \"mag_err\"},\n    \"asinh\": {\"flux_asinh\", \"err_log1p\"},\n}\n\n# ----------------------------\n# 6) Reader for cleaned parts\n# ----------------------------\ndef _read_clean_part(path: str) -> pd.DataFrame:\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(f\"Clean part missing: {p}\")\n\n    if p.suffix == \".parquet\":\n        df = pd.read_parquet(p)\n    elif p.name.endswith(\".csv.gz\"):\n        df = pd.read_csv(p, compression=\"gzip\")\n    else:\n        df = pd.read_csv(p)\n\n    df.columns = [c.strip() for c in df.columns]\n\n    global TOKEN_MODE, FEATURE_NAMES, FEATURE_DIM\n\n    cols = set(df.columns)\n    # choose mode with priority:\n    # 1) TOKEN_MODE_FORCE if available\n    # 2) fallback auto detect\n    if TOKEN_MODE is None:\n        chosen = None\n        if TOKEN_MODE_FORCE in MODE_COLS and BASE_COLS.issubset(cols) and MODE_COLS[TOKEN_MODE_FORCE].issubset(cols):\n            chosen = TOKEN_MODE_FORCE\n        else:\n            if BASE_COLS.issubset(cols) and MODE_COLS[\"asinh\"].issubset(cols):\n                chosen = \"asinh\"\n            elif BASE_COLS.issubset(cols) and MODE_COLS[\"mag\"].issubset(cols):\n                chosen = \"mag\"\n\n        if chosen is None:\n            raise RuntimeError(\n                \"Cannot detect cleaned schema.\\n\"\n                f\"Found cols={list(df.columns)}\\n\"\n                \"Expected MAG or ASINH schema from STAGE 4.\"\n            )\n\n        TOKEN_MODE = chosen\n\n        # feature names by set\n        if FEATURE_SET == \"v2\":\n            core = [\"t_rel_log\", \"dt_log\", \"dt_band_log\", \"signal\", \"err_log\", \"snr_tanh\", \"detected\", \"band_change\", \"delta_signal\"]\n        else:\n            core = [\"t_rel_log\", \"dt_log\", \"signal\", \"err_log\", \"snr_tanh\", \"detected\"]\n\n        meta_add = []\n        if ADD_OBJ_META_PER_TOKEN:\n            meta_add = [f\"meta_{c}\" for c in OBJ_META_COLS]\n\n        FEATURE_NAMES = core + meta_add\n        FEATURE_DIM = len(FEATURE_NAMES)\n\n    # validate required columns\n    req = set(BASE_COLS) | set(MODE_COLS[TOKEN_MODE])\n    miss = sorted(list(req - set(df.columns)))\n    if miss:\n        raise RuntimeError(f\"Clean part missing columns: {miss} | file={p}\")\n\n    df[\"object_id\"] = _safe_string_series(df[\"object_id\"])\n    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n    df[\"snr\"] = pd.to_numeric(df[\"snr\"], errors=\"coerce\").astype(np.float32)\n    df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n\n    if TOKEN_MODE == \"mag\":\n        df[\"mag\"] = pd.to_numeric(df[\"mag\"], errors=\"coerce\").astype(np.float32)\n        df[\"mag_err\"] = pd.to_numeric(df[\"mag_err\"], errors=\"coerce\").astype(np.float32)\n    else:\n        df[\"flux_asinh\"] = pd.to_numeric(df[\"flux_asinh\"], errors=\"coerce\").astype(np.float32)\n        df[\"err_log1p\"] = pd.to_numeric(df[\"err_log1p\"], errors=\"coerce\").astype(np.float32)\n\n    if DROP_BAD_TIME_ROWS:\n        df = df[np.isfinite(df[\"mjd\"].to_numpy(copy=False))]\n\n    return df\n\n# ----------------------------\n# 7) Truncation (same)\n# ----------------------------\ndef _smart_truncate(mjd, det, snr, Lmax: int):\n    n = len(mjd)\n    if n <= Lmax:\n        return np.arange(n, dtype=np.int64)\n\n    idx_all = np.arange(n, dtype=np.int64)\n    keep = set()\n    if KEEP_EDGE:\n        keep.add(0); keep.add(n - 1)\n\n    det_idx = idx_all[det.astype(bool)]\n    k_det = int(max(0, min(len(det_idx), int(np.floor(Lmax * KEEP_DET_FRAC)))))\n    if k_det > 0 and len(det_idx) > 0:\n        score = np.abs(snr[det_idx])\n        top = det_idx[np.argsort(-score)[:k_det]]\n        for i in top.tolist():\n            keep.add(int(i))\n\n    if len(keep) < Lmax:\n        rem = [i for i in idx_all.tolist() if i not in keep]\n        need = Lmax - len(keep)\n        if rem and need > 0:\n            pick = np.linspace(0, len(rem) - 1, num=need, dtype=int)\n            for p in pick.tolist():\n                keep.add(int(rem[p]))\n\n    out = np.array(sorted(keep), dtype=np.int64)\n    if len(out) > Lmax:\n        pos = np.linspace(0, len(out) - 1, num=Lmax, dtype=int)\n        out = out[pos]\n    return out\n\n# ----------------------------\n# 8) Build tokens per object (feature v2 + meta optional)\n# ----------------------------\ndef _get_obj_meta_vec(meta_df: pd.DataFrame, oid: str) -> np.ndarray:\n    if (not ADD_OBJ_META_PER_TOKEN) or (oid not in meta_df.index):\n        return np.zeros((len(OBJ_META_COLS),), dtype=np.float32)\n\n    vals = []\n    for c in OBJ_META_COLS:\n        if c in meta_df.columns:\n            v = meta_df.loc[oid, c]\n            v = float(v) if (v is not None and np.isfinite(v)) else 0.0\n        else:\n            v = 0.0\n        vals.append(v)\n    return np.asarray(vals, dtype=np.float32)\n\ndef build_empty_tokens(meta_dim: int):\n    X = np.zeros((1, int(FEATURE_DIM)), dtype=np.float32)\n    B = np.full((1,), -1, dtype=np.int8)\n    return X, B, 0, 1\n\ndef build_object_tokens(df_obj: pd.DataFrame, meta_df: pd.DataFrame, z_val: float = 0.0):\n    if df_obj is None or df_obj.empty:\n        return build_empty_tokens(meta_dim=len(OBJ_META_COLS))\n\n    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n\n    order = np.lexsort((band, mjd))\n    mjd = mjd[order]; band = band[order]; snr = snr[order]; det = det[order]\n\n    z = float(z_val) if (z_val is not None and np.isfinite(z_val)) else 0.0\n    denom = (1.0 + max(z, 0.0)) if USE_RESTFRAME_TIME else 1.0\n\n    t0 = mjd[0]\n    t_rel = (mjd - t0) / np.float32(denom)\n    dt = np.empty_like(t_rel); dt[0] = 0.0\n    if len(t_rel) > 1:\n        dt[1:] = np.maximum(t_rel[1:] - t_rel[:-1], 0.0)\n\n    if TIME_CLIP_MAX_DAYS is not None:\n        mx = np.float32(TIME_CLIP_MAX_DAYS)\n        t_rel = np.clip(t_rel, 0.0, mx)\n        dt    = np.clip(dt,    0.0, mx)\n\n    t_rel_log = np.log1p(t_rel).astype(np.float32)\n    dt_log    = np.log1p(dt).astype(np.float32)\n\n    # NEW: dt per-band\n    dt_band = np.zeros_like(dt, dtype=np.float32)\n    last_t = {}\n    for i in range(len(mjd)):\n        b = int(band[i])\n        if b in last_t:\n            dt_band[i] = max(float((mjd[i] - last_t[b]) / denom), 0.0)\n        last_t[b] = float(mjd[i])\n    dt_band_log = np.log1p(dt_band).astype(np.float32)\n\n    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n    det_f = det.astype(np.float32)\n\n    if TOKEN_MODE == \"mag\":\n        sig = df_obj[\"mag\"].to_numpy(dtype=np.float32, copy=False)[order]\n        err = df_obj[\"mag_err\"].to_numpy(dtype=np.float32, copy=False)[order]\n        sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        err = np.nan_to_num(err, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        err = np.maximum(err, np.float32(0.0))\n        err_log = np.log1p(err).astype(np.float32)\n    else:\n        sig = df_obj[\"flux_asinh\"].to_numpy(dtype=np.float32, copy=False)[order]\n        err_log = df_obj[\"err_log1p\"].to_numpy(dtype=np.float32, copy=False)[order]\n        sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        err_log = np.nan_to_num(err_log, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n\n    # NEW: band_change + delta_signal\n    band_change = np.zeros((len(band),), dtype=np.float32)\n    delta_signal = np.zeros((len(sig),), dtype=np.float32)\n    if len(band) > 1:\n        band_change[1:] = (band[1:] != band[:-1]).astype(np.float32)\n        delta_signal[1:] = (sig[1:] - sig[:-1]).astype(np.float32)\n\n    if FEATURE_SET == \"v2\":\n        X = np.stack(\n            [t_rel_log, dt_log, dt_band_log, sig, err_log, snr_tanh, det_f, band_change, delta_signal],\n            axis=1\n        ).astype(np.float32)\n    else:\n        X = np.stack([t_rel_log, dt_log, sig, err_log, snr_tanh, det_f], axis=1).astype(np.float32)\n\n    # append meta-per-token (constant)\n    if ADD_OBJ_META_PER_TOKEN:\n        oid = str(df_obj[\"object_id\"].iloc[0])\n        mv = _get_obj_meta_vec(meta_df, oid)  # (d_meta,)\n        mv_rep = np.repeat(mv[None, :], repeats=X.shape[0], axis=0).astype(np.float32)\n        X = np.concatenate([X, mv_rep], axis=1).astype(np.float32)\n\n    L0 = int(X.shape[0])\n    if L_MAX and int(L_MAX) > 0 and X.shape[0] > int(L_MAX):\n        if TRUNC_POLICY == \"smart\":\n            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n        elif TRUNC_POLICY == \"head\":\n            keep = np.arange(int(L_MAX), dtype=np.int64)\n        else:\n            keep = np.arange(X.shape[0], dtype=np.int64)\n\n        if len(keep) != X.shape[0]:\n            X = X[keep]\n            band = band[keep]\n\n            # recompute dt_log\n            sel_mjd = mjd[keep]\n            sel_t = (sel_mjd - sel_mjd[0]) / np.float32(denom)\n            sel_dt = np.empty_like(sel_t); sel_dt[0] = 0.0\n            if len(sel_t) > 1:\n                sel_dt[1:] = np.maximum(sel_t[1:] - sel_t[:-1], 0.0)\n            # dt_log column index:\n            # v2: dt_log is col 1; v1: dt_log col 1\n            X[:, 1] = np.log1p(sel_dt).astype(np.float32)\n\n    return X, band.astype(np.int8), L0, int(X.shape[0])\n\n# ----------------------------\n# 9) Shard writer + reuse manifest (same)\n# ----------------------------\ndef save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    obj_arr = np.asarray(object_ids, dtype=\"S\")\n    if COMPRESS_NPZ:\n        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n    else:\n        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n\ndef reconstruct_manifest_from_shards(split_name: str, which: str, out_dir: Path):\n    rows = []\n    for sp in sorted(out_dir.glob(\"shard_*.npz\")):\n        data = np.load(sp, allow_pickle=False)\n        obj = data[\"object_id\"]\n        offsets = data[\"offsets\"].astype(np.int64)\n        if offsets.ndim != 1 or offsets.size < 2:\n            continue\n        lengths = offsets[1:] - offsets[:-1]\n        for i in range(len(lengths)):\n            oid = obj[i]\n            oid = oid.decode(\"utf-8\", errors=\"ignore\") if isinstance(oid, (bytes, np.bytes_)) else str(oid)\n            rows.append({\n                \"object_id\": oid,\n                \"split\": split_name,\n                \"which\": which,\n                \"shard\": str(sp),\n                \"start\": int(offsets[i]),\n                \"length\": int(lengths[i]),\n            })\n    return rows\n\n# ----------------------------\n# 10) Bucket builder (opt: groupby bucket instead of np.unique loop)\n# ----------------------------\ndef build_sequences_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 256):\n    try:\n        import pyarrow as pa\n        import pyarrow.parquet as pq\n    except Exception as e:\n        raise RuntimeError(\"pyarrow tidak tersedia. Di Kaggle biasanya ada.\") from e\n\n    parts = get_clean_parts(split_name, which)\n    if not parts:\n        raise RuntimeError(f\"Tidak ada cleaned parts untuk {split_name}/{which}. Cek STAGE 4 output.\")\n\n    tmp_dir = Path(ART_DIR) / \"tmp_seq_buckets\" / split_name / which\n    if tmp_dir.exists():\n        shutil.rmtree(tmp_dir, ignore_errors=True)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n\n    writers = {}\n    kept_rows = 0\n    t0 = time.time()\n\n    def bucket_idx(series_objid: pd.Series) -> np.ndarray:\n        h = pd.util.hash_pandas_object(series_objid, index=False).to_numpy(dtype=np.uint64, copy=False)\n        return (h % np.uint64(num_buckets)).astype(np.int16)\n\n    try:\n        for p in parts:\n            df = _read_clean_part(p)\n            if df.empty:\n                continue\n\n            # filter expected ids (safety)\n            df = df[df[\"object_id\"].isin(expected_ids)]\n            if df.empty:\n                continue\n\n            kept_rows += int(len(df))\n            bidx = bucket_idx(df[\"object_id\"])\n            df[\"_b\"] = bidx\n\n            # faster than np.unique + mask repeatedly\n            for b, sub in df.groupby(\"_b\", sort=False):\n                sub = sub.drop(columns=[\"_b\"])\n                if sub.empty:\n                    continue\n                fp = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n                table = pa.Table.from_pandas(sub, preserve_index=False)\n                if int(b) not in writers:\n                    writers[int(b)] = pq.ParquetWriter(fp, table.schema, compression=\"snappy\")\n                writers[int(b)].write_table(table)\n\n            del df\n            gc.collect()\n\n    finally:\n        for w in list(writers.values()):\n            try: w.close()\n            except Exception: pass\n\n    meta = df_train_meta if which == \"train\" else df_test_meta\n\n    manifest_rows = []\n    shard_idx = 0\n    batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n    built_ids = set()\n    len_before, len_after = [], []\n\n    def flush_shard_local():\n        nonlocal shard_idx, batch_obj_ids, batch_X, batch_B, batch_len, manifest_rows\n        if not batch_obj_ids:\n            return\n        lengths = np.asarray(batch_len, dtype=np.int64)\n        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n        offsets[1:] = np.cumsum(lengths)\n\n        Xc = np.concatenate(batch_X, axis=0).astype(np.float32)\n        Bc = np.concatenate(batch_B, axis=0).astype(np.int8)\n\n        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n        save_shard(shard_path, batch_obj_ids, Xc, Bc, offsets)\n\n        for i, oid in enumerate(batch_obj_ids):\n            manifest_rows.append({\n                \"object_id\": oid,\n                \"split\": split_name,\n                \"which\": which,\n                \"shard\": str(shard_path),\n                \"start\": int(offsets[i]),\n                \"length\": int(lengths[i]),\n            })\n\n        shard_idx += 1\n        batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n        gc.collect()\n\n    for bf in sorted(tmp_dir.glob(\"bucket_*.parquet\")):\n        dfb = pd.read_parquet(bf)\n        if dfb.empty:\n            continue\n\n        for oid, g in dfb.groupby(\"object_id\", sort=False):\n            oid = str(oid)\n            if oid in built_ids:\n                continue\n\n            z_val = float(meta.loc[oid, \"Z\"]) if (USE_RESTFRAME_TIME and oid in meta.index and \"Z\" in meta.columns) else 0.0\n            X, B, lb, la = build_object_tokens(g, meta_df=meta, z_val=z_val)\n\n            len_before.append(lb)\n            len_after.append(la)\n\n            batch_obj_ids.append(oid)\n            batch_X.append(X)\n            batch_B.append(B)\n            batch_len.append(int(X.shape[0]))\n            built_ids.add(oid)\n\n            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                flush_shard_local()\n\n        del dfb\n        gc.collect()\n\n    missing_ids = list(expected_ids - built_ids)\n    if missing_ids:\n        for oid in missing_ids:\n            oid = str(oid)\n            X, B, lb, la = build_empty_tokens(meta_dim=len(OBJ_META_COLS))\n            len_before.append(lb)\n            len_after.append(la)\n            batch_obj_ids.append(oid)\n            batch_X.append(X)\n            batch_B.append(B)\n            batch_len.append(int(X.shape[0]))\n            built_ids.add(oid)\n\n            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                flush_shard_local()\n\n    flush_shard_local()\n    shutil.rmtree(tmp_dir, ignore_errors=True)\n\n    st = {\n        \"kept_rows\": int(kept_rows),\n        \"built_objects\": int(len(built_ids)),\n        \"missing_filled\": int(len(missing_ids)),\n        \"len_before_mean\": float(np.mean(len_before)) if len_before else 0.0,\n        \"len_before_p95\": float(np.quantile(len_before, 0.95)) if len_before else 0.0,\n        \"len_after_mean\": float(np.mean(len_after)) if len_after else 0.0,\n        \"len_after_p95\": float(np.quantile(len_after, 0.95)) if len_after else 0.0,\n        \"truncated_frac\": float(np.mean([a < b for a, b in zip(len_after, len_before)])) if len_before else 0.0,\n        \"time_s\": float(time.time() - t0),\n    }\n    return manifest_rows, st\n\n# ----------------------------\n# 11) RUN\n# ----------------------------\nsplits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLITS_TO_CONSIDER\nall_manifest_train, all_manifest_test, split_run_stats = [], [], []\n\ndef expected_set_for(split_name: str, which: str) -> set:\n    return set(train_ids_by_split.get(split_name, [])) if which == \"train\" else set(test_ids_by_split.get(split_name, []))\n\nfor split_name in splits_to_run:\n    for which in [\"train\", \"test\"]:\n        out_dir = SEQ_DIR / split_name / which\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        expected_ids = expected_set_for(split_name, which)\n        if len(expected_ids) == 0:\n            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}.\")\n\n        shard_exists = any(out_dir.glob(\"shard_*.npz\"))\n        if REBUILD_MODE == \"reuse_if_exists\" and shard_exists:\n            print(f\"\\n[Stage 5] REUSE (exists): {split_name}/{which}\")\n            man_rows = reconstruct_manifest_from_shards(split_name, which, out_dir)\n            if not man_rows:\n                raise RuntimeError(f\"REUSE mode aktif tapi gagal rekonstruksi manifest: {out_dir}\")\n            if which == \"train\":\n                all_manifest_train.extend(man_rows)\n            else:\n                all_manifest_test.extend(man_rows)\n            split_run_stats.append({\n                \"split\": split_name, \"which\": which,\n                \"kept_rows\": 0,\n                \"built_objects\": len(man_rows),\n                \"missing_filled\": 0,\n                \"len_before_mean\": 0.0,\n                \"len_before_p95\": 0.0,\n                \"len_after_mean\": 0.0,\n                \"len_after_p95\": 0.0,\n                \"truncated_frac\": 0.0,\n                \"time_s\": 0.0,\n            })\n            continue\n        else:\n            for f in out_dir.glob(\"shard_*.npz\"):\n                try: f.unlink()\n                except Exception: pass\n\n        print(f\"\\n[Stage 5] {split_name}/{which} | expected={len(expected_ids):,} | L_MAX={L_MAX} | TRUNC={TRUNC_POLICY} | mode_force={TOKEN_MODE_FORCE}\")\n\n        manifest_rows, st = build_sequences_bucket(\n            split_name=split_name,\n            which=which,\n            expected_ids=expected_ids,\n            out_dir=out_dir,\n            num_buckets=NUM_BUCKETS\n        )\n\n        print(f\"[Stage 5] OK: built={st['built_objects']:,} (missing_filled={st['missing_filled']:,}) | \"\n              f\"kept_rows={st['kept_rows']:,} | \"\n              f\"len_mean {st['len_before_mean']:.1f}->{st['len_after_mean']:.1f} | \"\n              f\"p95 {st['len_before_p95']:.1f}->{st['len_after_p95']:.1f} | \"\n              f\"trunc%={st['truncated_frac']*100:.1f}% | \"\n              f\"time={st['time_s']:.2f}s | mode={TOKEN_MODE} | feat={FEATURE_SET} | add_meta={ADD_OBJ_META_PER_TOKEN}\")\n\n        split_run_stats.append({\"split\": split_name, \"which\": which, **st})\n\n        if which == \"train\":\n            all_manifest_train.extend(manifest_rows)\n        else:\n            all_manifest_test.extend(manifest_rows)\n\n        gc.collect()\n\n# ----------------------------\n# 12) Save manifests + stats + config\n# ----------------------------\ndf_m_train = pd.DataFrame(all_manifest_train)\ndf_m_test  = pd.DataFrame(all_manifest_test)\n\nif not df_m_train.empty:\n    df_m_train = df_m_train.sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\nif not df_m_test.empty:\n    df_m_test = df_m_test.sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n\nmtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\nmtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\ndf_m_train.to_csv(mtrain_path, index=False)\ndf_m_test.to_csv(mtest_path, index=False)\n\ndf_stats = pd.DataFrame(split_run_stats)\nstats_path = SEQ_DIR / \"seq_build_stats.csv\"\ndf_stats.to_csv(stats_path, index=False)\n\ncfg = {\n    \"token_mode\": TOKEN_MODE,\n    \"token_mode_force\": TOKEN_MODE_FORCE,\n    \"feature_set\": FEATURE_SET,\n    \"feature_names\": FEATURE_NAMES,\n    \"feature_dim\": int(FEATURE_DIM) if FEATURE_DIM is not None else None,\n    \"obj_meta_cols\": OBJ_META_COLS,\n    \"add_meta_per_token\": bool(ADD_OBJ_META_PER_TOKEN),\n    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n    \"compress_npz\": bool(COMPRESS_NPZ),\n    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n    \"num_buckets\": int(NUM_BUCKETS),\n    \"L_MAX\": int(L_MAX),\n    \"TRUNC_POLICY\": str(TRUNC_POLICY),\n    \"KEEP_DET_FRAC\": float(KEEP_DET_FRAC),\n    \"USE_RESTFRAME_TIME\": bool(USE_RESTFRAME_TIME),\n    \"REBUILD_MODE\": str(REBUILD_MODE),\n    \"RUN_DIR_USED\": str(RUN_DIR),\n    \"ART_DIR_USED\": str(ART_DIR),\n    \"LC_CLEAN_DIR_USED\": str(LC_CLEAN_DIR),\n    \"manifest_csv\": str(manifest_csv),\n}\ncfg_path = SEQ_DIR / \"seq_config.json\"\ncfg_path.write_text(json.dumps(cfg, indent=2))\n\nprint(\"\\n[Stage 5] DONE\")\nprint(f\"- token_mode : {TOKEN_MODE} (force={TOKEN_MODE_FORCE})\")\nprint(f\"- feature_set: {FEATURE_SET} | dim={FEATURE_DIM}\")\nprint(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\nprint(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\nprint(f\"- Saved: {stats_path}\")\nprint(f\"- Saved: {cfg_path}\")\n\n# ----------------------------\n# 13) Smoke test\n# ----------------------------\ndef load_sequence(object_id: str, which: str):\n    object_id = str(object_id).strip()\n    m = df_m_train if which == \"train\" else df_m_test\n    row = m[m[\"object_id\"] == object_id]\n    if row.empty:\n        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n    r = row.iloc[0]\n    data = np.load(r[\"shard\"], allow_pickle=False)\n    start = int(r[\"start\"]); length = int(r[\"length\"])\n    X = data[\"x\"][start:start+length]\n    B = data[\"band\"][start:start+length]\n    return X, B\n\n_smoke_oid = str(df_train_meta.index[0])\nX_sm, B_sm = load_sequence(_smoke_oid, \"train\")\nprint(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\nprint(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n\nglobals().update({\n    \"RUN_DIR\": RUN_DIR,\n    \"ART_DIR\": ART_DIR,\n    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n    \"SEQ_DIR\": SEQ_DIR,\n    \"seq_manifest_train\": df_m_train,\n    \"seq_manifest_test\": df_m_test,\n    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n    \"SEQ_FEATURE_DIM\": int(FEATURE_DIM) if FEATURE_DIM is not None else None,\n    \"SEQ_TOKEN_MODE\": TOKEN_MODE,\n    \"get_clean_parts\": get_clean_parts,\n    \"load_sequence\": load_sequence,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sequence Length Policy (Padding, Truncation, Windowing)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n# ONE CELL, Kaggle CPU-SAFE — REVISI FULL v2.4 (IMPROVED + HARDENED)\n#\n# IMPROVE v2.4:\n# - Mode-aware scoring weights: asinh -> reduce W_VAL/W_DET to avoid redundant scoring\n# - Empty/stub safety: represent empty as 1 dummy token with mask=1 (anti NaN pooling)\n# - Safer band embedding: default SHIFT_BAND_IDS=True (PAD=0, real bands 1..6)\n# - Lower RAM: np.load(..., mmap_mode=\"r\")\n# - Keep hardened guards + reuse mode\n#\n# Output:\n# - artifacts/fixed_seq/{train|test}_{X|B|M}.dat  (memmap)\n# - artifacts/fixed_seq/{train|test}_ids.npy\n# - artifacts/fixed_seq/train_y.npy\n# - artifacts/fixed_seq/{train|test}_origlen.npy, {train|test}_winstart.npy, {train|test}_winend.npy\n# - artifacts/fixed_seq/length_policy_config.json\n# ============================================================\n\nimport gc, json, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"seq_manifest_train\", \"seq_manifest_test\", \"SEQ_FEATURE_NAMES\",\n             \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 -> 5 dulu.\")\n\nART_DIR = Path(ART_DIR)\n\nm_train = seq_manifest_train.copy()\nm_test  = seq_manifest_test.copy()\n\nSEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\nfeat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n\n# ----------------------------\n# 0b) Detect token_mode (MAG vs ASINH)\n# ----------------------------\nSEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\nif SEQ_TOKEN_MODE is None:\n    if (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n        SEQ_TOKEN_MODE = \"asinh\"\n    elif (\"mag\" in feat) and (\"mag_err_log\" in feat):\n        SEQ_TOKEN_MODE = \"mag\"\n    else:\n        raise ValueError(\n            \"Cannot infer SEQ_TOKEN_MODE from SEQ_FEATURE_NAMES.\\n\"\n            f\"SEQ_FEATURE_NAMES={SEQ_FEATURE_NAMES}\\n\"\n            \"Expected either (flux_asinh, err_log1p) or (mag, mag_err_log).\"\n        )\n\nREQ_COMMON = [\"t_rel_log\", \"dt_log\", \"snr_tanh\", \"detected\"]\nfor k in REQ_COMMON:\n    if k not in feat:\n        raise ValueError(f\"SEQ_FEATURE_NAMES must include '{k}'. Found: {SEQ_FEATURE_NAMES}\")\n\nif SEQ_TOKEN_MODE == \"asinh\":\n    if \"flux_asinh\" not in feat:\n        raise ValueError(\"token_mode=asinh requires 'flux_asinh'.\")\n    SCORE_VALUE_FEAT = \"flux_asinh\"\nelif SEQ_TOKEN_MODE == \"mag\":\n    if \"mag\" not in feat:\n        raise ValueError(\"token_mode=mag requires 'mag'.\")\n    SCORE_VALUE_FEAT = \"mag\"\nelse:\n    raise ValueError(f\"Unknown SEQ_TOKEN_MODE={SEQ_TOKEN_MODE}\")\n\nprint(f\"[Stage 6] token_mode={SEQ_TOKEN_MODE} | score_value_feat={SCORE_VALUE_FEAT} | F={len(SEQ_FEATURE_NAMES)}\")\n\n# ----------------------------\n# 1) Settings\n# ----------------------------\nFORCE_MAX_LEN = None          # e.g. 256 (kalau mau paksa)\nMAXLEN_CAPS = (256, 384, 512) # CPU-safe choices\n\n# Score weights (MODE-AWARE: asinh -> reduce redundancy)\nif SEQ_TOKEN_MODE == \"asinh\":\n    W_SNR = 1.00\n    W_VAL = 0.05   # flux_asinh often correlates with SNR already\n    W_DET = 0.05\nelse:\n    W_SNR = 1.00\n    W_VAL = 0.35\n    W_DET = 0.25\n\n# Padding / empty safety\nPAD_BAND_ID = 0\nDUMMY_TOKEN_FOR_EMPTY = True  # IMPORTANT: avoid NaN pooling if mask.sum()==0\n\n# Band policy (RECOMMENDED)\nSHIFT_BAND_IDS = True               # PAD=0; band 0..5 -> 1..6\nAUTO_SHIFT_IF_NEGATIVE_BANDS = False  # no need if SHIFT_BAND_IDS already True\n\n# Build policy\nREBUILD_MODE = \"wipe_all\"     # \"wipe_all\" atau \"reuse_if_exists\"\nDTYPE_X = np.float32          # bisa np.float16 kalau disk ketat\n\n# ----------------------------\n# 2) Inspect length distribution -> choose MAX_LEN\n# ----------------------------\ndef describe_lengths(m: pd.DataFrame, name: str):\n    L = pd.to_numeric(m[\"length\"], errors=\"coerce\").fillna(0).to_numpy(dtype=np.int32, copy=False)\n    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n    print(f\"\\n{name} length stats\")\n    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[7])} | p95={int(q[8])} | p99={int(q[10])} | max={int(q[-1])}\")\n    return q\n\nq_tr = describe_lengths(m_train, \"TRAIN\")\nq_te = describe_lengths(m_test,  \"TEST\")\n\np95 = int(max(q_tr[8], q_te[8]))\nif FORCE_MAX_LEN is not None:\n    MAX_LEN = int(FORCE_MAX_LEN)\nelse:\n    if p95 <= 256:\n        MAX_LEN = 256\n    elif p95 <= 384:\n        MAX_LEN = 384\n    else:\n        MAX_LEN = 512\n\nif MAX_LEN not in MAXLEN_CAPS and FORCE_MAX_LEN is None:\n    MAX_LEN = int(min(MAXLEN_CAPS, key=lambda x: abs(x - MAX_LEN)))\n\nprint(f\"\\n[Stage 6] MAX_LEN={MAX_LEN} (based on p95={p95})\")\nprint(f\"[Stage 6] Weights: W_SNR={W_SNR} | W_VAL={W_VAL} | W_DET={W_DET} | SHIFT_BAND_IDS={SHIFT_BAND_IDS} | DUMMY_TOKEN_FOR_EMPTY={DUMMY_TOKEN_FOR_EMPTY}\")\n\n# ----------------------------\n# 3) Window scoring (adaptive)\n# ----------------------------\ndef _brightness_proxy_from_mag(mag: np.ndarray) -> np.ndarray:\n    mag = np.nan_to_num(mag, nan=np.float32(0.0), posinf=np.float32(0.0), neginf=np.float32(0.0)).astype(np.float32, copy=False)\n    if mag.size == 0:\n        return np.zeros_like(mag, dtype=np.float32)\n    med = np.float32(np.median(mag))\n    br = np.maximum(med - mag, np.float32(0.0))\n    br = np.log1p(br).astype(np.float32, copy=False)\n    return br\n\ndef _score_tokens(X: np.ndarray) -> np.ndarray:\n    snr = np.abs(X[:, feat[\"snr_tanh\"]]).astype(np.float32, copy=False)\n    det = X[:, feat[\"detected\"]].astype(np.float32, copy=False)\n\n    if SEQ_TOKEN_MODE == \"asinh\":\n        val = np.abs(X[:, feat[\"flux_asinh\"]]).astype(np.float32, copy=False)\n    else:\n        mag = X[:, feat[\"mag\"]].astype(np.float32, copy=False)\n        val = _brightness_proxy_from_mag(mag)\n\n    score = (np.float32(W_SNR) * snr) + (np.float32(W_VAL) * val) + (np.float32(W_DET) * det)\n    score = np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n    return score\n\ndef select_best_window(score: np.ndarray, max_len: int) -> tuple[int, int]:\n    L = int(score.shape[0])\n    if L <= max_len:\n        return 0, L\n\n    cs = np.empty(L + 1, dtype=np.float32)\n    cs[0] = 0.0\n    np.cumsum(score.astype(np.float32, copy=False), out=cs[1:])\n\n    ws = cs[max_len:] - cs[:-max_len]\n    if not np.isfinite(ws).any():\n        start = (L - max_len) // 2\n    else:\n        start = int(np.argmax(ws))\n    end = start + max_len\n    return start, end\n\ndef _is_empty_stub(X: np.ndarray, B: np.ndarray) -> bool:\n    # STAGE 5 empty: len=1, band=-1, features all ~0\n    try:\n        if X is None or B is None:\n            return True\n        if int(X.shape[0]) != 1 or int(B.shape[0]) != 1:\n            return False\n        if int(B[0]) >= 0:\n            return False\n        return bool(np.all(np.abs(X.astype(np.float32, copy=False)) <= np.float32(1e-12)))\n    except Exception:\n        return False\n\ndef pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n    F = int(X.shape[1]) if (X is not None and X.ndim == 2) else int(len(SEQ_FEATURE_NAMES))\n\n    Xp = np.zeros((max_len, F), dtype=DTYPE_X)\n    Bp = np.full((max_len,), PAD_BAND_ID, dtype=np.int8)\n    Mp = np.zeros((max_len,), dtype=np.int8)\n\n    # unified empty handling\n    empty_like = (X is None or B is None or X.size == 0 or B.size == 0 or _is_empty_stub(X, B))\n    if empty_like:\n        if DUMMY_TOKEN_FOR_EMPTY:\n            Mp[0] = 1\n            Bp[0] = np.int8(PAD_BAND_ID)\n            # Xp[0] tetap 0\n            return Xp, Bp, Mp, 0, 0, 1\n        else:\n            return Xp, Bp, Mp, 0, 0, 0\n\n    L = int(X.shape[0])\n    if L <= max_len:\n        Xw = X\n        Bw = B\n        ws, we = 0, L\n    else:\n        sc = _score_tokens(X)\n        ws, we = select_best_window(sc, max_len=max_len)\n        Xw = X[ws:we]\n        Bw = B[ws:we]\n\n    lw = int(Xw.shape[0])\n    Xp[:lw] = Xw.astype(DTYPE_X, copy=False)\n\n    if SHIFT_BAND_IDS:\n        # PAD=0; band 0..K-1 -> 1..K\n        Bw16 = Bw.astype(np.int16, copy=False)\n        Bw_shift = np.clip(Bw16 + 1, 0, 127).astype(np.int8, copy=False)\n        Bp[:lw] = Bw_shift\n    else:\n        Bp[:lw] = Bw.astype(np.int8, copy=False)\n\n    Mp[:lw] = 1\n    return Xp, Bp, Mp, int(L), int(ws), int(we)\n\n# ----------------------------\n# 4) Fixed cache builder setup\n# ----------------------------\nFIX_DIR = Path(ART_DIR) / \"fixed_seq\"\nFIX_DIR.mkdir(parents=True, exist_ok=True)\n\ntrain_ids = df_train_meta.index.astype(\"string\").str.strip().astype(str).to_list()\n\n_y_col = None\nfor cand in [\"target\", \"y\", \"label\", \"class\", \"target_id\"]:\n    if cand in df_train_meta.columns:\n        _y_col = cand\n        break\nif _y_col is None:\n    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols={list(df_train_meta.columns)[:30]}\")\n\ny_train = pd.to_numeric(df_train_meta[_y_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n\ndef _try_load_sample_sub_ids():\n    if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in globals()[\"df_sub\"].columns:\n        return globals()[\"df_sub\"][\"object_id\"].astype(str).str.strip().to_list()\n    if \"PATHS\" in globals() and isinstance(PATHS, dict):\n        for k in [\"SAMPLE_SUB\", \"SAMPLE_SUBMISSION\", \"sample_submission\", \"sample_sub\", \"SAMPLE\"]:\n            p = PATHS.get(k, None)\n            if p and Path(p).exists():\n                df = pd.read_csv(p)\n                if \"object_id\" in df.columns:\n                    return df[\"object_id\"].astype(str).str.strip().to_list()\n    return None\n\ntest_ids = _try_load_sample_sub_ids()\nif test_ids is None:\n    test_ids = df_test_meta.index.astype(\"string\").str.strip().astype(str).to_list()\n\n# strict unique ids\nif len(set(train_ids)) != len(train_ids):\n    raise RuntimeError(\"train_ids contains duplicates. Check df_train_meta.index.\")\nif len(set(test_ids)) != len(test_ids):\n    raise RuntimeError(\"test_ids contains duplicates. Check ordering source (df_sub/sample_sub/df_test_meta).\")\n\ntrain_row = {oid: i for i, oid in enumerate(train_ids)}\ntest_row  = {oid: i for i, oid in enumerate(test_ids)}\n\nNTR = len(train_ids)\nNTE = len(test_ids)\nF = len(SEQ_FEATURE_NAMES)\n\ndef _gb(nbytes): return float(nbytes) / (1024**3)\nsize_tr = NTR * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\nsize_te = NTE * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\nprint(f\"\\n[Stage 6] Memmap X sizes approx: train={_gb(size_tr):.2f} GB | test={_gb(size_te):.2f} GB | dtype={DTYPE_X}\")\n\n# memmap paths\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\ntest_X_path  = FIX_DIR / \"test_X.dat\"\ntest_B_path  = FIX_DIR / \"test_B.dat\"\ntest_M_path  = FIX_DIR / \"test_M.dat\"\n\ntrain_len_path = FIX_DIR / \"train_origlen.npy\"\ntrain_ws_path  = FIX_DIR / \"train_winstart.npy\"\ntrain_we_path  = FIX_DIR / \"train_winend.npy\"\ntest_len_path  = FIX_DIR / \"test_origlen.npy\"\ntest_ws_path   = FIX_DIR / \"test_winstart.npy\"\ntest_we_path   = FIX_DIR / \"test_winend.npy\"\n\n# ----------------------------\n# 4c) Rebuild handling\n# ----------------------------\ndef _all_exist(paths):\n    return all(Path(p).exists() for p in paths)\n\nreuse_paths = [\n    train_X_path, train_B_path, train_M_path,\n    test_X_path, test_B_path, test_M_path,\n    FIX_DIR / \"train_ids.npy\", FIX_DIR / \"test_ids.npy\", FIX_DIR / \"train_y.npy\",\n    train_len_path, train_ws_path, train_we_path,\n    test_len_path, test_ws_path, test_we_path,\n    FIX_DIR / \"length_policy_config.json\"\n]\n\nif REBUILD_MODE == \"reuse_if_exists\" and _all_exist(reuse_paths):\n    print(\"[Stage 6] REUSE (exists): fixed_seq cache already present.\")\n    globals().update({\n        \"FIX_DIR\": FIX_DIR, \"MAX_LEN\": MAX_LEN,\n        \"FIX_TRAIN_X_PATH\": train_X_path, \"FIX_TRAIN_B_PATH\": train_B_path, \"FIX_TRAIN_M_PATH\": train_M_path,\n        \"FIX_TEST_X_PATH\": test_X_path,  \"FIX_TEST_B_PATH\": test_B_path,  \"FIX_TEST_M_PATH\": test_M_path,\n        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n        \"FIX_POLICY_CFG_PATH\": FIX_DIR / \"length_policy_config.json\",\n        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n        \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n    })\n    gc.collect()\n\nelse:\n    # ----------------------------\n    # 5) Create memmaps\n    # ----------------------------\n    Xtr = np.memmap(train_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTR, MAX_LEN, F))\n    Btr = np.memmap(train_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n    Mtr = np.memmap(train_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n\n    Xte = np.memmap(test_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTE, MAX_LEN, F))\n    Bte = np.memmap(test_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n    Mte = np.memmap(test_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n\n    origlen_tr  = np.zeros((NTR,), dtype=np.int32)\n    winstart_tr = np.zeros((NTR,), dtype=np.int32)\n    winend_tr   = np.zeros((NTR,), dtype=np.int32)\n\n    origlen_te  = np.zeros((NTE,), dtype=np.int32)\n    winstart_te = np.zeros((NTE,), dtype=np.int32)\n    winend_te   = np.zeros((NTE,), dtype=np.int32)\n\n    filled_tr = np.zeros((NTR,), dtype=np.uint8)\n    filled_te = np.zeros((NTE,), dtype=np.uint8)\n\n    # ----------------------------\n    # 6) Fill memmaps per shard\n    # ----------------------------\n    def process_manifest_into_memmap(m: pd.DataFrame, which: str):\n        if which == \"train\":\n            row_map = train_row\n            Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n            origlen, ws_arr, we_arr = origlen_tr, winstart_tr, winend_tr\n            filled_mask = filled_tr\n            expected_n = NTR\n        else:\n            row_map = test_row\n            Xmm, Bmm, Mmm = Xte, Bte, Mte\n            origlen, ws_arr, we_arr = origlen_te, winstart_te, winend_te\n            filled_mask = filled_te\n            expected_n = NTE\n\n        for c in [\"object_id\", \"shard\", \"start\", \"length\"]:\n            if c not in m.columns:\n                raise RuntimeError(f\"Manifest missing column '{c}'. cols={list(m.columns)}\")\n\n        m2 = m.copy()\n        m2[\"object_id\"] = m2[\"object_id\"].astype(str)\n        m2[\"shard\"] = m2[\"shard\"].astype(str)\n        m2[\"start\"] = pd.to_numeric(m2[\"start\"], errors=\"coerce\").fillna(-1).astype(np.int64)\n        m2[\"length\"] = pd.to_numeric(m2[\"length\"], errors=\"coerce\").fillna(0).astype(np.int64)\n\n        shard_paths = m2[\"shard\"].unique().tolist()\n        miss_sh = [p for p in shard_paths if not Path(p).exists()]\n        if miss_sh:\n            raise RuntimeError(f\"Missing shard files ({which}): count={len(miss_sh)} | ex={miss_sh[:5]}\")\n\n        filled = 0\n        dup = 0\n        empty = 0\n        dropped_bad = 0\n\n        t0 = time.time()\n        get = row_map.get\n\n        for shard_path, g in m2.groupby(\"shard\", sort=True):\n            # mmap_mode=\"r\" for RAM stability\n            data = np.load(shard_path, mmap_mode=\"r\", allow_pickle=False)\n            x_all = data[\"x\"]\n            b_all = data[\"band\"]\n\n            oids = g[\"object_id\"].to_numpy(copy=False)\n            starts = g[\"start\"].to_numpy(dtype=np.int64, copy=False)\n            lens   = g[\"length\"].to_numpy(dtype=np.int64, copy=False)\n\n            idxs = np.empty(len(oids), dtype=np.int64)\n            for i in range(len(oids)):\n                idxs[i] = get(oids[i], -1)\n\n            valid = (idxs >= 0) & (starts >= 0) & (lens >= 0)\n            if not valid.any():\n                continue\n\n            oids_v = oids[valid]\n            idxs_v = idxs[valid]\n            st_v   = starts[valid]\n            ln_v   = lens[valid]\n\n            for oid, idx, st, ln in zip(oids_v, idxs_v, st_v, ln_v):\n                if ln <= 0:\n                    empty += 1\n                    continue\n                if filled_mask[idx]:\n                    dup += 1\n                    continue\n\n                end = int(st + ln)\n                if st < 0 or end > x_all.shape[0] or end > b_all.shape[0]:\n                    dropped_bad += 1\n                    continue\n\n                X = x_all[st:end]\n                B = b_all[st:end]\n\n                Xp, Bp, Mp, L0, ws, we = pad_to_fixed(X, B, max_len=MAX_LEN)\n\n                Xmm[idx, :, :] = Xp\n                Bmm[idx, :] = Bp\n                Mmm[idx, :] = Mp\n                origlen[idx] = int(L0)\n                ws_arr[idx] = int(ws)\n                we_arr[idx] = int(we)\n                filled_mask[idx] = 1\n                filled += 1\n\n                if filled % 2000 == 0:\n                    gc.collect()\n\n        elapsed = time.time() - t0\n        return {\n            \"filled\": int(filled),\n            \"dup_skipped\": int(dup),\n            \"empty_len\": int(empty),\n            \"dropped_bad_slices\": int(dropped_bad),\n            \"time_s\": float(elapsed),\n            \"expected\": int(expected_n)\n        }\n\n    print(\"\\n[Stage 6] Building fixed cache (TRAIN)...\")\n    st_tr = process_manifest_into_memmap(m_train, \"train\")\n    print(f\"[Stage 6] TRAIN filled={st_tr['filled']:,}/{st_tr['expected']:,} | dup={st_tr['dup_skipped']:,} | empty={st_tr['empty_len']:,} | dropped_bad={st_tr['dropped_bad_slices']:,} | time={st_tr['time_s']:.2f}s\")\n\n    print(\"\\n[Stage 6] Building fixed cache (TEST)...\")\n    st_te = process_manifest_into_memmap(m_test, \"test\")\n    print(f\"[Stage 6] TEST  filled={st_te['filled']:,}/{st_te['expected']:,} | dup={st_te['dup_skipped']:,} | empty={st_te['empty_len']:,} | dropped_bad={st_te['dropped_bad_slices']:,} | time={st_te['time_s']:.2f}s\")\n\n    Xtr.flush(); Btr.flush(); Mtr.flush()\n    Xte.flush(); Bte.flush(); Mte.flush()\n\n    # ----------------------------\n    # 7) Hard sanity: must be 100% filled\n    # ----------------------------\n    miss_tr = np.where(filled_tr == 0)[0]\n    miss_te = np.where(filled_te == 0)[0]\n    if len(miss_tr) > 0:\n        ex = [train_ids[i] for i in miss_tr[:10]]\n        raise RuntimeError(f\"[Stage 6] TRAIN missing filled rows: {len(miss_tr):,}/{NTR:,} | ex={ex}\")\n    if len(miss_te) > 0:\n        ex = [test_ids[i] for i in miss_te[:10]]\n        raise RuntimeError(f\"[Stage 6] TEST missing filled rows: {len(miss_te):,}/{NTE:,} | ex={ex}\")\n\n    # ----------------------------\n    # 8) Save ids + y + meta arrays\n    # ----------------------------\n    np.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n    np.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids, dtype=\"S\"))\n    np.save(FIX_DIR / \"train_y.npy\",   y_train)\n\n    np.save(train_len_path, origlen_tr)\n    np.save(train_ws_path,  winstart_tr)\n    np.save(train_we_path,  winend_tr)\n\n    np.save(test_len_path, origlen_te)\n    np.save(test_ws_path,  winstart_te)\n    np.save(test_we_path,  winend_te)\n\n    # ----------------------------\n    # 9) Quick sanity samples\n    # ----------------------------\n    def sanity_samples(which: str, n_show: int = 3, seed: int = 2025):\n        rng = np.random.default_rng(seed)\n        if which == \"train\":\n            Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n            ids = train_ids\n            ol = origlen_tr\n        else:\n            Xmm, Bmm, Mmm = Xte, Bte, Mte\n            ids = test_ids\n            ol = origlen_te\n\n        idxs = rng.choice(len(ids), size=min(n_show, len(ids)), replace=False)\n        print(f\"\\n[Stage 6] Sanity samples ({which}):\")\n        for i in idxs:\n            kept = int(Mmm[i].sum())\n            bands = sorted(set(Bmm[i, :kept].tolist())) if kept > 0 else []\n            print(f\"- idx={i} oid={ids[i]} orig_len={int(ol[i])} kept={kept} bands_unique={bands}\")\n\n    sanity_samples(\"train\", 3)\n    sanity_samples(\"test\", 3)\n\n    # ----------------------------\n    # 10) Save config\n    # ----------------------------\n    policy_cfg = {\n        \"token_mode\": SEQ_TOKEN_MODE,\n        \"max_len\": int(MAX_LEN),\n        \"feature_names\": list(SEQ_FEATURE_NAMES),\n        \"score_weights\": {\"W_SNR\": float(W_SNR), \"W_VAL\": float(W_VAL), \"W_DET\": float(W_DET)},\n        \"score_value_feat\": SCORE_VALUE_FEAT,\n        \"window_policy\": \"best_contiguous_window_by_max_sum(score)\",\n        \"padding\": {\n            \"PAD_BAND_ID\": int(PAD_BAND_ID),\n            \"SHIFT_BAND_IDS\": bool(SHIFT_BAND_IDS),\n            \"DUMMY_TOKEN_FOR_EMPTY\": bool(DUMMY_TOKEN_FOR_EMPTY),\n        },\n        \"dtype_X\": str(DTYPE_X),\n        \"order\": {\n            \"train\": \"df_train_meta.index\",\n            \"test\": (\"df_sub.object_id\" if (\"df_sub\" in globals() and isinstance(df_sub, pd.DataFrame) and \"object_id\" in df_sub.columns) else \"df_test_meta.index / sample_submission fallback\"),\n            \"y_col\": str(_y_col),\n        },\n        \"stats\": {\"train\": st_tr, \"test\": st_te},\n        \"files\": {\n            \"train_X\": str(train_X_path), \"train_B\": str(train_B_path), \"train_M\": str(train_M_path),\n            \"test_X\": str(test_X_path),   \"test_B\": str(test_B_path),   \"test_M\": str(test_M_path),\n            \"train_y\": str(FIX_DIR / \"train_y.npy\"),\n            \"train_ids\": str(FIX_DIR / \"train_ids.npy\"),\n            \"test_ids\": str(FIX_DIR / \"test_ids.npy\"),\n            \"train_origlen\": str(train_len_path), \"train_winstart\": str(train_ws_path), \"train_winend\": str(train_we_path),\n            \"test_origlen\": str(test_len_path),   \"test_winstart\": str(test_ws_path),   \"test_winend\": str(test_we_path),\n        }\n    }\n    cfg_path = FIX_DIR / \"length_policy_config.json\"\n    with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(policy_cfg, f, indent=2)\n\n    print(\"\\n[Stage 6] DONE\")\n    print(f\"- FIX_DIR: {FIX_DIR}\")\n    print(f\"- Saved config: {cfg_path}\")\n\n    globals().update({\n        \"FIX_DIR\": FIX_DIR,\n        \"MAX_LEN\": MAX_LEN,\n        \"FIX_TRAIN_X_PATH\": train_X_path,\n        \"FIX_TRAIN_B_PATH\": train_B_path,\n        \"FIX_TRAIN_M_PATH\": train_M_path,\n        \"FIX_TEST_X_PATH\": test_X_path,\n        \"FIX_TEST_B_PATH\": test_B_path,\n        \"FIX_TEST_M_PATH\": test_M_path,\n        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n        \"FIX_POLICY_CFG_PATH\": cfg_path,\n        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n        \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n    })\n\n    gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CV Split (Object-Level, Stratified)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v2.4 (FASTER INDEXER + CLEAN HOLDOUT + HARDENED)\n#\n# Output:\n# - artifacts/cv/cv_folds.csv\n# - artifacts/cv/cv_folds.npz   (train_idx_f + val_idx_f + optional holdout_val_mask)\n# - artifacts/cv/cv_report.txt\n# - artifacts/cv/cv_config.json\n# - (optional) artifacts/cv/cv_holdout_val_mask.npy   (only if holdout)\n# - globals: fold_assign, folds, n_splits, CV_DIR\n# ============================================================\n\nimport gc, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nfor need in [\"df_train_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 2 dulu (df_train_meta & ART_DIR).\")\n\nSEED = int(globals().get(\"SEED\", 2025))\nART_DIR = Path(ART_DIR)\n\n# ----------------------------\n# 1) CV Settings\n# ----------------------------\nDEFAULT_SPLITS = 5\nFORCE_N_SPLITS = None              # set int kalau mau paksa (mis. 3), else None\nMIN_POS_PER_FOLD = 3               # 3–10 umum\nENFORCE_MIN_POS_PER_FOLD = True    # kalau True: n_splits turun otomatis sampai min_pos>=MIN_POS_PER_FOLD (atau fallback holdout)\n\nUSE_GROUP_BY_SPLIT = False         # True => prefer StratifiedGroupKFold (groups=df_train_meta[\"split\"])\nAUTO_FALLBACK_GROUP = True         # True => kalau group-cv tidak bisa, fallback ke StratifiedKFold\n\nHOLDOUT_FALLBACK = True            # True => kalau CV tidak mungkin, pakai holdout\nHOLDOUT_FRAC = 0.20                # target val fraction untuk holdout\n\nprint(f\"[Stage 7] seed={SEED} | default_splits={DEFAULT_SPLITS} | MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} \"\n      f\"| enforce_minpos={ENFORCE_MIN_POS_PER_FOLD} | group_by_split={USE_GROUP_BY_SPLIT} | fallback_group={AUTO_FALLBACK_GROUP}\")\n\n# ----------------------------\n# 2) Helpers\n# ----------------------------\ndef _decode_ids(arr) -> list:\n    out = []\n    for x in arr.tolist():\n        if isinstance(x, (bytes, bytearray, np.bytes_)):\n            s = x.decode(\"utf-8\", errors=\"ignore\")\n        else:\n            s = str(x)\n        out.append(s.strip())\n    return out\n\ndef _find_train_ids_npy(art_dir: Path):\n    # priority 1: FIX_DIR\n    if \"FIX_DIR\" in globals():\n        p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n        if p.exists():\n            return p\n    # priority 2: ART_DIR/fixed_seq\n    p = art_dir / \"fixed_seq\" / \"train_ids.npy\"\n    if p.exists():\n        return p\n    # priority 3: scan mallorn_run runs (latest mtime)\n    root = Path(\"/kaggle/working/mallorn_run\")\n    if root.exists():\n        cands = list(root.glob(\"run_*/artifacts/fixed_seq/train_ids.npy\"))\n        if cands:\n            cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n            return cands[0]\n    return None\n\ndef _safe_str_list(idx) -> list:\n    # consistent str/strip for ids\n    return pd.Index(idx).astype(\"string\").str.strip().astype(str).tolist()\n\n# ----------------------------\n# 3) Determine train_ids ordering (prefer fixed cache from STAGE 6)\n# ----------------------------\np_ids = _find_train_ids_npy(ART_DIR)\nif p_ids is not None:\n    raw = np.load(p_ids, allow_pickle=False)\n    train_ids = _decode_ids(raw)\n    order_source = str(p_ids)\nelse:\n    train_ids = _safe_str_list(df_train_meta.index)\n    order_source = \"df_train_meta.index\"\n\n# uniqueness check train_ids\nif len(train_ids) != len(set(train_ids)):\n    s = pd.Series(train_ids)\n    dup = s[s.duplicated()].iloc[:10].tolist()\n    raise RuntimeError(f\"[Stage 7] train_ids has duplicates (examples): {dup}\")\n\nN = len(train_ids)\n\n# ----------------------------\n# 4) Normalize meta index (string+strip) + fast indexer mapping\n# ----------------------------\nmeta = df_train_meta.copy()\n\nmeta_ids = _safe_str_list(meta.index)\nif len(meta_ids) != len(set(meta_ids)):\n    vc = pd.Series(meta_ids).value_counts()\n    dup = vc[vc > 1].index.tolist()[:10]\n    raise RuntimeError(f\"[Stage 7] df_train_meta index has duplicates after str/strip (examples): {dup}\")\n\nmeta.index = pd.Index(meta_ids, name=\"object_id\")\n\nmeta_index = meta.index\npos_idx = meta_index.get_indexer(train_ids)  # fast vectorized\nmissing_mask = (pos_idx < 0)\nif missing_mask.any():\n    ex = [train_ids[i] for i in np.where(missing_mask)[0][:10]]\n    raise RuntimeError(\n        \"[Stage 7] Some train_ids not found in df_train_meta (after str/strip index).\\n\"\n        f\"Missing count={int(missing_mask.sum())} | ex={ex}\\n\"\n        \"Solusi: pastikan df_train_meta memang object-level meta dan index-nya object_id.\"\n    )\n\n# ----------------------------\n# 5) Robust target column -> y (ordered by train_ids)\n# ----------------------------\ntarget_col = None\nfor cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n    if cand in meta.columns:\n        target_col = cand\n        break\nif target_col is None:\n    raise RuntimeError(f\"[Stage 7] Cannot find target column in df_train_meta. cols(sample)={list(meta.columns)[:40]}\")\n\n# grab y in train_ids order without slow loops\ny_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).to_numpy(copy=False)\ny = y_all[pos_idx]\ny = (y > 0).astype(np.int8)\n\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\nif pos == 0 or neg == 0:\n    raise RuntimeError(f\"[Stage 7] Invalid class distribution: pos={pos}, neg={neg}. Cannot do stratified split.\")\n\npos_rate = pos / max(N, 1)\npos_weight = float(neg / max(pos, 1))\nprint(f\"[Stage 7] N={N:,} pos={pos:,} neg={neg:,} pos%={pos_rate*100:.6f}% | pos_weight~{pos_weight:.4f} | order_source={order_source}\")\n\n# ----------------------------\n# 6) Optional groups (by split)\n# ----------------------------\ngroups = None\ngroup_col = None\nif USE_GROUP_BY_SPLIT:\n    for cand in [\"split\", \"split_id\", \"split_name\", \"split_idx\"]:\n        if cand in meta.columns:\n            group_col = cand\n            break\n    if group_col is None:\n        if not AUTO_FALLBACK_GROUP:\n            raise RuntimeError(\"[Stage 7] USE_GROUP_BY_SPLIT=True but no split column found in df_train_meta.\")\n        print(\"[Stage 7] WARN: split column not found; fallback to StratifiedKFold.\")\n        USE_GROUP_BY_SPLIT = False\n    else:\n        g_all = meta[group_col].astype(\"string\").str.strip().astype(str).to_numpy(copy=False)\n        groups = g_all[pos_idx]\n\n# ----------------------------\n# 7) Choose n_splits safely + auto-adjust\n# ----------------------------\nmax_splits_by_pos = pos\nmax_splits_by_neg = neg\nmax_splits_by_minpos = max(1, pos // max(int(MIN_POS_PER_FOLD), 1))\n\nn0 = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg, max_splits_by_minpos)\nif FORCE_N_SPLITS is not None:\n    n0 = int(FORCE_N_SPLITS)\n\nprint(f\"[Stage 7] Candidate n_splits={n0} | enforce_minpos={ENFORCE_MIN_POS_PER_FOLD} | MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}\")\n\n# ----------------------------\n# 8) Build folds (sklearn) with robust fallback\n# ----------------------------\ntry:\n    from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n    try:\n        from sklearn.model_selection import StratifiedGroupKFold\n    except Exception:\n        StratifiedGroupKFold = None\nexcept Exception as e:\n    raise RuntimeError(\"scikit-learn is not available in this environment.\") from e\n\ndef _try_split_kfold(k: int, use_group: bool):\n    fold_assign = np.full(N, -1, dtype=np.int16)\n    folds = []\n    per = []\n\n    if use_group:\n        if StratifiedGroupKFold is None:\n            return (False, \"StratifiedGroupKFold(unavailable)\", None, None, None)\n        splitter = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(np.zeros(N), y, groups=groups)\n        cv_type = f\"StratifiedGroupKFold({group_col})\"\n    else:\n        splitter = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(np.zeros(N), y)\n        cv_type = \"StratifiedKFold\"\n\n    try:\n        for fold, (tr_idx, val_idx) in enumerate(split_iter):\n            fold_assign[val_idx] = fold\n            yf = y[val_idx]\n            pf = int((yf == 1).sum())\n            nf = int((yf == 0).sum())\n            per.append((len(val_idx), pf, nf))\n            folds.append({\n                \"fold\": int(fold),\n                \"train_idx\": tr_idx.astype(np.int32, copy=False),\n                \"val_idx\": val_idx.astype(np.int32, copy=False),\n            })\n    except Exception as e:\n        return (False, f\"{cv_type} (error: {type(e).__name__}: {e})\", None, None, None)\n\n    if (fold_assign < 0).any():\n        return (False, f\"{cv_type} (unassigned)\", None, None, None)\n\n    # hard check: each fold must have pos>=1 and neg>=1\n    for (_, pf, nf) in per:\n        if pf == 0 or nf == 0:\n            return (False, f\"{cv_type} (empty class in fold)\", None, None, None)\n\n    return (True, cv_type, fold_assign, folds, per)\n\ndef _make_holdout():\n    # Holdout using StratifiedShuffleSplit\n    if pos < 2 or neg < 2:\n        raise RuntimeError(\n            f\"[Stage 7] Cannot build holdout safely. Need pos>=2 and neg>=2. Got pos={pos}, neg={neg}.\"\n        )\n\n    val_n = int(round(N * float(HOLDOUT_FRAC)))\n    val_n = max(val_n, 2)\n    val_n = min(val_n, N - 2)\n\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=(val_n / N), random_state=SEED)\n    tr_idx, val_idx = next(splitter.split(np.zeros(N), y))\n\n    # IMPORTANT: keep fold_assign without -1 to avoid downstream surprises\n    fold_assign = np.zeros(N, dtype=np.int16)  # all \"fold 0\"\n    val_mask = np.zeros(N, dtype=np.uint8)\n    val_mask[val_idx] = 1\n\n    folds = [{\n        \"fold\": 0,\n        \"train_idx\": tr_idx.astype(np.int32, copy=False),\n        \"val_idx\": val_idx.astype(np.int32, copy=False),\n    }]\n    per = [(len(val_idx), int((y[val_idx] == 1).sum()), int((y[val_idx] == 0).sum()))]\n    return 1, \"Holdout(StratifiedShuffleSplit)\", fold_assign, folds, per, val_mask\n\nbest = None\nval_mask_holdout = None\nuse_group_flag = bool(USE_GROUP_BY_SPLIT)\n\nif n0 >= 2:\n    for k in range(n0, 1, -1):\n        ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=use_group_flag)\n\n        if (not ok) and use_group_flag and AUTO_FALLBACK_GROUP:\n            ok2, cv_type2, fa2, folds2, per2 = _try_split_kfold(k, use_group=False)\n            if ok2:\n                ok, cv_type, fa, folds, per = ok2, cv_type2, fa2, folds2, per2\n                use_group_flag = False\n\n        if not ok:\n            continue\n\n        min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n        if ENFORCE_MIN_POS_PER_FOLD and (min_pos_seen < MIN_POS_PER_FOLD) and (FORCE_N_SPLITS is None):\n            continue\n\n        best = (k, cv_type, fa, folds, per, min_pos_seen)\n        break\n\n# if enforce failed completely, pick first valid\nif best is None and n0 >= 2:\n    for k in range(n0, 1, -1):\n        ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=bool(USE_GROUP_BY_SPLIT))\n        if (not ok) and USE_GROUP_BY_SPLIT and AUTO_FALLBACK_GROUP:\n            ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=False)\n        if ok:\n            min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n            best = (k, cv_type, fa, folds, per, min_pos_seen)\n            print(f\"[Stage 7] NOTE: Could not satisfy MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}. Using k={k} with min_pos={min_pos_seen}.\")\n            break\n\n# fallback to holdout\nif best is None:\n    if HOLDOUT_FALLBACK:\n        n_splits, cv_type, fold_assign, folds, per, val_mask_holdout = _make_holdout()\n        min_pos_seen = per[0][1] if per else 0\n        best = (n_splits, cv_type, fold_assign, folds, per, min_pos_seen)\n        print(f\"[Stage 7] FALLBACK -> {cv_type} | val_pos={min_pos_seen}\")\n    else:\n        raise RuntimeError(\"[Stage 7] Failed to build a valid CV split. Try smaller DEFAULT_SPLITS / FORCE_N_SPLITS, or enable HOLDOUT_FALLBACK.\")\n\nn_splits, cv_type, fold_assign, folds, per, min_pos_seen = best\n\nprint(f\"[Stage 7] FINAL: n_splits={n_splits} | cv_type={cv_type} | min_pos_in_fold={min_pos_seen}\")\n\n# ----------------------------\n# 9) Report\n# ----------------------------\nlines = []\nlines.append(f\"CV={cv_type} n_splits={n_splits} seed={SEED}\")\nlines.append(f\"Order source: {order_source}\")\nlines.append(f\"Target column: {target_col}\")\nlines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos_rate*100:.6f}% | pos_weight~{pos_weight:.6f}\")\nif USE_GROUP_BY_SPLIT:\n    lines.append(f\"Group col requested: {group_col} | used_group={('Group' in cv_type)}\")\n\nlines.append(\"Per-fold distribution (val):\")\nif n_splits >= 2:\n    for f in range(n_splits):\n        idx = np.where(fold_assign == f)[0]\n        yf = y[idx]\n        pf = int((yf == 1).sum())\n        nf = int((yf == 0).sum())\n        lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:9.6f}%\")\nelse:\n    # holdout mode: use folds[0]['val_idx']\n    vidx = folds[0][\"val_idx\"]\n    yf = y[vidx]\n    pf = int((yf == 1).sum())\n    nf = int((yf == 0).sum())\n    lines.append(f\"- holdout val: n={len(vidx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(vidx),1))*100:9.6f}%\")\n    lines.append(\"NOTE: holdout mode uses folds[0].train_idx / folds[0].val_idx; fold_assign is all zeros (no -1).\")\n\n# ----------------------------\n# 10) Save artifacts\n# ----------------------------\nCV_DIR = ART_DIR / \"cv\"\nCV_DIR.mkdir(parents=True, exist_ok=True)\n\ndf_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\nfolds_csv = CV_DIR / \"cv_folds.csv\"\ndf_folds.to_csv(folds_csv, index=False)\n\nnpz_path = CV_DIR / \"cv_folds.npz\"\nnpz_kwargs = {}\nfor fd in folds:\n    f = int(fd[\"fold\"])\n    npz_kwargs[f\"train_idx_{f}\"] = fd[\"train_idx\"].astype(np.int32, copy=False)\n    npz_kwargs[f\"val_idx_{f}\"]   = fd[\"val_idx\"].astype(np.int32, copy=False)\n\nif val_mask_holdout is not None:\n    npz_kwargs[\"holdout_val_mask\"] = val_mask_holdout.astype(np.uint8, copy=False)\n    np.save(CV_DIR / \"cv_holdout_val_mask.npy\", val_mask_holdout.astype(np.uint8, copy=False))\n\nnp.savez(npz_path, **npz_kwargs)\n\nreport_path = CV_DIR / \"cv_report.txt\"\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\ncfg_path = CV_DIR / \"cv_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"seed\": SEED,\n            \"n_splits\": int(n_splits),\n            \"cv_type\": cv_type,\n            \"min_pos_per_fold\": int(MIN_POS_PER_FOLD),\n            \"enforce_min_pos_per_fold\": bool(ENFORCE_MIN_POS_PER_FOLD),\n            \"use_group_by_split_requested\": bool(USE_GROUP_BY_SPLIT),\n            \"auto_fallback_group\": bool(AUTO_FALLBACK_GROUP),\n            \"holdout_fallback\": bool(HOLDOUT_FALLBACK),\n            \"holdout_frac\": float(HOLDOUT_FRAC),\n            \"order_source\": order_source,\n            \"target_col\": target_col,\n            \"group_col\": group_col,\n            \"pos_weight_hint\": float(pos_weight),\n            \"artifacts\": {\n                \"folds_csv\": str(folds_csv),\n                \"folds_npz\": str(npz_path),\n                \"report_txt\": str(report_path),\n                \"holdout_val_mask_npy\": (str(CV_DIR / \"cv_holdout_val_mask.npy\") if val_mask_holdout is not None else None),\n            },\n        },\n        f,\n        indent=2,\n    )\n\nprint(\"\\n[Stage 7] CV split OK\")\nprint(f\"- Saved: {folds_csv}\")\nprint(f\"- Saved: {npz_path}\")\nprint(f\"- Saved: {report_path}\")\nprint(f\"- Saved: {cfg_path}\")\n\ntail_n = min(len(lines), 12)\nprint(\"\\n\".join(lines[-tail_n:]))\n\n# ----------------------------\n# 11) Export globals for next stage\n# ----------------------------\nglobals().update({\n    \"CV_DIR\": CV_DIR,\n    \"n_splits\": int(n_splits),\n    \"train_ids_ordered\": train_ids,\n    \"y_ordered\": y,\n    \"fold_assign\": fold_assign,\n    \"folds\": folds,\n    \"CV_FOLDS_CSV\": folds_csv,\n    \"CV_FOLDS_NPZ\": npz_path,\n    \"CV_CFG_PATH\": cfg_path,\n    \"CV_TYPE\": cv_type,\n    \"CV_ORDER_SOURCE\": order_source,\n    \"POS_WEIGHT_HINT\": float(pos_weight),\n    \"HOLDOUT_VAL_MASK\": (val_mask_holdout if val_mask_holdout is not None else None),\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Model (CPU-Safe Configuration)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 8 — Train Multiband Event Transformer (CPU-Safe)\n# REVISI FULL v4.0 (BATCH AUG TORCH + FAST INDEXER + EMA + WD PARAM GROUPS + HOLDOUT SAFE)\n#\n# Output:\n# - checkpoints/fold_*.pt\n# - oof/oof_prob.npy + oof/oof_prob.csv\n# - oof/fold_metrics.json\n# - logs/train_cfg_stage8.json + global_feature_spec.json + global_meta_cols.json\n# ============================================================\n\nimport os, gc, json, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require minimal previous stages\n# ----------------------------\nneed_min = [\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_train_meta\",\"n_splits\",\"folds\"]\nfor k in need_min:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n\n# ----------------------------\n# 0a) Resolve train_ids ordering + labels (FAST + robust)\n# ----------------------------\ndef _decode_ids(arr):\n    out = []\n    for x in arr.tolist():\n        if isinstance(x, (bytes, bytearray, np.bytes_)):\n            s = x.decode(\"utf-8\", errors=\"ignore\")\n        else:\n            s = str(x)\n        out.append(s.strip())\n    return out\n\n# ordering\nif \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n    train_ids = [str(x).strip() for x in list(globals()[\"train_ids_ordered\"])]\nelse:\n    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n    if p.exists():\n        raw = np.load(p, allow_pickle=False)\n        train_ids = _decode_ids(raw)\n    else:\n        train_ids = pd.Index(df_train_meta.index).astype(\"string\").str.strip().astype(str).tolist()\n\n# target column robust\ntarget_col = None\nfor cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n    if cand in df_train_meta.columns:\n        target_col = cand\n        break\nif target_col is None:\n    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols(sample)={list(df_train_meta.columns)[:40]}\")\n\n# normalize meta index (string+strip) + fast indexer\nmeta = df_train_meta.copy()\nmeta.index = pd.Index(pd.Index(meta.index).astype(\"string\").str.strip().astype(str), name=\"object_id\")\n\npos_idx = meta.index.get_indexer(train_ids)  # vectorized\nif (pos_idx < 0).any():\n    miss = [train_ids[i] for i in np.where(pos_idx < 0)[0][:10]]\n    raise RuntimeError(f\"Some train_ids not found in df_train_meta.index (after str/strip). ex={miss}\")\n\ny_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\ny = y_all[pos_idx]\ny = (y > 0).astype(np.int8)\n\n# ----------------------------\n# 0b) Ensure output dirs exist\n# ----------------------------\nif \"RUN_DIR\" in globals() and globals()[\"RUN_DIR\"] is not None:\n    RUN_DIR = Path(globals()[\"RUN_DIR\"])\nelse:\n    if \"ART_DIR\" in globals() and globals()[\"ART_DIR\"] is not None:\n        RUN_DIR = Path(globals()[\"ART_DIR\"]).parent\n    else:\n        RUN_DIR = Path(\"/kaggle/working/mallorn_run\")\n\nART_DIR = Path(globals().get(\"ART_DIR\", RUN_DIR / \"artifacts\"))\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nCKPT_DIR = Path(globals().get(\"CKPT_DIR\", RUN_DIR / \"checkpoints\"))\nOOF_DIR  = Path(globals().get(\"OOF_DIR\",  RUN_DIR / \"oof\"))\nLOG_DIR  = Path(globals().get(\"LOG_DIR\",  RUN_DIR / \"logs\"))\nCKPT_DIR.mkdir(parents=True, exist_ok=True)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nglobals().update({\"RUN_DIR\": RUN_DIR, \"ART_DIR\": ART_DIR, \"CKPT_DIR\": CKPT_DIR, \"OOF_DIR\": OOF_DIR, \"LOG_DIR\": LOG_DIR})\n\n# ----------------------------\n# 1) Torch imports + CPU safety\n# ----------------------------\ntry:\n    import torch\n    import torch.nn as nn\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\ndevice = torch.device(\"cpu\")\n\n# thread guard\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\ntry:\n    from sklearn.metrics import roc_auc_score\nexcept Exception as e:\n    raise RuntimeError(\"scikit-learn metrics tidak tersedia.\") from e\n\n# ----------------------------\n# 2) Open memmaps (fixed seq) — NO RAM load\n# ----------------------------\nFIX_DIR = Path(globals()[\"FIX_DIR\"])\nN = len(train_ids)\nL = int(globals()[\"MAX_LEN\"])\nSEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\nFdim = len(SEQ_FEATURE_NAMES)\nfeat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\n\nfor p in [train_X_path, train_B_path, train_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\n# ----------------------------\n# 2b) Read Stage6 policy to detect SHIFT_BAND_IDS + dtype_X\n# ----------------------------\nSHIFT_BAND_IDS = False\nPAD_BAND_ID = 0\nDTYPE_X_MEMMAP = np.float32\n\npolicy_path = FIX_DIR / \"length_policy_config.json\"\nif policy_path.exists():\n    try:\n        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n            pol = json.load(f)\n        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n        dt = str(pol.get(\"dtype_X\", \"float32\")).lower()\n        if (\"float16\" in dt) or (\"fp16\" in dt):\n            DTYPE_X_MEMMAP = np.float16\n        else:\n            DTYPE_X_MEMMAP = np.float32\n    except Exception:\n        pass\n\nX_mm = np.memmap(train_X_path, dtype=DTYPE_X_MEMMAP, mode=\"r\", shape=(N, L, Fdim))\nB_mm = np.memmap(train_B_path, dtype=np.int8,        mode=\"r\", shape=(N, L))\nM_mm = np.memmap(train_M_path, dtype=np.int8,        mode=\"r\", shape=(N, L))\n\n# detect token mode\nSEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\nif SEQ_TOKEN_MODE is None:\n    if (\"mag\" in feat) and (\"mag_err_log\" in feat):\n        SEQ_TOKEN_MODE = \"mag\"\n    elif (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n        SEQ_TOKEN_MODE = \"asinh\"\n    else:\n        raise RuntimeError(f\"Cannot infer token_mode from features: {SEQ_FEATURE_NAMES}\")\n\nfor k in [\"snr_tanh\",\"detected\"]:\n    if k not in feat:\n        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n\nVAL_FEAT = \"mag\" if SEQ_TOKEN_MODE == \"mag\" else \"flux_asinh\"\nif VAL_FEAT not in feat:\n    raise RuntimeError(f\"Feature '{VAL_FEAT}' missing for token_mode={SEQ_TOKEN_MODE}.\")\n\n# ----------------------------\n# 3) Build RAW meta global features (no leak)\n# ----------------------------\nEBV_used = None\nif (\"EBV_clip\" in meta.columns):\n    EBV_used = pd.to_numeric(meta[\"EBV_clip\"], errors=\"coerce\")\nelif (\"EBV\" in meta.columns):\n    EBV_used = pd.to_numeric(meta[\"EBV\"], errors=\"coerce\")\nelse:\n    EBV_used = pd.Series(np.zeros((len(meta),), dtype=np.float32), index=meta.index)\n\n# ensure base cols exist (create in a local df to avoid mutating upstream)\nBASE_G_COLS = [\"Z\",\"Z_err\",\"EBV_used\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\n\ntmp_meta = meta.copy()\ntmp_meta[\"EBV_used\"] = EBV_used\n\nfor c in BASE_G_COLS:\n    if c not in tmp_meta.columns:\n        tmp_meta[c] = 0.0\n\nG_meta = tmp_meta.iloc[pos_idx][BASE_G_COLS].copy()\nfor c in BASE_G_COLS:\n    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\nG_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n\nwith open(Path(LOG_DIR)/\"global_meta_cols.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"cols\": BASE_G_COLS}, f, indent=2)\n\n# ----------------------------\n# 3b) Sequence aggregate features (global + per-band) — BOOST\n# (Sama konsepnya, tapi tetap CPU-safe)\n# FIX: band mapping correct when SHIFT_BAND_IDS=True (real=1..6 -> 0..5)\n# ----------------------------\nUSE_AGG_SEQ_FEATURES = True\nN_BANDS = 6\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1.0)\n\ndef build_agg_seq_features(X_mm, B_mm, M_mm, chunk=1024):\n    snr_i = feat[\"snr_tanh\"]\n    det_i = feat[\"detected\"]\n    val_i = feat[VAL_FEAT]\n\n    out_chunks = []\n    for s in range(0, N, chunk):\n        e = min(N, s + chunk)\n\n        Xc = np.asarray(X_mm[s:e])  # (B,L,F)\n        Bc = np.asarray(B_mm[s:e])  # (B,L)\n        Mc = np.asarray(M_mm[s:e])  # (B,L)\n\n        real = (Mc == 1)\n        tok_count = real.sum(axis=1).astype(np.float32)\n\n        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32, copy=False)\n        det = (Xc[:, :, det_i] > 0.5).astype(np.float32, copy=False)\n        val = Xc[:, :, val_i].astype(np.float32, copy=False)\n\n        snr_r = snr * real\n        det_r = det * real\n\n        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n        max_abs_snr = np.where(tok_count > 0, snr_r.max(axis=1), 0.0).astype(np.float32)\n\n        if SEQ_TOKEN_MODE == \"mag\":\n            val_r = np.where(real, val, np.nan)\n            mean_val = np.nan_to_num(np.nanmean(val_r, axis=1).astype(np.float32), nan=0.0)\n            std_val  = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32),  nan=0.0)\n            min_val  = np.nan_to_num(np.nanmin(val_r, axis=1).astype(np.float32),  nan=0.0)\n            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n        else:\n            aval = np.abs(val).astype(np.float32, copy=False)\n            aval_r = aval * real\n            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n            val_r = np.where(real, val, np.nan)\n            std_val = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32), nan=0.0)\n            max_aval = np.where(tok_count > 0, aval_r.max(axis=1), 0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n\n        # band adjust for aggregation only\n        if SHIFT_BAND_IDS:\n            Badj = Bc.astype(np.int16, copy=False).copy()\n            Badj[real] = np.clip(Badj[real] - 1, 0, N_BANDS - 1)\n        else:\n            Badj = Bc.astype(np.int16, copy=False)\n\n        per_band = []\n        for b in range(N_BANDS):\n            bm = (Badj == b) & real\n            cnt = bm.sum(axis=1).astype(np.float32)\n\n            detb = (det * bm).sum(axis=1).astype(np.float32)\n            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n\n            det_frac_b = _safe_div(detb, cnt)\n            mean_abs_snr_b = _safe_div(snrb, cnt)\n\n            if SEQ_TOKEN_MODE == \"mag\":\n                vb = np.where(bm, val, np.nan)\n                mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n            else:\n                ab = (np.abs(val).astype(np.float32) * bm).sum(axis=1).astype(np.float32)\n                mean_val_b = _safe_div(ab, cnt).astype(np.float32)\n\n            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n\n        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n\n        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n\n        out_chunks.append(agg)\n\n        del Xc, Bc, Mc, Badj\n        if ((s // chunk) % 4) == 0:\n            gc.collect()\n\n    return np.concatenate(out_chunks, axis=0).astype(np.float32)\n\nif USE_AGG_SEQ_FEATURES:\n    print(\"[Stage 8] Building AGG sequence features (one-time)...\")\n    t0 = time.time()\n    G_seq_np = build_agg_seq_features(X_mm, B_mm, M_mm, chunk=1024)\n    print(f\"[Stage 8] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\nelse:\n    G_seq_np = np.zeros((N,0), dtype=np.float32)\n\nG_raw_np = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\ng_dim = int(G_raw_np.shape[1])\n\nwith open(Path(LOG_DIR)/\"global_feature_spec.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"meta_cols\": BASE_G_COLS,\n            \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n            \"token_mode\": SEQ_TOKEN_MODE,\n            \"val_feat\": VAL_FEAT,\n            \"agg_dim\": int(G_seq_np.shape[1]),\n            \"total_g_dim\": int(g_dim),\n            \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n            \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n            \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n        },\n        f,\n        indent=2,\n    )\n\n# ----------------------------\n# 4) Dataset / Loader (MIN COPY, no per-sample float32 convert)\n# ----------------------------\nclass MemmapSeqDataset(torch.utils.data.Dataset):\n    def __init__(self, idx, X_mm, B_mm, M_mm, G_raw_np, y=None):\n        self.idx = np.asarray(idx, dtype=np.int32)\n        self.X_mm = X_mm\n        self.B_mm = B_mm\n        self.M_mm = M_mm\n        self.G_raw = G_raw_np\n        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n\n    def __len__(self):\n        return len(self.idx)\n\n    def __getitem__(self, i):\n        j = int(self.idx[i])\n        # NOTE: no astype float32 here; keep original dtype, convert in torch (batch)\n        X = np.asarray(self.X_mm[j])           # (L,F) float16/float32\n        B = np.asarray(self.B_mm[j])           # (L,)  int8\n        M = np.asarray(self.M_mm[j])           # (L,)  int8\n        G0 = np.asarray(self.G_raw[j], dtype=np.float32)\n\n        Xt = torch.from_numpy(X)              # keep dtype\n        Bt = torch.from_numpy(B.astype(np.int64, copy=False))\n        Mt = torch.from_numpy(M.astype(np.int64, copy=False))\n        Gt = torch.from_numpy(G0)\n\n        if self.y is None:\n            return Xt, Bt, Mt, Gt\n\n        yy = float(self.y[j])\n        return Xt, Bt, Mt, Gt, torch.tensor(yy, dtype=torch.float32)\n\ndef make_loader(ds, batch_size, shuffle, sampler=None):\n    return torch.utils.data.DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=(sampler is None and shuffle),\n        sampler=sampler,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n# ----------------------------\n# 5) EMA helper\n# ----------------------------\nclass EMA:\n    def __init__(self, model, decay=0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    def store(self, model):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.backup[name] = p.detach().clone()\n\n    def copy_to(self, model):\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                p.data.copy_(self.shadow[name].data)\n\n    def restore(self, model):\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                p.data.copy_(self.backup[name].data)\n        self.backup = {}\n\n# ----------------------------\n# 6) Model — stronger pooling + shift band in-model\n# ----------------------------\nclass MultibandEventTransformer(nn.Module):\n    def __init__(\n        self,\n        feat_dim, max_len, n_bands=6,\n        d_model=160, n_heads=4, n_layers=3, ff_mult=2,\n        dropout=0.14, g_dim=0,\n        shift_band_ids=False\n    ):\n        super().__init__()\n        self.n_bands = int(n_bands)\n        self.d_model = int(d_model)\n        self.max_len = int(max_len)\n        self.shift_band_ids = bool(shift_band_ids)\n\n        self.x_proj = nn.Sequential(\n            nn.Linear(feat_dim, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n        self.band_emb = nn.Embedding(self.n_bands, d_model)\n\n        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=int(d_model * ff_mult),\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        self.attn = nn.Linear(d_model, 1)\n        self.pool_ln = nn.LayerNorm(d_model)\n\n        g_out = max(32, d_model // 2)\n        self.g_mean = None\n        self.g_std = None\n        self.g_proj = nn.Sequential(\n            nn.Linear(g_dim, g_out),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model + g_out, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def set_global_scaler(self, mean_np, std_np):\n        # buffers for fold-wise scaler (avoid per-sample numpy scaling)\n        mean_t = torch.tensor(mean_np, dtype=torch.float32)\n        std_t  = torch.tensor(std_np, dtype=torch.float32)\n        self.register_buffer(\"g_mean_buf\", mean_t, persistent=False)\n        self.register_buffer(\"g_std_buf\", std_t, persistent=False)\n\n    def forward(self, X, band_id, mask, G_raw):\n        # X: float16/float32 -> cast once per batch\n        X = X.to(torch.float32)\n        band_id = band_id.to(torch.long)\n        mask = mask.to(torch.long)\n\n        # shift band ids only on real tokens (mask==1): (1..6)->(0..5)\n        if self.shift_band_ids:\n            real = (mask == 1)\n            if real.any():\n                band2 = band_id.clone()\n                band2[real] = (band2[real] - 1).clamp(0, self.n_bands - 1)\n                band2[~real] = 0\n                band_id = band2\n            else:\n                band_id = torch.zeros_like(band_id)\n\n        band_id = band_id.clamp(0, self.n_bands - 1)\n\n        pad_mask = (mask == 0)  # True=pad\n        all_pad = pad_mask.all(dim=1)\n        if all_pad.any():\n            pad_mask = pad_mask.clone()\n            pad_mask[all_pad, 0] = False\n\n        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        # attention pooling\n        a = self.attn(h).squeeze(-1)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)\n        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n\n        # mean pooling\n        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n        denom = valid.sum(dim=1).clamp_min(1.0)\n        pooled_mean = (h * valid).sum(dim=1) / denom\n\n        # max pooling (masked)\n        h_masked = h.masked_fill(pad_mask.unsqueeze(-1), -1e9)\n        pooled_max = torch.max(h_masked, dim=1).values\n        pooled_max = torch.where(torch.isfinite(pooled_max), pooled_max, torch.zeros_like(pooled_max))\n\n        pooled = (0.50 * pooled_attn) + (0.30 * pooled_mean) + (0.20 * pooled_max)\n        pooled = self.pool_ln(pooled)\n\n        # scale global inside model\n        G = G_raw.to(torch.float32)\n        if hasattr(self, \"g_mean_buf\") and hasattr(self, \"g_std_buf\"):\n            G = (G - self.g_mean_buf) / self.g_std_buf\n\n        g = self.g_proj(G)\n        z = torch.cat([pooled, g], dim=1)\n        return self.head(z).squeeze(-1)\n\n# ----------------------------\n# 7) Training config (CPU safe)\n# ----------------------------\nCFG = {\n    \"d_model\": 160,\n    \"n_heads\": 4,\n    \"n_layers\": 3,\n    \"ff_mult\": 2,\n    \"dropout\": 0.14,\n\n    \"batch_size\": 16,\n    \"grad_accum\": 2,\n\n    \"epochs\": 16,\n    \"lr\": 5e-4,\n    \"weight_decay\": 0.02,\n\n    \"patience\": 5,            # early stop by AUC\n    \"max_grad_norm\": 1.0,\n\n    # imbalance strategy: \"sampler\" | \"pos_weight\" | \"both\" | \"none\"\n    \"balance_mode\": \"both\",\n\n    \"label_smoothing\": 0.03,\n    \"scheduler\": \"onecycle\",\n\n    # EMA\n    \"use_ema\": True,\n    \"ema_decay\": 0.999,\n\n    # batch aug (torch)\n    \"aug_tokendrop_p\": 0.06,\n    \"aug_value_noise\": 0.012,\n    \"aug_featdrop_p\": 0.00,   # kalau mau: 0.02 misalnya\n}\n\n# auto soften for long seq (CPU guard)\nif L >= 512:\n    CFG[\"d_model\"] = 128\n    CFG[\"n_heads\"] = 4\n    CFG[\"n_layers\"] = 2\n    CFG[\"batch_size\"] = 12\n    CFG[\"grad_accum\"] = 2\n    CFG[\"lr\"] = 4e-4\n\ncfg_path = Path(LOG_DIR) / \"train_cfg_stage8.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(CFG, f, indent=2)\n\npos_all = int((y == 1).sum())\nneg_all = int((y == 0).sum())\nprint(\"[Stage 8] TRAIN CONFIG (CPU)\")\nprint(f\"- N={N:,} | pos={pos_all:,} | neg={neg_all:,} | pos%={pos_all/max(N,1)*100:.6f}%\")\nprint(f\"- token_mode={SEQ_TOKEN_MODE} | val_feat={VAL_FEAT} | g_dim={g_dim} | use_agg_seq={USE_AGG_SEQ_FEATURES}\")\nprint(f\"- SHIFT_BAND_IDS(from stage6)={SHIFT_BAND_IDS} | PAD_BAND_ID(from stage6)={PAD_BAND_ID} | X_dtype={DTYPE_X_MEMMAP}\")\nprint(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\nprint(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\nprint(f\"- balance_mode={CFG['balance_mode']} | label_smoothing={CFG['label_smoothing']} | ema={CFG['use_ema']}({CFG['ema_decay']})\")\nprint(f\"- CKPT_DIR={CKPT_DIR}\")\nprint(f\"- OOF_DIR ={OOF_DIR}\")\nprint(f\"- LOG_DIR ={LOG_DIR}\")\n\n# ----------------------------\n# 8) Helpers\n# ----------------------------\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef f1_binary(y_true, y_pred01):\n    y_true = y_true.astype(np.int32)\n    y_pred01 = y_pred01.astype(np.int32)\n    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n    if tp == 0:\n        return 0.0\n    prec = tp / max(tp + fp, 1)\n    rec  = tp / max(tp + fn, 1)\n    if prec + rec == 0:\n        return 0.0\n    return float(2 * prec * rec / (prec + rec))\n\n@torch.no_grad()\ndef eval_model(model, loader, criterion, use_ema=False, ema=None):\n    model.eval()\n    if use_ema and (ema is not None):\n        ema.store(model)\n        ema.copy_to(model)\n\n    losses, logits_all, y_all = [], [], []\n    for batch in loader:\n        Xb, Bb, Mb, Gb, yb = batch\n        Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n        logit = model(Xb, Bb, Mb, Gb)\n        loss = criterion(logit, yb)\n        losses.append(float(loss.item()))\n        logits_all.append(logit.detach().cpu().numpy())\n        y_all.append(yb.detach().cpu().numpy())\n\n    if use_ema and (ema is not None):\n        ema.restore(model)\n\n    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n\n    probs = sigmoid_np(logits_all)\n    pred01 = (probs >= 0.5).astype(np.int8)\n    f1 = f1_binary(y_all, pred01)\n    auc = float(roc_auc_score(y_all, probs)) if (len(np.unique(y_all)) == 2) else float(\"nan\")\n    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1, auc\n\ndef fit_scaler_fold(G_raw_np, tr_idx):\n    X = G_raw_np[tr_idx]\n    mean = X.mean(axis=0).astype(np.float32)\n    std  = X.std(axis=0).astype(np.float32)\n    std  = np.where(std < 1e-6, 1.0, std).astype(np.float32)\n    return mean, std\n\ndef make_adamw_param_groups(model, weight_decay):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        # no weight decay for bias & norms\n        if name.endswith(\".bias\") or (\"norm\" in name.lower()) or (\"ln\" in name.lower()):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n    return [\n        {\"params\": decay, \"weight_decay\": float(weight_decay)},\n        {\"params\": no_decay, \"weight_decay\": 0.0},\n    ]\n\n# ----------------------------\n# 9) Batch augment (torch, vectorized)\n# ----------------------------\ndef apply_batch_aug(Xb, Bb, Mb, cfg, feat_map, val_feat_name):\n    # Xb float32, Bb long, Mb long\n    p_drop = float(cfg.get(\"aug_tokendrop_p\", 0.0))\n    noise  = float(cfg.get(\"aug_value_noise\", 0.0))\n    p_fdrop = float(cfg.get(\"aug_featdrop_p\", 0.0))\n\n    if p_drop and p_drop > 0:\n        real = (Mb == 1)\n        if real.any():\n            rnd = torch.rand_like(Mb.float())\n            drop = (rnd < p_drop) & real\n            # ensure at least 1 real token remains per sample\n            nreal = real.sum(dim=1)\n            ndrop = drop.sum(dim=1)\n            bad = (nreal > 0) & (ndrop >= nreal)\n            if bad.any():\n                bad_idx = torch.where(bad)[0].tolist()\n                for bi in bad_idx:\n                    pos = torch.where(real[bi])[0]\n                    if pos.numel() > 0:\n                        keep_one = pos[int(torch.randint(0, pos.numel(), (1,)).item())]\n                        drop[bi, keep_one] = False\n            Mb = Mb.clone()\n            Mb[drop] = 0\n\n    if noise and noise > 0:\n        vi = int(feat_map[val_feat_name])\n        real = (Mb == 1)\n        if real.any():\n            eps = torch.randn((int(real.sum().item()),), device=Xb.device, dtype=Xb.dtype) * float(noise)\n            Xb = Xb.clone()\n            Xb[real, vi] = Xb[real, vi] + eps\n\n    if p_fdrop and p_fdrop > 0:\n        # feature dropout on real tokens (except time features by default)\n        real = (Mb == 1)\n        if real.any():\n            # drop on value + snr only (safer)\n            cand_feats = []\n            for nm in [val_feat_name, \"snr_tanh\"]:\n                if nm in feat_map:\n                    cand_feats.append(int(feat_map[nm]))\n            if cand_feats:\n                Xb = Xb.clone()\n                for fi in cand_feats:\n                    mask_drop = (torch.rand_like(Mb.float()) < p_fdrop) & real\n                    Xb[mask_drop, fi] = 0.0\n\n    return Xb, Bb, Mb\n\n# ----------------------------\n# 10) CV Train\n# ----------------------------\noof_prob = np.full((N,), np.nan, dtype=np.float32)  # holdout-safe\nfold_metrics = []\n\nstart_time = time.time()\nn_splits = int(globals()[\"n_splits\"])\ncv_type = str(globals().get(\"CV_TYPE\", \"\"))\n\nfor fold_info in globals()[\"folds\"]:\n    fold = int(fold_info.get(\"fold\", 0))\n\n    tr_idx = np.asarray(fold_info[\"train_idx\"], dtype=np.int32)\n    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n\n    y_tr = y[tr_idx]\n    pos = int((y_tr == 1).sum())\n    neg = int((y_tr == 0).sum())\n    if pos == 0:\n        raise RuntimeError(f\"[Stage 8] Fold {fold}: no positives in training split.\")\n\n    balance_mode = str(CFG.get(\"balance_mode\", \"both\")).lower()\n    use_sampler = balance_mode in (\"sampler\", \"both\")\n    use_posw    = balance_mode in (\"pos_weight\", \"both\")\n\n    pos_weight = float(neg / max(pos, 1))\n    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\n\n    # label smoothing (applied only to TRAIN targets)\n    ls = float(CFG.get(\"label_smoothing\", 0.0))\n    def smooth(yb):\n        if ls <= 0:\n            return yb\n        return yb * (1.0 - ls) + 0.5 * ls\n\n    if use_posw:\n        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n\n    print(f\"\\n[Stage 8] FOLD {fold} | train={len(tr_idx):,} val={len(val_idx):,} \"\n          f\"| pos={pos:,} neg={neg:,} | pos_weight={pos_weight:.4f} | balance_mode={balance_mode}\")\n\n    # fold-wise scaler (NO leakage)\n    g_mean, g_std = fit_scaler_fold(G_raw_np, tr_idx)\n\n    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_raw_np, y=y)\n    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_raw_np, y=y)\n\n    sampler = None\n    if use_sampler:\n        # weights align with ds_tr order (len=|tr_idx|)\n        ytr_local = y[tr_idx]\n        w = np.ones((len(tr_idx),), dtype=np.float32)\n        w[ytr_local == 1] = float(neg / max(pos, 1))\n        sampler = torch.utils.data.WeightedRandomSampler(\n            weights=torch.from_numpy(w),\n            num_samples=len(tr_idx),\n            replacement=True\n        )\n\n    dl_tr = make_loader(ds_tr, batch_size=int(CFG[\"batch_size\"]), shuffle=True, sampler=sampler)\n    dl_va = make_loader(ds_va, batch_size=int(CFG[\"batch_size\"]), shuffle=False)\n\n    model = MultibandEventTransformer(\n        feat_dim=Fdim,\n        max_len=L,\n        n_bands=6,\n        d_model=int(CFG[\"d_model\"]),\n        n_heads=int(CFG[\"n_heads\"]),\n        n_layers=int(CFG[\"n_layers\"]),\n        ff_mult=int(CFG[\"ff_mult\"]),\n        dropout=float(CFG[\"dropout\"]),\n        g_dim=g_dim,\n        shift_band_ids=bool(SHIFT_BAND_IDS),\n    ).to(device)\n    model.set_global_scaler(g_mean, g_std)\n\n    # AdamW param groups (better stability)\n    param_groups = make_adamw_param_groups(model, weight_decay=float(CFG[\"weight_decay\"]))\n    opt = torch.optim.AdamW(param_groups, lr=float(CFG[\"lr\"]))\n\n    # scheduler OneCycle (by OPT steps)\n    scheduler = None\n    grad_accum = int(CFG[\"grad_accum\"])\n    if str(CFG.get(\"scheduler\",\"\")).lower() == \"onecycle\":\n        steps_per_epoch_opt = int(math.ceil(len(dl_tr) / max(grad_accum, 1)))\n        steps_per_epoch_opt = max(steps_per_epoch_opt, 1)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            opt,\n            max_lr=float(CFG[\"lr\"]),\n            epochs=int(CFG[\"epochs\"]),\n            steps_per_epoch=steps_per_epoch_opt,\n            pct_start=0.1,\n            anneal_strategy=\"cos\",\n            div_factor=10.0,\n            final_div_factor=50.0,\n        )\n\n    use_ema = bool(CFG.get(\"use_ema\", True))\n    ema = EMA(model, decay=float(CFG.get(\"ema_decay\", 0.999))) if use_ema else None\n\n    best_val_auc = -1e9\n    best_val_loss = float(\"inf\")\n    best_epoch = -1\n    best_probs = None\n    patience_left = int(CFG[\"patience\"])\n\n    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        total_loss = 0.0\n        n_batches = 0\n        accum = 0\n        opt_steps = 0\n\n        for batch in dl_tr:\n            Xb, Bb, Mb, Gb, yb = batch\n            Xb = Xb.to(device)\n            Bb = Bb.to(device)\n            Mb = Mb.to(device)\n            Gb = Gb.to(device)\n            yb = yb.to(device)\n\n            # single cast per batch\n            Xb = Xb.to(torch.float32)\n\n            # batch aug\n            Xb, Bb, Mb = apply_batch_aug(Xb, Bb, Mb, CFG, feat, VAL_FEAT)\n\n            yb_s = smooth(yb)\n\n            logit = model(Xb, Bb, Mb, Gb)\n            loss = criterion(logit, yb_s)\n\n            total_loss += float(loss.item())\n            n_batches += 1\n\n            (loss / float(grad_accum)).backward()\n            accum += 1\n\n            if accum == grad_accum:\n                if CFG[\"max_grad_norm\"] is not None:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n                opt.step()\n                opt.zero_grad(set_to_none=True)\n                opt_steps += 1\n                accum = 0\n                if scheduler is not None:\n                    scheduler.step()\n                if ema is not None:\n                    ema.update(model)\n\n        # remainder step\n        if accum > 0:\n            if CFG[\"max_grad_norm\"] is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n            opt.step()\n            opt.zero_grad(set_to_none=True)\n            opt_steps += 1\n            if scheduler is not None:\n                scheduler.step()\n            if ema is not None:\n                ema.update(model)\n\n        train_loss = total_loss / max(n_batches, 1)\n\n        # validate (EMA weights if enabled)\n        val_loss, probs, y_val, f1_05, val_auc = eval_model(model, dl_va, criterion, use_ema=use_ema, ema=ema)\n\n        improved = (val_auc > best_val_auc + 1e-6) or (math.isnan(best_val_auc) and not math.isnan(val_auc))\n        if (not improved) and (abs(val_auc - best_val_auc) <= 1e-6) and (val_loss < best_val_loss - 1e-6):\n            improved = True\n\n        if improved:\n            best_val_auc = float(val_auc)\n            best_val_loss = float(val_loss)\n            best_epoch = int(epoch)\n            best_probs = probs.copy()\n\n            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n            payload = {\n                \"fold\": fold,\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"cfg\": CFG,\n                \"seq_feature_names\": SEQ_FEATURE_NAMES,\n                \"max_len\": L,\n                \"token_mode\": SEQ_TOKEN_MODE,\n                \"val_feat\": VAL_FEAT,\n                \"global_meta_cols\": BASE_G_COLS,\n                \"use_agg_seq_features\": bool(USE_AGG_SEQ_FEATURES),\n                \"global_scaler\": {\"mean\": g_mean.astype(np.float32), \"std\": g_std.astype(np.float32)},\n                \"pos_weight_train\": float(pos_weight),\n                \"balance_mode\": balance_mode,\n                \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n                \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n                \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n                \"cv_type\": str(cv_type),\n            }\n            if ema is not None:\n                payload[\"ema_shadow\"] = {k: v.detach().cpu() for k, v in ema.shadow.items()}\n                payload[\"ema_decay\"] = float(ema.decay)\n\n            torch.save(payload, ckpt_path)\n            patience_left = int(CFG[\"patience\"])\n        else:\n            patience_left -= 1\n\n        lr_now = opt.param_groups[0][\"lr\"]\n        print(f\"  epoch {epoch:02d} | lr={lr_now:.2e} | opt_steps={opt_steps:4d} | \"\n              f\"train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | val_auc={val_auc:.5f} | f1@0.5={f1_05:.4f} | \"\n              f\"best_ep={best_epoch} | pat={patience_left}\")\n\n        if patience_left <= 0:\n            break\n\n    if best_probs is None:\n        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n\n    # fill OOF for val only (holdout-safe)\n    oof_prob[val_idx] = best_probs.astype(np.float32)\n\n    pred01 = (best_probs >= 0.5).astype(np.int8)\n    best_f1_05 = f1_binary(y[val_idx], pred01)\n\n    fold_metrics.append({\n        \"fold\": fold,\n        \"val_size\": int(len(val_idx)),\n        \"best_epoch\": int(best_epoch),\n        \"best_val_auc\": float(best_val_auc),\n        \"best_val_loss\": float(best_val_loss),\n        \"f1_at_0p5\": float(best_f1_05),\n        \"pos_weight_train\": float(pos_weight),\n        \"g_dim\": int(g_dim),\n        \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n        \"balance_mode\": balance_mode,\n        \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n        \"ema_used\": bool(use_ema),\n    })\n\n    del model, opt, ds_tr, ds_va, dl_tr, dl_va, ema\n    gc.collect()\n\nelapsed = time.time() - start_time\n\n# ----------------------------\n# 11) Save OOF artifacts + summary (NaN-aware)\n# ----------------------------\noof_path_npy = OOF_DIR / \"oof_prob.npy\"\nnp.save(oof_path_npy, oof_prob)\n\ndf_oof = pd.DataFrame({\"object_id\": train_ids, \"target\": y.astype(int), \"oof_prob\": oof_prob.astype(np.float32)})\noof_path_csv = OOF_DIR / \"oof_prob.csv\"\ndf_oof.to_csv(oof_path_csv, index=False)\n\nmetrics_path = OOF_DIR / \"fold_metrics.json\"\nwith open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed), \"cv_type\": str(cv_type)}, f, indent=2)\n\n# compute OOF metrics on valid rows only\nvalid = np.isfinite(oof_prob)\nif valid.any() and len(np.unique(y[valid])) == 2:\n    oof_auc = float(roc_auc_score(y[valid], oof_prob[valid]))\nelse:\n    oof_auc = float(\"nan\")\n\nif valid.any():\n    oof_pred01 = (oof_prob[valid] >= 0.5).astype(np.int8)\n    oof_f1_05 = f1_binary(y[valid], oof_pred01)\nelse:\n    oof_f1_05 = float(\"nan\")\n\nprint(\"\\n[Stage 8] TRAIN DONE\")\nprint(f\"- elapsed: {elapsed/60:.2f} min\")\nprint(f\"- OOF saved: {oof_path_npy}\")\nprint(f\"- OOF saved: {oof_path_csv}\")\nprint(f\"- fold metrics: {metrics_path}\")\nprint(f\"- OOF rows valid: {int(valid.sum()):,}/{N:,}  (holdout mode -> hanya val yang valid)\")\nprint(f\"- OOF AUC (valid-only): {oof_auc:.5f}\")\nprint(f\"- OOF F1@0.5 (valid-only): {oof_f1_05:.4f}\")\n\nglobals().update({\n    \"oof_prob\": oof_prob,\n    \"OOF_PROB_PATH\": oof_path_npy,\n    \"OOF_CSV_PATH\": oof_path_csv,\n    \"FOLD_METRICS_PATH\": metrics_path,\n    \"TRAIN_CFG_PATH\": cfg_path,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OOF Prediction + Threshold Tuning","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v4.0 (HOLDOUT/NaN SAFE + FAST INDEXER + >= / > boundary + robust candidates)\n#\n# Upgrade utama vs v3.2:\n# - HOLDOUT SAFE: kalau Stage 8 pakai oof_prob NaN utk non-val, Stage 9 hanya tuning pada rows valid (finite).\n# - Alignment lebih cepat: pakai Index.get_indexer() (bukan Series map + loop).\n# - Candidate thresholds lebih kuat: unique + nextafter(unique, +inf) => bisa menangkap batas \"exclude equals\".\n# - Tambah AUC (threshold-free) sebagai sanity.\n# ============================================================\n\nimport gc, json, math, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\"OOF_DIR\", \"df_train_meta\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n\nOOF_DIR = Path(OOF_DIR)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Helper: robust stringify id\n# ----------------------------\ndef _to_str_list(ids):\n    out = []\n    for x in ids:\n        if isinstance(x, (bytes, np.bytes_, bytearray)):\n            out.append(x.decode(\"utf-8\", errors=\"ignore\").strip())\n        else:\n            out.append(str(x).strip())\n    return out\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a.reshape(1)\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\n# ----------------------------\n# Detect target column in df_train_meta\n# ----------------------------\ndef _detect_target_col(df):\n    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\nTARGET_COL = _detect_target_col(df_train_meta)\nif TARGET_COL is None:\n    raise RuntimeError(\n        \"Cannot detect target column in df_train_meta. \"\n        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n    )\n\n# normalize meta index to string+strip once\nmeta = df_train_meta.copy()\nmeta.index = pd.Index(pd.Index(meta.index).astype(\"string\").str.strip().astype(str), name=\"object_id\")\n\n# ----------------------------\n# Load OOF (prefer CSV)\n# ----------------------------\ndef _load_oof():\n    pcsv = OOF_DIR / \"oof_prob.csv\"\n    if pcsv.exists():\n        df = pd.read_csv(pcsv)\n        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n            ids = df[\"object_id\"].astype(str).str.strip().tolist()\n            prob = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n            return ids, prob, \"csv(oof_prob.csv)\"\n\n    if \"oof_prob\" in globals():\n        prob = _as_1d_float32(globals()[\"oof_prob\"])\n        if isinstance(prob, np.ndarray) and prob.ndim == 1 and len(prob) > 0:\n            if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n                ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n                return ids, prob, \"globals(oof_prob + train_ids_ordered)\"\n            if len(prob) == len(meta):\n                ids = _to_str_list(meta.index.tolist())\n                return ids, prob, \"globals(oof_prob + df_train_meta.index)\"\n\n    pnpy = OOF_DIR / \"oof_prob.npy\"\n    if pnpy.exists():\n        prob = _as_1d_float32(np.load(pnpy, allow_pickle=False))\n        if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n            ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n            return ids, prob, \"npy(oof_prob.npy + train_ids_ordered)\"\n        if len(prob) == len(meta):\n            ids = _to_str_list(meta.index.tolist())\n            return ids, prob, \"npy(oof_prob.npy + df_train_meta.index)\"\n\n    raise FileNotFoundError(\"OOF prob not found (csv/globals/npy). Jalankan STAGE 8 dulu.\")\n\ntrain_ids, oof_prob, src = _load_oof()\n\noof_prob = _as_1d_float32(oof_prob).astype(np.float32)\nif len(train_ids) != len(oof_prob):\n    raise RuntimeError(f\"OOF length mismatch: len(train_ids)={len(train_ids)} vs len(oof_prob)={len(oof_prob)}\")\n\n# IMPORTANT: HOLDOUT/NaN safe — jangan nan_to_num jadi 0 (itu bikin tuning bias).\nvalid = np.isfinite(oof_prob)\nif not valid.any():\n    raise RuntimeError(\"All oof_prob are non-finite (NaN/inf). Check STAGE 8 output.\")\n\n# align y by train_ids via fast indexer\nidx = meta.index.get_indexer(train_ids)\nif (idx < 0).any():\n    bad = [train_ids[i] for i in np.where(idx < 0)[0][:10]]\n    raise KeyError(\n        f\"OOF ids not found in df_train_meta.index (string-normalized). ex={bad} | missing_n={int((idx<0).sum())}\"\n    )\n\ny_raw = pd.to_numeric(meta[TARGET_COL], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\ny_all = (y_raw[idx] > 0).astype(np.int8)\n\n# filter valid rows (holdout-safe)\ntrain_ids_v = [train_ids[i] for i in np.where(valid)[0]]\noof_prob_v = np.clip(oof_prob[valid].astype(np.float32), 0.0, 1.0)\ny_v = y_all[valid].astype(np.int8)\n\nN_all = int(len(y_all))\nN = int(len(y_v))\npos = int((y_v == 1).sum())\nneg = int((y_v == 0).sum())\n\nprint(f\"[Stage 9] Loaded OOF from: {src}\")\nprint(f\"[Stage 9] Valid rows: {N:,}/{N_all:,} (holdout mode => valid << total)\")\nprint(f\"[Stage 9] pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}% | target_col={TARGET_COL}\")\n\nuy = set(np.unique(y_v).tolist())\nif not uy.issubset({0, 1}):\n    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\nif len(uy) < 2:\n    print(\"[Stage 9] WARN: y(valid) has only one class; threshold tuning metrics (F1/MCC/BACC) are not meaningful.\")\n\n# optional AUC sanity (threshold-free)\ntry:\n    from sklearn.metrics import roc_auc_score\n    auc_oof = float(roc_auc_score(y_v, oof_prob_v)) if (len(uy) == 2 and N > 1) else float(\"nan\")\nexcept Exception:\n    auc_oof = float(\"nan\")\n\n# ----------------------------\n# 1) Metrics helpers (vectorized-safe)\n# ----------------------------\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1e-12)\n\ndef _metrics_from_counts(tp, fp, fn, tn):\n    tp = tp.astype(np.float64); fp = fp.astype(np.float64)\n    fn = fn.astype(np.float64); tn = tn.astype(np.float64)\n\n    prec = _safe_div(tp, tp + fp)\n    rec  = _safe_div(tp, tp + fn)\n    f1   = _safe_div(2 * prec * rec, prec + rec)\n\n    acc  = _safe_div(tp + tn, tp + fp + fn + tn)\n\n    tpr  = _safe_div(tp, tp + fn)\n    tnr  = _safe_div(tn, tn + fp)\n    bacc = 0.5 * (tpr + tnr)\n\n    num = tp * tn - fp * fn\n    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n\n    return f1, prec, rec, acc, bacc, mcc\n\n# ----------------------------\n# 2) Threshold candidates (grid + quantiles + unique + nextafter)\n# ----------------------------\n# grid (lebih halus di tengah)\ngrid = np.concatenate([\n    np.linspace(0.00, 0.10, 41, dtype=np.float32),\n    np.linspace(0.10, 0.90, 161, dtype=np.float32),\n    np.linspace(0.90, 1.00, 41, dtype=np.float32),\n]).astype(np.float32)\n\n# quantile thresholds (valid-only)\nqs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\ntry:\n    quant_thr = np.quantile(oof_prob_v, qs).astype(np.float32)\nexcept Exception:\n    quant_thr = np.array([], dtype=np.float32)\n\n# unique probs + boundary \"just above\" (to model strict >)\nuniq = np.unique(oof_prob_v.astype(np.float32))\n# cap uniq size\nif len(uniq) > 8000:\n    take = np.linspace(0, len(uniq) - 1, 8000, dtype=int)\n    uniq = uniq[take].astype(np.float32)\n\nuniq_up = np.nextafter(uniq, np.float32(1.0)).astype(np.float32)  # slightly higher than each unique\n# clip to [0,1]\nuniq = np.clip(uniq, 0.0, 1.0).astype(np.float32)\nthr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr, uniq, uniq_up, np.array([0.0, 1.0], np.float32)]), 0.0, 1.0)).astype(np.float32)\n\n# safety cap candidates (CPU)\nif len(thr_candidates) > 20000:\n    take = np.linspace(0, len(thr_candidates) - 1, 20000, dtype=int)\n    thr_candidates = thr_candidates[take].astype(np.float32)\n\n# ----------------------------\n# 3) FAST sweep via sorting + cumulative counts\n#    Predict positive if prob >= thr  (exact, include equals)\n# ----------------------------\np = oof_prob_v.astype(np.float32)\ny = y_v.astype(np.int8)\n\nord_desc = np.argsort(-p)\np_sorted = p[ord_desc]\ny_sorted = y[ord_desc]\n\npos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\nneg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\n\npos_total = int(pos_prefix[-1]) if N > 0 else 0\nneg_total = int(neg_prefix[-1]) if N > 0 else 0\n\n# k = count(prob >= thr)\nk = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"right\").astype(np.int64)\nk = np.clip(k, 0, N).astype(np.int64)\n\ntp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\nfp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\nfn = (pos_total - tp).astype(np.int64)\ntn = (neg_total - fp).astype(np.int64)\n\nf1, prec, rec, acc, bacc, mcc = _metrics_from_counts(tp, fp, fn, tn)\npos_pred = k.astype(np.int64)\n\nthr_table = pd.DataFrame({\n    \"thr\": thr_candidates.astype(np.float32),\n    \"f1\": f1.astype(np.float32),\n    \"precision\": prec.astype(np.float32),\n    \"recall\": rec.astype(np.float32),\n    \"accuracy\": acc.astype(np.float32),\n    \"balanced_accuracy\": bacc.astype(np.float32),\n    \"mcc\": mcc.astype(np.float32),\n    \"tp\": tp.astype(np.int64),\n    \"fp\": fp.astype(np.int64),\n    \"fn\": fn.astype(np.int64),\n    \"tn\": tn.astype(np.int64),\n    \"pos_pred\": pos_pred.astype(np.int64),\n})\n\n# ----------------------------\n# 4) Pick best thresholds with tie-breakers\n# ----------------------------\ndef _pick_best(df, primary, tie_cols):\n    sort_cols = [primary] + tie_cols\n    asc = [False] * len(sort_cols)\n    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n\nbest_f1_row  = _pick_best(thr_table, \"f1\", [\"mcc\", \"balanced_accuracy\", \"recall\", \"precision\"])\nbest_mcc_row = _pick_best(thr_table, \"mcc\", [\"f1\", \"balanced_accuracy\", \"accuracy\"])\nbest_bac_row = _pick_best(thr_table, \"balanced_accuracy\", [\"mcc\", \"accuracy\", \"f1\"])\nbest_acc_row = _pick_best(thr_table, \"accuracy\", [\"balanced_accuracy\", \"mcc\", \"f1\"])\n\ndef _eval_at(thr):\n    thr = float(thr)\n    k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"right\"))\n    k0 = max(0, min(k0, N))\n    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n    fn0 = int(pos_total - tp0)\n    tn0 = int(neg_total - fp0)\n\n    prec0 = tp0 / max(tp0 + fp0, 1)\n    rec0  = tp0 / max(tp0 + fn0, 1)\n    f10 = 0.0 if (tp0 == 0 or (prec0 + rec0) == 0) else (2 * prec0 * rec0 / (prec0 + rec0))\n    acc0 = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n\n    return {\n        \"thr\": thr,\n        \"f1\": float(f10),\n        \"precision\": float(prec0),\n        \"recall\": float(rec0),\n        \"accuracy\": float(acc0),\n        \"balanced_accuracy\": float(bacc0),\n        \"mcc\": float(mcc0),\n        \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0,\n        \"pos_pred\": int(k0),\n    }\n\nbase05 = _eval_at(0.5)\n\nBEST_THR_F1   = float(best_f1_row[\"thr\"])\nBEST_THR_MCC  = float(best_mcc_row[\"thr\"])\nBEST_THR_BACC = float(best_bac_row[\"thr\"])\nBEST_THR_ACC  = float(best_acc_row[\"thr\"])\n\nbest_f1_full  = _eval_at(BEST_THR_F1)\nbest_mcc_full = _eval_at(BEST_THR_MCC)\nbest_bac_full = _eval_at(BEST_THR_BACC)\nbest_acc_full = _eval_at(BEST_THR_ACC)\n\n# Default threshold (tetap F1), tapi kamu bisa ganti ke MCC/BACC jika itu lebih stabil di imbalance ekstrem\nBEST_THR = BEST_THR_F1\n\n# ----------------------------\n# 5) Save artifacts\n# ----------------------------\nout_json = OOF_DIR / \"threshold_tuning.json\"\nout_txt  = OOF_DIR / \"threshold_report.txt\"\nout_csv  = OOF_DIR / \"threshold_table_top500.csv\"\n\npayload = {\n    \"version\": \"v4.0\",\n    \"source\": src,\n    \"target_col\": TARGET_COL,\n    \"n_total_rows\": int(N_all),\n    \"n_valid_rows\": int(N),\n    \"pos_valid\": int(pos),\n    \"neg_valid\": int(neg),\n    \"oof_auc_valid_only\": float(auc_oof),\n    \"baseline_thr_0p5\": base05,\n    \"best_thr_f1\": best_f1_full,\n    \"best_thr_mcc\": best_mcc_full,\n    \"best_thr_balanced_accuracy\": best_bac_full,\n    \"best_thr_accuracy\": best_acc_full,\n    \"default_best_thr\": {\"metric\": \"f1\", \"thr\": float(BEST_THR)},\n}\n\nwith open(out_json, \"w\", encoding=\"utf-8\") as f:\n    json.dump(payload, f, indent=2)\n\nthr_table.sort_values([\"f1\",\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"], ascending=[False, False, False, False, False]).head(500).to_csv(out_csv, index=False)\ntop_f1 = thr_table.sort_values([\"f1\",\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"], ascending=[False, False, False, False, False]).head(10).reset_index(drop=True)\n\nlines = []\nlines.append(\"OOF Threshold Tuning Report (v4.0)\")\nlines.append(f\"- source={src}\")\nlines.append(f\"- target_col={TARGET_COL}\")\nlines.append(f\"- total_rows={N_all} | valid_rows={N} | pos_valid={pos} | neg_valid={neg} | pos%={pos/max(N,1)*100:.6f}%\")\nlines.append(f\"- OOF AUC (valid-only) = {auc_oof:.6f}\")\nlines.append(\"\")\nlines.append(\"Baseline @ thr=0.5\")\nlines.append(f\"- F1={base05['f1']:.6f} | P={base05['precision']:.6f} | R={base05['recall']:.6f} | \"\n             f\"ACC={base05['accuracy']:.6f} | BACC={base05['balanced_accuracy']:.6f} | MCC={base05['mcc']:.6f}\")\nlines.append(f\"- tp={base05['tp']} fp={base05['fp']} fn={base05['fn']} tn={base05['tn']} | pos_pred={base05['pos_pred']}\")\nlines.append(\"\")\nlines.append(f\"BEST-F1   @ thr={best_f1_full['thr']:.6f} | F1={best_f1_full['f1']:.6f} | P={best_f1_full['precision']:.6f} | R={best_f1_full['recall']:.6f} | pos_pred={best_f1_full['pos_pred']}\")\nlines.append(f\"BEST-MCC  @ thr={best_mcc_full['thr']:.6f} | MCC={best_mcc_full['mcc']:.6f} | F1={best_mcc_full['f1']:.6f} | BACC={best_mcc_full['balanced_accuracy']:.6f}\")\nlines.append(f\"BEST-BACC @ thr={best_bac_full['thr']:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f} | MCC={best_bac_full['mcc']:.6f} | F1={best_bac_full['f1']:.6f}\")\nlines.append(f\"BEST-ACC  @ thr={best_acc_full['thr']:.6f} | ACC={best_acc_full['accuracy']:.6f} | BACC={best_acc_full['balanced_accuracy']:.6f} | MCC={best_acc_full['mcc']:.6f}\")\nlines.append(\"\")\nlines.append(\"Top 10 by F1 (tie: MCC -> BACC -> Recall -> Precision):\")\nfor i in range(len(top_f1)):\n    r = top_f1.iloc[i]\n    lines.append(f\"{i+1:02d}. thr={float(r['thr']):.6f} | f1={float(r['f1']):.6f} | mcc={float(r['mcc']):.6f} | \"\n                 f\"P={float(r['precision']):.6f} | R={float(r['recall']):.6f} | \"\n                 f\"ACC={float(r['accuracy']):.6f} | BACC={float(r['balanced_accuracy']):.6f} | pos_pred={int(r['pos_pred'])}\")\n\nwith open(out_txt, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\nprint(\"[Stage 9] DONE\")\nprint(f\"- Saved: {out_json}\")\nprint(f\"- Saved: {out_txt}\")\nprint(f\"- Saved: {out_csv}\")\nprint(f\"- OOF AUC (valid-only): {auc_oof:.6f}\")\nprint(f\"- BEST_THR (default=F1) ={BEST_THR:.6f} | F1={best_f1_full['f1']:.6f} (P={best_f1_full['precision']:.6f} R={best_f1_full['recall']:.6f})\")\nprint(f\"- BEST_THR_MCC          ={BEST_THR_MCC:.6f} | MCC={best_mcc_full['mcc']:.6f}\")\nprint(f\"- BEST_THR_BACC         ={BEST_THR_BACC:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f}\")\nprint(f\"- BEST_THR_ACC          ={BEST_THR_ACC:.6f} | ACC={best_acc_full['accuracy']:.6f}\")\n\nglobals().update({\n    \"train_ids_oof_all\": train_ids,      # full ordering from OOF source\n    \"train_ids_oof_valid\": train_ids_v,  # only finite rows used for tuning\n    \"oof_prob_all\": oof_prob,\n    \"oof_prob_valid\": oof_prob_v,\n    \"y_oof_valid\": y_v,\n    \"BEST_THR\": float(BEST_THR),          # default tetap F1\n    \"BEST_THR_F1\": float(BEST_THR_F1),\n    \"BEST_THR_MCC\": float(BEST_THR_MCC),\n    \"BEST_THR_BACC\": float(BEST_THR_BACC),\n    \"BEST_THR_ACC\": float(BEST_THR_ACC),\n    \"thr_table\": thr_table,\n    \"THR_JSON_PATH\": out_json,\n    \"THR_REPORT_PATH\": out_txt,\n    \"THR_TABLE_CSV_PATH\": out_csv,\n    \"OOF_AUC_VALID_ONLY\": float(auc_oof),\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Inference (Fold Ensemble)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL, Kaggle CPU-SAFE)\n# REVISI FULL v4.0 (MATCH STAGE 8 EXACT + dtype_X from policy + meta_cols from ckpt)\n#\n# Upgrade utama vs v3.3:\n# - MATCH STAGE 8 global meta cols: pakai meta[\"global_meta_cols\"] dari ckpt (bukan hardcode)\n# - EBV_used: otomatis dibuat (prefer EBV_clip jika ada) bila diminta oleh global_meta_cols\n# - dtype_X memmap: baca dari Stage6 length_policy_config.json (support float16/float32)\n# - Agg seq feats: dibangun hanya jika diperlukan (g_dim > len(meta_cols))\n# - Band-id padding safety: selalu pastikan token PAD tidak bikin embedding out-of-range\n# ============================================================\n\nimport os, gc, json, re, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True.*\")\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\"ART_DIR\",\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_test_meta\",\"CKPT_DIR\",\"n_splits\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n\n# Torch\ntry:\n    import torch\n    import torch.nn as nn\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\ndevice = torch.device(\"cpu\")\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# Thread guard (CPU)\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\nFIX_DIR = Path(FIX_DIR)\nART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\nCKPT_DIR = Path(CKPT_DIR)\n\nOUT_DIR = ART_DIR / \"preds\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# helper: normalize id robustly\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_, bytearray)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _load_ids_npy(path: Path):\n    arr = np.load(path, allow_pickle=False)\n    xs = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n    return [_norm_id(z) for z in xs]\n\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\n# ----------------------------\n# 0b) Read Stage6 policy (SHIFT_BAND_IDS, PAD_BAND_ID, dtype_X)\n# ----------------------------\nSHIFT_BAND_IDS = False\nPAD_BAND_ID = 0\nDTYPE_X_MEMMAP = np.float32\n\npolicy_path = FIX_DIR / \"length_policy_config.json\"\nif policy_path.exists():\n    try:\n        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n            pol = json.load(f)\n        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n\n        dt = str(pol.get(\"dtype_X\", \"float32\")).lower()\n        if (\"float16\" in dt) or (\"fp16\" in dt):\n            DTYPE_X_MEMMAP = np.float16\n        else:\n            DTYPE_X_MEMMAP = np.float32\n    except Exception:\n        pass\n\n# ----------------------------\n# 1) Load TEST ordering (must match STAGE 6)\n# ----------------------------\ntest_ids_path = FIX_DIR / \"test_ids.npy\"\nif not test_ids_path.exists():\n    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n\ntest_ids = _load_ids_npy(test_ids_path)\nNTE = len(test_ids)\nif NTE <= 0:\n    raise RuntimeError(\"test_ids kosong (NTE=0). Pastikan STAGE 6 sukses membuat test_ids.npy.\")\n\n# Align df_test_meta.index via string-map (HARD)\ndf_test_meta = df_test_meta.copy(deep=False)\nmeta_ids = [_norm_id(z) for z in df_test_meta.index.tolist()]\ndf_test_meta.index = pd.Index(meta_ids, name=df_test_meta.index.name)\n\npos_idx = df_test_meta.index.get_indexer(test_ids)\nif (pos_idx < 0).any():\n    bad = [test_ids[i] for i in np.where(pos_idx < 0)[0][:10]]\n    raise KeyError(f\"Some test_ids not found in df_test_meta.index (string-mapped). ex={bad} | missing_n={int((pos_idx<0).sum())}\")\n\npos_idx = pos_idx.astype(np.int32)\n\nif len(set(test_ids)) != len(test_ids):\n    s = pd.Series(test_ids)\n    dup = s[s.duplicated()].head(10).tolist()\n    raise ValueError(f\"Duplicate object_id in test_ids ordering (examples): {dup}\")\n\n# ----------------------------\n# 2) Open fixed-length TEST memmaps (dtype from Stage6 policy)\n# ----------------------------\nSEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\nFdim = len(SEQ_FEATURE_NAMES)\nL = int(MAX_LEN)\n\ntest_X_path = FIX_DIR / \"test_X.dat\"\ntest_B_path = FIX_DIR / \"test_B.dat\"\ntest_M_path = FIX_DIR / \"test_M.dat\"\nfor p in [test_X_path, test_B_path, test_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\nXte = np.memmap(test_X_path, dtype=DTYPE_X_MEMMAP, mode=\"r\", shape=(NTE, L, Fdim))\nBte = np.memmap(test_B_path, dtype=np.int8,        mode=\"r\", shape=(NTE, L))\nMte = np.memmap(test_M_path, dtype=np.int8,        mode=\"r\", shape=(NTE, L))\n\nfeat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n\n# detect token mode (same as STAGE 8)\nif (\"mag\" in feat) and (\"mag_err_log\" in feat):\n    SEQ_TOKEN_MODE = \"mag\"\n    VAL_FEAT = \"mag\"\nelif (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n    SEQ_TOKEN_MODE = \"asinh\"\n    VAL_FEAT = \"flux_asinh\"\nelse:\n    raise RuntimeError(f\"Cannot infer token_mode from SEQ_FEATURE_NAMES. Found={SEQ_FEATURE_NAMES}\")\n\nfor k in [\"snr_tanh\",\"detected\"]:\n    if k not in feat:\n        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\nif VAL_FEAT not in feat:\n    raise RuntimeError(f\"Feature '{VAL_FEAT}' missing for token_mode={SEQ_TOKEN_MODE}.\")\n\n# ----------------------------\n# 3) Checkpoints (fold_*.pt)\n# ----------------------------\nn_splits = int(n_splits)\nckpts = []\nfor f in range(n_splits):\n    p = CKPT_DIR / f\"fold_{f}.pt\"\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n    ckpts.append(p)\n\n# ----------------------------\n# 4) Safe/compat checkpoint loader\n# ----------------------------\ndef torch_load_compat(path: Path):\n    try:\n        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n        if isinstance(obj, dict) and (\"model_state\" in obj or \"cfg\" in obj or \"global_scaler\" in obj):\n            return obj\n        return torch.load(path, map_location=\"cpu\", weights_only=False)\n    except TypeError:\n        return torch.load(path, map_location=\"cpu\")\n    except Exception:\n        return torch.load(path, map_location=\"cpu\", weights_only=False)\n\ndef extract_state_and_meta(ckpt_obj):\n    if isinstance(ckpt_obj, dict) and \"model_state\" in ckpt_obj and isinstance(ckpt_obj[\"model_state\"], dict):\n        return ckpt_obj[\"model_state\"], ckpt_obj\n    if isinstance(ckpt_obj, dict):\n        any_tensor = any(hasattr(v, \"shape\") for v in ckpt_obj.values())\n        if any_tensor:\n            return ckpt_obj, {}\n        return ckpt_obj, ckpt_obj\n    raise RuntimeError(f\"Unsupported ckpt object type: {type(ckpt_obj)}\")\n\n# ----------------------------\n# 5) Infer architecture from state_dict\n# ----------------------------\ndef infer_from_state(sd: dict):\n    keys = set(sd.keys())\n\n    if \"band_emb.weight\" not in sd:\n        raise RuntimeError(\"state_dict missing band_emb.weight.\")\n    n_bands = int(sd[\"band_emb.weight\"].shape[0])\n    d_model = int(sd[\"band_emb.weight\"].shape[1])\n\n    if \"pos_emb\" not in sd:\n        raise RuntimeError(\"state_dict missing pos_emb.\")\n    max_len_ckpt = int(sd[\"pos_emb\"].shape[1])\n\n    if \"x_proj.0.weight\" in keys:\n        feat_dim = int(sd[\"x_proj.0.weight\"].shape[1])\n    elif \"x_proj.weight\" in keys:\n        feat_dim = int(sd[\"x_proj.weight\"].shape[1])\n    else:\n        raise RuntimeError(\"state_dict missing x_proj.*.weight\")\n\n    if \"g_proj.0.weight\" in keys:\n        g_dim = int(sd[\"g_proj.0.weight\"].shape[1])\n        g_hidden = int(sd[\"g_proj.0.weight\"].shape[0])\n    else:\n        g_dim = 0\n        g_hidden = max(32, d_model // 2)\n\n    layer_ids = set()\n    for k in keys:\n        m = re.match(r\"encoder\\.layers\\.(\\d+)\\.\", k)\n        if m:\n            layer_ids.add(int(m.group(1)))\n    n_layers = (max(layer_ids) + 1) if layer_ids else 0\n    if n_layers <= 0:\n        raise RuntimeError(\"Cannot infer n_layers from state_dict (encoder.layers.* not found).\")\n\n    k_lin1 = \"encoder.layers.0.linear1.weight\"\n    if k_lin1 in sd:\n        dim_ff = int(sd[k_lin1].shape[0])\n    else:\n        lin1_keys = [k for k in keys if k.endswith(\"linear1.weight\")]\n        if not lin1_keys:\n            raise RuntimeError(\"Cannot infer dim_feedforward (linear1.weight not found).\")\n        dim_ff = int(sd[sorted(lin1_keys)[0]].shape[0])\n\n    has_pool_ln = (\"pool_ln.weight\" in keys and \"pool_ln.bias\" in keys)\n\n    head_w_idx = []\n    for k in keys:\n        m = re.match(r\"head\\.(\\d+)\\.weight\", k)\n        if m:\n            head_w_idx.append(int(m.group(1)))\n    if not head_w_idx:\n        raise RuntimeError(\"Cannot infer head structure (head.*.weight not found).\")\n    head_final_idx = max(sorted(set(head_w_idx)))\n\n    return {\n        \"n_bands\": n_bands,\n        \"d_model\": d_model,\n        \"max_len_ckpt\": max_len_ckpt,\n        \"feat_dim\": feat_dim,\n        \"g_dim\": g_dim,\n        \"g_hidden\": g_hidden,\n        \"n_layers\": n_layers,\n        \"dim_ff\": dim_ff,\n        \"has_pool_ln\": has_pool_ln,\n        \"head_final_idx\": head_final_idx,\n    }\n\n# ----------------------------\n# 6) Model that matches STAGE 8 forward\n# ----------------------------\nclass FlexMultibandEventTransformer(nn.Module):\n    def __init__(self, feat_dim, max_len, n_bands, d_model, n_heads, n_layers, dim_ff, dropout,\n                 g_dim, g_hidden, has_pool_ln=True, head_final_idx=3):\n        super().__init__()\n        self.n_bands = int(n_bands)\n        self.max_len = int(max_len)\n        self.d_model = int(d_model)\n\n        self.x_proj = nn.Sequential(\n            nn.Linear(int(feat_dim), int(d_model)),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n        )\n        self.band_emb = nn.Embedding(int(n_bands), int(d_model))\n\n        self.pos_emb = nn.Parameter(torch.zeros(1, int(max_len), int(d_model)))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=int(d_model),\n            nhead=int(n_heads),\n            dim_feedforward=int(dim_ff),\n            dropout=float(dropout),\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=int(n_layers))\n\n        self.attn = nn.Linear(int(d_model), 1)\n\n        self.has_pool_ln = bool(has_pool_ln)\n        if self.has_pool_ln:\n            self.pool_ln = nn.LayerNorm(int(d_model))\n\n        self.g_proj = nn.Sequential(\n            nn.Linear(int(g_dim), int(g_hidden)),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n        )\n\n        in_head = int(d_model + g_hidden)\n        if head_final_idx == 3:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.GELU(),\n                nn.Dropout(float(dropout)),\n                nn.Linear(int(d_model), 1),\n            )\n        elif head_final_idx == 2:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.GELU(),\n                nn.Linear(int(d_model), 1),\n            )\n        else:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.Linear(int(d_model), 1),\n            )\n\n    def forward(self, X, band_id, mask, G):\n        X = X.to(torch.float32)\n        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n        mask = mask.to(torch.long)\n\n        pad_mask = (mask == 0)  # True=pad\n        all_pad = pad_mask.all(dim=1)\n        if all_pad.any():\n            pad_mask = pad_mask.clone()\n            pad_mask[all_pad, 0] = False\n\n        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        a = self.attn(h).squeeze(-1)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)\n        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n\n        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n        denom = valid.sum(dim=1).clamp_min(1.0)\n        pooled_mean = (h * valid).sum(dim=1) / denom\n\n        pooled = 0.6 * pooled_attn + 0.4 * pooled_mean\n        if self.has_pool_ln:\n            pooled = self.pool_ln(pooled)\n\n        g = self.g_proj(G.to(torch.float32))\n        z = torch.cat([pooled, g], dim=1)\n        return self.head(z).squeeze(-1)  # logit\n\n# ----------------------------\n# 7) Determine meta cols + whether need agg (from FIRST ckpt meta)\n# ----------------------------\nfirst_obj = torch_load_compat(ckpts[0])\n_, first_meta = extract_state_and_meta(first_obj)\n\n# meta cols (match Stage8)\nDEFAULT_META_COLS = [\"Z\",\"Z_err\",\"EBV_used\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\nMETA_COLS = first_meta.get(\"global_meta_cols\", DEFAULT_META_COLS) if isinstance(first_meta, dict) else DEFAULT_META_COLS\nif not isinstance(META_COLS, (list, tuple)) or len(META_COLS) == 0:\n    META_COLS = DEFAULT_META_COLS\nMETA_COLS = [str(c) for c in META_COLS]\n\n# check if any fold needs agg (g_dim > len(META_COLS))\nneed_agg = False\narch_used = None\nfold_arch = []\nfor p in ckpts:\n    obj = torch_load_compat(p)\n    sd, meta = extract_state_and_meta(obj)\n    arch = infer_from_state(sd)\n    fold_arch.append(arch)\n    if arch_used is None:\n        arch_used = dict(arch)\n    if int(arch.get(\"g_dim\", 0)) > len(META_COLS):\n        need_agg = True\n\n# ----------------------------\n# 8) Build TEST global features (meta + optional agg seq feats) — MATCH STAGE 8\n# ----------------------------\nN_BANDS = 6\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1.0)\n\ndef _prepare_meta_cols(df):\n    # If EBV_used requested, create it (prefer EBV_clip if exists)\n    if \"EBV_used\" in META_COLS and \"EBV_used\" not in df.columns:\n        if (\"EBV_clip\" in df.columns) and (\"EBV\" in df.columns):\n            df[\"EBV_used\"] = df[\"EBV_clip\"]\n        elif \"EBV\" in df.columns:\n            df[\"EBV_used\"] = df[\"EBV\"]\n        elif \"EBV_clip\" in df.columns:\n            df[\"EBV_used\"] = df[\"EBV_clip\"]\n        else:\n            df[\"EBV_used\"] = 0.0\n\n    # ensure requested cols exist\n    for c in META_COLS:\n        if c not in df.columns:\n            df[c] = 0.0\n\n    Gm = df.iloc[pos_idx][META_COLS].copy()\n    for c in META_COLS:\n        Gm[c] = pd.to_numeric(Gm[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n    return Gm.to_numpy(dtype=np.float32, copy=False)\n\ndef build_agg_seq_features_memmap(Xmm, Bmm, Mmm, chunk=512):\n    snr_i = feat[\"snr_tanh\"]\n    det_i = feat[\"detected\"]\n    val_i = feat[VAL_FEAT]\n\n    out = np.zeros((NTE, 31), dtype=np.float32)\n\n    for start in range(0, NTE, int(chunk)):\n        end = min(NTE, start + int(chunk))\n        Xc = np.asarray(Xmm[start:end])  # (B,L,F) fp16/fp32\n        Bc = np.asarray(Bmm[start:end])  # (B,L)\n        Mc = np.asarray(Mmm[start:end])  # (B,L)\n\n        real = (Mc == 1)\n\n        # match Stage8: shift band ids only when SHIFT_BAND_IDS True\n        if SHIFT_BAND_IDS:\n            Bc2 = Bc.astype(np.int16, copy=True)\n            if real.any():\n                Bc2[real] = np.clip(Bc2[real] - 1, 0, N_BANDS - 1)\n            Bc2[~real] = 0\n            Bc = Bc2.astype(np.int8, copy=False)\n        else:\n            # safety: pad band forced to 0 so embedding safe\n            if real.any():\n                Bc = Bc.copy()\n                Bc[~real] = 0\n\n        tok_count = real.sum(axis=1).astype(np.float32)\n\n        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32, copy=False)\n        det = (Xc[:, :, det_i] > 0.5).astype(np.float32, copy=False)\n        val = Xc[:, :, val_i].astype(np.float32, copy=False)\n\n        snr_r = snr * real\n        det_r = det * real\n\n        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n        max_abs_snr = np.where(tok_count > 0, snr_r.max(axis=1), 0.0).astype(np.float32)\n\n        if SEQ_TOKEN_MODE == \"mag\":\n            val_r = np.where(real, val, np.nan)\n            mean_val = np.nanmean(val_r, axis=1).astype(np.float32)\n            std_val  = np.nanstd(val_r,  axis=1).astype(np.float32)\n            min_val  = np.nanmin(val_r,  axis=1).astype(np.float32)\n            mean_val = np.nan_to_num(mean_val, nan=0.0).astype(np.float32)\n            std_val  = np.nan_to_num(std_val,  nan=0.0).astype(np.float32)\n            min_val  = np.nan_to_num(min_val,  nan=0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n        else:\n            aval = np.abs(val).astype(np.float32, copy=False)\n            aval_r = aval * real\n            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n            val_r = np.where(real, val, np.nan)\n            std_val = np.nanstd(val_r, axis=1).astype(np.float32)\n            std_val = np.nan_to_num(std_val, nan=0.0).astype(np.float32)\n            max_aval = np.where(tok_count > 0, aval_r.max(axis=1), 0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n\n        per_band = []\n        for b in range(N_BANDS):\n            bm = (Bc == b) & real\n            cnt = bm.sum(axis=1).astype(np.float32)\n\n            detb = (det * bm).sum(axis=1).astype(np.float32)\n            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n\n            det_frac_b = _safe_div(detb, cnt)\n            mean_abs_snr_b = _safe_div(snrb, cnt)\n\n            if SEQ_TOKEN_MODE == \"mag\":\n                val_b = np.where(bm, val, np.nan)\n                mean_val_b = np.nanmean(val_b, axis=1).astype(np.float32)\n                mean_val_b = np.nan_to_num(mean_val_b, nan=0.0).astype(np.float32)\n            else:\n                aval_b = np.abs(val).astype(np.float32, copy=False) * bm\n                mean_val_b = _safe_div(aval_b.sum(axis=1).astype(np.float32), cnt).astype(np.float32)\n\n            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n\n        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n\n        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n\n        out[start:end] = agg\n\n        del Xc, Bc, Mc\n        if (start // int(chunk)) % 4 == 0:\n            gc.collect()\n\n    return out\n\nprint(f\"[Stage 10] token_mode={SEQ_TOKEN_MODE} | val_feat={VAL_FEAT} | X_dtype={DTYPE_X_MEMMAP}\")\nprint(f\"[Stage 10] SHIFT_BAND_IDS(from stage6)={SHIFT_BAND_IDS} | PAD_BAND_ID={PAD_BAND_ID}\")\nprint(f\"[Stage 10] META_COLS(from ckpt0)={META_COLS} | need_agg={need_agg}\")\n\n# META\nG_meta_np = _prepare_meta_cols(df_test_meta)\n\n# AGG (optional)\nif need_agg:\n    print(\"[Stage 10] Building AGG seq features for TEST (one-time)...\")\n    t0 = time.time()\n    G_seq_np = build_agg_seq_features_memmap(Xte, Bte, Mte, chunk=512)\n    print(f\"[Stage 10] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\n    G_raw_default = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)  # meta + 31\n    agg_dim = 31\nelse:\n    G_seq_np = None\n    G_raw_default = G_meta_np.astype(np.float32, copy=False)\n    agg_dim = 0\n\n# ----------------------------\n# 9) Inference per fold (logit ensemble)\n# ----------------------------\n@torch.no_grad()\ndef predict_logits_batchwise(model, Xmm, Bmm, Mmm, G_raw, mean=None, std=None, batch_size=64):\n    model.eval()\n    out = np.zeros((Xmm.shape[0],), dtype=np.float32)\n    N0 = int(Xmm.shape[0])\n\n    for i in range(0, N0, int(batch_size)):\n        j = min(N0, i + int(batch_size))\n        Xb_np = np.asarray(Xmm[i:j]).astype(np.float32, copy=False)\n        Bb_np = np.asarray(Bmm[i:j])\n        Mb_np = np.asarray(Mmm[i:j])\n\n        real = (Mb_np == 1)\n\n        # band id mapping matches Stage8 dataset\n        if SHIFT_BAND_IDS:\n            Bb_np = Bb_np.astype(np.int16, copy=True)\n            if real.any():\n                Bb_np[real] = np.clip(Bb_np[real] - 1, 0, N_BANDS - 1)\n            Bb_np[~real] = 0\n            Bb_np = Bb_np.astype(np.int64, copy=False)\n        else:\n            # safety: pad band forced to 0\n            Bb_np = Bb_np.copy()\n            Bb_np[~real] = 0\n            Bb_np = Bb_np.astype(np.int64, copy=False)\n\n        Xb = torch.from_numpy(Xb_np)\n        Bb = torch.from_numpy(Bb_np)\n        Mb = torch.from_numpy(Mb_np.astype(np.int64, copy=False))\n\n        Gb_np = G_raw[i:j]\n        if mean is not None and std is not None:\n            Gb_np = ((Gb_np - mean) / std).astype(np.float32, copy=False)\n        Gb = torch.from_numpy(Gb_np.astype(np.float32, copy=False))\n\n        logit = model(Xb.to(device), Bb.to(device), Mb.to(device), Gb.to(device))\n        out[i:j] = logit.detach().cpu().numpy().astype(np.float32, copy=False)\n\n    return out\n\nBATCH_SIZE = 64\ntest_logit_folds = np.zeros((NTE, n_splits), dtype=np.float32)\n\nprint(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} | ensemble=mean_logits_then_sigmoid\")\n\nfor fold, ckpt_path in enumerate(ckpts):\n    ckpt_obj = torch_load_compat(ckpt_path)\n    sd, meta = extract_state_and_meta(ckpt_obj)\n\n    arch = fold_arch[fold]  # already inferred\n    cfg = meta.get(\"cfg\", {}) if isinstance(meta, dict) else {}\n    dropout = float(cfg.get(\"dropout\", 0.0)) if isinstance(cfg, dict) else 0.0\n\n    # n_heads: prefer ckpt cfg; fallback safe\n    n_heads = int(cfg.get(\"n_heads\", 4)) if isinstance(cfg, dict) else 4\n    if n_heads <= 0:\n        n_heads = 4\n    if (arch[\"d_model\"] % n_heads) != 0:\n        for h in [4, 8, 2, 1, 16, 32]:\n            if h > 0 and (arch[\"d_model\"] % h) == 0:\n                n_heads = h\n                break\n        if (arch[\"d_model\"] % n_heads) != 0:\n            raise RuntimeError(f\"Fold {fold}: cannot choose valid n_heads for d_model={arch['d_model']}\")\n\n    # HARD checks\n    if arch[\"feat_dim\"] != Fdim:\n        raise RuntimeError(\n            f\"Fold {fold}: feature_dim mismatch.\\n\"\n            f\"- ckpt expects feat_dim={arch['feat_dim']}\\n\"\n            f\"- memmap has Fdim={Fdim}\\n\"\n            \"Solusi: pastikan STAGE 6 feature list sama saat training ckpt dibuat.\"\n        )\n    if arch[\"max_len_ckpt\"] != L:\n        raise RuntimeError(\n            f\"Fold {fold}: max_len mismatch.\\n\"\n            f\"- ckpt max_len={arch['max_len_ckpt']}\\n\"\n            f\"- memmap MAX_LEN={L}\\n\"\n        )\n\n    # Decide G_raw to match ckpt g_dim\n    g_dim = int(arch[\"g_dim\"])\n    if g_dim <= 0:\n        G_raw = np.zeros((NTE, 0), dtype=np.float32)\n        g_mean = None\n        g_std = None\n    else:\n        if G_raw_default.shape[1] == g_dim:\n            G_raw = G_raw_default\n        elif G_raw_default.shape[1] > g_dim:\n            G_raw = G_raw_default[:, :g_dim]\n        else:\n            pad = np.zeros((NTE, g_dim - G_raw_default.shape[1]), dtype=np.float32)\n            G_raw = np.concatenate([G_raw_default, pad], axis=1).astype(np.float32, copy=False)\n\n        scaler = meta.get(\"global_scaler\", None) if isinstance(meta, dict) else None\n        if scaler is not None and isinstance(scaler, dict) and (\"mean\" in scaler) and (\"std\" in scaler):\n            g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32).reshape(-1)\n            g_std  = np.asarray(scaler[\"std\"],  dtype=np.float32).reshape(-1)\n            if g_mean.shape[0] != g_dim or g_std.shape[0] != g_dim:\n                g_mean = None; g_std = None\n            else:\n                g_std = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n        else:\n            g_mean = None; g_std = None\n\n    model = FlexMultibandEventTransformer(\n        feat_dim=arch[\"feat_dim\"],\n        max_len=arch[\"max_len_ckpt\"],\n        n_bands=arch[\"n_bands\"],\n        d_model=arch[\"d_model\"],\n        n_heads=n_heads,\n        n_layers=arch[\"n_layers\"],\n        dim_ff=arch[\"dim_ff\"],\n        dropout=dropout,\n        g_dim=g_dim,\n        g_hidden=arch[\"g_hidden\"],\n        has_pool_ln=arch[\"has_pool_ln\"],\n        head_final_idx=arch[\"head_final_idx\"],\n    ).to(device)\n\n    model.load_state_dict(sd, strict=True)\n\n    logits = predict_logits_batchwise(\n        model, Xte, Bte, Mte, G_raw, mean=g_mean, std=g_std, batch_size=BATCH_SIZE\n    )\n    test_logit_folds[:, fold] = logits\n\n    probs_tmp = sigmoid_np(logits)\n    print(f\"  fold {fold}: d_model={arch['d_model']} n_heads={n_heads} g_dim={g_dim} | \"\n          f\"logit_mean={float(logits.mean()):.6f} | prob_mean={float(probs_tmp.mean()):.6f} | prob_std={float(probs_tmp.std()):.6f}\")\n\n    del model, logits, probs_tmp\n    gc.collect()\n\n# ensemble on logits\ntest_logit_ens = test_logit_folds.mean(axis=1).astype(np.float32)\ntest_prob_folds = sigmoid_np(test_logit_folds).astype(np.float32)\ntest_prob_ens   = sigmoid_np(test_logit_ens).astype(np.float32)\n\n# ----------------------------\n# 10) Save artifacts\n# ----------------------------\nlogit_fold_path = OUT_DIR / \"test_logit_folds.npy\"\nlogit_ens_path  = OUT_DIR / \"test_logit_ens.npy\"\nprob_fold_path  = OUT_DIR / \"test_prob_folds.npy\"\nprob_ens_path   = OUT_DIR / \"test_prob_ens.npy\"\ncsv_path        = OUT_DIR / \"test_prob_ens.csv\"\ncfg_path        = OUT_DIR / \"test_infer_config.json\"\n\nnp.save(logit_fold_path, test_logit_folds)\nnp.save(logit_ens_path,  test_logit_ens)\nnp.save(prob_fold_path,  test_prob_folds)\nnp.save(prob_ens_path,   test_prob_ens)\n\npd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens}).to_csv(csv_path, index=False)\n\ninfer_cfg = {\n    \"seed\": int(SEED),\n    \"n_splits\": int(n_splits),\n    \"ensemble\": \"mean_logits_then_sigmoid\",\n    \"batch_size\": int(BATCH_SIZE),\n    \"max_len\": int(L),\n    \"feature_dim\": int(Fdim),\n    \"token_mode\": SEQ_TOKEN_MODE,\n    \"val_feat\": VAL_FEAT,\n    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n    \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n    \"global_meta_cols\": META_COLS,\n    \"need_agg_seq\": bool(need_agg),\n    \"global_agg_dim\": int(agg_dim),\n    \"global_default_dim\": int(G_raw_default.shape[1]),\n    \"ckpt_dir\": str(CKPT_DIR),\n    \"ckpts\": [str(p) for p in ckpts],\n    \"arch_inferred_from_first_fold\": arch_used,\n    \"outputs\": {\n        \"test_logit_folds\": str(logit_fold_path),\n        \"test_logit_ens\": str(logit_ens_path),\n        \"test_prob_folds\": str(prob_fold_path),\n        \"test_prob_ens\": str(prob_ens_path),\n        \"test_prob_ens_csv\": str(csv_path),\n    }\n}\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(infer_cfg, f, indent=2)\n\nprint(\"\\n[Stage 10] DONE\")\nprint(f\"- Saved logits folds: {logit_fold_path}\")\nprint(f\"- Saved logits ens  : {logit_ens_path}\")\nprint(f\"- Saved probs folds : {prob_fold_path}\")\nprint(f\"- Saved probs ens   : {prob_ens_path}\")\nprint(f\"- Saved csv         : {csv_path}\")\nprint(f\"- Saved config      : {cfg_path}\")\nprint(f\"- ens prob mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | \"\n      f\"min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n\nglobals().update({\n    \"test_ids\": test_ids,\n    \"test_logit_folds\": test_logit_folds,\n    \"test_logit_ens\": test_logit_ens,\n    \"test_prob_folds\": test_prob_folds,\n    \"test_prob_ens\": test_prob_ens,\n    \"TEST_LOGIT_FOLDS_PATH\": logit_fold_path,\n    \"TEST_LOGIT_ENS_PATH\": logit_ens_path,\n    \"TEST_PROB_FOLDS_PATH\": prob_fold_path,\n    \"TEST_PROB_ENS_PATH\": prob_ens_path,\n    \"TEST_PROB_CSV_PATH\": csv_path,\n    \"TEST_INFER_CFG_PATH\": cfg_path,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation ","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ONE CELL — EVALUATION (Precision / Recall / F1) + Threshold Sweep (OOF)\n# REVISI FULL v3.2 (FIX >=thr exact + faster align + add BACC/MCC + better tie-break)\n#\n# Input minimal:\n# - df_train_meta (index: object_id)\n# - oof_prob (globals) OR OOF_DIR/oof_prob.npy OR OOF_DIR/oof_prob.csv\n#\n# Output:\n# - Print ringkasan metrik\n# - Save: eval_report.txt + eval_threshold_table.csv + eval_threshold_table_top500.csv + eval_summary.json\n# - Export globals: BEST_THR_F1, BEST_THR_F05, BEST_THR_F2, thr_table_eval\n# ============================================================\n\nimport gc, json, math, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require minimal\n# ----------------------------\nif \"df_train_meta\" not in globals():\n    raise RuntimeError(\"Missing df_train_meta. Jalankan stage meta dulu.\")\n\nART_DIR = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\nOOF_DIR = Path(globals().get(\"OOF_DIR\", ART_DIR / \"oof\"))\nOOF_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Utils: id normalize + robust 1D float32\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\ndef _sanitize_prob(p):\n    p = np.asarray(p, dtype=np.float32)\n    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n    p = np.clip(p, 0.0, 1.0)\n    return p.astype(np.float32)\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1e-12)\n\n# ----------------------------\n# 0a) Normalize meta index\n# ----------------------------\ndf_train_meta = df_train_meta.copy(deep=False)\ndf_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\n\n# ----------------------------\n# 0b) Detect target column (robust)\n# ----------------------------\ndef _detect_target_col(df):\n    for cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\nTARGET_COL = _detect_target_col(df_train_meta)\nif TARGET_COL is None:\n    raise RuntimeError(\n        \"Cannot detect target column in df_train_meta. \"\n        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n    )\n\n# Build robust target map (handles duplicate index safely)\n# Policy: binary target = (numeric > 0); if dup ids exist -> take max (OR)\ny_series = pd.to_numeric(df_train_meta[TARGET_COL], errors=\"coerce\").fillna(0.0)\ny_bin = (y_series.to_numpy(dtype=np.float32) > 0).astype(np.int8)\ny_map = pd.Series(y_bin, index=df_train_meta.index)\nif y_map.index.has_duplicates:\n    y_map = y_map.groupby(level=0).max()\n\n# ----------------------------\n# 1) Load oof_prob (prefer csv for safest alignment)\n# ----------------------------\ndef load_oof():\n    pcsv = OOF_DIR / \"oof_prob.csv\"\n    if pcsv.exists():\n        df = pd.read_csv(pcsv)\n        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n            df = df[[\"object_id\", \"oof_prob\"]].copy()\n            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n            p = _sanitize_prob(_as_1d_float32(df[\"oof_prob\"].to_numpy()))\n            if len(p) != len(df):\n                raise RuntimeError(\"oof_prob.csv: length mismatch after parsing.\")\n            df[\"oof_prob\"] = p\n            return p, df, \"csv(oof_prob.csv)\"\n\n    if \"oof_prob\" in globals():\n        p = _as_1d_float32(globals()[\"oof_prob\"])\n        if isinstance(p, np.ndarray) and p.ndim != 0:\n            return _sanitize_prob(p), None, \"globals(oof_prob)\"\n\n    pnpy = OOF_DIR / \"oof_prob.npy\"\n    if pnpy.exists():\n        p = _sanitize_prob(_as_1d_float32(np.load(pnpy, allow_pickle=False)))\n        return p, None, \"npy(oof_prob.npy)\"\n\n    raise FileNotFoundError(\"OOF prob tidak ditemukan (oof_prob.csv / globals oof_prob / oof_prob.npy).\")\n\noof_prob, df_oof_csv, oof_src = load_oof()\nif not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n    raise TypeError(f\"Invalid oof_prob (scalar/unsized). type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}\")\n\n# ----------------------------\n# 2) Align y (target) to OOF order (super robust)\n# ----------------------------\ntrain_ids = None\n\nif df_oof_csv is not None:\n    # Handle duplicate ids in oof: mean per object_id but keep first-seen order\n    ids_first = pd.unique(df_oof_csv[\"object_id\"].to_numpy())\n    if len(ids_first) != len(df_oof_csv):\n        df_mean = df_oof_csv.groupby(\"object_id\", as_index=True)[\"oof_prob\"].mean()\n        df_oof_csv = pd.DataFrame({\"object_id\": ids_first})\n        df_oof_csv[\"oof_prob\"] = df_mean.reindex(ids_first).to_numpy(dtype=np.float32)\n        oof_prob = _sanitize_prob(df_oof_csv[\"oof_prob\"].to_numpy())\n\n    train_ids = df_oof_csv[\"object_id\"].tolist()\n\n    # Drop ids not in meta target map (vectorized, faster)\n    ok = pd.Index(train_ids).isin(y_map.index).to_numpy()\n    if not ok.all():\n        bad = [train_ids[i] for i in np.where(~ok)[0][:10]]\n        print(f\"[WARN] oof_prob.csv has ids not in df_train_meta: missing_n={int((~ok).sum())} examples={bad}\")\n        df_oof_csv = df_oof_csv.loc[ok].reset_index(drop=True)\n        train_ids = df_oof_csv[\"object_id\"].tolist()\n        oof_prob = _sanitize_prob(df_oof_csv[\"oof_prob\"].to_numpy())\n\n    y = y_map.reindex(train_ids).to_numpy(dtype=np.int8, copy=True)\n\nelif \"train_ids_ordered\" in globals():\n    ids = [_norm_id(z) for z in list(globals()[\"train_ids_ordered\"])]\n    if len(ids) == len(oof_prob):\n        missing = [oid for oid in ids if oid not in y_map.index]\n        if missing:\n            raise KeyError(f\"train_ids_ordered contains ids not in df_train_meta. ex={missing[:10]} missing_n={len(missing)}\")\n        train_ids = ids\n        y = y_map.reindex(train_ids).to_numpy(dtype=np.int8, copy=True)\n    else:\n        raise RuntimeError(\"train_ids_ordered length mismatch with oof_prob. Gunakan oof_prob.csv agar alignment aman.\")\n\nelse:\n    # fallback only if same length and meta index unique\n    if len(oof_prob) != len(df_train_meta):\n        raise RuntimeError(\n            f\"Tidak bisa align y. len(oof_prob)={len(oof_prob)} != len(df_train_meta)={len(df_train_meta)} \"\n            \"dan tidak ada oof_prob.csv (object_id) atau train_ids_ordered.\"\n        )\n    if df_train_meta.index.has_duplicates:\n        raise RuntimeError(\n            \"df_train_meta.index has duplicates, tapi oof source tidak punya object_id ordering (csv/train_ids_ordered). \"\n            \"Solusi: simpan oof_prob.csv dari Stage 8 (object_id + oof_prob) atau sediakan train_ids_ordered.\"\n        )\n    train_ids = df_train_meta.index.astype(str).tolist()\n    y = y_map.reindex(train_ids).to_numpy(dtype=np.int8, copy=True)\n\nif len(y) != len(oof_prob):\n    raise RuntimeError(f\"Length mismatch: y={len(y)} vs oof_prob={len(oof_prob)}\")\n\nuy = set(np.unique(y).tolist())\nif not uy.issubset({0, 1}):\n    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n\nN = int(len(y))\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\n\nprint(f\"[Eval] OOF source={oof_src} | target_col={TARGET_COL}\")\nprint(f\"[Eval] N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}%\")\n\n# ----------------------------\n# 3) Optional ranking metrics (AUCs)\n# ----------------------------\nroc_auc = None\npr_auc = None\ntry:\n    from sklearn.metrics import roc_auc_score, average_precision_score\n    if (y.max() == 1) and (y.min() == 0):\n        roc_auc = float(roc_auc_score(y, oof_prob))\n        pr_auc  = float(average_precision_score(y, oof_prob))\nexcept Exception:\n    pass\n\n# ----------------------------\n# 4) Threshold candidates (grid + quantiles + sampled uniques)\n# ----------------------------\ngrid = np.concatenate([\n    np.linspace(0.00, 0.10, 41),\n    np.linspace(0.10, 0.90, 161),\n    np.linspace(0.90, 1.00, 41),\n]).astype(np.float32)\n\nqs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\nquant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n\nuniq = np.unique(oof_prob)\nif len(uniq) > 5000:\n    take = np.linspace(0, len(uniq) - 1, 5000, dtype=int)\n    uniq = uniq[take].astype(np.float32)\n\nthr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr, uniq]), 0.0, 1.0)).astype(np.float32)\n\n# ----------------------------\n# 5) FAST sweep via sorting + cumulative counts\n#    Predict positive if oof_prob >= thr\n#    FIX IMPORTANT: use side=\"right\" so equality is included (>=)\n# ----------------------------\nord_desc = np.argsort(-oof_prob)\np_sorted = oof_prob[ord_desc]\ny_sorted = y[ord_desc].astype(np.int8)\n\npos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\nneg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\n\npos_total = int(pos_prefix[-1]) if N > 0 else 0\nneg_total = int(neg_prefix[-1]) if N > 0 else 0\n\n# k(thr) = number of items with prob >= thr\nk = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"right\").astype(np.int64)\n\ntp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\nfp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\nfn = (pos_total - tp).astype(np.int64)\ntn = (neg_total - fp).astype(np.int64)\n\nprec = _safe_div(tp, tp + fp)\nrec  = _safe_div(tp, tp + fn)\nf1   = _safe_div(2 * prec * rec, prec + rec)\n\ndef fbeta_vec(prec, rec, beta):\n    b2 = beta * beta\n    return _safe_div((1.0 + b2) * prec * rec, b2 * prec + rec)\n\nf05 = fbeta_vec(prec, rec, beta=0.5)\nf2  = fbeta_vec(prec, rec, beta=2.0)\n\nacc  = _safe_div(tp + tn, tp + fp + fn + tn)\ntpr  = _safe_div(tp, tp + fn)\ntnr  = _safe_div(tn, tn + fp)\nbacc = 0.5 * (tpr + tnr)\n\nnum = tp * tn - fp * fn\nden = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\nmcc = np.where(den > 0, num / np.sqrt(den.astype(np.float64)), 0.0).astype(np.float32)\n\npos_pred = k.astype(np.int64)\n\nthr_table = pd.DataFrame({\n    \"thr\": thr_candidates.astype(np.float32),\n    \"f1\": f1.astype(np.float32),\n    \"f0.5\": f05.astype(np.float32),\n    \"f2\": f2.astype(np.float32),\n    \"precision\": prec.astype(np.float32),\n    \"recall\": rec.astype(np.float32),\n    \"acc\": acc.astype(np.float32),\n    \"bacc\": bacc.astype(np.float32),\n    \"mcc\": mcc.astype(np.float32),\n    \"tp\": tp.astype(np.int64),\n    \"fp\": fp.astype(np.int64),\n    \"fn\": fn.astype(np.int64),\n    \"tn\": tn.astype(np.int64),\n    \"pos_pred\": pos_pred.astype(np.int64),\n})\n\ndef _eval_at(thr):\n    thr = float(thr)\n    k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"right\"))  # FIX >=\n    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n    fn0 = int(pos_total - tp0)\n    tn0 = int(neg_total - fp0)\n\n    p0 = tp0 / max(tp0 + fp0, 1)\n    r0 = tp0 / max(tp0 + fn0, 1)\n    f10 = 0.0 if (tp0 == 0 or (p0 + r0) == 0) else (2 * p0 * r0 / (p0 + r0))\n    f05_0 = 0.0 if (0.25 * p0 + r0) == 0 else ((1.25) * p0 * r0 / (0.25 * p0 + r0))\n    f2_0  = 0.0 if (4.0 * p0 + r0) == 0 else ((5.0) * p0 * r0 / (4.0 * p0 + r0))\n\n    acc0  = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n\n    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n\n    return {\n        \"thr\": thr, \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0, \"pos_pred\": k0,\n        \"precision\": float(p0), \"recall\": float(r0), \"f1\": float(f10),\n        \"f0.5\": float(f05_0), \"f2\": float(f2_0),\n        \"acc\": float(acc0), \"bacc\": float(bacc0), \"mcc\": float(mcc0),\n    }\n\nbase = _eval_at(0.5)\n\n# ----------------------------\n# 6) Pick best thresholds (with tie-breakers)\n# ----------------------------\ndef _pick_best(df, primary, tie_cols):\n    sort_cols = [primary] + tie_cols\n    asc = [False] * len(sort_cols)\n    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n\nbest_f1_row  = _pick_best(thr_table, \"f1\",   [\"bacc\", \"mcc\", \"recall\", \"precision\", \"acc\"])\nbest_f05_row = _pick_best(thr_table, \"f0.5\", [\"precision\", \"mcc\", \"f1\", \"acc\"])\nbest_f2_row  = _pick_best(thr_table, \"f2\",   [\"recall\", \"mcc\", \"f1\", \"bacc\", \"acc\"])\n\nBEST_THR_F1  = float(best_f1_row[\"thr\"])\nBEST_THR_F05 = float(best_f05_row[\"thr\"])\nBEST_THR_F2  = float(best_f2_row[\"thr\"])\n\nbest_f1  = _eval_at(BEST_THR_F1)\nbest_f05 = _eval_at(BEST_THR_F05)\nbest_f2  = _eval_at(BEST_THR_F2)\n\n# sort table for viewing\nthr_table_sorted = thr_table.sort_values([\"f1\", \"bacc\", \"mcc\", \"recall\", \"precision\"], ascending=[False, False, False, False, False]).reset_index(drop=True)\n\n# ----------------------------\n# 7) Print report\n# ----------------------------\nprint(\"\\nEVALUATION (OOF) — Precision/Recall/F1 (+BACC/MCC)\")\nif roc_auc is not None:\n    print(f\"- ROC-AUC={roc_auc:.6f} | PR-AUC={pr_auc:.6f}\")\n\nprint(\"\\nBaseline @ thr=0.5\")\nprint(f\"- F1={base['f1']:.6f} | F0.5={base['f0.5']:.6f} | F2={base['f2']:.6f} | P={base['precision']:.6f} | R={base['recall']:.6f} | ACC={base['acc']:.6f} | BACC={base['bacc']:.6f} | MCC={base['mcc']:.6f}\")\nprint(f\"  tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\")\n\nprint(f\"\\nBEST-F1  @ thr={BEST_THR_F1:.6f}\")\nprint(f\"- F1={best_f1['f1']:.6f} | P={best_f1['precision']:.6f} | R={best_f1['recall']:.6f} | ACC={best_f1['acc']:.6f} | BACC={best_f1['bacc']:.6f} | MCC={best_f1['mcc']:.6f}\")\nprint(f\"  tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\")\n\nprint(f\"\\nBEST-F0.5 @ thr={BEST_THR_F05:.6f} (precision-leaning)\")\nprint(f\"- F0.5={best_f05['f0.5']:.6f} | P={best_f05['precision']:.6f} | R={best_f05['recall']:.6f} | F1={best_f05['f1']:.6f} | MCC={best_f05['mcc']:.6f}\")\n\nprint(f\"\\nBEST-F2   @ thr={BEST_THR_F2:.6f} (recall-leaning)\")\nprint(f\"- F2={best_f2['f2']:.6f} | P={best_f2['precision']:.6f} | R={best_f2['recall']:.6f} | F1={best_f2['f1']:.6f} | MCC={best_f2['mcc']:.6f}\")\n\nprint(\"\\nTop 10 thresholds by F1:\")\nfor i in range(min(10, len(thr_table_sorted))):\n    r = thr_table_sorted.iloc[i]\n    print(f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | f0.5={r['f0.5']:.6f} | f2={r['f2']:.6f} | \"\n          f\"P={r['precision']:.6f} R={r['recall']:.6f} | bacc={r['bacc']:.6f} mcc={r['mcc']:.6f} | \"\n          f\"tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n\n# ----------------------------\n# 8) Save artifacts\n# ----------------------------\nout_txt   = OOF_DIR / \"eval_report.txt\"\nout_csv   = OOF_DIR / \"eval_threshold_table.csv\"\nout_csv_t = OOF_DIR / \"eval_threshold_table_top500.csv\"\nout_json  = OOF_DIR / \"eval_summary.json\"\n\nlines = []\nlines.append(\"OOF Evaluation Report (Precision/Recall/F1 + BACC/MCC)\")\nlines.append(f\"source={oof_src} | target_col={TARGET_COL}\")\nlines.append(f\"N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.10f}%\")\nif roc_auc is not None:\n    lines.append(f\"ROC-AUC={roc_auc:.10f} | PR-AUC={pr_auc:.10f}\")\nlines.append(\"\")\nlines.append(\"Baseline @ thr=0.5\")\nlines.append(f\"F1={base['f1']:.10f} | F0.5={base['f0.5']:.10f} | F2={base['f2']:.10f} | P={base['precision']:.10f} | R={base['recall']:.10f} | ACC={base['acc']:.10f} | BACC={base['bacc']:.10f} | MCC={base['mcc']:.10f}\")\nlines.append(f\"tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\")\nlines.append(\"\")\nlines.append(f\"BEST-F1 @ thr={BEST_THR_F1:.10f}\")\nlines.append(f\"F1={best_f1['f1']:.10f} | P={best_f1['precision']:.10f} | R={best_f1['recall']:.10f} | ACC={best_f1['acc']:.10f} | BACC={best_f1['bacc']:.10f} | MCC={best_f1['mcc']:.10f}\")\nlines.append(f\"tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\")\nlines.append(\"\")\nlines.append(f\"BEST-F0.5 @ thr={BEST_THR_F05:.10f}\")\nlines.append(f\"F0.5={best_f05['f0.5']:.10f} | P={best_f05['precision']:.10f} | R={best_f05['recall']:.10f} | F1={best_f05['f1']:.10f} | MCC={best_f05['mcc']:.10f}\")\nlines.append(\"\")\nlines.append(f\"BEST-F2 @ thr={BEST_THR_F2:.10f}\")\nlines.append(f\"F2={best_f2['f2']:.10f} | P={best_f2['precision']:.10f} | R={best_f2['recall']:.10f} | F1={best_f2['f1']:.10f} | MCC={best_f2['mcc']:.10f}\")\nlines.append(\"\")\nlines.append(\"Top 10 thresholds by F1:\")\nfor i in range(min(10, len(thr_table_sorted))):\n    r = thr_table_sorted.iloc[i]\n    lines.append(\n        f\"{i+1:02d}. thr={float(r['thr']):.10f} | f1={float(r['f1']):.10f} | f0.5={float(r['f0.5']):.10f} | f2={float(r['f2']):.10f} | \"\n        f\"P={float(r['precision']):.10f} R={float(r['recall']):.10f} | bacc={float(r['bacc']):.10f} mcc={float(r['mcc']):.10f} | \"\n        f\"tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\"\n    )\n\nwith open(out_txt, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\nthr_table_sorted.to_csv(out_csv, index=False)\nthr_table_sorted.head(500).to_csv(out_csv_t, index=False)\n\npayload = {\n    \"source\": oof_src,\n    \"target_col\": TARGET_COL,\n    \"N\": N, \"pos\": pos, \"neg\": neg,\n    \"roc_auc\": roc_auc, \"pr_auc\": pr_auc,\n    \"baseline_thr_0p5\": base,\n    \"best_f1\": best_f1,\n    \"best_f0.5\": best_f05,\n    \"best_f2\": best_f2,\n    \"paths\": {\"report\": str(out_txt), \"table\": str(out_csv), \"table_top500\": str(out_csv_t), \"summary\": str(out_json)},\n}\nwith open(out_json, \"w\", encoding=\"utf-8\") as f:\n    json.dump(payload, f, indent=2)\n\nprint(\"\\nSaved:\")\nprint(f\"- {out_txt}\")\nprint(f\"- {out_csv}\")\nprint(f\"- {out_csv_t}\")\nprint(f\"- {out_json}\")\n\nglobals().update({\n    \"BEST_THR_F1\": BEST_THR_F1,\n    \"BEST_THR_F05\": BEST_THR_F05,\n    \"BEST_THR_F2\": BEST_THR_F2,\n    \"thr_table_eval\": thr_table_sorted,\n    \"EVAL_REPORT_PATH\": out_txt,\n    \"EVAL_TABLE_PATH\": out_csv,\n    \"EVAL_TABLE_TOP500_PATH\": out_csv_t,\n    \"EVAL_SUMMARY_PATH\": out_json,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission Build","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 11 — Submission Build (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL v3.3\n#\n# Upgrade v3.3:\n# - Anti mismatch object_id: read_csv dtype object_id=str (sample + pred)\n# - CSV pred prob auto-detect (prob/prediction/kolom float tunggal)\n# - Threshold fallback tambah: THR_JSON_PATH / EVAL_SUMMARY_PATH jika ada\n# - Diagnostic lebih jelas (pred extra/missing vs sample)\n#\n# Output:\n# - /kaggle/working/submission.csv\n# - SUB_DIR/submission.csv (copy)\n# ============================================================\n\nimport gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 0 globals\n# ----------------------------\nfor need in [\"PATHS\", \"SUB_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n\nsample_path = Path(PATHS[\"SAMPLE_SUB\"])\nif not sample_path.exists():\n    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n\n# IMPORTANT: dtype object_id=str to prevent ID corruption\ndf_sub = pd.read_csv(sample_path, dtype={\"object_id\": str})\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\ndef _sanitize_prob(p):\n    p = np.asarray(p, dtype=np.float32)\n    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n    p = np.clip(p, 0.0, 1.0)\n    return p.astype(np.float32)\n\ndef _load_ids_npy(path: Path):\n    arr = np.load(path, allow_pickle=False)\n    ids = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n    return [_norm_id(x) for x in ids]\n\ndef _try_load_json(p):\n    try:\n        p = Path(p)\n        if not p.exists():\n            return None\n        with open(p, \"r\", encoding=\"utf-8\") as f:\n            obj = json.load(f)\n        return obj if isinstance(obj, dict) else None\n    except Exception:\n        return None\n\ndef _try_load_stage10_cfg():\n    p = globals().get(\"TEST_INFER_CFG_PATH\", None)\n    if p is None:\n        return None\n    return _try_load_json(p)\n\ndef _detect_prob_col(df):\n    # Prefer explicit names\n    for c in [\"prob\", \"prediction\", \"oof_prob\", \"pred\", \"proba\"]:\n        if c in df.columns:\n            return c\n\n    # else: pick a single float-like column besides object_id\n    cand = [c for c in df.columns if c != \"object_id\"]\n    floatish = []\n    for c in cand:\n        s = pd.to_numeric(df[c], errors=\"coerce\")\n        if s.notna().mean() > 0.95:  # mostly numeric\n            floatish.append(c)\n    if len(floatish) == 1:\n        return floatish[0]\n\n    return None\n\ndef _load_pred_df():\n    \"\"\"\n    Return (df_pred, src_str) where df_pred has columns: object_id, prob\n    Priority:\n      A) globals: test_ids + test_prob_ens\n      B) STAGE10 config json -> outputs.test_prob_ens_csv\n      C) csv fallbacks (ART_DIR/preds/test_prob_ens.csv etc)\n      D) npy fallback: FIX_DIR/test_ids.npy + test_prob_ens.npy\n    \"\"\"\n    # ---- A) globals ----\n    if (\"test_prob_ens\" in globals()) and (globals()[\"test_prob_ens\"] is not None) and \\\n       (\"test_ids\" in globals()) and (globals()[\"test_ids\"] is not None):\n        ids = [_norm_id(x) for x in list(globals()[\"test_ids\"])]\n        prob = _sanitize_prob(_as_1d_float32(globals()[\"test_prob_ens\"]))\n        if isinstance(prob, np.ndarray) and prob.ndim != 0 and len(ids) == len(prob) and len(ids) > 0:\n            df = pd.DataFrame({\"object_id\": ids, \"prob\": prob})\n            return df, \"globals(test_ids + test_prob_ens)\"\n\n    # ---- B) STAGE 10 config json ----\n    cfg = _try_load_stage10_cfg()\n    if isinstance(cfg, dict):\n        out = cfg.get(\"outputs\", {}) if isinstance(cfg.get(\"outputs\", {}), dict) else {}\n        csvp = out.get(\"test_prob_ens_csv\", None)\n        if csvp:\n            csvp = Path(csvp)\n            if csvp.exists():\n                df = pd.read_csv(csvp, dtype={\"object_id\": str})\n                if \"object_id\" in df.columns:\n                    colp = _detect_prob_col(df)\n                    if colp is None:\n                        raise RuntimeError(f\"Cannot detect prob column in: {csvp} | cols={list(df.columns)}\")\n                    df = df.copy()\n                    df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n                    prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n                    if len(prob) != len(df):\n                        raise RuntimeError(f\"CSV prob length mismatch: {csvp}\")\n                    return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob}), f\"stage10_cfg_csv({csvp})\"\n\n    # ---- C) csv fallback ----\n    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n    preds_dir = art_dir / \"preds\"\n    cand_csv = []\n    if \"TEST_PROB_CSV_PATH\" in globals() and globals()[\"TEST_PROB_CSV_PATH\"] is not None:\n        cand_csv.append(Path(globals()[\"TEST_PROB_CSV_PATH\"]))\n    cand_csv += [preds_dir / \"test_prob_ens.csv\", art_dir / \"test_prob_ens.csv\"]\n\n    for p in cand_csv:\n        if p.exists():\n            df = pd.read_csv(p, dtype={\"object_id\": str})\n            if \"object_id\" in df.columns:\n                colp = _detect_prob_col(df)\n                if colp is None:\n                    raise RuntimeError(f\"Cannot detect prob column in: {p} | cols={list(df.columns)}\")\n                df = df.copy()\n                df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n                prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n                if len(prob) != len(df):\n                    raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n                return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob}), f\"csv({p})\"\n\n    # ---- D) npy fallback ----\n    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n    p_ids = fix_dir / \"test_ids.npy\"\n    if not p_ids.exists():\n        raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat fixed_seq/test_ids.npy atau STAGE 10 export test_ids.\")\n\n    ids = _load_ids_npy(p_ids)\n    if len(ids) == 0:\n        raise RuntimeError(\"test_ids.npy kosong.\")\n\n    cand_npy = []\n    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n        cand_npy.append(Path(globals()[\"TEST_PROB_ENS_PATH\"]))\n    cand_npy += [preds_dir / \"test_prob_ens.npy\", art_dir / \"test_prob_ens.npy\"]\n\n    prob = None\n    used = None\n    for p in cand_npy:\n        if p.exists():\n            prob = _sanitize_prob(_as_1d_float32(np.load(p, allow_pickle=False)))\n            used = p\n            break\n    if prob is None:\n        raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n\n    if not isinstance(prob, np.ndarray) or prob.ndim == 0:\n        raise TypeError(f\"Invalid test_prob (scalar/unsized). type={type(prob)} ndim={getattr(prob,'ndim',None)}\")\n\n    if len(prob) != len(ids):\n        raise RuntimeError(f\"Length mismatch (NPY): test_prob={len(prob)} vs test_ids={len(ids)}\")\n\n    return pd.DataFrame({\"object_id\": ids, \"prob\": prob}), f\"npy({used}) + ids({p_ids})\"\n\ndef _load_best_threshold_fallback():\n    \"\"\"\n    Fallback cari threshold dari file tuning jika globals tidak ada.\n    Priority:\n      0) globals THR_JSON_PATH / EVAL_SUMMARY_PATH (kalau ada)\n      1) OOF_DIR/threshold_tuning.json  -> payload['best_thr_f1']['thr']\n      2) OOF_DIR/eval_summary.json      -> payload['best_f1']['thr']\n    \"\"\"\n    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n    oof_dir = Path(globals().get(\"OOF_DIR\", art_dir / \"oof\"))\n\n    cand = []\n    if \"THR_JSON_PATH\" in globals() and globals()[\"THR_JSON_PATH\"] is not None:\n        cand.append(Path(globals()[\"THR_JSON_PATH\"]))\n    if \"EVAL_SUMMARY_PATH\" in globals() and globals()[\"EVAL_SUMMARY_PATH\"] is not None:\n        cand.append(Path(globals()[\"EVAL_SUMMARY_PATH\"]))\n\n    cand += [oof_dir / \"threshold_tuning.json\", oof_dir / \"eval_summary.json\"]\n\n    for p in cand:\n        obj = _try_load_json(p)\n        if not isinstance(obj, dict):\n            continue\n        if Path(p).name == \"threshold_tuning.json\":\n            b = obj.get(\"best_thr_f1\", None)\n            if isinstance(b, dict) and \"thr\" in b:\n                return float(b[\"thr\"])\n        if Path(p).name == \"eval_summary.json\":\n            b = obj.get(\"best_f1\", None)\n            if isinstance(b, dict) and \"thr\" in b:\n                return float(b[\"thr\"])\n        # generic fallback\n        for k in [\"BEST_THR_F1\", \"BEST_THR\"]:\n            if k in obj:\n                try:\n                    return float(obj[k])\n                except Exception:\n                    pass\n    return None\n\n# ----------------------------\n# 1) Load prediction df\n# ----------------------------\ndf_pred, pred_src = _load_pred_df()\nif df_pred is None or df_pred.empty:\n    raise RuntimeError(\"df_pred empty (unexpected).\")\n\ndf_pred = df_pred.copy()\ndf_pred[\"object_id\"] = df_pred[\"object_id\"].apply(_norm_id)\n\nif df_pred[\"object_id\"].duplicated().any():\n    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n\np = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\nif not np.isfinite(p).all():\n    bad = int((~np.isfinite(p)).sum())\n    raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\ndf_pred[\"prob\"] = _sanitize_prob(p)\n\np2 = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\nprint(\"[Stage 11] Loaded test predictions\")\nprint(f\"- source: {pred_src}\")\nprint(f\"- N_pred={len(df_pred):,} | prob_mean={float(p2.mean()):.6f} | std={float(p2.std()):.6f} | min={float(p2.min()):.6f} | max={float(p2.max()):.6f}\")\n\n# ----------------------------\n# 2) Threshold selection (priority)\n# ----------------------------\nFORCE_THR = None  # set manual if you want, e.g. 0.37\n\nthr_src = None\nif FORCE_THR is not None:\n    thr = float(FORCE_THR); thr_src = \"FORCE_THR\"\nelif \"BEST_THR_F1\" in globals() and globals()[\"BEST_THR_F1\"] is not None:\n    thr = float(globals()[\"BEST_THR_F1\"]); thr_src = \"globals(BEST_THR_F1)\"\nelif \"BEST_THR\" in globals() and globals()[\"BEST_THR\"] is not None:\n    thr = float(globals()[\"BEST_THR\"]); thr_src = \"globals(BEST_THR)\"\nelse:\n    fb = _load_best_threshold_fallback()\n    if fb is not None:\n        thr = float(fb); thr_src = \"json_fallback(threshold_tuning/eval_summary)\"\n    else:\n        thr = 0.5; thr_src = \"default(0.5)\"\n\nthr = float(np.clip(thr, 0.0, 1.0))\n\n# ----------------------------\n# 3) Align to sample_submission order + build BINARY prediction (0/1)\n# ----------------------------\ndf_sub = df_sub.copy()\ndf_sub[\"object_id\"] = df_sub[\"object_id\"].apply(_norm_id)\n\nif df_sub[\"object_id\"].duplicated().any():\n    dup = df_sub.loc[df_sub[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(f\"sample_submission has duplicate object_id (unexpected). examples={dup}\")\n\n# quick diagnostics before merge\nsample_ids = pd.Index(df_sub[\"object_id\"].tolist())\npred_ids = pd.Index(df_pred[\"object_id\"].tolist())\nprint(f\"[Stage 11] ID coverage check: pred_in_sample={int(pred_ids.isin(sample_ids).sum()):,} / {len(pred_ids):,} | sample_in_pred={int(sample_ids.isin(pred_ids).sum()):,} / {len(sample_ids):,}\")\n\ndf_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n\nif df_out[\"prob\"].isna().any():\n    missing_n = int(df_out[\"prob\"].isna().sum())\n    miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(\n        f\"Some sample_submission object_id have no prediction: missing_n={missing_n}. Examples: {miss_ids}\\n\"\n        \"Penyebab umum: object_id kebaca numeric (leading zero hilang) atau pred tidak lengkap.\"\n    )\n\n# IMPORTANT: binary with >= thr\ndf_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\ndf_out = df_out[[\"object_id\", \"prediction\"]]\n\nu = set(np.unique(df_out[\"prediction\"].to_numpy()).tolist())\nif not u.issubset({0, 1}):\n    raise RuntimeError(f\"submission prediction contains values outside {{0,1}}: {sorted(list(u))}\")\n\nif len(df_out) != len(df_sub):\n    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n\npos_pred = int(df_out[\"prediction\"].sum())\nprint(\"\\n[Stage 11] SUBMISSION READY (BINARY 0/1)\")\nprint(f\"- threshold_used={thr:.6f} | thr_source={thr_src}\")\nprint(f\"- rows={len(df_out):,} | pos_pred={pos_pred:,} ({pos_pred/max(len(df_out),1)*100:.6f}%)\")\n\n# ----------------------------\n# 4) Write files\n# ----------------------------\nSUB_DIR = Path(SUB_DIR)\nSUB_DIR.mkdir(parents=True, exist_ok=True)\n\nout_main = Path(\"/kaggle/working/submission.csv\")\nout_copy = SUB_DIR / \"submission.csv\"\n\ndf_out.to_csv(out_main, index=False)\ndf_out.to_csv(out_copy, index=False)\n\nprint(f\"- wrote: {out_main}\")\nprint(f\"- copy : {out_copy}\")\nprint(\"\\nPreview:\")\nprint(df_out.head(8).to_string(index=False))\n\nglobals().update({\n    \"SUBMISSION_PATH\": out_main,\n    \"SUBMISSION_COPY_PATH\": out_copy,\n    \"SUBMISSION_MODE\": \"binary\",\n    \"SUBMISSION_THRESHOLD\": thr,\n    \"SUBMISSION_THRESHOLD_SOURCE\": thr_src,\n    \"SUBMISSION_PRED_SOURCE\": pred_src,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}