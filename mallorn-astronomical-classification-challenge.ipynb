{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14010596,"sourceType":"datasetVersion","datasetId":8925232}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Kaggle CPU Environment Setup","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Environment + Paths + Health Checks (ONE CELL)\n# REVISI FULL v8.1 (BRUTAL-LB READY + CONSISTENT SIGNAL POLICY)\n#\n# v8.1 changes vs your v8.0:\n# - SIGNAL policy dibuat konsisten dgn saran: flux-safe (asinh_flux) + snr_tanh (bukan mag_ulim)\n# - Tambah policy penting untuk stage lanjut:\n#     * DELTA_POLICY=\"per_band\" (hindari delta lintas band)\n#     * USE_REST_FRAME_TIME=True (t/(1+z))\n#     * WINDOW_POLICY=\"peak_centered\" + MULTI_WINDOW_K (TTA windows)\n#     * Calibration & threshold robust knobs\n# - Health report tambah ringkas statistik Z_err test (domain shift)\n# ============================================================\n\nimport os, sys, gc, json, time, random, hashlib, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# Repro / run identity\n# ----------------------------\nSEED = 2025\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\n# ----------------------------\n# Device detect\n# ----------------------------\ntry:\n    import torch\n    TORCH_OK = True\n    _cuda_ok = torch.cuda.is_available()\nexcept Exception:\n    torch = None\n    TORCH_OK = False\n    _cuda_ok = False\n\nDEVICE = \"cuda\" if _cuda_ok else \"cpu\"\n\n# ----------------------------\n# Thread policy (anti-freeze, tapi brutal mode boleh lebih tinggi)\n# ----------------------------\ndef _pick_threads(device: str) -> int:\n    if device == \"cuda\":\n        return 2\n    cpu = os.cpu_count() or 4\n    # sedikit lebih fleksibel untuk feature engineering, tapi tetap cap\n    return int(min(10, max(2, cpu // 2)))\n\nTHREADS = _pick_threads(DEVICE)\nfor k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n    os.environ.setdefault(k, str(THREADS))\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\nif TORCH_OK:\n    torch.manual_seed(SEED)\n    try:\n        torch.set_num_threads(THREADS)\n        torch.set_num_interop_threads(1)\n    except Exception:\n        pass\n\n# ----------------------------\n# Helpers\n# ----------------------------\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\ndef _must_exist(p: Path, what: str):\n    if not p.exists():\n        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef _normalize_split(x):\n    if pd.isna(x):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    if s.isdigit():\n        return f\"split_{int(s):02d}\"\n    s = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s.startswith(\"split_\"):\n        tail = s.split(\"split_\", 1)[1].strip(\"_\")\n        if tail.isdigit():\n            return f\"split_{int(tail):02d}\"\n    return s\n\ndef _discover_data_root(default_root: Path) -> Path:\n    \"\"\"\n    Cari folder di /kaggle/input yang punya:\n    - train_log.csv, test_log.csv, sample_submission.csv\n    - split_01..split_20 (minimal split_01 dan split_20)\n    \"\"\"\n    if default_root.exists():\n        return default_root\n    base = Path(\"/kaggle/input\")\n    if not base.exists():\n        return default_root\n\n    candidates = []\n    for d in base.iterdir():\n        if not d.is_dir():\n            continue\n        if (d / \"train_log.csv\").exists() and (d / \"test_log.csv\").exists() and (d / \"sample_submission.csv\").exists():\n            has_01 = (d / \"split_01\").exists() or (d / \"split_1\").exists()\n            has_20 = (d / \"split_20\").exists()\n            if has_01 and has_20:\n                candidates.append(d)\n\n    if len(candidates) == 1:\n        return candidates[0]\n    if len(candidates) > 1:\n        candidates = sorted(candidates, key=lambda x: (not (x / \"split_01\").exists(), x.name))\n        return candidates[0]\n    return default_root\n\ndef _hash_cfg(d: dict) -> str:\n    s = json.dumps(d, sort_keys=True, ensure_ascii=True)\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:10]\n\ndef _safe_float(x, default=0.0):\n    try:\n        v = float(x)\n        if np.isfinite(v):\n            return v\n    except Exception:\n        pass\n    return float(default)\n\ndef _sample_ids_per_split(df_log: pd.DataFrame, split_name: str, n: int, seed: int) -> list:\n    s = df_log.loc[df_log[\"split\"] == split_name, \"object_id\"].astype(str)\n    if len(s) == 0:\n        return []\n    if len(s) <= n:\n        return s.tolist()\n    return s.sample(n=n, random_state=seed).tolist()\n\ndef _scan_lightcurve_for_ids(csv_path: Path, target_ids: set, chunk_rows: int = 200_000):\n    \"\"\"\n    Scan streaming untuk memastikan object_id target benar-benar muncul di file lightcurve.\n    Kembalikan: found_ids, obs_count_by_id, band_mask_by_id\n    \"\"\"\n    found = set()\n    obs_count = {oid: 0 for oid in target_ids}\n    band_mask = {oid: 0 for oid in target_ids}  # bitmask u,g,r,i,z,y (0..5)\n\n    band_to_bit = {\"u\": 1<<0, \"g\": 1<<1, \"r\": 1<<2, \"i\": 1<<3, \"z\": 1<<4, \"y\": 1<<5}\n\n    usecols = [\"object_id\", \"Filter\"]\n    it = pd.read_csv(\n        csv_path,\n        usecols=usecols,\n        dtype={\"object_id\": \"string\"},\n        chunksize=chunk_rows,\n        **SAFE_READ_KW\n    )\n\n    for chunk in it:\n        chunk = _norm_cols(chunk)\n        if \"object_id\" not in chunk.columns or \"Filter\" not in chunk.columns:\n            raise ValueError(f\"[BAD LIGHTCURVE COLUMNS] {csv_path.name}: {list(chunk.columns)}\")\n\n        f = chunk[\"Filter\"].astype(\"string\").str.strip().str.lower()\n        oid = chunk[\"object_id\"].astype(\"string\").str.strip()\n\n        mask = oid.isin(target_ids)\n        if not mask.any():\n            continue\n\n        sub_oid = oid[mask].astype(str).values\n        sub_flt = f[mask].astype(str).values\n\n        for o, flt in zip(sub_oid, sub_flt):\n            found.add(o)\n            obs_count[o] = obs_count.get(o, 0) + 1\n            band_mask[o] = band_mask.get(o, 0) | band_to_bit.get(flt, 0)\n\n        if len(found) == len(target_ids):\n            break\n\n    return found, obs_count, band_mask\n\ndef _full_scan_lightcurve_object_ids(csv_path: Path, chunk_rows: int = 200_000):\n    found = set()\n    it = pd.read_csv(\n        csv_path,\n        usecols=[\"object_id\"],\n        dtype={\"object_id\": \"string\"},\n        chunksize=chunk_rows,\n        **SAFE_READ_KW\n    )\n    for chunk in it:\n        chunk = _norm_cols(chunk)\n        ids = chunk[\"object_id\"].astype(\"string\").str.strip()\n        found.update(ids.dropna().astype(str).unique().tolist())\n    return found\n\n# ----------------------------\n# PATHS (auto-discovery)\n# ----------------------------\nDEFAULT_DATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\nDATA_ROOT = _discover_data_root(DEFAULT_DATA_ROOT)\n\nPATHS = {\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n}\n\n# ----------------------------\n# WORKDIR (versioned run)\n# ----------------------------\nWORKDIR = Path(\"/kaggle/working\")\nBASE_RUN_DIR = WORKDIR / \"mallorn_run\"\nBASE_RUN_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# CFG — Brutal LB defaults (selaras dengan saran)\n# ----------------------------\nCFG = {\n    # Run intent\n    \"RUN_MODE\": \"brutal_lb\",\n    \"DEVICE_PREFERRED\": \"auto\",             # auto/cpu/cuda\n\n    # Multi-seed (ensemble)\n    \"SEEDS\": [2025, 2026, 2027],\n    \"CV_REPEATS\": 1,\n\n    # Model switches\n    \"USE_GBDT\": True,                       # baseline kuat (tabular)\n    \"USE_TRANSFORMER\": True,\n    \"USE_HYBRID_BLEND\": True,\n    \"USE_STACKING\": False,\n\n    # F1 essentials\n    \"USE_THRESHOLD_TUNING\": True,\n    \"THR_STRATEGY\": \"per_fold_median\",      # per_fold_median / global_best\n    \"USE_PROBA_CALIBRATION\": True,\n    \"CALIBRATION_METHOD\": \"temperature\",    # temperature / platt / isotonic\n\n    # Photometry / signal policy (KUNCI)\n    \"USE_DEEXTINCTION\": True,\n    # SIGNAL_CHANNELS: dibuat flux-safe (bukan mag_ulim) supaya Stage6 scoring & Stage8 agg konsisten\n    \"SIGNAL_CHANNELS\": [\"asinh_flux\", \"snr_tanh\"],\n\n    # flux-safe transform details (dipakai stage 4/5)\n    \"FLUX_SCALE_POLICY\": \"per_object_median_fluxerr\",  # global_constant / per_object_median_fluxerr\n    \"GLOBAL_FLUX_SCALE\": 1.0,            # dipakai jika policy global_constant\n    \"MIN_FLUXERR\": 1e-6,\n\n    # SNR handling\n    \"SNR_CLIP\": 30.0,\n    \"SNR_DET_THR_LIST\": [2.0, 3.0, 4.0],\n    \"SNR_STRONG_THR\": 5.0,\n    \"SNR_TANH_K\": 10.0,                  # snr_tanh = tanh(snr / k)\n\n    # Time policy (fisika)\n    \"USE_REST_FRAME_TIME\": True,         # t_rest = t/(1+z)\n    \"REST_FRAME_EPS\": 1e-6,\n\n    # Delta policy (hindari noise lintas band)\n    \"DELTA_POLICY\": \"per_band\",          # per_band / global (JANGAN global)\n\n    # IO\n    \"CHUNK_ROWS\": 200_000,\n\n    # CV\n    \"N_FOLDS\": 10,\n    \"CV_STRATIFY\": True,\n    \"CV_USE_SPLIT_COL\": True,\n    \"CV_FORCE_POS_EACH_FOLD\": True,\n\n    # Sequence length + windowing (kunci untuk transformer)\n    \"MAX_LEN_LIST\": [384, 512],\n    \"WINDOW_POLICY\": \"peak_centered\",    # peak_centered / best_contiguous / multi_window\n    \"TRAIN_RANDOM_CROP\": True,           # augment crop sekitar peak\n    \"MULTI_WINDOW_K\": 3,                 # TTA: 2-3 window saat inference (kalau CPU kuat)\n    \"PEAK_SCORE\": \"snr_pos\",             # snr_pos / abs_signal\n\n    # Training defaults\n    \"DEEP_EPOCHS\": 25,\n    \"DEEP_BS\": 128,\n    \"DEEP_LR\": 3e-4,\n    \"DEEP_WEIGHT_DECAY\": 0.02,\n    \"DEEP_POS_WEIGHT_MODE\": \"auto\",\n    \"DEEP_USE_EMA\": True,\n\n    # Stage0 validation\n    \"STAGE0_LC_VALIDATE_MODE\": \"sample\", # off/sample/full\n    \"STAGE0_LC_SAMPLE_PER_SPLIT\": 80,\n    \"STAGE0_FAIL_FAST_MISSING_RATE\": 0.01,\n}\n\nCFG_HASH = _hash_cfg(CFG)\nRUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\nRUN_DIR = BASE_RUN_DIR / f\"run_{RUN_TAG}_{CFG_HASH}\"\n\nART_DIR   = RUN_DIR / \"artifacts\"\nCACHE_DIR = RUN_DIR / \"cache\"\nOOF_DIR   = RUN_DIR / \"oof\"\nSUB_DIR   = RUN_DIR / \"submissions\"\nLOG_DIR   = RUN_DIR / \"logs\"\nFEAT_DIR  = CACHE_DIR / \"features\"\nSEQ_DIR   = CACHE_DIR / \"seq\"\n\nfor d in [RUN_DIR, ART_DIR, CACHE_DIR, OOF_DIR, SUB_DIR, LOG_DIR, FEAT_DIR, SEQ_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Validate existence (files + split folders)\n# ----------------------------\n_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n_must_exist(PATHS[\"TRAIN_LOG\"],  \"train_log.csv\")\n_must_exist(PATHS[\"TEST_LOG\"],   \"test_log.csv\")\n\nfor sd in PATHS[\"SPLITS\"]:\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    _must_exist(tr, f\"{sd.name}/train_full_lightcurves.csv\")\n    _must_exist(te, f\"{sd.name}/test_full_lightcurves.csv\")\n\n# ----------------------------\n# Load logs + sample\n# ----------------------------\ndf_sub = _norm_cols(pd.read_csv(PATHS[\"SAMPLE_SUB\"], dtype={\"object_id\": \"string\"}, **SAFE_READ_KW))\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission must have object_id,prediction. Found: {list(df_sub.columns)}\")\nif df_sub[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in sample_submission: {int(df_sub['object_id'].duplicated().sum())}\")\n\ndf_train_log = _norm_cols(pd.read_csv(PATHS[\"TRAIN_LOG\"], dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\ndf_test_log  = _norm_cols(pd.read_csv(PATHS[\"TEST_LOG\"],  dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n\nneed_train = {\"object_id\",\"EBV\",\"Z\",\"split\",\"target\"}\nneed_test  = {\"object_id\",\"EBV\",\"Z\",\"split\"}\nmissing_train = sorted(list(need_train - set(df_train_log.columns)))\nmissing_test  = sorted(list(need_test - set(df_test_log.columns)))\nif missing_train:\n    raise ValueError(f\"train_log missing: {missing_train}\")\nif missing_test:\n    raise ValueError(f\"test_log missing: {missing_test}\")\n\ndf_train_log[\"split\"] = df_train_log[\"split\"].map(_normalize_split)\ndf_test_log[\"split\"]  = df_test_log[\"split\"].map(_normalize_split)\n\nvalid_splits = {f\"split_{i:02d}\" for i in range(1, 21)}\nbad_tr = sorted(set(df_train_log[\"split\"]) - valid_splits)\nbad_te = sorted(set(df_test_log[\"split\"]) - valid_splits)\nif bad_tr:\n    raise ValueError(f\"Invalid split in train_log (examples): {bad_tr[:10]}\")\nif bad_te:\n    raise ValueError(f\"Invalid split in test_log  (examples): {bad_te[:10]}\")\n\nfor col in [\"EBV\", \"Z\"]:\n    df_train_log[col] = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n    df_test_log[col]  = pd.to_numeric(df_test_log[col],  errors=\"coerce\")\n\ndf_train_log[\"EBV_missing\"] = df_train_log[\"EBV\"].isna().astype(\"int8\")\ndf_train_log[\"Z_missing\"]   = df_train_log[\"Z\"].isna().astype(\"int8\")\ndf_test_log[\"EBV_missing\"]  = df_test_log[\"EBV\"].isna().astype(\"int8\")\ndf_test_log[\"Z_missing\"]    = df_test_log[\"Z\"].isna().astype(\"int8\")\n\nebv_med = float(df_train_log[\"EBV\"].median(skipna=True)) if df_train_log[\"EBV\"].notna().any() else 0.0\nz_med   = float(df_train_log[\"Z\"].median(skipna=True))   if df_train_log[\"Z\"].notna().any()   else 0.0\n\ndf_train_log[\"EBV\"] = df_train_log[\"EBV\"].fillna(ebv_med)\ndf_train_log[\"Z\"]   = df_train_log[\"Z\"].fillna(z_med)\ndf_test_log[\"EBV\"]  = df_test_log[\"EBV\"].fillna(ebv_med)\ndf_test_log[\"Z\"]    = df_test_log[\"Z\"].fillna(z_med)\n\ndf_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\")\nif df_train_log[\"target\"].isna().any():\n    raise ValueError(f\"train_log target NaN after coercion: {int(df_train_log['target'].isna().sum())}\")\nu = set(pd.unique(df_train_log[\"target\"]).tolist())\nif not u.issubset({0, 1}):\n    raise ValueError(f\"train_log target must be 0/1. Found: {sorted(list(u))}\")\n\n# Z_err handling (domain shift info)\nif \"Z_err\" not in df_test_log.columns:\n    df_test_log[\"Z_err\"] = np.nan\ndf_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\ndf_test_log[\"has_zerr\"] = (~df_test_log[\"Z_err\"].isna()).astype(\"int8\")\ndf_test_log[\"Z_err\"] = df_test_log[\"Z_err\"].fillna(0.0)\n\nif \"Z_err\" not in df_train_log.columns:\n    df_train_log[\"Z_err\"] = 0.0\ndf_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\").fillna(0.0)\ndf_train_log[\"has_zerr\"] = np.zeros(len(df_train_log), dtype=np.int8)\n\nif df_train_log[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in train_log: {int(df_train_log['object_id'].duplicated().sum())}\")\nif df_test_log[\"object_id\"].duplicated().any():\n    raise ValueError(f\"Duplicated object_id in test_log:  {int(df_test_log['object_id'].duplicated().sum())}\")\n\nsub_ids  = df_sub[\"object_id\"].astype(\"string\").str.strip()\ntest_ids = df_test_log[\"object_id\"].astype(\"string\").str.strip()\n\nif len(sub_ids) != len(test_ids):\n    raise ValueError(f\"Row mismatch: sample_submission={len(sub_ids)} vs test_log={len(test_ids)}\")\n\ns_sub = set(sub_ids.tolist())\ns_tst = set(test_ids.tolist())\nif s_sub != s_tst:\n    missing_in_test = list(s_sub - s_tst)[:5]\n    missing_in_sub  = list(s_tst - s_sub)[:5]\n    raise ValueError(\n        \"sample_submission and test_log object_id set mismatch.\\n\"\n        f\"- sample not in test_log (up to5): {missing_in_test}\\n\"\n        f\"- test_log not in sample (up to5): {missing_in_sub}\"\n    )\n\nSUB_ORDER = sub_ids.tolist()\nOID2SPLIT_TRAIN = dict(zip(df_train_log[\"object_id\"].astype(str), df_train_log[\"split\"].astype(str)))\nOID2SPLIT_TEST  = dict(zip(df_test_log[\"object_id\"].astype(str),  df_test_log[\"split\"].astype(str)))\n\n# ----------------------------\n# Health report: per split meta summary\n# ----------------------------\nsplit_rows = []\nfor sp in sorted(valid_splits):\n    tr = df_train_log[df_train_log[\"split\"] == sp]\n    te = df_test_log[df_test_log[\"split\"] == sp]\n    pos = int((tr[\"target\"] == 1).sum())\n    tot = int(len(tr))\n    split_rows.append({\n        \"split\": sp,\n        \"train_n\": tot,\n        \"train_pos\": pos,\n        \"train_pos_pct\": (pos / max(tot, 1)) * 100.0,\n        \"test_n\": int(len(te)),\n        \"train_Z_med\": _safe_float(tr[\"Z\"].median(), 0.0) if tot else 0.0,\n        \"train_EBV_med\": _safe_float(tr[\"EBV\"].median(), 0.0) if tot else 0.0,\n        \"test_has_zerr_pct\": float(te[\"has_zerr\"].mean() * 100.0) if len(te) else 0.0,\n        \"test_Zerr_med\": _safe_float(te[\"Z_err\"].median(), 0.0) if len(te) else 0.0,\n    })\ndf_split_summary = pd.DataFrame(split_rows).sort_values(\"split\").reset_index(drop=True)\n\n# ----------------------------\n# FAIL-FAST Lightcurve validation (off/sample/full)\n# ----------------------------\nlc_mode = str(CFG.get(\"STAGE0_LC_VALIDATE_MODE\", \"sample\")).lower().strip()\nlc_sample_n = int(CFG.get(\"STAGE0_LC_SAMPLE_PER_SPLIT\", 80))\nchunk_rows = int(CFG.get(\"CHUNK_ROWS\", 200_000))\nfail_rate = float(CFG.get(\"STAGE0_FAIL_FAST_MISSING_RATE\", 0.01))\n\nlc_diag = {\n    \"mode\": lc_mode,\n    \"sample_per_split\": lc_sample_n,\n    \"chunk_rows\": chunk_rows,\n    \"missing_train_ids\": [],\n    \"missing_test_ids\": [],\n    \"missing_train_count\": 0,\n    \"missing_test_count\": 0,\n    \"missing_train_rate\": 0.0,\n    \"missing_test_rate\": 0.0,\n    \"sample_obs_stats\": {},\n    \"sample_band_coverage\": {}\n}\n\nif lc_mode not in [\"off\", \"sample\", \"full\"]:\n    raise ValueError(\"CFG['STAGE0_LC_VALIDATE_MODE'] must be one of: off/sample/full\")\n\nif lc_mode != \"off\":\n    print(\"\\n[STAGE0] Lightcurve validation:\", lc_mode)\n\n    missing_train = []\n    missing_test = []\n    sample_obs_stats = {}\n    sample_band_cov = {}\n\n    for sp in sorted(valid_splits):\n        split_dir = DATA_ROOT / sp\n        tr_path = split_dir / \"train_full_lightcurves.csv\"\n        te_path = split_dir / \"test_full_lightcurves.csv\"\n\n        tr_ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(str).tolist()\n        te_ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(str).tolist()\n\n        if lc_mode == \"full\":\n            found_tr = _full_scan_lightcurve_object_ids(tr_path, chunk_rows=chunk_rows) if len(tr_ids) else set()\n            found_te = _full_scan_lightcurve_object_ids(te_path, chunk_rows=chunk_rows) if len(te_ids) else set()\n\n            miss_tr = sorted(list(set(tr_ids) - found_tr))\n            miss_te = sorted(list(set(te_ids) - found_te))\n\n            if miss_tr:\n                missing_train.extend([(sp, x) for x in miss_tr[:50]])\n            if miss_te:\n                missing_test.extend([(sp, x) for x in miss_te[:50]])\n\n            sample_obs_stats[sp] = {\n                \"full_mode\": True,\n                \"train_missing\": len(miss_tr),\n                \"test_missing\": len(miss_te),\n            }\n            sample_band_cov[sp] = {\"full_mode\": True}\n\n        else:\n            tr_s = _sample_ids_per_split(df_train_log, sp, lc_sample_n, SEED)\n            te_s = _sample_ids_per_split(df_test_log,  sp, lc_sample_n, SEED + 1)\n\n            band_cov_bits = 0\n\n            if len(tr_s):\n                found, obs_count, band_mask = _scan_lightcurve_for_ids(tr_path, set(tr_s), chunk_rows=chunk_rows)\n                miss = [x for x in tr_s if x not in found]\n                missing_train.extend([(sp, x) for x in miss])\n\n                counts = [obs_count.get(x, 0) for x in tr_s]\n                bandbits = [band_mask.get(x, 0) for x in tr_s]\n                band_cov_bits |= int(np.bitwise_or.reduce(bandbits)) if len(bandbits) else 0\n\n                sample_obs_stats.setdefault(sp, {})\n                sample_obs_stats[sp].update({\n                    \"train_sample_n\": len(tr_s),\n                    \"train_sample_missing\": len(miss),\n                    \"train_sample_obs_min\": int(np.min(counts)) if len(counts) else 0,\n                    \"train_sample_obs_med\": float(np.median(counts)) if len(counts) else 0.0,\n                    \"train_sample_obs_p95\": float(np.percentile(counts, 95)) if len(counts) else 0.0,\n                })\n\n            if len(te_s):\n                found, obs_count, band_mask = _scan_lightcurve_for_ids(te_path, set(te_s), chunk_rows=chunk_rows)\n                miss = [x for x in te_s if x not in found]\n                missing_test.extend([(sp, x) for x in miss])\n\n                counts = [obs_count.get(x, 0) for x in te_s]\n                bandbits = [band_mask.get(x, 0) for x in te_s]\n                band_cov_bits |= int(np.bitwise_or.reduce(bandbits)) if len(bandbits) else 0\n\n                sample_obs_stats.setdefault(sp, {})\n                sample_obs_stats[sp].update({\n                    \"test_sample_n\": len(te_s),\n                    \"test_sample_missing\": len(miss),\n                    \"test_sample_obs_min\": int(np.min(counts)) if len(counts) else 0,\n                    \"test_sample_obs_med\": float(np.median(counts)) if len(counts) else 0.0,\n                    \"test_sample_obs_p95\": float(np.percentile(counts, 95)) if len(counts) else 0.0,\n                })\n\n            bit_to_band = [(1<<0,\"u\"),(1<<1,\"g\"),(1<<2,\"r\"),(1<<3,\"i\"),(1<<4,\"z\"),(1<<5,\"y\")]\n            bands_present = [b for bit,b in bit_to_band if (band_cov_bits & bit)]\n            sample_band_cov[sp] = {\n                \"bands_present_in_sample\": bands_present,\n                \"bands_present_count\": len(bands_present),\n            }\n\n    lc_diag[\"missing_train_ids\"] = missing_train[:200]\n    lc_diag[\"missing_test_ids\"] = missing_test[:200]\n    lc_diag[\"missing_train_count\"] = len(missing_train)\n    lc_diag[\"missing_test_count\"] = len(missing_test)\n\n    if lc_mode == \"full\":\n        lc_diag[\"missing_train_rate\"] = None\n        lc_diag[\"missing_test_rate\"]  = None\n    else:\n        total_train_sample = sum(v.get(\"train_sample_n\", 0) for v in sample_obs_stats.values())\n        total_test_sample  = sum(v.get(\"test_sample_n\", 0)  for v in sample_obs_stats.values())\n        lc_diag[\"missing_train_rate\"] = (len(missing_train) / max(total_train_sample, 1))\n        lc_diag[\"missing_test_rate\"]  = (len(missing_test)  / max(total_test_sample, 1))\n\n    lc_diag[\"sample_obs_stats\"] = sample_obs_stats\n    lc_diag[\"sample_band_coverage\"] = sample_band_cov\n\n    if lc_mode == \"sample\":\n        if lc_diag[\"missing_test_rate\"] > fail_rate or lc_diag[\"missing_train_rate\"] > fail_rate:\n            raise RuntimeError(\n                \"[FAIL-FAST] Lightcurve validation indicates missing object_id in lightcurve files.\\n\"\n                f\"- missing_train_rate={lc_diag['missing_train_rate']:.3%}\\n\"\n                f\"- missing_test_rate ={lc_diag['missing_test_rate']:.3%}\\n\"\n                \"Periksa split routing / file path / object_id normalization.\"\n            )\n        if len(missing_test) > 0 or len(missing_train) > 0:\n            print(\"[WARN] Ada sample object_id yang tidak ditemukan di lightcurve file.\")\n            print(\"       Contoh missing_test:\", missing_test[:5])\n            print(\"       Contoh missing_train:\", missing_train[:5])\n\n# ----------------------------\n# Basic counts\n# ----------------------------\npos = int((df_train_log[\"target\"] == 1).sum())\nneg = int((df_train_log[\"target\"] == 0).sum())\ntot = int(len(df_train_log))\n\nprint(\"ENV OK (Stage0)\")\nprint(f\"- DEVICE: {DEVICE} | THREADS: {THREADS}\")\nprint(f\"- DATA_ROOT: {DATA_ROOT}\")\nprint(f\"- Python: {sys.version.split()[0]}\")\nprint(f\"- Numpy:  {np.__version__}\")\nprint(f\"- Pandas: {pd.__version__}\")\nif TORCH_OK:\n    print(f\"- Torch:  {torch.__version__} | CUDA: {_cuda_ok}\")\nelse:\n    print(\"- Torch:  not available\")\n\nprint(\"\\nDATA OK\")\nprint(f\"- train_log objects: {tot:,} | pos={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.3f}%\")\nprint(f\"- test_log objects:  {len(df_test_log):,}\")\nprint(f\"- sample_submission: {len(df_sub):,}\")\nprint(f\"- splits: {len(PATHS['SPLITS'])} folders (01..20)\")\nprint(f\"- RUN_DIR: {RUN_DIR}\")\n\nprint(\"\\nSPLIT SUMMARY (meta)\")\ntry:\n    display(df_split_summary.head(10))\nexcept Exception:\n    print(df_split_summary.head(10).to_string(index=False))\n\n# ----------------------------\n# Save diagnostics snapshot\n# ----------------------------\ndiag = {\n    \"SEED\": SEED,\n    \"DEVICE\": DEVICE,\n    \"THREADS\": THREADS,\n    \"CFG\": CFG,\n    \"CFG_HASH\": CFG_HASH,\n    \"RUN_DIR\": str(RUN_DIR),\n    \"DATA_ROOT\": str(DATA_ROOT),\n    \"counts\": {\n        \"train_objects\": int(len(df_train_log)),\n        \"train_pos\": int(pos),\n        \"train_neg\": int(neg),\n        \"test_objects\": int(len(df_test_log)),\n        \"sample_rows\": int(len(df_sub)),\n    },\n    \"split_meta_summary\": df_split_summary.to_dict(orient=\"records\"),\n    \"lightcurve_validation\": lc_diag,\n}\n\nwith open(RUN_DIR / \"config_stage0.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"SEED\": SEED,\n            \"DEVICE\": DEVICE,\n            \"THREADS\": THREADS,\n            \"CFG\": CFG,\n            \"CFG_HASH\": CFG_HASH,\n            \"RUN_DIR\": str(RUN_DIR),\n            \"DATA_ROOT\": str(DATA_ROOT),\n        },\n        f, indent=2\n    )\n\nwith open(RUN_DIR / \"run_diagnostics.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(diag, f, indent=2)\n\n# ----------------------------\n# Export globals\n# ----------------------------\nglobals().update({\n    \"SEED\": SEED, \"DEVICE\": DEVICE, \"THREADS\": THREADS,\n    \"CFG\": CFG, \"CFG_HASH\": CFG_HASH,\n    \"PATHS\": PATHS, \"DATA_ROOT\": DATA_ROOT, \"RUN_DIR\": RUN_DIR,\n    \"ART_DIR\": ART_DIR, \"CACHE_DIR\": CACHE_DIR, \"OOF_DIR\": OOF_DIR,\n    \"SUB_DIR\": SUB_DIR, \"LOG_DIR\": LOG_DIR, \"FEAT_DIR\": FEAT_DIR, \"SEQ_DIR\": SEQ_DIR,\n    \"df_sub\": df_sub, \"df_train_log\": df_train_log, \"df_test_log\": df_test_log,\n    \"OID2SPLIT_TRAIN\": OID2SPLIT_TRAIN, \"OID2SPLIT_TEST\": OID2SPLIT_TEST,\n    \"SUB_ORDER\": SUB_ORDER,\n    \"df_split_summary\": df_split_summary,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Verify Dataset Paths & Split Discovery","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — Split Routing + LC Profiling + Object Quality Features (ONE CELL)\n# REVISI FULL v5.1 (BRUTAL-LB READY + ANTI-AGN + REST-FRAME READY)\n#\n# v5.1 vs v5.0:\n# - Object quality tambah fitur kuat:\n#   * det_count abs + POS/NEG split (anti-AGN)\n#   * multi-threshold det counts (ikut CFG[\"SNR_DET_THR_LIST\"])\n#   * snr_pos_max / snr_neg_min\n#   * flux_max/min/mean/std + ferr_mean (valid numeric rows)\n#   * per-band det counts (base thr) + n_bands_det\n#   * timespan_rest + cadence_proxy_rest jika USE_REST_FRAME_TIME=True\n# - Fix: chunk[\"Filter\"] dinormalisasi sebelum band counts groupby\n# - Robust: ignore rows object_id yang tidak ada di log index (tidak crash)\n# ============================================================\n\nimport re, gc, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require STAGE 0 globals\n# ----------------------------\nneed0 = [\"PATHS\", \"df_train_log\", \"df_test_log\", \"RUN_DIR\", \"LOG_DIR\", \"CFG\", \"SEED\"]\nfor need in need0:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n\nDATA_ROOT = Path(PATHS[\"DATA_ROOT\"])\nRUN_DIR = Path(RUN_DIR)\nLOG_DIR = Path(LOG_DIR)\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nCFG_LOCAL = globals().get(\"CFG\", {}) or {}\nSEED = int(globals().get(\"SEED\", 2025))\n\n# deterministic split list (ikuti PATHS[\"SPLITS\"])\nif \"SPLITS\" not in PATHS or not isinstance(PATHS[\"SPLITS\"], (list, tuple)) or len(PATHS[\"SPLITS\"]) == 0:\n    raise RuntimeError(\"PATHS['SPLITS'] tidak valid. Pastikan STAGE 0 sukses.\")\nSPLIT_LIST = [Path(p).name for p in PATHS[\"SPLITS\"]]\nSPLIT_LIST = [s for s in SPLIT_LIST if s.startswith(\"split_\")]\nSPLIT_LIST = sorted(SPLIT_LIST)  # split_01..split_20\n\nVALID_SPLITS = set([f\"split_{i:02d}\" for i in range(1, 21)])\nif set(SPLIT_LIST) != VALID_SPLITS:\n    raise RuntimeError(\n        \"SPLIT_LIST mismatch dengan expected split_01..split_20.\\n\"\n        f\"Found (first 10): {sorted(list(set(SPLIT_LIST)))[:10]}\"\n    )\n\nSPLIT_DIRS = {Path(p).name: Path(p) for p in PATHS[\"SPLITS\"]}\n\n# ----------------------------\n# 1) Safe read config\n# ----------------------------\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\n# micro profiling knobs\nHEAD_ROWS = int(CFG_LOCAL.get(\"STAGE1_HEAD_ROWS\", CFG_LOCAL.get(\"LC_HEAD_ROWS\", 4000)))\n\n# sample ID presence knobs\nSAMPLE_ID_PER_SPLIT = int(\n    CFG_LOCAL.get(\n        \"STAGE1_ID_SAMPLE_PER_SPLIT\",\n        CFG_LOCAL.get(\"STAGE0_LC_SAMPLE_PER_SPLIT\", CFG_LOCAL.get(\"SAMPLE_ID_PER_SPLIT\", 12))\n    )\n)\n\nCHUNK_ROWS = int(CFG_LOCAL.get(\"CHUNK_ROWS\", 200_000))\n\n# adaptive scan caps\nMAX_CHUNKS_PER_FILE = int(CFG_LOCAL.get(\"MAX_CHUNKS_PER_FILE\", 6))\nMAX_CHUNKS_HARD = int(CFG_LOCAL.get(\"MAX_CHUNKS_HARD\", 30))\n\n# numeric sanity thresholds\nMAX_TIME_NA_FRAC = float(CFG_LOCAL.get(\"MAX_TIME_NA_FRAC\", 0.02))\nMAX_FERR_NA_FRAC = float(CFG_LOCAL.get(\"MAX_FERR_NA_FRAC\", 0.02))\nMIN_SAMPLE_ROWS  = int(CFG_LOCAL.get(\"MIN_SAMPLE_ROWS\", 200))\n\n# ID miss handling\nID_MISS_FAIL_FRAC = float(CFG_LOCAL.get(\"ID_MISS_FAIL_FRAC\", 0.80))\nFAIL_FAST_MISSING_RATE = float(\n    CFG_LOCAL.get(\"STAGE1_FAIL_FAST_MISSING_RATE\", CFG_LOCAL.get(\"STAGE0_FAIL_FAST_MISSING_RATE\", 0.01))\n)\n\n# Stage1 validate mode\nLC_VALIDATE_MODE = str(\n    CFG_LOCAL.get(\"STAGE1_LC_VALIDATE_MODE\", CFG_LOCAL.get(\"STAGE0_LC_VALIDATE_MODE\", \"sample\"))\n).lower().strip()\nif LC_VALIDATE_MODE not in [\"off\", \"sample\", \"full\"]:\n    raise ValueError(\"CFG['STAGE1_LC_VALIDATE_MODE'] must be one of: off/sample/full\")\n\n# SNR policy\nSNR_CLIP = float(CFG_LOCAL.get(\"SNR_CLIP\", 30.0))\nMIN_FLUXERR = float(CFG_LOCAL.get(\"MIN_FLUXERR\", 1e-6))\n\n# det threshold list (brutal): simpan multi-threshold count per object\n_thr_list = CFG_LOCAL.get(\"SNR_DET_THR_LIST\", None)\nif isinstance(_thr_list, (list, tuple)) and len(_thr_list) > 0:\n    SNR_DET_THR_LIST = [float(x) for x in _thr_list]\nelse:\n    SNR_DET_THR_LIST = [float(CFG_LOCAL.get(\"SNR_DET_THR\", 3.0))]\n# sanitize\nSNR_DET_THR_LIST = sorted(list(dict.fromkeys([float(x) for x in SNR_DET_THR_LIST if np.isfinite(float(x)) and float(x) > 0])))\nif len(SNR_DET_THR_LIST) == 0:\n    SNR_DET_THR_LIST = [3.0]\n\n# base threshold for “per-band det count”\nSNR_DET_THR = float(SNR_DET_THR_LIST[0])\nSNR_STRONG_THR = float(CFG_LOCAL.get(\"SNR_STRONG_THR\", 5.0))\n\n# rest-frame policy\nUSE_REST = bool(CFG_LOCAL.get(\"USE_REST_FRAME_TIME\", False))\nREST_EPS = float(CFG_LOCAL.get(\"REST_FRAME_EPS\", 1e-6))\n\n# NEW: build full object quality table\nBUILD_OBJECT_QUALITY = bool(CFG_LOCAL.get(\"STAGE1_BUILD_OBJECT_QUALITY\", True))\nOBJ_QUALITY_CHUNK_ROWS = int(CFG_LOCAL.get(\"STAGE1_OBJ_QUALITY_CHUNK_ROWS\", CHUNK_ROWS))\nZEROOBS_FAIL_RATE = float(CFG_LOCAL.get(\"STAGE1_ZEROOBS_FAIL_RATE\", 0.0005))  # 0.05% default\n\n# ----------------------------\n# 2) Helpers\n# ----------------------------\nREQ_LC_COLS = [\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"]\nREQ_LC_COLS_SET = set([c.strip() for c in REQ_LC_COLS])\nALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\nBANDS = [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]\nBAND_TO_IDX = {b:i for i,b in enumerate(BANDS)}\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef normalize_split_name(x) -> str:\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s2.isdigit():\n        return f\"split_{int(s2):02d}\"\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    return s2\n\ndef sizeof_mb(p: Path) -> float:\n    try:\n        return p.stat().st_size / (1024**2)\n    except Exception:\n        return float(\"nan\")\n\n_HEADER_CACHE = {}\n\ndef _get_usecols(csv_path: Path, required_trimmed: set):\n    key = str(csv_path)\n    if key in _HEADER_CACHE:\n        cols0, trim2orig = _HEADER_CACHE[key]\n    else:\n        df0 = pd.read_csv(csv_path, nrows=0, **SAFE_READ_KW)\n        cols0 = list(df0.columns)\n        trim2orig = {}\n        for c in cols0:\n            ct = str(c).strip()\n            if ct not in trim2orig:\n                trim2orig[ct] = c\n        _HEADER_CACHE[key] = (cols0, trim2orig)\n\n    missing = sorted(list(required_trimmed - set(trim2orig.keys())))\n    if missing:\n        found_trim = sorted(list(trim2orig.keys()))\n        raise ValueError(\n            f\"[LC SCHEMA] {csv_path} missing required columns (trim-aware): {missing}\\n\"\n            f\"Found columns (trimmed, first 50): {found_trim[:50]}\"\n        )\n    usecols = [trim2orig[c] for c in required_trimmed]\n    return usecols\n\ndef _read_sample_df(p: Path, nrows: int):\n    usecols = _get_usecols(p, REQ_LC_COLS_SET)\n    dfh = pd.read_csv(p, usecols=usecols, nrows=nrows, **SAFE_READ_KW)\n    dfh = _norm_cols(dfh)\n    return dfh\n\ndef _numeric_and_filter_stats(dfh: pd.DataFrame):\n    out = {\"n_sample\": int(len(dfh))}\n    if len(dfh) == 0:\n        out.update({k: np.nan for k in [\n            \"time_na_frac\",\"flux_na_frac\",\"ferr_na_frac\",\n            \"time_min\",\"time_max\",\"time_span\",\n            \"flux_neg_frac\",\"flux_p01\",\"flux_p50\",\"flux_p99\",\n            \"ferr_min\",\"ferr_p50\",\"ferr_p99\",\n            \"snr_abs_p50\",\"snr_abs_p95\",\"snr_ge_det_frac\",\"snr_ge_strong_frac\",\n            \"ferr_zero_frac\"\n        ]})\n        out.update({\"filter_bad\": \"\", \"filter_sample\": \"\"})\n        for b in BANDS:\n            out[f\"frac_{b}\"] = 0.0\n        out[\"ferr_neg_any\"] = 0\n        return out\n\n    filt = dfh[\"Filter\"].astype(\"string\").str.strip().str.lower()\n    filt = filt[~filt.isna()]\n    uniq = sorted(set(filt.tolist()))\n    bad = sorted([v for v in uniq if v not in ALLOWED_FILTERS])\n    out[\"filter_bad\"] = \",\".join(bad[:10]) if bad else \"\"\n    out[\"filter_sample\"] = \",\".join(uniq[:10]) if uniq else \"\"\n\n    if len(filt) > 0:\n        vc = filt.value_counts()\n        denom = float(vc.sum())\n        for b in BANDS:\n            out[f\"frac_{b}\"] = float(vc.get(b, 0) / denom)\n    else:\n        for b in BANDS:\n            out[f\"frac_{b}\"] = 0.0\n\n    t = pd.to_numeric(dfh[\"Time (MJD)\"], errors=\"coerce\")\n    f = pd.to_numeric(dfh[\"Flux\"], errors=\"coerce\")\n    e = pd.to_numeric(dfh[\"Flux_err\"], errors=\"coerce\")\n\n    out[\"time_na_frac\"] = float(t.isna().mean())\n    out[\"flux_na_frac\"] = float(f.isna().mean())\n    out[\"ferr_na_frac\"] = float(e.isna().mean())\n\n    if (~t.isna()).any():\n        out[\"time_min\"] = float(t.min())\n        out[\"time_max\"] = float(t.max())\n        out[\"time_span\"] = float(t.max() - t.min())\n    else:\n        out[\"time_min\"] = np.nan\n        out[\"time_max\"] = np.nan\n        out[\"time_span\"] = np.nan\n\n    if (~f.isna()).any():\n        fd = f.dropna()\n        out[\"flux_neg_frac\"] = float((fd < 0).mean())\n        out[\"flux_p01\"] = float(np.quantile(fd, 0.01))\n        out[\"flux_p50\"] = float(np.quantile(fd, 0.50))\n        out[\"flux_p99\"] = float(np.quantile(fd, 0.99))\n    else:\n        out[\"flux_neg_frac\"] = np.nan\n        out[\"flux_p01\"] = np.nan\n        out[\"flux_p50\"] = np.nan\n        out[\"flux_p99\"] = np.nan\n\n    if (~e.isna()).any():\n        ed = e.dropna()\n        out[\"ferr_min\"] = float(ed.min())\n        out[\"ferr_p50\"] = float(np.quantile(ed, 0.50))\n        out[\"ferr_p99\"] = float(np.quantile(ed, 0.99))\n        out[\"ferr_neg_any\"] = int((ed < 0).any())\n        out[\"ferr_zero_frac\"] = float((ed <= 0).mean())\n    else:\n        out[\"ferr_min\"] = np.nan\n        out[\"ferr_p50\"] = np.nan\n        out[\"ferr_p99\"] = np.nan\n        out[\"ferr_neg_any\"] = 0\n        out[\"ferr_zero_frac\"] = np.nan\n\n    if (~f.isna()).any() and (~e.isna()).any():\n        ff = f.to_numpy()\n        ee = e.to_numpy()\n        m = np.isfinite(ff) & np.isfinite(ee)\n        if m.any():\n            ee2 = np.maximum(ee[m], MIN_FLUXERR)\n            snr = ff[m] / ee2\n            snr = np.clip(snr, -SNR_CLIP, SNR_CLIP)\n            snr_abs = np.abs(snr)\n            out[\"snr_abs_p50\"] = float(np.quantile(snr_abs, 0.50))\n            out[\"snr_abs_p95\"] = float(np.quantile(snr_abs, 0.95))\n            out[\"snr_ge_det_frac\"] = float((snr_abs >= SNR_DET_THR).mean())\n            out[\"snr_ge_strong_frac\"] = float((snr_abs >= SNR_STRONG_THR).mean())\n        else:\n            out[\"snr_abs_p50\"] = np.nan\n            out[\"snr_abs_p95\"] = np.nan\n            out[\"snr_ge_det_frac\"] = np.nan\n            out[\"snr_ge_strong_frac\"] = np.nan\n    else:\n        out[\"snr_abs_p50\"] = np.nan\n        out[\"snr_abs_p95\"] = np.nan\n        out[\"snr_ge_det_frac\"] = np.nan\n        out[\"snr_ge_strong_frac\"] = np.nan\n\n    return out\n\ndef _sample_id_presence_adaptive(csv_path: Path, want_ids: set, chunk_rows: int,\n                                 max_chunks_init: int, max_chunks_hard: int):\n    if not want_ids:\n        return 0, set(), 0, max_chunks_init\n\n    usecols = _get_usecols(csv_path, {\"object_id\"})\n    remaining = set(want_ids)\n    found = set()\n    total_chunks_read = 0\n    used_cap = max_chunks_init\n\n    cap = max_chunks_init\n    while True:\n        nread = 0\n        for i, chunk in enumerate(pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_rows, **SAFE_READ_KW)):\n            if i < total_chunks_read:\n                continue\n            nread += 1\n            total_chunks_read += 1\n            chunk = _norm_cols(chunk)\n            ids = set(chunk[\"object_id\"].astype(\"string\").dropna().str.strip().astype(str).tolist())\n            hit = remaining & ids\n            if hit:\n                found |= hit\n                remaining -= hit\n            if not remaining:\n                break\n            if nread >= cap:\n                break\n\n        used_cap = cap\n        if not remaining:\n            break\n        if total_chunks_read >= max_chunks_hard:\n            break\n        cap = min(cap * 2, max_chunks_hard)\n\n    return len(found), remaining, total_chunks_read, used_cap\n\n# ----------------------------\n# 3) Normalize split col in logs (idempotent)\n# ----------------------------\nfor df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n    if \"split\" not in df.columns:\n        raise ValueError(f\"{name} missing 'split' column.\")\n    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n\nbad_train_split = sorted(set(df_train_log[\"split\"].unique()) - VALID_SPLITS)\nbad_test_split  = sorted(set(df_test_log[\"split\"].unique())  - VALID_SPLITS)\nif bad_train_split:\n    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\nif bad_test_split:\n    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n\n# ----------------------------\n# 4) Verify disk splits set + required files exist\n# ----------------------------\ndisk_splits = set(SPLIT_DIRS.keys())\nmissing_dirs = sorted(list(VALID_SPLITS - disk_splits))\nextra_dirs   = sorted(list(disk_splits - VALID_SPLITS))\nif missing_dirs or extra_dirs:\n    msg = []\n    if missing_dirs: msg.append(f\"Missing split folders: {missing_dirs[:10]}\")\n    if extra_dirs:   msg.append(f\"Unexpected split folders: {extra_dirs[:10]}\")\n    raise RuntimeError(\"Split folder set mismatch.\\n\" + \"\\n\".join(msg))\n\nmissing_files = []\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    for kind in [\"train\", \"test\"]:\n        p = sd / f\"{kind}_full_lightcurves.csv\"\n        if not p.exists():\n            missing_files.append(str(p))\nif missing_files:\n    raise FileNotFoundError(\"Some lightcurve files missing (showing up to 10):\\n\" + \"\\n\".join(missing_files[:10]))\n\n# ----------------------------\n# 5) Build routing manifest\n# ----------------------------\ntrain_counts = df_train_log[\"split\"].value_counts().to_dict()\ntest_counts  = df_test_log[\"split\"].value_counts().to_dict()\n\nrouting_rows = []\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    routing_rows.append({\n        \"split\": sp,\n        \"train_csv\": str(tr),\n        \"test_csv\": str(te),\n        \"train_mb\": sizeof_mb(tr),\n        \"test_mb\": sizeof_mb(te),\n        \"n_train_objects_log\": int(train_counts.get(sp, 0)),\n        \"n_test_objects_log\":  int(test_counts.get(sp, 0)),\n    })\n\ndf_routing = pd.DataFrame(routing_rows)\nrouting_path = LOG_DIR / \"split_routing.csv\"\ndf_routing.to_csv(routing_path, index=False)\n\n# ----------------------------\n# 6) Micro profiling + ID crosscheck\n# ----------------------------\nstats_rows = []\nid_warn_rows = []\nwarn_flux_na_files = 0\n\nt0 = time.time()\n\nagg_id_total = 0\nagg_id_missing = 0\n\nprint(f\"[STAGE1] LC_VALIDATE_MODE={LC_VALIDATE_MODE} | HEAD_ROWS={HEAD_ROWS} | SAMPLE_ID_PER_SPLIT={SAMPLE_ID_PER_SPLIT}\")\nprint(f\"[STAGE1] SNR_DET_THR_LIST={SNR_DET_THR_LIST} | SNR_STRONG_THR={SNR_STRONG_THR} | USE_REST={USE_REST}\")\n\nfor sp in SPLIT_LIST:\n    sd = SPLIT_DIRS[sp]\n    for kind in [\"train\", \"test\"]:\n        p = sd / f\"{kind}_full_lightcurves.csv\"\n\n        dfh = _read_sample_df(p, nrows=HEAD_ROWS)\n        if len(dfh) < MIN_SAMPLE_ROWS:\n            raise ValueError(f\"[LC SAMPLE] Too few rows sampled from {p} (n={len(dfh)}). Possible read issue.\")\n\n        st = _numeric_and_filter_stats(dfh)\n\n        if st.get(\"filter_bad\", \"\"):\n            raise ValueError(f\"[LC FILTER] Unexpected Filter values in {p}: {st['filter_bad']} (sample={st.get('filter_sample','')})\")\n\n        if st.get(\"time_na_frac\", 0.0) > MAX_TIME_NA_FRAC:\n            raise ValueError(f\"[LC NUM] Time(MJD) NaN too high in sample: {p} frac={st['time_na_frac']:.4f}\")\n        if st.get(\"ferr_na_frac\", 0.0) > MAX_FERR_NA_FRAC:\n            raise ValueError(f\"[LC NUM] Flux_err NaN too high in sample: {p} frac={st['ferr_na_frac']:.4f}\")\n        if int(st.get(\"ferr_neg_any\", 0)) == 1:\n            raise ValueError(f\"[LC NUM] Negative Flux_err detected in sample of {p} (should be >=0).\")\n\n        if st.get(\"flux_na_frac\", 0.0) > 0:\n            warn_flux_na_files += 1\n\n        id_k = 0\n        id_found = 0\n        id_missing = 0\n        id_scan_chunks = 0\n        id_scan_cap_used = 0\n        miss_ids_list = []\n\n        if LC_VALIDATE_MODE in [\"sample\", \"full\"]:\n            if kind == \"train\":\n                ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(\"string\").dropna().str.strip()\n            else:\n                ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(\"string\").dropna().str.strip()\n\n            if LC_VALIDATE_MODE == \"full\":\n                usecols = _get_usecols(p, {\"object_id\"})\n                found_all = set()\n                for chunk in pd.read_csv(p, usecols=usecols, chunksize=CHUNK_ROWS, **SAFE_READ_KW):\n                    chunk = _norm_cols(chunk)\n                    found_all.update(chunk[\"object_id\"].astype(\"string\").dropna().str.strip().astype(str).unique().tolist())\n                miss = sorted(list(set(ids.astype(str).tolist()) - found_all))\n                id_k = int(len(ids))\n                id_missing = int(len(miss))\n                id_found = id_k - id_missing\n                id_scan_chunks = None\n                id_scan_cap_used = None\n                miss_ids_list = miss[:20]\n            else:\n                id_k = int(min(SAMPLE_ID_PER_SPLIT, len(ids)))\n                want = set(ids.sample(n=id_k, random_state=SEED + (0 if kind==\"train\" else 7)).astype(str).tolist()) if id_k > 0 else set()\n                found_n, missing_ids, chunks_read, cap_used = _sample_id_presence_adaptive(\n                    p, want, CHUNK_ROWS, MAX_CHUNKS_PER_FILE, MAX_CHUNKS_HARD\n                )\n                id_found = int(found_n)\n                id_missing = int(len(missing_ids))\n                id_scan_chunks = int(chunks_read)\n                id_scan_cap_used = int(cap_used)\n                miss_ids_list = list(missing_ids)[:10]\n\n                miss_frac = (id_missing / max(id_k, 1)) if id_k else 0.0\n                agg_id_total += id_k\n                agg_id_missing += id_missing\n\n                if id_k and miss_frac >= ID_MISS_FAIL_FRAC:\n                    raise ValueError(\n                        f\"[LC ID] Severe mismatch within adaptive scan: {p} missing {id_missing}/{id_k} \"\n                        f\"(chunks_read={chunks_read}, hard_cap={MAX_CHUNKS_HARD}). Example missing: {miss_ids_list[:3]}\"\n                    )\n\n                if id_k and id_missing > 0:\n                    id_warn_rows.append({\n                        \"split\": sp, \"kind\": kind, \"file\": str(p),\n                        \"k\": id_k, \"missing\": id_missing,\n                        \"chunks_read\": chunks_read, \"cap_used\": cap_used,\n                        \"example_missing\": \",\".join(miss_ids_list[:5]),\n                    })\n\n        row = {\n            \"split\": sp,\n            \"kind\": kind,\n            \"file\": str(p),\n            \"file_mb\": sizeof_mb(p),\n            \"n_sample\": st.get(\"n_sample\", 0),\n            \"time_na_frac\": st.get(\"time_na_frac\", np.nan),\n            \"flux_na_frac\": st.get(\"flux_na_frac\", np.nan),\n            \"ferr_na_frac\": st.get(\"ferr_na_frac\", np.nan),\n            \"time_min\": st.get(\"time_min\", np.nan),\n            \"time_max\": st.get(\"time_max\", np.nan),\n            \"time_span\": st.get(\"time_span\", np.nan),\n            \"flux_neg_frac\": st.get(\"flux_neg_frac\", np.nan),\n            \"flux_p01\": st.get(\"flux_p01\", np.nan),\n            \"flux_p50\": st.get(\"flux_p50\", np.nan),\n            \"flux_p99\": st.get(\"flux_p99\", np.nan),\n            \"ferr_min\": st.get(\"ferr_min\", np.nan),\n            \"ferr_p50\": st.get(\"ferr_p50\", np.nan),\n            \"ferr_p99\": st.get(\"ferr_p99\", np.nan),\n            \"ferr_zero_frac\": st.get(\"ferr_zero_frac\", np.nan),\n            \"snr_abs_p50\": st.get(\"snr_abs_p50\", np.nan),\n            \"snr_abs_p95\": st.get(\"snr_abs_p95\", np.nan),\n            \"snr_ge_det_frac\": st.get(\"snr_ge_det_frac\", np.nan),\n            \"snr_ge_strong_frac\": st.get(\"snr_ge_strong_frac\", np.nan),\n            \"snr_det_thr_base\": float(SNR_DET_THR),\n            \"snr_strong_thr\": float(SNR_STRONG_THR),\n            \"filter_sample\": st.get(\"filter_sample\", \"\"),\n            \"id_check_k\": int(id_k),\n            \"id_found\": int(id_found),\n            \"id_missing\": int(id_missing),\n            \"id_scan_chunks\": id_scan_chunks,\n            \"id_scan_cap_used\": id_scan_cap_used,\n        }\n        for b in BANDS:\n            row[f\"frac_{b}\"] = st.get(f\"frac_{b}\", 0.0)\n\n        stats_rows.append(row)\n\ndf_lc_stats = pd.DataFrame(stats_rows)\nlc_stats_path = LOG_DIR / \"lc_sample_stats.csv\"\ndf_lc_stats.to_csv(lc_stats_path, index=False)\n\ndf_id_warn = pd.DataFrame(id_warn_rows)\nid_warn_path = LOG_DIR / \"lc_id_presence_warnings.csv\"\ndf_id_warn.to_csv(id_warn_path, index=False)\n\nagg_missing_rate = (agg_id_missing / max(agg_id_total, 1)) if (LC_VALIDATE_MODE == \"sample\") else None\nif LC_VALIDATE_MODE == \"sample\":\n    if agg_missing_rate > FAIL_FAST_MISSING_RATE:\n        raise RuntimeError(\n            \"[FAIL-FAST] Aggregate sample-ID missing rate terlalu tinggi.\\n\"\n            f\"- agg_missing_rate={agg_missing_rate:.3%} (missing={agg_id_missing}, total_sample={agg_id_total})\\n\"\n            f\"- threshold={FAIL_FAST_MISSING_RATE:.3%}\\n\"\n            \"Ini indikasi routing/split/object_id normalization bermasalah.\"\n        )\n\n# ----------------------------\n# 7) Build object_quality tables (train/test) — streaming scan\n# ----------------------------\nobjq_train_path = None\nobjq_test_path = None\nsplitq_path = None\n\nif BUILD_OBJECT_QUALITY:\n    print(\"\\n[STAGE1] Building object_quality (full streaming scan) ...\")\n\n    _tr = df_train_log.copy()\n    _te = df_test_log.copy()\n    _tr[\"object_id\"] = _tr[\"object_id\"].astype(\"string\").str.strip()\n    _te[\"object_id\"] = _te[\"object_id\"].astype(\"string\").str.strip()\n\n    # include has_zerr if exists\n    keep_tr = [\"split\",\"target\",\"Z\",\"Z_err\",\"EBV\"]\n    keep_te = [\"split\",\"Z\",\"Z_err\",\"EBV\"]\n    if \"has_zerr\" in _tr.columns: keep_tr.append(\"has_zerr\")\n    if \"has_zerr\" in _te.columns: keep_te.append(\"has_zerr\")\n\n    obj_train = _tr.set_index(\"object_id\")[keep_tr].copy()\n    obj_test  = _te.set_index(\"object_id\")[keep_te].copy()\n\n    # numeric holders\n    def _init_obj_table(df):\n        df = df.copy()\n\n        df[\"n_obs_total\"] = np.int32(0)\n        for b in BANDS:\n            df[f\"n_{b}\"] = np.int32(0)\n\n        # per-band det counts for base det thr\n        for b in BANDS:\n            df[f\"det_{b}\"] = np.int32(0)\n\n        # aggregate counters\n        df[\"neg_count\"] = np.int32(0)\n        df[\"pos_count\"] = np.int32(0)\n        df[\"ferr0_count\"] = np.int32(0)\n\n        # multi-threshold det counts (abs + pos + neg)\n        for thr in SNR_DET_THR_LIST:\n            key = int(round(thr * 10))  # 2.0->20, 3.0->30\n            df[f\"snr_det_abs_{key}\"] = np.int32(0)\n            df[f\"snr_det_pos_{key}\"] = np.int32(0)\n            df[f\"snr_det_neg_{key}\"] = np.int32(0)\n\n        # strong counts (abs + pos + neg)\n        df[\"snr_strong_abs\"] = np.int32(0)\n        df[\"snr_strong_pos\"] = np.int32(0)\n        df[\"snr_strong_neg\"] = np.int32(0)\n\n        # maxima/minima\n        df[\"snr_abs_max\"] = np.float32(0.0)\n        df[\"snr_pos_max\"] = np.float32(0.0)\n        df[\"snr_neg_min\"] = np.float32(0.0)  # negative (<=0), more negative => stronger neg excursion\n\n        df[\"flux_max\"] = np.float32(-np.inf)\n        df[\"flux_min\"] = np.float32(np.inf)\n\n        # sums for mean/std\n        df[\"flux_sum\"] = np.float64(0.0)\n        df[\"flux_sumsq\"] = np.float64(0.0)\n        df[\"ferr_sum\"] = np.float64(0.0)\n        df[\"n_valid\"] = np.int32(0)\n\n        # time range\n        df[\"tmin\"] = np.float64(np.inf)\n        df[\"tmax\"] = np.float64(-np.inf)\n\n        return df\n\n    obj_train = _init_obj_table(obj_train)\n    obj_test  = _init_obj_table(obj_test)\n\n    usecols = _get_usecols(SPLIT_DIRS[\"split_01\"] / \"train_full_lightcurves.csv\", REQ_LC_COLS_SET)  # schema assumed same\n\n    def _update_obj_stats(obj_df: pd.DataFrame, csv_path: Path, unknown_counter: dict):\n        idx_master = obj_df.index\n\n        for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=OBJ_QUALITY_CHUNK_ROWS, **SAFE_READ_KW):\n            chunk = _norm_cols(chunk)\n            chunk[\"object_id\"] = chunk[\"object_id\"].astype(\"string\").str.strip()\n            chunk = chunk[chunk[\"object_id\"].notna()]\n            if len(chunk) == 0:\n                continue\n\n            # normalize Filter into chunk column (important!)\n            chunk[\"Filter\"] = chunk[\"Filter\"].astype(\"string\").str.strip().str.lower().fillna(\"\")\n            bad = set(chunk[\"Filter\"].unique().tolist()) - ALLOWED_FILTERS - {\"\"}\n            if bad:\n                raise ValueError(f\"[LC FILTER] Unexpected Filter values in {csv_path}: {sorted(list(bad))[:10]}\")\n\n            # restrict to known object_id (avoid crash if file contains extra ids)\n            oid = chunk[\"object_id\"].astype(str)\n            known_mask = oid.isin(idx_master)\n            if not known_mask.any():\n                unknown_counter[\"unknown_rows\"] += int(len(chunk))\n                continue\n            if (~known_mask).any():\n                unknown_counter[\"unknown_rows\"] += int((~known_mask).sum())\n                chunk = chunk.loc[known_mask].copy()\n                oid = chunk[\"object_id\"].astype(str)\n\n            # numeric\n            t = pd.to_numeric(chunk[\"Time (MJD)\"], errors=\"coerce\")\n            f = pd.to_numeric(chunk[\"Flux\"], errors=\"coerce\")\n            e = pd.to_numeric(chunk[\"Flux_err\"], errors=\"coerce\")\n\n            # total n per object from ALL rows\n            n_all = oid.value_counts()\n            idx_all = n_all.index\n            obj_df.loc[idx_all, \"n_obs_total\"] = (obj_df.loc[idx_all, \"n_obs_total\"].astype(np.int64) + n_all.astype(np.int64)).astype(np.int32)\n\n            # time min/max (numeric coerce)\n            tmin = t.groupby(oid).min()\n            tmax = t.groupby(oid).max()\n            obj_df.loc[idx_all, \"tmin\"] = np.minimum(obj_df.loc[idx_all, \"tmin\"].to_numpy(), tmin.reindex(idx_all).to_numpy())\n            obj_df.loc[idx_all, \"tmax\"] = np.maximum(obj_df.loc[idx_all, \"tmax\"].to_numpy(), tmax.reindex(idx_all).to_numpy())\n\n            # band counts\n            bc = chunk.groupby([oid, chunk[\"Filter\"]]).size().unstack(fill_value=0)\n            for b in BANDS:\n                if b in bc.columns:\n                    obj_df.loc[bc.index, f\"n_{b}\"] = (obj_df.loc[bc.index, f\"n_{b}\"].astype(np.int64) + bc[b].astype(np.int64)).astype(np.int32)\n\n            # valid numeric rows for SNR/flux stats\n            ff = f.to_numpy()\n            ee = e.to_numpy()\n            mm = np.isfinite(ff) & np.isfinite(ee)\n            if not mm.any():\n                continue\n\n            oid_m = oid.to_numpy()[mm]\n            flt_m = chunk[\"Filter\"].to_numpy()[mm]\n            f_m = ff[mm].astype(np.float64, copy=False)\n            e_raw_m = ee[mm].astype(np.float64, copy=False)\n\n            # ferr0\n            ferr0 = (e_raw_m <= 0).astype(np.int32)\n            # clip for snr\n            e_m = np.maximum(e_raw_m, MIN_FLUXERR)\n            snr = (f_m / e_m).astype(np.float64, copy=False)\n            snr = np.clip(snr, -SNR_CLIP, SNR_CLIP)\n            snr_abs = np.abs(snr)\n\n            neg = (f_m < 0).astype(np.int32)\n            pos = (f_m > 0).astype(np.int32)\n\n            # build tmp frame\n            tmp = pd.DataFrame({\n                \"object_id\": oid_m,\n                \"filter\": flt_m,\n                \"flux\": f_m,\n                \"ferr\": e_raw_m,\n                \"neg\": neg,\n                \"pos\": pos,\n                \"ferr0\": ferr0,\n                \"snr\": snr,\n                \"snr_abs\": snr_abs,\n                \"snr_pos\": np.maximum(snr, 0.0),\n                \"snr_neg\": np.minimum(snr, 0.0),\n            })\n\n            # multi-threshold det flags (abs + pos + neg)\n            for thr in SNR_DET_THR_LIST:\n                key = int(round(thr * 10))\n                tmp[f\"det_abs_{key}\"] = (tmp[\"snr_abs\"].to_numpy() >= thr).astype(np.int32)\n                tmp[f\"det_pos_{key}\"] = (tmp[\"snr\"].to_numpy() >= thr).astype(np.int32)\n                tmp[f\"det_neg_{key}\"] = (tmp[\"snr\"].to_numpy() <= -thr).astype(np.int32)\n\n            # strong flags\n            tmp[\"strong_abs\"] = (tmp[\"snr_abs\"].to_numpy() >= SNR_STRONG_THR).astype(np.int32)\n            tmp[\"strong_pos\"] = (tmp[\"snr\"].to_numpy() >= SNR_STRONG_THR).astype(np.int32)\n            tmp[\"strong_neg\"] = (tmp[\"snr\"].to_numpy() <= -SNR_STRONG_THR).astype(np.int32)\n\n            # aggregate per object\n            agg_dict = {\n                \"neg_count\": (\"neg\", \"sum\"),\n                \"pos_count\": (\"pos\", \"sum\"),\n                \"ferr0_count\": (\"ferr0\", \"sum\"),\n                \"snr_abs_max\": (\"snr_abs\", \"max\"),\n                \"snr_pos_max\": (\"snr_pos\", \"max\"),\n                \"snr_neg_min\": (\"snr_neg\", \"min\"),\n                \"flux_max\": (\"flux\", \"max\"),\n                \"flux_min\": (\"flux\", \"min\"),\n                \"flux_sum\": (\"flux\", \"sum\"),\n                \"ferr_sum\": (\"ferr\", \"sum\"),\n                \"n_valid\": (\"flux\", \"count\"),\n            }\n            agg = tmp.groupby(\"object_id\").agg(**agg_dict)\n\n            # sumsq for std\n            tmp[\"flux2\"] = tmp[\"flux\"].to_numpy() * tmp[\"flux\"].to_numpy()\n            agg2 = tmp.groupby(\"object_id\").agg(flux_sumsq=(\"flux2\", \"sum\"))\n\n            idx = agg.index\n            obj_df.loc[idx, \"neg_count\"] = (obj_df.loc[idx, \"neg_count\"].astype(np.int64) + agg[\"neg_count\"].astype(np.int64)).astype(np.int32)\n            obj_df.loc[idx, \"pos_count\"] = (obj_df.loc[idx, \"pos_count\"].astype(np.int64) + agg[\"pos_count\"].astype(np.int64)).astype(np.int32)\n            obj_df.loc[idx, \"ferr0_count\"] = (obj_df.loc[idx, \"ferr0_count\"].astype(np.int64) + agg[\"ferr0_count\"].astype(np.int64)).astype(np.int32)\n\n            obj_df.loc[idx, \"flux_sum\"] = obj_df.loc[idx, \"flux_sum\"].to_numpy() + agg[\"flux_sum\"].to_numpy()\n            obj_df.loc[idx, \"flux_sumsq\"] = obj_df.loc[idx, \"flux_sumsq\"].to_numpy() + agg2[\"flux_sumsq\"].reindex(idx).to_numpy()\n            obj_df.loc[idx, \"ferr_sum\"] = obj_df.loc[idx, \"ferr_sum\"].to_numpy() + agg[\"ferr_sum\"].to_numpy()\n            obj_df.loc[idx, \"n_valid\"] = (obj_df.loc[idx, \"n_valid\"].astype(np.int64) + agg[\"n_valid\"].astype(np.int64)).astype(np.int32)\n\n            obj_df.loc[idx, \"snr_abs_max\"] = np.maximum(obj_df.loc[idx, \"snr_abs_max\"].to_numpy(), agg[\"snr_abs_max\"].to_numpy()).astype(np.float32)\n            obj_df.loc[idx, \"snr_pos_max\"] = np.maximum(obj_df.loc[idx, \"snr_pos_max\"].to_numpy(), agg[\"snr_pos_max\"].to_numpy()).astype(np.float32)\n            obj_df.loc[idx, \"snr_neg_min\"] = np.minimum(obj_df.loc[idx, \"snr_neg_min\"].to_numpy(), agg[\"snr_neg_min\"].to_numpy()).astype(np.float32)\n\n            obj_df.loc[idx, \"flux_max\"] = np.maximum(obj_df.loc[idx, \"flux_max\"].to_numpy(), agg[\"flux_max\"].to_numpy()).astype(np.float32)\n            obj_df.loc[idx, \"flux_min\"] = np.minimum(obj_df.loc[idx, \"flux_min\"].to_numpy(), agg[\"flux_min\"].to_numpy()).astype(np.float32)\n\n            # multi-threshold det counts update\n            det_aggs = {}\n            for thr in SNR_DET_THR_LIST:\n                key = int(round(thr * 10))\n                det_aggs[f\"det_abs_{key}\"] = (f\"det_abs_{key}\", \"sum\")\n                det_aggs[f\"det_pos_{key}\"] = (f\"det_pos_{key}\", \"sum\")\n                det_aggs[f\"det_neg_{key}\"] = (f\"det_neg_{key}\", \"sum\")\n            det_sum = tmp.groupby(\"object_id\").agg(**det_aggs)\n\n            for thr in SNR_DET_THR_LIST:\n                key = int(round(thr * 10))\n                obj_df.loc[det_sum.index, f\"snr_det_abs_{key}\"] = (obj_df.loc[det_sum.index, f\"snr_det_abs_{key}\"].astype(np.int64) + det_sum[f\"det_abs_{key}\"].astype(np.int64)).astype(np.int32)\n                obj_df.loc[det_sum.index, f\"snr_det_pos_{key}\"] = (obj_df.loc[det_sum.index, f\"snr_det_pos_{key}\"].astype(np.int64) + det_sum[f\"det_pos_{key}\"].astype(np.int64)).astype(np.int32)\n                obj_df.loc[det_sum.index, f\"snr_det_neg_{key}\"] = (obj_df.loc[det_sum.index, f\"snr_det_neg_{key}\"].astype(np.int64) + det_sum[f\"det_neg_{key}\"].astype(np.int64)).astype(np.int32)\n\n            # strong counts\n            strong_sum = tmp.groupby(\"object_id\").agg(\n                strong_abs=(\"strong_abs\", \"sum\"),\n                strong_pos=(\"strong_pos\", \"sum\"),\n                strong_neg=(\"strong_neg\", \"sum\"),\n            )\n            obj_df.loc[strong_sum.index, \"snr_strong_abs\"] = (obj_df.loc[strong_sum.index, \"snr_strong_abs\"].astype(np.int64) + strong_sum[\"strong_abs\"].astype(np.int64)).astype(np.int32)\n            obj_df.loc[strong_sum.index, \"snr_strong_pos\"] = (obj_df.loc[strong_sum.index, \"snr_strong_pos\"].astype(np.int64) + strong_sum[\"strong_pos\"].astype(np.int64)).astype(np.int32)\n            obj_df.loc[strong_sum.index, \"snr_strong_neg\"] = (obj_df.loc[strong_sum.index, \"snr_strong_neg\"].astype(np.int64) + strong_sum[\"strong_neg\"].astype(np.int64)).astype(np.int32)\n\n            # per-band det counts (base thr only; abs>=SNR_DET_THR)\n            tmp[\"det_base\"] = (tmp[\"snr_abs\"].to_numpy() >= SNR_DET_THR).astype(np.int32)\n            det_band = tmp[tmp[\"det_base\"] == 1].groupby([\"object_id\",\"filter\"]).size().unstack(fill_value=0)\n            for b in BANDS:\n                if b in det_band.columns:\n                    obj_df.loc[det_band.index, f\"det_{b}\"] = (obj_df.loc[det_band.index, f\"det_{b}\"].astype(np.int64) + det_band[b].astype(np.int64)).astype(np.int32)\n\n    unknown_tr = {\"unknown_rows\": 0}\n    unknown_te = {\"unknown_rows\": 0}\n\n    for sp in SPLIT_LIST:\n        sd = SPLIT_DIRS[sp]\n        _update_obj_stats(obj_train, sd / \"train_full_lightcurves.csv\", unknown_tr)\n        _update_obj_stats(obj_test,  sd / \"test_full_lightcurves.csv\", unknown_te)\n\n    def _finalize_obj(df: pd.DataFrame, is_train: bool):\n        df = df.copy()\n        n = df[\"n_obs_total\"].astype(np.float64).to_numpy()\n        n_safe = np.maximum(n, 1.0)\n\n        # time features\n        df[\"timespan\"] = (df[\"tmax\"] - df[\"tmin\"]).astype(np.float64)\n        df.loc[~np.isfinite(df[\"timespan\"]), \"timespan\"] = np.nan\n\n        df.loc[np.isinf(df[\"tmin\"]), \"tmin\"] = np.nan\n        df.loc[np.isinf(df[\"tmax\"]), \"tmax\"] = np.nan\n        df.loc[np.isinf(df[\"flux_max\"]), \"flux_max\"] = np.nan\n        df.loc[np.isinf(df[\"flux_min\"]), \"flux_min\"] = np.nan\n\n        # ratios\n        df[\"neg_flux_frac\"] = (df[\"neg_count\"].astype(np.float64) / n_safe).astype(np.float32)\n        df[\"pos_flux_frac\"] = (df[\"pos_count\"].astype(np.float64) / n_safe).astype(np.float32)\n\n        # multi-threshold fractions\n        for thr in SNR_DET_THR_LIST:\n            key = int(round(thr * 10))\n            df[f\"snr_det_abs_frac_{key}\"] = (df[f\"snr_det_abs_{key}\"].astype(np.float64) / n_safe).astype(np.float32)\n            df[f\"snr_det_pos_frac_{key}\"] = (df[f\"snr_det_pos_{key}\"].astype(np.float64) / n_safe).astype(np.float32)\n            df[f\"snr_det_neg_frac_{key}\"] = (df[f\"snr_det_neg_{key}\"].astype(np.float64) / n_safe).astype(np.float32)\n\n        df[\"snr_strong_abs_frac\"] = (df[\"snr_strong_abs\"].astype(np.float64) / n_safe).astype(np.float32)\n        df[\"snr_strong_pos_frac\"] = (df[\"snr_strong_pos\"].astype(np.float64) / n_safe).astype(np.float32)\n        df[\"snr_strong_neg_frac\"] = (df[\"snr_strong_neg\"].astype(np.float64) / n_safe).astype(np.float32)\n\n        # band coverage\n        band_present = []\n        for b in BANDS:\n            band_present.append((df[f\"n_{b}\"].to_numpy() > 0).astype(np.int8))\n            df[f\"frac_{b}\"] = (df[f\"n_{b}\"].astype(np.float64) / n_safe).astype(np.float32)\n\n        df[\"n_bands_present\"] = np.clip(np.sum(np.vstack(band_present), axis=0), 0, 6).astype(np.int8)\n\n        # det band coverage (base thr)\n        det_present = []\n        for b in BANDS:\n            det_present.append((df[f\"det_{b}\"].to_numpy() > 0).astype(np.int8))\n            df[f\"det_frac_{b}\"] = (df[f\"det_{b}\"].astype(np.float64) / n_safe).astype(np.float32)\n        df[\"n_bands_det\"] = np.clip(np.sum(np.vstack(det_present), axis=0), 0, 6).astype(np.int8)\n\n        # cadence proxy\n        df[\"cadence_proxy\"] = (df[\"timespan\"].astype(np.float64) / np.maximum(df[\"n_obs_total\"].astype(np.float64) - 1.0, 1.0)).astype(np.float32)\n\n        # flux / err stats from sums\n        nv = np.maximum(df[\"n_valid\"].astype(np.float64).to_numpy(), 1.0)\n        df[\"flux_mean\"] = (df[\"flux_sum\"].astype(np.float64).to_numpy() / nv).astype(np.float32)\n        # var = E[x^2] - mean^2\n        ex2 = (df[\"flux_sumsq\"].astype(np.float64).to_numpy() / nv)\n        var = np.maximum(ex2 - (df[\"flux_mean\"].astype(np.float64).to_numpy() ** 2), 0.0)\n        df[\"flux_std\"] = np.sqrt(var).astype(np.float32)\n        df[\"ferr_mean\"] = (df[\"ferr_sum\"].astype(np.float64).to_numpy() / nv).astype(np.float32)\n\n        # rest-frame\n        if USE_REST and (\"Z\" in df.columns):\n            z = pd.to_numeric(df[\"Z\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float64)\n            denom = np.maximum(1.0 + z, REST_EPS)\n            df[\"timespan_rest\"] = (df[\"timespan\"].astype(np.float64).to_numpy() / denom).astype(np.float32)\n            df[\"cadence_proxy_rest\"] = (df[\"cadence_proxy\"].astype(np.float64).to_numpy() / denom).astype(np.float32)\n            # zerr_rel (domain shift)\n            if \"Z_err\" in df.columns:\n                zerr = pd.to_numeric(df[\"Z_err\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float64)\n                df[\"zerr_rel\"] = (zerr / denom).astype(np.float32)\n\n        return df\n\n    obj_train_f = _finalize_obj(obj_train, is_train=True)\n    obj_test_f  = _finalize_obj(obj_test,  is_train=False)\n\n    # fail-fast: zero obs objects\n    z0_tr = int((obj_train_f[\"n_obs_total\"] == 0).sum())\n    z0_te = int((obj_test_f[\"n_obs_total\"] == 0).sum())\n    r0_tr = z0_tr / max(len(obj_train_f), 1)\n    r0_te = z0_te / max(len(obj_test_f), 1)\n\n    if (r0_tr > ZEROOBS_FAIL_RATE) or (r0_te > ZEROOBS_FAIL_RATE):\n        raise RuntimeError(\n            \"[FAIL-FAST] Terlalu banyak object_id dengan 0 observasi (routing/reading issue).\\n\"\n            f\"- train_zero_obs: {z0_tr}/{len(obj_train_f)} ({r0_tr:.3%})\\n\"\n            f\"- test_zero_obs : {z0_te}/{len(obj_test_f)} ({r0_te:.3%})\\n\"\n            f\"- threshold      : {ZEROOBS_FAIL_RATE:.3%}\\n\"\n        )\n\n    objq_train_path = LOG_DIR / \"object_quality_train.csv\"\n    objq_test_path  = LOG_DIR / \"object_quality_test.csv\"\n    obj_train_f.reset_index().to_csv(objq_train_path, index=False)\n    obj_test_f.reset_index().to_csv(objq_test_path, index=False)\n\n    # split-level summary\n    def _split_summary(df: pd.DataFrame, is_train: bool):\n        g = df.groupby(\"split\", dropna=False)\n        rows = []\n        for sp, d in g:\n            r = {\n                \"split\": sp,\n                \"n_objects\": int(len(d)),\n                \"n_obs_total_sum\": int(d[\"n_obs_total\"].sum()),\n                \"n_obs_total_med\": float(d[\"n_obs_total\"].median()),\n                \"timespan_med\": float(d[\"timespan\"].median(skipna=True)),\n                \"neg_flux_frac_med\": float(d[\"neg_flux_frac\"].median(skipna=True)),\n                \"pos_flux_frac_med\": float(d[\"pos_flux_frac\"].median(skipna=True)),\n                \"snr_abs_max_p95\": float(np.nanpercentile(d[\"snr_abs_max\"].to_numpy(), 95)),\n                \"snr_pos_max_p95\": float(np.nanpercentile(d[\"snr_pos_max\"].to_numpy(), 95)),\n                \"snr_neg_min_p05\": float(np.nanpercentile(d[\"snr_neg_min\"].to_numpy(), 5)),\n                \"n_bands_present_med\": float(d[\"n_bands_present\"].median()),\n                \"n_bands_det_med\": float(d[\"n_bands_det\"].median()),\n                \"cadence_proxy_med\": float(d[\"cadence_proxy\"].median(skipna=True)),\n                \"flux_std_med\": float(d[\"flux_std\"].median(skipna=True)),\n            }\n            # include base det fractions\n            key0 = int(round(SNR_DET_THR * 10))\n            if f\"snr_det_abs_frac_{key0}\" in d.columns:\n                r[\"snr_det_abs_frac_med_base\"] = float(d[f\"snr_det_abs_frac_{key0}\"].median(skipna=True))\n                r[\"snr_det_pos_frac_med_base\"] = float(d[f\"snr_det_pos_frac_{key0}\"].median(skipna=True))\n                r[\"snr_det_neg_frac_med_base\"] = float(d[f\"snr_det_neg_frac_{key0}\"].median(skipna=True))\n\n            for b in BANDS:\n                r[f\"frac_{b}_med\"] = float(d[f\"frac_{b}\"].median(skipna=True))\n                r[f\"det_frac_{b}_med\"] = float(d[f\"det_frac_{b}\"].median(skipna=True))\n\n            if USE_REST and \"timespan_rest\" in d.columns:\n                r[\"timespan_rest_med\"] = float(d[\"timespan_rest\"].median(skipna=True))\n                r[\"cadence_proxy_rest_med\"] = float(d[\"cadence_proxy_rest\"].median(skipna=True))\n\n            if is_train and \"target\" in d.columns:\n                r[\"pos\"] = int((d[\"target\"] == 1).sum())\n                r[\"pos_pct\"] = float((d[\"target\"] == 1).mean() * 100.0)\n            rows.append(r)\n        return pd.DataFrame(rows).sort_values(\"split\").reset_index(drop=True)\n\n    split_train = _split_summary(obj_train_f, is_train=True)\n    split_test  = _split_summary(obj_test_f,  is_train=False)\n    split_train[\"kind\"] = \"train\"\n    split_test[\"kind\"]  = \"test\"\n    df_splitq = pd.concat([split_train, split_test], ignore_index=True)\n\n    splitq_path = LOG_DIR / \"split_quality_summary.csv\"\n    df_splitq.to_csv(splitq_path, index=False)\n\n# ----------------------------\n# 8) Summary prints + JSON summary\n# ----------------------------\nelapsed = time.time() - t0\n\nworst_id_missing = (\n    df_lc_stats.sort_values([\"id_missing\", \"id_check_k\"], ascending=[False, False])\n    .head(10)[[\"split\",\"kind\",\"id_missing\",\"id_check_k\",\"id_scan_chunks\",\"id_scan_cap_used\",\"file_mb\"]]\n)\nworst_flux_neg = (\n    df_lc_stats.sort_values(\"flux_neg_frac\", ascending=False)\n    .head(10)[[\"split\",\"kind\",\"flux_neg_frac\",\"snr_ge_det_frac\",\"file_mb\"]]\n)\nworst_snr = (\n    df_lc_stats.sort_values(\"snr_ge_det_frac\", ascending=False)\n    .head(10)[[\"split\",\"kind\",\"snr_ge_det_frac\",\"snr_ge_strong_frac\",\"snr_abs_p95\",\"file_mb\"]]\n)\n\nsummary = {\n    \"stage\": \"stage1\",\n    \"data_root\": str(DATA_ROOT),\n    \"log_dir\": str(LOG_DIR),\n    \"lc_validate_mode\": LC_VALIDATE_MODE,\n    \"head_rows\": HEAD_ROWS,\n    \"sample_id_per_split\": SAMPLE_ID_PER_SPLIT,\n    \"chunk_rows\": CHUNK_ROWS,\n    \"obj_quality_chunk_rows\": OBJ_QUALITY_CHUNK_ROWS,\n    \"build_object_quality\": bool(BUILD_OBJECT_QUALITY),\n    \"snr\": {\n        \"SNR_CLIP\": SNR_CLIP,\n        \"SNR_DET_THR_LIST\": SNR_DET_THR_LIST,\n        \"SNR_STRONG_THR\": SNR_STRONG_THR,\n        \"MIN_FLUXERR\": MIN_FLUXERR,\n    },\n    \"rest_frame\": {\n        \"USE_REST_FRAME_TIME\": USE_REST,\n        \"REST_EPS\": REST_EPS,\n    },\n    \"thresholds\": {\n        \"MAX_TIME_NA_FRAC\": MAX_TIME_NA_FRAC,\n        \"MAX_FERR_NA_FRAC\": MAX_FERR_NA_FRAC,\n        \"ID_MISS_FAIL_FRAC\": ID_MISS_FAIL_FRAC,\n        \"FAIL_FAST_MISSING_RATE\": FAIL_FAST_MISSING_RATE,\n        \"MIN_SAMPLE_ROWS\": MIN_SAMPLE_ROWS,\n        \"ZEROOBS_FAIL_RATE\": ZEROOBS_FAIL_RATE,\n    },\n    \"aggregate_id_missing\": {\n        \"total_sample_ids\": int(agg_id_total),\n        \"missing_sample_ids\": int(agg_id_missing),\n        \"missing_rate\": float(agg_missing_rate) if agg_missing_rate is not None else None\n    },\n    \"warn_flux_na_files\": int(warn_flux_na_files),\n    \"routing_csv\": str(routing_path),\n    \"lc_sample_stats_csv\": str(lc_stats_path),\n    \"lc_id_presence_warnings_csv\": str(id_warn_path),\n    \"object_quality_train_csv\": str(objq_train_path) if objq_train_path else None,\n    \"object_quality_test_csv\": str(objq_test_path) if objq_test_path else None,\n    \"split_quality_summary_csv\": str(splitq_path) if splitq_path else None,\n    \"elapsed_sec\": float(elapsed),\n}\n\nsummary_path = LOG_DIR / \"stage1_summary.json\"\nwith open(summary_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"\\nSTAGE 1 OK — ROUTING + PROFILING + OBJECT_QUALITY READY\")\nprint(f\"- routing saved: {routing_path}\")\nprint(f\"- lc sample stats saved: {lc_stats_path}\")\nprint(f\"- id warnings saved: {id_warn_path}\")\nif objq_train_path:\n    print(f\"- object_quality_train: {objq_train_path}\")\n    print(f\"- object_quality_test : {objq_test_path}\")\n    print(f\"- split_quality_summary: {splitq_path}\")\nprint(f\"- summary json saved: {summary_path}\")\nprint(f\"- elapsed: {elapsed/60:.2f} min | warn_flux_na_files={warn_flux_na_files}\")\n\nprint(\"\\nTOP ISSUES (ID missing in sample/adaptive scan)\")\nprint(worst_id_missing.to_string(index=False))\n\nprint(\"\\nTOP PATTERN (highest negative flux fraction in sample head)\")\nprint(worst_flux_neg.to_string(index=False))\n\nprint(\"\\nTOP PATTERN (highest high-SNR fraction in sample head)\")\nprint(worst_snr.to_string(index=False))\n\n# ----------------------------\n# 9) Export to globals\n# ----------------------------\nglobals().update({\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SPLIT_DIRS\": SPLIT_DIRS,\n    \"SPLIT_LIST\": SPLIT_LIST,\n    \"df_split_routing\": df_routing,\n    \"df_lc_sample_stats\": df_lc_stats,\n    \"df_lc_id_presence_warnings\": df_id_warn,\n    \"STAGE1_SUMMARY_PATH\": summary_path,\n    \"OBJECT_QUALITY_TRAIN_PATH\": str(objq_train_path) if objq_train_path else None,\n    \"OBJECT_QUALITY_TEST_PATH\": str(objq_test_path) if objq_test_path else None,\n    \"SPLIT_QUALITY_SUMMARY_PATH\": str(splitq_path) if splitq_path else None,\n})\n\ngc.collect()\nprint(\"\\nStage 1 complete: splits verified + routing/stats + object_quality exported.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and Validate Train/Test Logs","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — Clean Meta Logs + CV Fold Assignment + Meta Enrichment (ONE CELL)\n# REVISI FULL v6.1 (STAGE1 v5.1 compatible + safe objq join + SNR alias + robust clipping)\n#\n# v6.1 upgrade:\n# - Join object_quality: drop duplicate cols (split/target/Z/Z_err/EBV/has_zerr) sebelum join (no _oq noise)\n# - Auto-detect & include ALL useful objq feature cols (multi-threshold + pos/neg + det band + flux stats)\n# - Alias safety:\n#   * snr_det_frac <= snr_det_abs_frac_{base_key} jika ada\n#   * snr_strong_frac <= snr_strong_abs_frac jika ada\n# - Z_err clip: prefer train photo-z pool, fallback test pool\n# - Optional fold balancing add mean_log1p_obs when objq exists\n# ============================================================\n\nimport re, gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 0/1 globals\n# ----------------------------\nfor need in [\"PATHS\", \"ART_DIR\", \"SPLIT_DIRS\", \"CFG\", \"SEED\", \"LOG_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 & STAGE 1 dulu.\")\n\nTRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\nTEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n\nART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\n\nSEED = int(SEED)\nN_FOLDS = int(CFG.get(\"N_FOLDS\", 5))\nCV_USE_SPLIT_COL = bool(CFG.get(\"CV_USE_SPLIT_COL\", True))\n\n# deterministic split list (sinkron STAGE 0/1)\nSPLIT_LIST = [Path(p).name for p in PATHS[\"SPLITS\"]]\nSPLIT_LIST = sorted([s for s in SPLIT_LIST if s.startswith(\"split_\")])\nVALID_SPLITS = set(SPLIT_LIST)\n\ndisk_splits = set(SPLIT_DIRS.keys())\nif disk_splits != VALID_SPLITS:\n    miss = sorted(list(VALID_SPLITS - disk_splits))\n    extra = sorted(list(disk_splits - VALID_SPLITS))\n    raise RuntimeError(\n        f\"SPLIT_DIRS mismatch. missing={miss[:5]} extra={extra[:5]} (jalankan ulang STAGE 1)\"\n    )\n\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\n# fold assignment tuning\nFOLD_QUOTA = int(CFG.get(\"SPLIT_PER_FOLD_QUOTA\", int(np.ceil(len(SPLIT_LIST)/max(N_FOLDS,1)))))\nRESTARTS = int(CFG.get(\"SPLIT_ASSIGN_RESTARTS\", 512))\nRESTARTS_HARD = int(CFG.get(\"SPLIT_ASSIGN_RESTARTS_HARD\", 2048))\nPENALTY_ZERO_POS = float(CFG.get(\"FOLD_BALANCE_PENALTY_ZERO_POS\", 3.0))\n\n# weights objective\nLAMBDA_COUNT = float(CFG.get(\"FOLD_BALANCE_LAMBDA_COUNT\", 0.25))\nLAMBDA_QUOTA = float(CFG.get(\"FOLD_BALANCE_LAMBDA_QUOTA\", 0.05))\nLAMBDA_ZMEAN = float(CFG.get(\"FOLD_BALANCE_LAMBDA_ZMEAN\", 0.15))\nLAMBDA_EBVMEAN = float(CFG.get(\"FOLD_BALANCE_LAMBDA_EBVMEAN\", 0.10))\nLAMBDA_OBSMEAN = float(CFG.get(\"FOLD_BALANCE_LAMBDA_OBSMEAN\", 0.10))  # aktif jika objq ada\n\n# clipping quantiles\nQLO = float(CFG.get(\"META_QLO\", 0.001))\nQHI = float(CFG.get(\"META_QHI\", 0.999))\n\n# split prior smoothing\nPRIOR_ALPHA = float(CFG.get(\"SPLIT_PRIOR_ALPHA\", 10.0))\n\n# optional join object_quality from stage1\nOBJQ_TRAIN = globals().get(\"OBJECT_QUALITY_TRAIN_PATH\", None)\nOBJQ_TEST  = globals().get(\"OBJECT_QUALITY_TEST_PATH\", None)\nif not OBJQ_TRAIN:\n    p = LOG_DIR / \"object_quality_train.csv\"\n    OBJQ_TRAIN = str(p) if p.exists() else None\nif not OBJQ_TEST:\n    p = LOG_DIR / \"object_quality_test.csv\"\n    OBJQ_TEST = str(p) if p.exists() else None\n\n# base snr det thr key (untuk alias snr_det_frac)\n_thr_list = CFG.get(\"SNR_DET_THR_LIST\", None)\nif isinstance(_thr_list, (list, tuple)) and len(_thr_list) > 0:\n    _thr_list = [float(x) for x in _thr_list if np.isfinite(float(x))]\n    _thr_list = sorted(list(dict.fromkeys(_thr_list)))\nelse:\n    _thr_list = [float(CFG.get(\"SNR_DET_THR\", 3.0))]\nBASE_DET_THR = float(_thr_list[0]) if len(_thr_list) else 3.0\nBASE_DET_KEY = int(round(BASE_DET_THR * 10))  # 3.0->30\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\ndef normalize_split_name(x) -> str:\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip()\n    if not s:\n        return \"\"\n    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n    if s2.isdigit():\n        return f\"split_{int(s2):02d}\"\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n    if m:\n        return f\"split_{int(m.group(1)):02d}\"\n    return s2\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [str(c).strip() for c in df.columns]\n    return df\n\ndef _coerce_float32(df: pd.DataFrame, col: str):\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\")\n\ndef _safe_clip(series: pd.Series, lo: float, hi: float) -> pd.Series:\n    # handle NaN bounds\n    if not np.isfinite(lo): lo = float(np.nanmin(series.values.astype(float))) if np.isfinite(series.values.astype(float)).any() else 0.0\n    if not np.isfinite(hi): hi = float(np.nanmax(series.values.astype(float))) if np.isfinite(series.values.astype(float)).any() else 0.0\n    if hi < lo:\n        lo, hi = hi, lo\n    return series.clip(lower=np.float32(lo), upper=np.float32(hi)).astype(\"float32\")\n\ndef _qclip_bounds(arr: np.ndarray, qlo=0.001, qhi=0.999, default=(0.0, 0.0)):\n    x = np.asarray(arr, dtype=float)\n    x = x[np.isfinite(x)]\n    if len(x) == 0:\n        return float(default[0]), float(default[1])\n    lo, hi = np.quantile(x, [qlo, qhi])\n    lo = float(lo); hi = float(hi)\n    if not np.isfinite(lo): lo = float(default[0])\n    if not np.isfinite(hi): hi = float(default[1])\n    if hi < lo: lo, hi = hi, lo\n    return lo, hi\n\ndef _load_or_use_global(global_name: str, path: Path) -> pd.DataFrame:\n    if global_name in globals() and isinstance(globals()[global_name], pd.DataFrame):\n        return _norm_cols(globals()[global_name].copy())\n    return _norm_cols(pd.read_csv(path, dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n\ndef _fill_z(df: pd.DataFrame, split_med: dict, gmed: float):\n    z = df[\"Z\"].copy()\n    if z.isna().any():\n        z = z.fillna(df[\"split\"].map(split_med))\n        z = z.fillna(np.float32(gmed))\n    return z.astype(\"float32\")\n\ndef _read_objq(path: str) -> pd.DataFrame:\n    df = pd.read_csv(path, dtype={\"object_id\": \"string\"})\n    df = _norm_cols(df)\n    if \"object_id\" not in df.columns:\n        raise ValueError(f\"object_quality missing object_id: {path}\")\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n\n    # drop columns that duplicate meta log content (avoid _oq collisions)\n    drop_dup = set([\"split\",\"target\",\"Z\",\"Z_err\",\"EBV\",\"has_zerr\",\"is_photoz\"])\n    keep_cols = [\"object_id\"] + [c for c in df.columns if c not in drop_dup and c != \"object_id\"]\n\n    df = df[keep_cols].copy()\n\n    # coerce numerics where possible (do not crash if some are strings)\n    for c in df.columns:\n        if c == \"object_id\":\n            continue\n        if df[c].dtype == \"O\":\n            # attempt numeric\n            df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n        # if still object, drop it (keep stage2 meta numeric)\n        if df[c].dtype == \"O\":\n            df.drop(columns=[c], inplace=True)\n\n    return df.set_index(\"object_id\", drop=True)\n\ndef _assign_splits_to_folds_greedy_multi(sp_stat: pd.DataFrame, n_folds: int, quota: int, seed: int,\n                                        lam_count: float, lam_quota: float, lam_z: float, lam_ebv: float, lam_obs: float,\n                                        penalty_zero_pos: float, restarts: int):\n    rng = np.random.default_rng(seed)\n\n    global_pos_rate = float(sp_stat[\"pos\"].sum() / max(sp_stat[\"n\"].sum(), 1))\n    target_fold_n = float(sp_stat[\"n\"].sum() / max(n_folds, 1))\n\n    wsum = float(sp_stat[\"n\"].sum())\n    g_z = float((sp_stat[\"n\"] * sp_stat[\"mean_log1pZ\"]).sum() / max(wsum, 1.0))\n    g_e = float((sp_stat[\"n\"] * sp_stat[\"mean_EBV\"]).sum() / max(wsum, 1.0))\n    g_o = float((sp_stat[\"n\"] * sp_stat[\"mean_log1pObs\"]).sum() / max(wsum, 1.0)) if \"mean_log1pObs\" in sp_stat.columns else 0.0\n\n    base_order = sp_stat.sort_values([\"pos\",\"n\"], ascending=False).reset_index(drop=True)\n\n    best = None\n\n    for _ in range(max(restarts, 1)):\n        order = base_order.copy()\n        order[\"_j\"] = rng.normal(0, 1e-6, size=len(order))\n        order = order.sort_values([\"pos\",\"n\",\"_j\"], ascending=[False,False,True]).drop(columns=[\"_j\"]).reset_index(drop=True)\n\n        fold_n = np.zeros(n_folds, dtype=float)\n        fold_pos = np.zeros(n_folds, dtype=float)\n        fold_k = np.zeros(n_folds, dtype=int)\n        fold_zsum = np.zeros(n_folds, dtype=float)\n        fold_esum = np.zeros(n_folds, dtype=float)\n        fold_osum = np.zeros(n_folds, dtype=float)\n\n        split2fold = {}\n\n        for _, r in order.iterrows():\n            sp = r[\"split\"]; n = float(r[\"n\"]); p = float(r[\"pos\"])\n            zmean = float(r[\"mean_log1pZ\"]); eme = float(r[\"mean_EBV\"])\n            omean = float(r[\"mean_log1pObs\"]) if \"mean_log1pObs\" in r else 0.0\n\n            cand = np.where(fold_k < quota)[0]\n            if len(cand) == 0:\n                cand = np.arange(n_folds)\n\n            scores = []\n            for f in cand:\n                n2 = fold_n[f] + n\n                p2 = fold_pos[f] + p\n                pr2 = (p2 / n2) if n2 > 0 else global_pos_rate\n\n                z2 = (fold_zsum[f] + n * zmean) / max(n2, 1.0)\n                e2 = (fold_esum[f] + n * eme) / max(n2, 1.0)\n                o2 = (fold_osum[f] + n * omean) / max(n2, 1.0)\n\n                score = abs(pr2 - global_pos_rate) \\\n                        + lam_count * abs(n2 - target_fold_n) / max(target_fold_n, 1.0) \\\n                        + lam_z * abs(z2 - g_z) \\\n                        + lam_ebv * abs(e2 - g_e) \\\n                        + lam_obs * abs(o2 - g_o) \\\n                        + lam_quota * (fold_k[f] / max(quota, 1))\n                scores.append(score)\n\n            scores = np.asarray(scores, dtype=float)\n            best_idx = np.where(scores == scores.min())[0]\n            choose = int(cand[int(rng.choice(best_idx))]) if len(best_idx) > 1 else int(cand[int(best_idx[0])])\n\n            split2fold[sp] = choose\n            fold_n[choose] += n\n            fold_pos[choose] += p\n            fold_k[choose] += 1\n            fold_zsum[choose] += n * zmean\n            fold_esum[choose] += n * eme\n            fold_osum[choose] += n * omean\n\n        fold_pr = np.divide(fold_pos, np.maximum(fold_n, 1e-9))\n        score = float(np.sum(np.abs(fold_pr - global_pos_rate))) \\\n                + float(lam_count * np.std(fold_n) / max(target_fold_n, 1.0))\n\n        zero_pos = int(np.sum(fold_pos == 0))\n        score += penalty_zero_pos * zero_pos\n\n        cand_pack = (score, split2fold, fold_n.copy(), fold_pos.copy(), fold_k.copy(),\n                     fold_zsum.copy(), fold_esum.copy(), fold_osum.copy(), zero_pos)\n\n        if best is None or cand_pack[0] < best[0]:\n            best = cand_pack\n\n        if best is not None and best[-1] == 0 and best[0] < 0.01:\n            break\n\n    return best\n\n# ----------------------------\n# 2) Load logs\n# ----------------------------\ndf_train = _load_or_use_global(\"df_train_log\", TRAIN_LOG_PATH)\ndf_test  = _load_or_use_global(\"df_test_log\",  TEST_LOG_PATH)\n\n# ----------------------------\n# 3) Required columns check\n# ----------------------------\nreq_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\nreq_train  = req_common | {\"target\"}\nreq_test   = req_common\n\nmiss_train = sorted(list(req_train - set(df_train.columns)))\nmiss_test  = sorted(list(req_test  - set(df_test.columns)))\nif miss_train:\n    raise ValueError(f\"train_log missing columns: {miss_train} | found={list(df_train.columns)}\")\nif miss_test:\n    raise ValueError(f\"test_log missing columns: {miss_test} | found={list(df_test.columns)}\")\n\n# ----------------------------\n# 4) Basic cleaning\n# ----------------------------\ndf_train[\"object_id\"] = df_train[\"object_id\"].astype(\"string\").str.strip()\ndf_test[\"object_id\"]  = df_test[\"object_id\"].astype(\"string\").str.strip()\n\ndf_train[\"split\"] = df_train[\"split\"].astype(\"string\").map(normalize_split_name)\ndf_test[\"split\"]  = df_test[\"split\"].astype(\"string\").map(normalize_split_name)\n\nbad_train_split = sorted(set(df_train[\"split\"].unique()) - VALID_SPLITS)\nbad_test_split  = sorted(set(df_test[\"split\"].unique())  - VALID_SPLITS)\nif bad_train_split:\n    raise ValueError(f\"train_log invalid split values: {bad_train_split[:10]}\")\nif bad_test_split:\n    raise ValueError(f\"test_log invalid split values: {bad_test_split[:10]}\")\n\n# ----------------------------\n# 5) Ensure Z_err exists + numeric coercion\n# ----------------------------\nif \"Z_err\" not in df_train.columns:\n    df_train[\"Z_err\"] = np.nan\nif \"Z_err\" not in df_test.columns:\n    df_test[\"Z_err\"] = np.nan\n\nfor c in [\"EBV\",\"Z\",\"Z_err\"]:\n    _coerce_float32(df_train, c)\n    _coerce_float32(df_test, c)\n\ndf_train[\"has_zerr\"] = (~pd.to_numeric(df_train[\"Z_err\"], errors=\"coerce\").isna()).astype(\"int8\")\ndf_test[\"has_zerr\"]  = (~pd.to_numeric(df_test[\"Z_err\"],  errors=\"coerce\").isna()).astype(\"int8\")\ndf_train[\"is_photoz\"] = df_train[\"has_zerr\"].astype(\"int8\")\ndf_test[\"is_photoz\"]  = df_test[\"has_zerr\"].astype(\"int8\")\n\n# ----------------------------\n# 6) Duplicate / overlap checks\n# ----------------------------\nif df_train[\"object_id\"].duplicated().any():\n    ex = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n    raise ValueError(f\"Duplicated object_id in train_log (examples): {ex}\")\nif df_test[\"object_id\"].duplicated().any():\n    ex = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n    raise ValueError(f\"Duplicated object_id in test_log (examples): {ex}\")\n\noverlap = set(df_train[\"object_id\"].tolist()) & set(df_test[\"object_id\"].tolist())\nif overlap:\n    raise ValueError(f\"object_id overlap train vs test (examples): {list(overlap)[:5]}\")\n\n# ----------------------------\n# 7) Target validation\n# ----------------------------\ndf_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\nif df_train[\"target\"].isna().any():\n    raise ValueError(f\"train_log target has NaN after coercion: {int(df_train['target'].isna().sum())} rows.\")\nuniq_t = set(pd.unique(df_train[\"target\"]).tolist())\nif not uniq_t.issubset({0,1}):\n    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\ndf_train[\"target\"] = df_train[\"target\"].astype(\"int8\")\n\n# ----------------------------\n# 8) Missing flags + fills (safe)\n# ----------------------------\nfor df in [df_train, df_test]:\n    df[\"EBV_missing\"]  = df[\"EBV\"].isna().astype(\"int8\")\n    df[\"Z_missing\"]    = df[\"Z\"].isna().astype(\"int8\")\n    df[\"Zerr_missing\"] = df[\"Z_err\"].isna().astype(\"int8\")\n\n# fill EBV with train median\nebv_med = float(np.nanmedian(df_train[\"EBV\"].values.astype(float))) if np.isfinite(df_train[\"EBV\"].values.astype(float)).any() else 0.0\ndf_train[\"EBV\"] = df_train[\"EBV\"].fillna(np.float32(ebv_med)).astype(\"float32\")\ndf_test[\"EBV\"]  = df_test[\"EBV\"].fillna(np.float32(ebv_med)).astype(\"float32\")\n\n# fill Z with split median then global median (from train only)\ntrain_split_med = df_train.groupby(\"split\")[\"Z\"].median().to_dict()\ntrain_gmed = float(np.nanmedian(df_train[\"Z\"].values.astype(float))) if np.isfinite(df_train[\"Z\"].values.astype(float)).any() else 0.0\ndf_train[\"Z\"] = _fill_z(df_train, train_split_med, train_gmed)\ndf_test[\"Z\"]  = _fill_z(df_test,  train_split_med, train_gmed)\n\n# fill Z_err with 0 (keep has_zerr as flag)\ndf_train[\"Z_err\"] = df_train[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\ndf_test[\"Z_err\"]  = df_test[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n\n# ----------------------------\n# 9) Clipping + derived meta features\n# ----------------------------\nEBV_LO, EBV_HI = _qclip_bounds(df_train[\"EBV\"].values, QLO, QHI)\nZ_LO,   Z_HI   = _qclip_bounds(df_train[\"Z\"].values,   QLO, QHI)\n\ndf_train[\"EBV_clip\"] = _safe_clip(df_train[\"EBV\"], EBV_LO, EBV_HI)\ndf_test[\"EBV_clip\"]  = _safe_clip(df_test[\"EBV\"],  EBV_LO, EBV_HI)\n\ndf_train[\"Z_clip\"] = _safe_clip(df_train[\"Z\"], Z_LO, Z_HI)\ndf_test[\"Z_clip\"]  = _safe_clip(df_test[\"Z\"],  Z_LO, Z_HI)\n\n# Z_err clip: prefer train photo-z pool, fallback to test photo-z pool\nZE_LO = 0.0\npool_tr = df_train.loc[df_train[\"has_zerr\"] == 1, \"Z_err\"].values\npool_te = df_test.loc[df_test[\"has_zerr\"] == 1, \"Z_err\"].values\nif np.isfinite(pool_tr.astype(float)).any():\n    _, ZE_HI = _qclip_bounds(pool_tr, QLO, QHI, default=(0.0, 0.0))\nelif np.isfinite(pool_te.astype(float)).any():\n    _, ZE_HI = _qclip_bounds(pool_te, QLO, QHI, default=(0.0, 0.0))\nelse:\n    ZE_HI = 0.0\nZE_HI = max(float(ZE_HI), 0.0)\n\ndf_train[\"Zerr_clip\"] = _safe_clip(df_train[\"Z_err\"], ZE_LO, ZE_HI)\ndf_test[\"Zerr_clip\"]  = _safe_clip(df_test[\"Z_err\"],  ZE_LO, ZE_HI)\n\neps = np.float32(1e-6)\n\nfor df in [df_train, df_test]:\n    df[\"log1pZ\"] = np.log1p(df[\"Z_clip\"]).astype(\"float32\")\n    df[\"inv_1pz\"] = (1.0 / (1.0 + df[\"Z_clip\"])).astype(\"float32\")\n    df[\"z2\"] = (df[\"Z_clip\"] * df[\"Z_clip\"]).astype(\"float32\")\n\n    df[\"log1pEBV\"] = np.log1p(df[\"EBV_clip\"]).astype(\"float32\")\n    df[\"ebv_over_1pz\"] = (df[\"EBV_clip\"] / (1.0 + df[\"Z_clip\"] + eps)).astype(\"float32\")\n    df[\"ebv_x_1pz\"] = (df[\"EBV_clip\"] * (1.0 + df[\"Z_clip\"])).astype(\"float32\")\n\n    df[\"log1pZerr\"] = np.log1p(df[\"Zerr_clip\"]).astype(\"float32\")\n    # safer denom\n    df[\"zerr_over_1pz\"] = (df[\"Zerr_clip\"] / (1.0 + df[\"Z_clip\"] + eps)).astype(\"float32\")\n    df[\"zerr_rel_z\"] = (df[\"Zerr_clip\"] / (df[\"Z_clip\"] + eps)).astype(\"float32\")\n    df[\"high_zerr\"] = (df[\"zerr_over_1pz\"] > np.float32(0.30)).astype(\"int8\")  # tunable\n\nsplit2id = {f\"split_{i:02d}\": i for i in range(1, 21)}\ndf_train[\"split_id\"] = df_train[\"split\"].map(split2id).astype(\"int16\")\ndf_test[\"split_id\"]  = df_test[\"split\"].map(split2id).astype(\"int16\")\n\n# ----------------------------\n# 10) JOIN object_quality from STAGE1 (lebih awal, bisa dipakai fold balancing juga)\n# ----------------------------\nobjq_joined = False\noq_train_cols = []\noq_test_cols = []\n\nif OBJQ_TRAIN and Path(OBJQ_TRAIN).exists():\n    oq_tr = _read_objq(OBJQ_TRAIN)\n    oq_train_cols = oq_tr.columns.tolist()\n    df_train = df_train.set_index(\"object_id\", drop=False).join(oq_tr, how=\"left\").reset_index(drop=True)\n    objq_joined = True\n\nif OBJQ_TEST and Path(OBJQ_TEST).exists():\n    oq_te = _read_objq(OBJQ_TEST)\n    oq_test_cols = oq_te.columns.tolist()\n    df_test = df_test.set_index(\"object_id\", drop=False).join(oq_te, how=\"left\").reset_index(drop=True)\n    objq_joined = True\n\n# unify objq cols intersection (train/test) for meta keep\noq_cols_common = sorted(list(set(oq_train_cols) & set(oq_test_cols)))\n\n# fill + alias safety\nif objq_joined and len(oq_cols_common) > 0:\n    for df in [df_train, df_test]:\n        # default fills\n        if \"n_obs_total\" in df.columns:\n            df[\"n_obs_total\"] = pd.to_numeric(df[\"n_obs_total\"], errors=\"coerce\").fillna(0).astype(\"int32\")\n            df[\"low_obs\"] = (df[\"n_obs_total\"] < 20).astype(\"int8\")\n        if \"n_bands_present\" in df.columns:\n            df[\"n_bands_present\"] = pd.to_numeric(df[\"n_bands_present\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n            df[\"low_bandcov\"] = (df[\"n_bands_present\"] <= 2).astype(\"int8\")\n\n        # alias det/strong fractions if stage1 v5.1 naming present\n        cand_det = f\"snr_det_abs_frac_{BASE_DET_KEY}\"\n        if (\"snr_det_frac\" not in df.columns) and (cand_det in df.columns):\n            df[\"snr_det_frac\"] = pd.to_numeric(df[cand_det], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n        if (\"snr_strong_frac\" not in df.columns) and (\"snr_strong_abs_frac\" in df.columns):\n            df[\"snr_strong_frac\"] = pd.to_numeric(df[\"snr_strong_abs_frac\"], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n\n        # common numeric fills\n        for c in [\"timespan\",\"cadence_proxy\",\"neg_flux_frac\",\"snr_det_frac\",\"snr_strong_frac\",\n                  \"snr_abs_max\",\"snr_pos_max\",\"snr_neg_min\",\"flux_mean\",\"flux_std\",\"ferr_mean\",\n                  \"timespan_rest\",\"cadence_proxy_rest\",\"zerr_rel\"]:\n            if c in df.columns:\n                df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n\n        # per band fracs\n        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n            c = f\"frac_{b}\"\n            if c in df.columns:\n                df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n            c2 = f\"det_frac_{b}\"\n            if c2 in df.columns:\n                df[c2] = pd.to_numeric(df[c2], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n\n# ----------------------------\n# 11) Fold assignment (split-aware, balanced incl Z/EBV (+ optional obs))\n# ----------------------------\ndf_train[\"fold\"] = -1\n\nif CV_USE_SPLIT_COL:\n    sp_pos = df_train.groupby(\"split\")[\"target\"].sum().reindex(SPLIT_LIST).fillna(0).astype(int)\n    sp_n   = df_train.groupby(\"split\")[\"target\"].count().reindex(SPLIT_LIST).fillna(0).astype(int)\n\n    sp_zm  = df_train.groupby(\"split\")[\"log1pZ\"].mean().reindex(SPLIT_LIST).fillna(0.0).astype(\"float32\")\n    sp_em  = df_train.groupby(\"split\")[\"EBV_clip\"].mean().reindex(SPLIT_LIST).fillna(0.0).astype(\"float32\")\n\n    # optional obs summary if objq exists\n    if \"n_obs_total\" in df_train.columns:\n        sp_om = df_train.groupby(\"split\")[\"n_obs_total\"].mean().reindex(SPLIT_LIST).fillna(0.0).astype(\"float32\")\n        sp_om = np.log1p(sp_om).astype(\"float32\")\n    else:\n        sp_om = pd.Series(np.zeros(len(SPLIT_LIST), dtype=\"float32\"), index=SPLIT_LIST)\n\n    sp_stat = pd.DataFrame({\n        \"split\": SPLIT_LIST,\n        \"n\": sp_n.values.astype(int),\n        \"pos\": sp_pos.values.astype(int),\n        \"mean_log1pZ\": sp_zm.values.astype(float),\n        \"mean_EBV\": sp_em.values.astype(float),\n        \"mean_log1pObs\": sp_om.values.astype(float),\n    })\n\n    best = _assign_splits_to_folds_greedy_multi(\n        sp_stat, n_folds=N_FOLDS, quota=FOLD_QUOTA, seed=SEED,\n        lam_count=LAMBDA_COUNT, lam_quota=LAMBDA_QUOTA,\n        lam_z=LAMBDA_ZMEAN, lam_ebv=LAMBDA_EBVMEAN, lam_obs=LAMBDA_OBSMEAN,\n        penalty_zero_pos=PENALTY_ZERO_POS, restarts=RESTARTS\n    )\n    if best is None:\n        raise RuntimeError(\"split->fold assignment failed unexpectedly.\")\n\n    if best[-1] > 0 and RESTARTS_HARD > RESTARTS:\n        best2 = _assign_splits_to_folds_greedy_multi(\n            sp_stat, n_folds=N_FOLDS, quota=FOLD_QUOTA, seed=SEED + 999,\n            lam_count=LAMBDA_COUNT, lam_quota=LAMBDA_QUOTA,\n            lam_z=LAMBDA_ZMEAN, lam_ebv=LAMBDA_EBVMEAN, lam_obs=LAMBDA_OBSMEAN,\n            penalty_zero_pos=PENALTY_ZERO_POS, restarts=RESTARTS_HARD\n        )\n        if best2 is not None and best2[0] <= best[0]:\n            best = best2\n\n    score, split2fold, fold_n, fold_pos, fold_k, *_rest, zero_pos = best\n    df_train[\"fold\"] = df_train[\"split\"].map(split2fold).astype(\"int16\")\n\n    uniq_folds = sorted(df_train[\"fold\"].unique().tolist())\n    if uniq_folds != list(range(N_FOLDS)):\n        print(f\"[WARN] split->fold tidak memakai semua fold. uniq_folds={uniq_folds}. Fallback StratifiedKFold.\")\n        CV_USE_SPLIT_COL = False\n    else:\n        with open(ART_DIR / \"split2fold.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump({k:int(v) for k,v in split2fold.items()}, f)\n\nif not CV_USE_SPLIT_COL:\n    from sklearn.model_selection import StratifiedKFold\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    y = df_train[\"target\"].to_numpy()\n    idx = np.arange(len(df_train))\n    for fold_id, (_, va_idx) in enumerate(skf.split(idx, y)):\n        df_train.iloc[va_idx, df_train.columns.get_loc(\"fold\")] = fold_id\n    df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n\nif (df_train[\"fold\"] < 0).any():\n    n_bad = int((df_train[\"fold\"] < 0).sum())\n    raise RuntimeError(f\"Fold assignment gagal: ada {n_bad} baris fold=-1\")\n\nfold_tab = (\n    df_train.groupby(\"fold\")[\"target\"]\n    .agg([\"count\",\"sum\"])\n    .rename(columns={\"sum\":\"pos\"})\n    .reindex(range(N_FOLDS)).fillna(0)\n)\nfold_tab[\"pos_rate\"] = fold_tab[\"pos\"] / fold_tab[\"count\"].clip(lower=1)\n\n# ----------------------------\n# 12) OOF split prior (smoothed)\n# ----------------------------\ng_pos = float(df_train[\"target\"].sum())\ng_n = float(len(df_train))\ng_rate = g_pos / max(g_n, 1.0)\n\nsp_all = df_train.groupby(\"split\")[\"target\"].agg([\"count\",\"sum\"]).rename(columns={\"count\":\"n\",\"sum\":\"pos\"})\nsp_all[\"prior\"] = (sp_all[\"pos\"] + PRIOR_ALPHA * g_rate) / (sp_all[\"n\"] + PRIOR_ALPHA)\n\ndf_test[\"split_pos_prior\"] = df_test[\"split\"].map(sp_all[\"prior\"]).fillna(g_rate).astype(\"float32\")\n\ndf_train[\"split_pos_prior_oof\"] = np.float32(g_rate)\nfor f in range(N_FOLDS):\n    tr_idx = df_train[\"fold\"] != f\n    sp_f = df_train.loc[tr_idx].groupby(\"split\")[\"target\"].agg([\"count\",\"sum\"]).rename(columns={\"count\":\"n\",\"sum\":\"pos\"})\n    g_rate_f = float(df_train.loc[tr_idx, \"target\"].sum() / max(tr_idx.sum(), 1))\n    sp_f[\"prior\"] = (sp_f[\"pos\"] + PRIOR_ALPHA * g_rate_f) / (sp_f[\"n\"] + PRIOR_ALPHA)\n    m = df_train[\"fold\"] == f\n    df_train.loc[m, \"split_pos_prior_oof\"] = df_train.loc[m, \"split\"].map(sp_f[\"prior\"]).fillna(g_rate_f).astype(\"float32\")\n\ndf_train[\"split_pos_prior\"] = df_train[\"split\"].map(sp_all[\"prior\"]).fillna(g_rate).astype(\"float32\")\n\n# ----------------------------\n# 13) Build meta tables (index=object_id)\n# ----------------------------\nbase_train_cols = [\n    \"object_id\",\"split\",\"split_id\",\n    \"EBV\",\"EBV_clip\",\"log1pEBV\",\"ebv_over_1pz\",\"ebv_x_1pz\",\n    \"Z\",\"Z_clip\",\"log1pZ\",\"inv_1pz\",\"z2\",\n    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_over_1pz\",\"zerr_rel_z\",\"high_zerr\",\n    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n    \"split_pos_prior\",\"split_pos_prior_oof\",\n    \"fold\",\"target\"\n]\nbase_test_cols = [\n    \"object_id\",\"split\",\"split_id\",\n    \"EBV\",\"EBV_clip\",\"log1pEBV\",\"ebv_over_1pz\",\"ebv_x_1pz\",\n    \"Z\",\"Z_clip\",\"log1pZ\",\"inv_1pz\",\"z2\",\n    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_over_1pz\",\"zerr_rel_z\",\"high_zerr\",\n    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n    \"split_pos_prior\"\n]\n\n# choose objq feature columns that exist in BOTH train/test\nobjq_cols = []\nfor c in oq_cols_common:\n    if c in df_train.columns and c in df_test.columns:\n        objq_cols.append(c)\n\n# ensure alias cols included if created\nfor c in [\"low_obs\",\"low_bandcov\",\"snr_det_frac\",\"snr_strong_frac\"]:\n    if (c in df_train.columns) and (c in df_test.columns) and (c not in objq_cols):\n        objq_cols.append(c)\n\nkeep_train = [c for c in base_train_cols if c in df_train.columns] + objq_cols\nkeep_test  = [c for c in base_test_cols  if c in df_test.columns]  + objq_cols\n\ndf_train_meta = df_train[keep_train].copy().set_index(\"object_id\", drop=True).sort_index()\ndf_test_meta  = df_test[keep_test].copy().set_index(\"object_id\", drop=True).sort_index()\n\nif not df_train_meta.index.is_unique:\n    raise RuntimeError(\"df_train_meta index (object_id) not unique after processing.\")\nif not df_test_meta.index.is_unique:\n    raise RuntimeError(\"df_test_meta index (object_id) not unique after processing.\")\n\nid2split_train = df_train_meta[\"split\"].to_dict()\nid2split_test  = df_test_meta[\"split\"].to_dict()\n\n# ----------------------------\n# 14) Save artifacts\n# ----------------------------\ntrain_pq = ART_DIR / \"train_meta.parquet\"\ntest_pq  = ART_DIR / \"test_meta.parquet\"\ntrain_csv = ART_DIR / \"train_meta.csv\"\ntest_csv  = ART_DIR / \"test_meta.csv\"\n\ntry:\n    df_train_meta.to_parquet(train_pq, index=True)\n    df_test_meta.to_parquet(test_pq, index=True)\n    saved_train, saved_test = str(train_pq), str(test_pq)\nexcept Exception:\n    df_train_meta.to_csv(train_csv, index=True)\n    df_test_meta.to_csv(test_csv, index=True)\n    saved_train, saved_test = str(train_csv), str(test_csv)\n\nsplit_stats = pd.DataFrame({\n    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n})\nsplit_stats.index.name = \"split\"\npos_by_split = df_train_meta.groupby(\"split\")[\"target\"].sum().reindex(SPLIT_LIST).fillna(0).astype(int)\nsplit_stats[\"train_pos\"] = pos_by_split.values\nsplit_stats[\"train_pos_rate\"] = (split_stats[\"train_pos\"] / split_stats[\"train_objects\"].clip(lower=1)).astype(\"float32\")\nsplit_stats_path = ART_DIR / \"split_stats.csv\"\nsplit_stats.to_csv(split_stats_path)\n\nfold_path = ART_DIR / \"train_folds.csv\"\ndf_train_meta.reset_index()[[\"object_id\",\"split\",\"fold\",\"target\"]].to_csv(fold_path, index=False)\n\nwith open(ART_DIR / \"id2split_train.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(id2split_train, f)\nwith open(ART_DIR / \"id2split_test.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(id2split_test, f)\n\ndrop_nonfeat = set([\"target\",\"fold\"])\nmeta_feature_cols = [c for c in df_train_meta.columns.tolist() if c not in drop_nonfeat]\nwith open(ART_DIR / \"meta_feature_cols.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(meta_feature_cols, f, indent=2)\n\npos = int((df_train_meta[\"target\"] == 1).sum())\nneg = int((df_train_meta[\"target\"] == 0).sum())\ntot = int(len(df_train_meta))\npos_rate = pos / max(tot, 1)\nscale_pos_weight = float(neg / max(pos, 1))\n\nstage2_summary = {\n    \"stage\": \"stage2\",\n    \"N_FOLDS\": int(N_FOLDS),\n    \"CV_USE_SPLIT_COL_USED\": bool(CV_USE_SPLIT_COL),\n    \"OBJQ_JOINED\": bool(objq_joined),\n    \"objq_cols_used\": int(len(objq_cols)),\n    \"counts\": {\n        \"train\": int(tot),\n        \"pos\": int(pos),\n        \"neg\": int(neg),\n        \"pos_rate\": float(pos_rate),\n        \"test\": int(len(df_test_meta))\n    },\n    \"clip_ranges\": {\n        \"EBV_train\": [float(EBV_LO), float(EBV_HI)],\n        \"Z_train\": [float(Z_LO), float(Z_HI)],\n        \"Zerr_used\": [float(ZE_LO), float(ZE_HI)]\n    },\n    \"split_prior\": {\n        \"alpha\": float(PRIOR_ALPHA),\n        \"global_pos_rate\": float(g_rate)\n    },\n    \"scale_pos_weight\": float(scale_pos_weight),\n    \"base_det_thr\": float(BASE_DET_THR),\n    \"artifacts\": {\n        \"train_meta\": saved_train,\n        \"test_meta\": saved_test,\n        \"split_stats\": str(split_stats_path),\n        \"train_folds\": str(fold_path),\n        \"id2split_train\": str(ART_DIR / \"id2split_train.json\"),\n        \"id2split_test\": str(ART_DIR / \"id2split_test.json\"),\n        \"split2fold\": str(ART_DIR / \"split2fold.json\") if (ART_DIR / \"split2fold.json\").exists() else None,\n        \"meta_feature_cols\": str(ART_DIR / \"meta_feature_cols.json\"),\n    }\n}\nwith open(ART_DIR / \"stage2_summary.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(stage2_summary, f, indent=2)\n\n# ----------------------------\n# 15) Print summary\n# ----------------------------\nprint(\"STAGE 2 OK — META READY (clean + folds + enriched)\")\nprint(f\"- CV_USE_SPLIT_COL_USED: {CV_USE_SPLIT_COL} | N_FOLDS={N_FOLDS} | split_quota={FOLD_QUOTA}\")\nprint(f\"- OBJQ_JOINED: {objq_joined} | objq_cols_used={len(objq_cols)} | base_det_thr={BASE_DET_THR}\")\nprint(f\"- train objects: {tot:,} | pos={pos:,} | neg={neg:,} | pos%={pos_rate*100:.3f}%\")\nprint(f\"- test objects : {len(df_test_meta):,}\")\nprint(f\"- saved train  : {saved_train}\")\nprint(f\"- saved test   : {saved_test}\")\nprint(f\"- saved stats  : {split_stats_path}\")\nprint(f\"- saved folds  : {fold_path}\")\nprint(f\"- saved meta_feature_cols: {ART_DIR / 'meta_feature_cols.json'}\")\nprint(f\"- scale_pos_weight (neg/pos): {scale_pos_weight:.3f}\")\n\nprint(\"\\nCLIP RANGES\")\nprint(f\"- EBV clip (train): [{EBV_LO:.6f}, {EBV_HI:.6f}]\")\nprint(f\"- Z   clip (train): [{Z_LO:.6f}, {Z_HI:.6f}]\")\nprint(f\"- Zerr clip used  : [{ZE_LO:.6f}, {ZE_HI:.6f}]\")\n\nprint(\"\\nFOLD BALANCE (count/pos/pos_rate)\")\nprint(fold_tab.to_string())\n\n# ----------------------------\n# 16) Export globals\n# ----------------------------\nglobals().update({\n    \"df_train_meta\": df_train_meta,\n    \"df_test_meta\": df_test_meta,\n    \"id2split_train\": id2split_train,\n    \"id2split_test\": id2split_test,\n    \"split_stats\": split_stats,\n    \"split2id\": split2id,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"CV_USE_SPLIT_COL_USED\": CV_USE_SPLIT_COL,\n    \"META_FEATURE_COLS_PATH\": str(ART_DIR / \"meta_feature_cols.json\"),\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Lightcurve Loading Strategy","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — Robust Lightcurve Loader Utilities + FULL Object-Quality Build (ONE CELL)\n# REVISI FULL v5.4 (UPGRADE dari v5.3: filter numeric-safe + OBJQ sanitize hard + smoke test anti-StopIteration)\n#\n# Upgrade v5.4 (dibanding v5.3):\n# - FIX filter \"0..5\" (string/angka) -> map ke u,g,r,i,z,y (tidak kebuang karena invalid)\n# - OBJQ join makin aman:\n#   * drop kolom \"Unnamed:*\"\n#   * drop kolom meta-like (heuristic) walau tidak overlap\n#   * keep only kolom objq valid (whitelist) biar objq csv kotor tidak nyusup ke meta\n#   * numeric coercion + dedup aggregator aman\n# - Smoke test: handle StopIteration (kalau chunk pertama habis terfilter / file kosong)\n#\n# Tetap:\n# - FAST dtype mode (float32 parse) + fallback SAFE mode (coerce)\n# - FULL Object Quality builder (streaming per split)\n# - Output: logs/object_quality_train.csv + logs/object_quality_test.csv\n# - Auto-join ke df_train_meta / df_test_meta + overwrite meta di ART_DIR\n# - Loader functions: iter_lightcurve_chunks, load_object_lightcurve, load_many_object_lightcurves, NPZ cache\n#\n# Output globals:\n# - OBJECT_QUALITY_TRAIN_PATH, OBJECT_QUALITY_TEST_PATH\n# - df_train_meta, df_test_meta updated (enriched)\n# ============================================================\n\nimport gc, re, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed_prev = [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"CFG\", \"SEED\", \"LOG_DIR\"]\nfor need in need_prev:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n\nART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\nSEED = int(SEED)\n\n# Optional CACHE_DIR (dari STAGE 0). Kalau tidak ada, fallback ke ART_DIR/cache\nCACHE_DIR = Path(globals().get(\"CACHE_DIR\", ART_DIR / \"cache\"))\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\n\n# Core config\nMIN_FLUXERR = float(CFG.get(\"MIN_FLUXERR\", 1e-6))\nCHUNK_ROWS_DEFAULT = int(CFG.get(\"CHUNK_ROWS\", 400_000))\nSNR_CLIP = float(CFG.get(\"SNR_CLIP\", 30.0))\nSNR_DET_THR = float(CFG.get(\"SNR_DET_THR\", 3.0))\nSNR_STRONG_THR = float(CFG.get(\"SNR_STRONG_THR\", 5.0))\n\n# Object-quality builder config (BRUTAL but CPU-safe)\nBUILD_OBJECT_QUALITY = bool(CFG.get(\"BUILD_OBJECT_QUALITY\", True))\nOBJQ_REFRESH = bool(CFG.get(\"OBJQ_REFRESH\", False))\nOBJQ_CHUNK_ROWS = int(CFG.get(\"OBJQ_CHUNK_ROWS\", max(200_000, CHUNK_ROWS_DEFAULT)))\nOBJQ_SAVE_PARQUET = bool(CFG.get(\"OBJQ_SAVE_PARQUET\", False))  # default csv for portability\nOBJQ_ONLY_IF_MISSING = bool(CFG.get(\"OBJQ_ONLY_IF_MISSING\", True))  # don't rebuild if files exist (unless refresh)\n\n# konsisten dengan STAGE 0/2\nSAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\nSAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n\nREQ_LC_KEYS = [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]\nALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\nALLOWED_FILTERS_TUP = (\"u\", \"g\", \"r\", \"i\", \"z\", \"y\")\nFILTER_ORDER = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\nFILTER2ID = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\nID2FILTER = {v:k for k,v in FILTER2ID.items()}\n\n# v5.4: support numeric filters \"0..5\"\nFILTER_NUM2STR = {\"0\":\"u\",\"1\":\"g\",\"2\":\"r\",\"3\":\"i\",\"4\":\"z\",\"5\":\"y\"}\n\n# Cache configs\n_LC_CFG_CACHE = {}  # (split_name, which) -> cfg dict\n_LC_OBJ_CACHE = {}  # in-memory small cache: (which, object_id) -> df\nLC_CACHE_DIR = CACHE_DIR / \"lightcurves_npz\"\nLC_CACHE_DIR.mkdir(parents=True, exist_ok=True)\nMAX_MEM_CACHE = int(CFG.get(\"LC_MEM_CACHE_MAX\", 64))  # max objects in RAM cache\n\n# ----------------------------\n# Utils: ID normalize (robust)\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_, bytearray)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\n# Normalize meta indices early (prevent mismatch)\ndf_train_meta = df_train_meta.copy(deep=False)\ndf_test_meta  = df_test_meta.copy(deep=False)\ndf_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index.tolist()], name=df_train_meta.index.name)\ndf_test_meta.index  = pd.Index([_norm_id(z) for z in df_test_meta.index.tolist()], name=df_test_meta.index.name)\n\nif \"split\" not in df_train_meta.columns or \"split\" not in df_test_meta.columns:\n    raise RuntimeError(\"Missing column `split` in df_train_meta/df_test_meta. Pastikan STAGE 2 membuat routing split.\")\n\n# ----------------------------\n# 1) Build split file mapping (train/test lightcurves)\n# ----------------------------\nSPLIT_FILES = {}\nfor s in SPLIT_LIST:\n    sd = Path(SPLIT_DIRS[s])\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    if (not tr.exists()) or (not te.exists()):\n        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n\n# Save split file manifest\nmanifest = []\nfor s in SPLIT_LIST:\n    p_tr = SPLIT_FILES[s][\"train\"]\n    p_te = SPLIT_FILES[s][\"test\"]\n    manifest.append({\n        \"split\": s,\n        \"train_path\": str(p_tr),\n        \"test_path\": str(p_te),\n        \"train_mb\": float(p_tr.stat().st_size) / (1024**2),\n        \"test_mb\":  float(p_te.stat().st_size) / (1024**2),\n    })\ndf_manifest = pd.DataFrame(manifest).sort_values(\"split\")\nmanifest_path = ART_DIR / \"split_file_manifest.csv\"\ndf_manifest.to_csv(manifest_path, index=False)\n\n# ----------------------------\n# 2) Build object routing by split\n# ----------------------------\ntrain_ids_by_split = {s: [] for s in SPLIT_LIST}\ntest_ids_by_split  = {s: [] for s in SPLIT_LIST}\n\ntr_groups = df_train_meta.groupby(\"split\").groups\nte_groups = df_test_meta.groupby(\"split\").groups\n\nfor sp, idx in tr_groups.items():\n    if sp in train_ids_by_split:\n        train_ids_by_split[sp] = pd.Index(idx).astype(str).map(_norm_id).tolist()\n\nfor sp, idx in te_groups.items():\n    if sp in test_ids_by_split:\n        test_ids_by_split[sp] = pd.Index(idx).astype(str).map(_norm_id).tolist()\n\nif sum(len(v) for v in train_ids_by_split.values()) != len(df_train_meta):\n    raise RuntimeError(\"Routing train_ids_by_split mismatch total vs df_train_meta length.\")\nif sum(len(v) for v in test_ids_by_split.values()) != len(df_test_meta):\n    raise RuntimeError(\"Routing test_ids_by_split mismatch total vs df_test_meta length.\")\n\ndf_counts = pd.DataFrame({\n    \"split\": SPLIT_LIST,\n    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n})\ncounts_path = ART_DIR / \"object_counts_by_split.csv\"\ndf_counts.to_csv(counts_path, index=False)\n\n# ----------------------------\n# 3) Robust header mapping -> canonical columns (FAST dtype + SAFE fallback)\n# ----------------------------\ndef _canon_col(x: str) -> str:\n    s = str(x).strip().lower()\n    s = s.replace(\"\\ufeff\", \"\")\n    s = re.sub(r\"\\s+\", \"\", s)\n    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n    s = s.replace(\"-\", \"_\")\n    return s\n\ndef _build_lc_read_cfg(p: Path):\n    h = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n    orig_cols = list(h.columns)\n\n    c2o = {}\n    for c in orig_cols:\n        k = _canon_col(c)\n        if k not in c2o:\n            c2o[k] = c\n\n    obj_col = c2o.get(\"object_id\", None)\n\n    time_col = None\n    for k in [\"time_mjd\", \"timemjd\", \"mjd\", \"time\"]:\n        if k in c2o:\n            time_col = c2o[k]\n            break\n\n    flux_col = c2o.get(\"flux\", None)\n\n    ferr_col = None\n    for k in [\"flux_err\", \"fluxerr\", \"fluxerror\"]:\n        if k in c2o:\n            ferr_col = c2o[k]\n            break\n\n    filt_col = c2o.get(\"filter\", None)\n\n    missing = []\n    if obj_col is None:  missing.append(\"object_id\")\n    if time_col is None: missing.append(\"Time (MJD)\")\n    if flux_col is None: missing.append(\"Flux\")\n    if ferr_col is None: missing.append(\"Flux_err\")\n    if filt_col is None: missing.append(\"Filter\")\n    if missing:\n        raise ValueError(\n            f\"Missing required lightcurve columns in {p.name}: {missing}. \"\n            f\"Header sample: {orig_cols[:20]}\"\n        )\n\n    usecols = [obj_col, time_col, flux_col, ferr_col, filt_col]\n    rename = {obj_col:\"object_id\", time_col:\"mjd\", flux_col:\"flux\", ferr_col:\"flux_err\", filt_col:\"filter\"}\n\n    dtype_fast = {\n        obj_col: \"string\",\n        filt_col: \"string\",\n        time_col: \"float32\",\n        flux_col: \"float32\",\n        ferr_col: \"float32\",\n    }\n    dtype_safe = {obj_col: \"string\", filt_col: \"string\"}\n\n    return {\"usecols\": usecols, \"dtype_fast\": dtype_fast, \"dtype_safe\": dtype_safe, \"rename\": rename}\n\ndef _normalize_lc_chunk(\n    df: pd.DataFrame,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n    encode_filter: bool = False,\n):\n    df = df[[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"]].copy()\n\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n\n    # v5.4: filter robust (support numeric 0..5)\n    f = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n    f = f.replace(FILTER_NUM2STR)  # \"0\"->\"u\", ...\n    df[\"filter\"] = f\n\n    df.loc[~df[\"filter\"].isin(ALLOWED_FILTERS_TUP), \"filter\"] = pd.NA\n\n    # numeric coercion (SAFE path)\n    if df[\"mjd\"].dtype == \"O\" or str(df[\"mjd\"].dtype).startswith(\"string\"):\n        df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\")\n    if df[\"flux\"].dtype == \"O\" or str(df[\"flux\"].dtype).startswith(\"string\"):\n        df[\"flux\"] = pd.to_numeric(df[\"flux\"], errors=\"coerce\")\n    if df[\"flux_err\"].dtype == \"O\" or str(df[\"flux_err\"].dtype).startswith(\"string\"):\n        df[\"flux_err\"] = pd.to_numeric(df[\"flux_err\"], errors=\"coerce\")\n\n    df[\"mjd\"] = df[\"mjd\"].astype(\"float32\")\n    df[\"flux\"] = df[\"flux\"].astype(\"float32\")\n    df[\"flux_err\"] = df[\"flux_err\"].astype(\"float32\")\n\n    # Guard flux_err\n    fe = df[\"flux_err\"]\n    if drop_bad_fluxerr:\n        df = df[fe.notna()]\n        fe = df[\"flux_err\"]\n        df = df[fe > 0]\n        fe = df[\"flux_err\"]\n\n    if MIN_FLUXERR > 0:\n        fe = df[\"flux_err\"]\n        df.loc[fe.notna() & (fe < MIN_FLUXERR), \"flux_err\"] = np.float32(MIN_FLUXERR)\n\n    # Drop empty id\n    df = df[df[\"object_id\"].notna() & (df[\"object_id\"] != \"\")]\n\n    if drop_bad_filter:\n        df = df[df[\"filter\"].notna()]\n    if drop_bad_mjd:\n        df = df[df[\"mjd\"].notna()]\n\n    df = df[REQ_LC_KEYS]\n\n    if encode_filter:\n        df = df.copy()\n        df[\"filter_id\"] = df[\"filter\"].map(FILTER2ID).astype(\"int8\")\n\n    return df\n\n# ----------------------------\n# 4) Chunked readers\n# ----------------------------\ndef iter_lightcurve_chunks(\n    split_name: str,\n    which: str,\n    chunksize: int = None,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n    encode_filter: bool = False,\n):\n    split_name = str(split_name).strip()\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Unknown split_name={split_name}. Known={list(SPLIT_FILES.keys())[:5]}..\")\n    if which not in (\"train\", \"test\"):\n        raise ValueError(\"which must be 'train' or 'test'\")\n\n    if chunksize is None:\n        chunksize = CHUNK_ROWS_DEFAULT\n\n    p = SPLIT_FILES[split_name][which]\n    key = (split_name, which)\n    if key not in _LC_CFG_CACHE:\n        _LC_CFG_CACHE[key] = _build_lc_read_cfg(p)\n    cfg = _LC_CFG_CACHE[key]\n\n    try:\n        reader = pd.read_csv(\n            p,\n            usecols=cfg[\"usecols\"],\n            dtype=cfg[\"dtype_fast\"],\n            chunksize=int(chunksize),\n            engine=\"c\",\n            memory_map=True,\n            **SAFE_READ_KW\n        )\n        for chunk in reader:\n            chunk = chunk.rename(columns=cfg[\"rename\"])\n            yield _normalize_lc_chunk(\n                chunk,\n                drop_bad_filter=drop_bad_filter,\n                drop_bad_mjd=drop_bad_mjd,\n                drop_bad_fluxerr=drop_bad_fluxerr,\n                encode_filter=encode_filter,\n            )\n    except Exception:\n        reader = pd.read_csv(\n            p,\n            usecols=cfg[\"usecols\"],\n            dtype=cfg[\"dtype_safe\"],\n            chunksize=int(chunksize),\n            engine=\"c\",\n            memory_map=True,\n            **SAFE_READ_KW\n        )\n        for chunk in reader:\n            chunk = chunk.rename(columns=cfg[\"rename\"])\n            yield _normalize_lc_chunk(\n                chunk,\n                drop_bad_filter=drop_bad_filter,\n                drop_bad_mjd=drop_bad_mjd,\n                drop_bad_fluxerr=drop_bad_fluxerr,\n                encode_filter=encode_filter,\n            )\n\ndef load_object_lightcurve(\n    object_id: str,\n    which: str,\n    chunksize: int = None,\n    sort_time: bool = True,\n    max_chunks: int = None,\n    stop_after_found_block: bool = True,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n):\n    object_id = _norm_id(object_id)\n\n    if which == \"train\":\n        if object_id not in df_train_meta.index:\n            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n        split_name = str(df_train_meta.loc[object_id, \"split\"]).strip()\n    elif which == \"test\":\n        if object_id not in df_test_meta.index:\n            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n        split_name = str(df_test_meta.loc[object_id, \"split\"]).strip()\n    else:\n        raise ValueError(\"which must be 'train' or 'test'\")\n\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Routing split not found in SPLIT_FILES: split={split_name} object_id={object_id}\")\n\n    if chunksize is None:\n        chunksize = CHUNK_ROWS_DEFAULT\n\n    pieces = []\n    seen = 0\n    found_any = False\n    last_hit = False\n\n    for ch in iter_lightcurve_chunks(\n        split_name, which, chunksize=chunksize,\n        drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd, drop_bad_fluxerr=drop_bad_fluxerr,\n        encode_filter=False\n    ):\n        seen += 1\n        sub = ch[ch[\"object_id\"] == object_id]\n        hit = (len(sub) > 0)\n        if hit:\n            pieces.append(sub)\n            found_any = True\n\n        if stop_after_found_block and found_any and last_hit and (not hit):\n            break\n        last_hit = hit\n\n        if max_chunks is not None and seen >= int(max_chunks):\n            break\n\n    if not pieces:\n        out = pd.DataFrame(columns=REQ_LC_KEYS)\n    else:\n        out = pd.concat(pieces, ignore_index=True)\n        if sort_time and len(out) > 1:\n            out[\"filter_ord\"] = out[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n            out = (\n                out.sort_values([\"mjd\", \"filter_ord\"], kind=\"mergesort\")\n                   .drop(columns=[\"filter_ord\"])\n                   .reset_index(drop=True)\n            )\n    return out\n\ndef load_many_object_lightcurves(\n    split_name: str,\n    which: str,\n    object_ids,\n    chunksize: int = None,\n    max_chunks: int = None,\n    sort_time: bool = True,\n    drop_bad_filter: bool = True,\n    drop_bad_mjd: bool = True,\n    drop_bad_fluxerr: bool = True,\n):\n    split_name = str(split_name).strip()\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Unknown split_name={split_name}\")\n    if which not in (\"train\",\"test\"):\n        raise ValueError(\"which must be train/test\")\n\n    if chunksize is None:\n        chunksize = CHUNK_ROWS_DEFAULT\n\n    oid_set = set([_norm_id(x) for x in object_ids if _norm_id(x) != \"\"])\n    if len(oid_set) == 0:\n        return {}\n\n    out = {oid: [] for oid in oid_set}\n\n    seen = 0\n    for ch in iter_lightcurve_chunks(\n        split_name, which, chunksize=chunksize,\n        drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd, drop_bad_fluxerr=drop_bad_fluxerr,\n        encode_filter=False\n    ):\n        seen += 1\n        m = ch[\"object_id\"].isin(oid_set)\n        if m.any():\n            sub = ch.loc[m]\n            for oid, g in sub.groupby(\"object_id\", sort=False):\n                out[_norm_id(oid)].append(g)\n\n        if max_chunks is not None and seen >= int(max_chunks):\n            break\n\n    final = {}\n    for oid, parts in out.items():\n        if not parts:\n            continue\n        df = pd.concat(parts, ignore_index=True)\n        if sort_time and len(df) > 1:\n            df[\"filter_ord\"] = df[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n            df = (\n                df.sort_values([\"mjd\",\"filter_ord\"], kind=\"mergesort\")\n                  .drop(columns=[\"filter_ord\"])\n                  .reset_index(drop=True)\n            )\n        final[oid] = df\n\n    return final\n\n# ----------------------------\n# 5) NPZ cache utilities\n# ----------------------------\ndef get_lc_cache_path(object_id: str, which: str) -> Path:\n    object_id = _norm_id(object_id)\n    which = str(which).strip()\n    return LC_CACHE_DIR / f\"{which}__{object_id}.npz\"\n\ndef _mem_cache_put(key, value):\n    _LC_OBJ_CACHE[key] = value\n    if len(_LC_OBJ_CACHE) > MAX_MEM_CACHE:\n        k0 = next(iter(_LC_OBJ_CACHE.keys()))\n        _LC_OBJ_CACHE.pop(k0, None)\n\ndef load_object_lightcurve_cached(\n    object_id: str,\n    which: str,\n    chunksize: int = None,\n    sort_time: bool = True,\n    max_chunks: int = None,\n    stop_after_found_block: bool = True,\n    use_npz_cache: bool = True,\n    refresh_cache: bool = False,\n):\n    object_id = _norm_id(object_id)\n    which = str(which).strip()\n\n    mem_key = (which, object_id)\n    if mem_key in _LC_OBJ_CACHE and (not refresh_cache):\n        return _LC_OBJ_CACHE[mem_key].copy()\n\n    npz_path = get_lc_cache_path(object_id, which)\n    if use_npz_cache and npz_path.exists() and (not refresh_cache):\n        z = np.load(npz_path, allow_pickle=False)\n        mjd = z[\"mjd\"].astype(\"float32\")\n        flux = z[\"flux\"].astype(\"float32\")\n        ferr = z[\"flux_err\"].astype(\"float32\")\n        filt_id = z[\"filter_id\"].astype(\"int8\")\n        filt = np.array([ID2FILTER[int(i)] for i in filt_id], dtype=object)\n\n        df = pd.DataFrame({\n            \"object_id\": object_id,\n            \"mjd\": mjd,\n            \"flux\": flux,\n            \"flux_err\": ferr,\n            \"filter\": filt,\n        })\n        if sort_time and len(df) > 1:\n            df[\"filter_ord\"] = df[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n            df = df.sort_values([\"mjd\",\"filter_ord\"], kind=\"mergesort\").drop(columns=[\"filter_ord\"]).reset_index(drop=True)\n        _mem_cache_put(mem_key, df)\n        return df.copy()\n\n    df = load_object_lightcurve(\n        object_id, which,\n        chunksize=chunksize,\n        sort_time=sort_time,\n        max_chunks=max_chunks,\n        stop_after_found_block=stop_after_found_block,\n        drop_bad_filter=True, drop_bad_mjd=True, drop_bad_fluxerr=True\n    )\n\n    if use_npz_cache:\n        try:\n            if len(df) > 0:\n                filt_id = df[\"filter\"].map(FILTER2ID).astype(\"int8\").to_numpy()\n                np.savez_compressed(\n                    npz_path,\n                    mjd=df[\"mjd\"].to_numpy(dtype=\"float32\"),\n                    flux=df[\"flux\"].to_numpy(dtype=\"float32\"),\n                    flux_err=df[\"flux_err\"].to_numpy(dtype=\"float32\"),\n                    filter_id=filt_id\n                )\n        except Exception:\n            pass\n\n    _mem_cache_put(mem_key, df)\n    return df.copy()\n\n# ----------------------------\n# 6) FULL Object-Quality builder (streaming, vectorized)\n# ----------------------------\ndef _build_object_quality(which: str, out_path: Path, refresh: bool = False, chunksize: int = 400_000):\n    which = str(which).strip()\n    if which not in (\"train\",\"test\"):\n        raise ValueError(\"which must be train/test\")\n\n    if out_path.exists() and (not refresh):\n        df = pd.read_csv(out_path, dtype={\"object_id\":\"string\"}, **SAFE_READ_KW)\n        if \"object_id\" not in df.columns:\n            raise RuntimeError(f\"Bad objq file (missing object_id): {out_path}\")\n        df[\"object_id\"] = df[\"object_id\"].astype(\"string\").map(_norm_id)\n        return df\n\n    idx = df_train_meta.index if which == \"train\" else df_test_meta.index\n    idx = pd.Index([_norm_id(z) for z in idx.astype(str).tolist()])\n    n_obj = int(len(idx))\n\n    n_obs = np.zeros(n_obj, dtype=np.int32)\n    mjd_min = np.full(n_obj, np.float32(np.inf), dtype=np.float32)\n    mjd_max = np.full(n_obj, np.float32(-np.inf), dtype=np.float32)\n\n    sum_flux = np.zeros(n_obj, dtype=np.float64)\n    sum_flux2 = np.zeros(n_obj, dtype=np.float64)\n    sum_abs_flux = np.zeros(n_obj, dtype=np.float64)\n    neg_cnt = np.zeros(n_obj, dtype=np.int32)\n\n    snr_abs_max = np.full(n_obj, np.float32(0.0), dtype=np.float32)\n    sum_snr_abs = np.zeros(n_obj, dtype=np.float64)\n    det_cnt = np.zeros(n_obj, dtype=np.int32)\n    strong_cnt = np.zeros(n_obj, dtype=np.int32)\n\n    band_cnt = np.zeros((6, n_obj), dtype=np.int32)\n\n    t0 = time.time()\n    rows_seen = 0\n\n    for sp in SPLIT_LIST:\n        for ch in iter_lightcurve_chunks(\n            sp, which,\n            chunksize=chunksize,\n            drop_bad_filter=True, drop_bad_mjd=True, drop_bad_fluxerr=True,\n            encode_filter=True\n        ):\n            if ch is None or len(ch) == 0:\n                continue\n\n            oids = ch[\"object_id\"].to_numpy(dtype=object, copy=False)\n            oid_idx = idx.get_indexer(oids)  # -1 if not found\n            m = oid_idx >= 0\n            if not np.any(m):\n                continue\n\n            oid_idx = oid_idx[m].astype(np.int64, copy=False)\n\n            mjd = ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False)[m]\n            flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)[m]\n            ferr = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)[m]\n            fid = ch[\"filter_id\"].to_numpy(dtype=np.int8, copy=False)[m]\n\n            if MIN_FLUXERR > 0:\n                ferr = np.maximum(ferr, np.float32(MIN_FLUXERR))\n\n            rows_seen += int(len(oid_idx))\n\n            bc = np.bincount(oid_idx, minlength=n_obj)\n            n_obs += bc.astype(np.int32)\n\n            fx64 = flux.astype(np.float64)\n            sum_flux += np.bincount(oid_idx, weights=fx64, minlength=n_obj)\n            sum_flux2 += np.bincount(oid_idx, weights=(fx64 * fx64), minlength=n_obj)\n            sum_abs_flux += np.bincount(oid_idx, weights=np.abs(fx64), minlength=n_obj)\n            neg_cnt += np.bincount(oid_idx, weights=(flux < 0).astype(np.int32), minlength=n_obj).astype(np.int32)\n\n            snr = (flux / ferr).astype(np.float32)\n            snr = np.clip(snr, -np.float32(SNR_CLIP), np.float32(SNR_CLIP))\n            snr_abs = np.abs(snr).astype(np.float32)\n\n            sum_snr_abs += np.bincount(oid_idx, weights=snr_abs.astype(np.float64), minlength=n_obj)\n            det_cnt += np.bincount(oid_idx, weights=(snr_abs >= np.float32(SNR_DET_THR)).astype(np.int32), minlength=n_obj).astype(np.int32)\n            strong_cnt += np.bincount(oid_idx, weights=(snr_abs >= np.float32(SNR_STRONG_THR)).astype(np.int32), minlength=n_obj).astype(np.int32)\n\n            for b in range(6):\n                mb = (fid == b)\n                if np.any(mb):\n                    band_cnt[b] += np.bincount(oid_idx[mb], minlength=n_obj).astype(np.int32)\n\n            order = np.argsort(oid_idx, kind=\"mergesort\")\n            idx_s = oid_idx[order]\n            mjd_s = mjd[order]\n            snr_s = snr_abs[order]\n\n            starts = np.r_[0, 1 + np.where(idx_s[1:] != idx_s[:-1])[0]]\n            uniq = idx_s[starts]\n\n            mn = np.minimum.reduceat(mjd_s, starts).astype(np.float32)\n            mx = np.maximum.reduceat(mjd_s, starts).astype(np.float32)\n            sx = np.maximum.reduceat(snr_s, starts).astype(np.float32)\n\n            mjd_min[uniq] = np.minimum(mjd_min[uniq], mn)\n            mjd_max[uniq] = np.maximum(mjd_max[uniq], mx)\n            snr_abs_max[uniq] = np.maximum(snr_abs_max[uniq], sx)\n\n    n_obs_f = np.maximum(n_obs.astype(np.float32), np.float32(1.0))\n    timespan = np.where(\n        np.isfinite(mjd_min) & np.isfinite(mjd_max),\n        (mjd_max - mjd_min).astype(np.float32),\n        np.float32(0.0)\n    )\n    cadence_proxy = (timespan / np.maximum(n_obs - 1, 1).astype(np.float32)).astype(np.float32)\n\n    flux_mean = (sum_flux / n_obs_f.astype(np.float64)).astype(np.float32)\n    flux_var = (sum_flux2 / n_obs_f.astype(np.float64) - (sum_flux / n_obs_f.astype(np.float64))**2)\n    flux_var = np.maximum(flux_var, 0.0)\n    flux_std = np.sqrt(flux_var).astype(np.float32)\n\n    abs_flux_mean = (sum_abs_flux / n_obs_f.astype(np.float64)).astype(np.float32)\n    neg_flux_frac = (neg_cnt.astype(np.float32) / n_obs_f).astype(np.float32)\n\n    snr_abs_mean = (sum_snr_abs / n_obs_f.astype(np.float64)).astype(np.float32)\n    snr_det_frac = (det_cnt.astype(np.float32) / n_obs_f).astype(np.float32)\n    snr_strong_frac = (strong_cnt.astype(np.float32) / n_obs_f).astype(np.float32)\n\n    n_bands_present = (band_cnt > 0).sum(axis=0).astype(np.int8)\n    frac_bands = (band_cnt.astype(np.float32) / n_obs_f[None, :]).astype(np.float32)\n\n    df_out = pd.DataFrame({\n        \"object_id\": pd.Series(idx, dtype=\"string\"),\n        \"n_obs_total\": n_obs.astype(np.int32),\n        \"n_bands_present\": n_bands_present,\n        \"timespan\": timespan,\n        \"cadence_proxy\": cadence_proxy,\n        \"neg_flux_frac\": neg_flux_frac,\n        \"flux_mean\": flux_mean,\n        \"flux_std\": flux_std,\n        \"abs_flux_mean\": abs_flux_mean,\n        \"snr_abs_mean\": snr_abs_mean,\n        \"snr_abs_max\": snr_abs_max.astype(np.float32),\n        \"snr_det_frac\": snr_det_frac,\n        \"snr_strong_frac\": snr_strong_frac,\n    })\n\n    for b, name in enumerate([\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]):\n        df_out[f\"n_{name}\"] = band_cnt[b].astype(np.int32)\n        df_out[f\"frac_{name}\"] = frac_bands[b].astype(np.float32)\n\n    df_out.to_csv(out_path, index=False)\n    if OBJQ_SAVE_PARQUET:\n        try:\n            df_out.to_parquet(out_path.with_suffix(\".parquet\"), index=False)\n        except Exception:\n            pass\n\n    elapsed = time.time() - t0\n    print(f\"[OBJQ] Built {which} object_quality: {out_path} | objects={len(df_out):,} | rows_seen={rows_seen:,} | {elapsed/60:.2f} min\")\n    return df_out\n\n# ----------------------------\n# 7) SAFE overwrite-join object-quality -> meta (v5.4 sanitize hard)\n# ----------------------------\nOBJECT_QUALITY_TRAIN_PATH = LOG_DIR / \"object_quality_train.csv\"\nOBJECT_QUALITY_TEST_PATH  = LOG_DIR / \"object_quality_test.csv\"\n\nMETA_PROTECT_COLS = set([\n    \"split\", \"split_id\", \"fold\",\n    \"target\", \"y\", \"label\", \"class\",\n    \"EBV\", \"EBV_clip\", \"EBV_used\", \"EBV_missing\", \"log1pEBV\",\n    \"Z\", \"Z_clip\", \"Z_err\", \"Z_missing\", \"Z_err_missing\", \"log1pZ\", \"log1pZerr\",\n    \"is_photoz\", \"photoz\", \"redshift\",\n    \"prior\", \"pos_prior\", \"neg_prior\"\n])\n\ndef _drop_unnamed_cols(df: pd.DataFrame) -> pd.DataFrame:\n    bad = [c for c in df.columns if str(c).startswith(\"Unnamed\")]\n    return df.drop(columns=bad, errors=\"ignore\") if bad else df\n\ndef _is_objq_valid_col(c: str) -> bool:\n    c = str(c)\n    base = {\n        \"n_obs_total\",\"n_bands_present\",\"timespan\",\"cadence_proxy\",\n        \"neg_flux_frac\",\"abs_flux_mean\",\n        \"flux_mean\",\"flux_std\",\n        \"snr_abs_mean\",\"snr_abs_max\",\"snr_det_frac\",\"snr_strong_frac\",\n        \"low_obs\",\"low_bandcov\",\n    }\n    if c in base:\n        return True\n    if c.startswith(\"n_\"):     # n_u..n_y\n        return True\n    if c.startswith(\"frac_\"):  # frac_u..frac_y\n        return True\n    return False\n\ndef _looks_like_meta_col(c: str) -> bool:\n    s = str(c).lower()\n    if s in {x.lower() for x in META_PROTECT_COLS}:\n        return True\n    meta_sub = [\n        \"split\", \"target\", \"label\", \"class\", \"fold\",\n        \"ebv\", \"redshift\", \"photoz\", \"z_err\", \"zerr\",\n        \"log1p\", \"_clip\", \"_missing\", \"prior\"\n    ]\n    return any(k in s for k in meta_sub)\n\ndef _clean_objq_df(df: pd.DataFrame, tag: str) -> pd.DataFrame:\n    if df is None or len(df) == 0:\n        return df\n    df = _drop_unnamed_cols(df).copy()\n\n    if \"object_id\" not in df.columns:\n        raise RuntimeError(f\"[OBJQ][{tag}] missing object_id col\")\n\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").map(_norm_id)\n\n    drop_metaish = [c for c in df.columns if c != \"object_id\" and _looks_like_meta_col(c)]\n    if drop_metaish:\n        print(f\"[OBJQ][{tag}] drop META-like cols from objq: {drop_metaish[:12]}{'...' if len(drop_metaish)>12 else ''}\")\n        df = df.drop(columns=drop_metaish, errors=\"ignore\")\n\n    keep = [\"object_id\"] + [c for c in df.columns if c != \"object_id\" and _is_objq_valid_col(c)]\n    dropped_other = [c for c in df.columns if c not in keep]\n    if dropped_other:\n        print(f\"[OBJQ][{tag}] drop non-objq cols: {dropped_other[:12]}{'...' if len(dropped_other)>12 else ''}\")\n    df = df[keep].copy()\n\n    for c in df.columns:\n        if c == \"object_id\":\n            continue\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    return df\n\ndef _dedup_objq(df: pd.DataFrame, tag: str):\n    if df is None or df.empty:\n        return df\n    df = df.copy()\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").map(_norm_id)\n\n    if not df[\"object_id\"].duplicated().any():\n        return df\n\n    print(f\"[WARN][OBJQ][{tag}] duplicated object_id detected -> aggregating\")\n    num_cols = [c for c in df.columns if c != \"object_id\"]\n    count_like = set([c for c in num_cols if c.startswith(\"n_\")] + [\"n_obs_total\",\"n_bands_present\",\"low_obs\",\"low_bandcov\"])\n\n    agg = {}\n    for c in num_cols:\n        agg[c] = \"max\" if c in count_like else \"mean\"\n\n    return df.groupby(\"object_id\", as_index=False).agg(agg)\n\ndef _add_objq_flags(objq_df: pd.DataFrame):\n    oq = objq_df.copy()\n    if \"n_obs_total\" in oq.columns:\n        oq[\"low_obs\"] = (pd.to_numeric(oq[\"n_obs_total\"], errors=\"coerce\").fillna(0).astype(int) < 20).astype(\"int8\")\n    if \"n_bands_present\" in oq.columns:\n        oq[\"low_bandcov\"] = (pd.to_numeric(oq[\"n_bands_present\"], errors=\"coerce\").fillna(0).astype(int) <= 2).astype(\"int8\")\n    return oq\n\ndef _safe_join_objq_overwrite(df_meta: pd.DataFrame, objq_df: pd.DataFrame, tag=\"train\"):\n    df_meta = df_meta.copy(deep=False)\n    df_meta.index = pd.Index([_norm_id(z) for z in df_meta.index.tolist()], name=df_meta.index.name)\n\n    oq = objq_df.copy(deep=False)\n    if \"object_id\" in oq.columns:\n        oq[\"object_id\"] = oq[\"object_id\"].astype(\"string\").map(_norm_id)\n        oq = oq.set_index(\"object_id\", drop=True)\n    else:\n        oq.index = pd.Index([_norm_id(z) for z in oq.index.tolist()], name=oq.index.name)\n\n    overlap = df_meta.columns.intersection(oq.columns)\n    if len(overlap) > 0:\n        print(f\"[OBJQ][{tag}] overlap detected -> overwrite from objq: {list(overlap[:12])}{'...' if len(overlap)>12 else ''}\")\n        df_meta = df_meta.drop(columns=list(overlap), errors=\"ignore\")\n\n    out = df_meta.join(oq, how=\"left\")\n    added_cols = [c for c in oq.columns if c in out.columns]\n    return out, added_cols\n\nobjq_train_df = None\nobjq_test_df  = None\nadded_cols_train = []\nadded_cols_test  = []\n\nif BUILD_OBJECT_QUALITY:\n    do_train = True\n    do_test = True\n    if OBJQ_ONLY_IF_MISSING and (OBJECT_QUALITY_TRAIN_PATH.exists() and OBJECT_QUALITY_TEST_PATH.exists()) and (not OBJQ_REFRESH):\n        do_train = do_test = False\n\n    if do_train:\n        objq_train_df = _build_object_quality(\"train\", OBJECT_QUALITY_TRAIN_PATH, refresh=OBJQ_REFRESH, chunksize=OBJQ_CHUNK_ROWS)\n    else:\n        objq_train_df = pd.read_csv(OBJECT_QUALITY_TRAIN_PATH, dtype={\"object_id\":\"string\"}, **SAFE_READ_KW)\n\n    if do_test:\n        objq_test_df = _build_object_quality(\"test\", OBJECT_QUALITY_TEST_PATH, refresh=OBJQ_REFRESH, chunksize=OBJQ_CHUNK_ROWS)\n    else:\n        objq_test_df = pd.read_csv(OBJECT_QUALITY_TEST_PATH, dtype={\"object_id\":\"string\"}, **SAFE_READ_KW)\n\n    objq_train_df = _clean_objq_df(objq_train_df, \"train\")\n    objq_test_df  = _clean_objq_df(objq_test_df,  \"test\")\n\n    objq_train_df = _dedup_objq(objq_train_df, \"train\")\n    objq_test_df  = _dedup_objq(objq_test_df,  \"test\")\n\n    objq_train_df = _add_objq_flags(objq_train_df)\n    objq_test_df  = _add_objq_flags(objq_test_df)\n\n    df_train_meta, added_cols_train = _safe_join_objq_overwrite(df_train_meta, objq_train_df, tag=\"train\")\n    df_test_meta,  added_cols_test  = _safe_join_objq_overwrite(df_test_meta,  objq_test_df,  tag=\"test\")\n\n    for dfm, added in [(df_train_meta, added_cols_train), (df_test_meta, added_cols_test)]:\n        for c in added:\n            if c in dfm.columns:\n                dfm[c] = pd.to_numeric(dfm[c], errors=\"coerce\").fillna(0)\n\n    try:\n        df_train_meta.to_parquet(ART_DIR / \"train_meta.parquet\", index=True)\n        df_test_meta.to_parquet(ART_DIR / \"test_meta.parquet\", index=True)\n        meta_saved = \"parquet\"\n    except Exception:\n        df_train_meta.to_csv(ART_DIR / \"train_meta.csv\", index=True)\n        df_test_meta.to_csv(ART_DIR / \"test_meta.csv\", index=True)\n        meta_saved = \"csv\"\n\n    with open(ART_DIR / \"stage3_objq_summary.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump({\n            \"stage\": \"stage3\",\n            \"version\": \"v5.4\",\n            \"BUILD_OBJECT_QUALITY\": bool(BUILD_OBJECT_QUALITY),\n            \"OBJQ_REFRESH\": bool(OBJQ_REFRESH),\n            \"OBJQ_CHUNK_ROWS\": int(OBJQ_CHUNK_ROWS),\n            \"object_quality_train\": str(OBJECT_QUALITY_TRAIN_PATH),\n            \"object_quality_test\": str(OBJECT_QUALITY_TEST_PATH),\n            \"meta_overwrite_format\": meta_saved,\n            \"added_objq_cols_train\": sorted(list(set(added_cols_train))),\n            \"added_objq_cols_test\": sorted(list(set(added_cols_test))),\n        }, f, indent=2)\n\n    print(f\"[OBJQ] Joined into meta (SAFE overwrite v5.4) + saved updated meta ({meta_saved}) in {ART_DIR}\")\n\n# ----------------------------\n# 8) Smoke test (schema + find a few objects quickly) — v5.4 safe StopIteration\n# ----------------------------\nrng = np.random.default_rng(SEED)\ncandidate_splits = []\nfor s in SPLIT_LIST:\n    if len(train_ids_by_split.get(s, [])) > 0 and len(test_ids_by_split.get(s, [])) > 0:\n        candidate_splits.append(s)\n    if len(candidate_splits) >= 2:\n        break\nif len(candidate_splits) == 0:\n    raise RuntimeError(\"Tidak ada split yang punya train & test objects (unexpected). Cek STAGE 2.\")\n\nSMOKE_CHUNK = int(CFG.get(\"SMOKE_CHUNK_ROWS\", 80_000))\nSMOKE_MAX_CHUNKS = int(CFG.get(\"SMOKE_MAX_CHUNKS\", 6))\nSMOKE_N_IDS = int(CFG.get(\"SMOKE_N_IDS_PER_SPLIT\", 2))\n\nfor s in candidate_splits:\n    try:\n        ch_tr = next(iter_lightcurve_chunks(s, \"train\", chunksize=SMOKE_CHUNK, encode_filter=False))\n        ch_te = next(iter_lightcurve_chunks(s, \"test\",  chunksize=SMOKE_CHUNK, encode_filter=False))\n    except StopIteration:\n        print(f\"[WARN][SMOKE] split={s} StopIteration (file empty / all rows filtered). Skip.\")\n        continue\n\n    if list(ch_tr.columns) != REQ_LC_KEYS:\n        raise RuntimeError(f\"Train chunk schema mismatch in {s}: {list(ch_tr.columns)}\")\n    if list(ch_te.columns) != REQ_LC_KEYS:\n        raise RuntimeError(f\"Test chunk schema mismatch in {s}: {list(ch_te.columns)}\")\n\n    tr_ids = train_ids_by_split[s]\n    te_ids = test_ids_by_split[s]\n    pick_tr = [tr_ids[i] for i in rng.integers(0, len(tr_ids), size=min(SMOKE_N_IDS, len(tr_ids)))]\n    pick_te = [te_ids[i] for i in rng.integers(0, len(te_ids), size=min(SMOKE_N_IDS, len(te_ids)))]\n\n    got_tr = load_many_object_lightcurves(s, \"train\", pick_tr, chunksize=SMOKE_CHUNK, max_chunks=SMOKE_MAX_CHUNKS, sort_time=False)\n    got_te = load_many_object_lightcurves(s, \"test\",  pick_te, chunksize=SMOKE_CHUNK, max_chunks=SMOKE_MAX_CHUNKS, sort_time=False)\n\n    miss_tr = [x for x in pick_tr if _norm_id(x) not in got_tr]\n    miss_te = [x for x in pick_te if _norm_id(x) not in got_te]\n\n    if miss_tr:\n        print(f\"[WARN][SMOKE] split={s} train not found within first {SMOKE_MAX_CHUNKS} chunks: {miss_tr[:2]}\")\n    if miss_te:\n        print(f\"[WARN][SMOKE] split={s} test not found within first {SMOKE_MAX_CHUNKS} chunks: {miss_te[:2]}\")\n\nprint(\"STAGE 3 OK — LIGHTCURVE UTILITIES READY (+OBJQ full if enabled)\")\nprint(f\"- Saved manifest: {manifest_path}\")\nprint(f\"- Saved counts  : {counts_path}\")\nprint(f\"- LC_CACHE_DIR  : {LC_CACHE_DIR}\")\nprint(f\"- OBJQ train    : {OBJECT_QUALITY_TRAIN_PATH} (exists={OBJECT_QUALITY_TRAIN_PATH.exists()})\")\nprint(f\"- OBJQ test     : {OBJECT_QUALITY_TEST_PATH} (exists={OBJECT_QUALITY_TEST_PATH.exists()})\")\nprint(f\"- Smoke splits  : {candidate_splits}\")\n\n# ----------------------------\n# 9) Export globals\n# ----------------------------\nglobals().update({\n    \"SPLIT_FILES\": SPLIT_FILES,\n    \"train_ids_by_split\": train_ids_by_split,\n    \"test_ids_by_split\": test_ids_by_split,\n    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n    \"load_object_lightcurve\": load_object_lightcurve,\n    \"load_many_object_lightcurves\": load_many_object_lightcurves,\n    \"load_object_lightcurve_cached\": load_object_lightcurve_cached,\n    \"get_lc_cache_path\": get_lc_cache_path,\n    \"LC_CACHE_DIR\": LC_CACHE_DIR,\n    \"REQ_LC_KEYS\": REQ_LC_KEYS,\n    \"ALLOWED_FILTERS\": ALLOWED_FILTERS,\n    \"FILTER_ORDER\": FILTER_ORDER,\n    \"FILTER2ID\": FILTER2ID,\n    \"ID2FILTER\": ID2FILTER,\n    \"OBJECT_QUALITY_TRAIN_PATH\": str(OBJECT_QUALITY_TRAIN_PATH),\n    \"OBJECT_QUALITY_TEST_PATH\": str(OBJECT_QUALITY_TEST_PATH),\n    \"df_train_meta\": df_train_meta,\n    \"df_test_meta\": df_test_meta,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — Photometric Cleaning (FORCE OVERWRITE) — REVISI FULL v7.1\n# - Dust de-extinction (Rubin DustValues().r_x) for ugrizy\n# - Output:\n#   * mag, mag_err (positive-only; non-detect uses DET_SIGMA*err as limit)\n#   * asinh_mag, asinh_mag_err (Luptitude-like; works for negative flux)\n#   * snr, snr_abs, snr_asinh (preserve sign via asinh)\n# - Manifest + summary + config JSON\n#\n# v7.1 upgrade (dibanding v7.0):\n# - filter lebih robust: support \"0..5\" + \"lsst_u\" + unexpected casing; invalid filter bisa drop (atau strict)\n# - EBV robust: kalau EBV/EBV_clip tidak ada -> auto zeros\n# - skip write untuk part kosong (hindari rows<=0 error)\n# - summary tambah counters: dropped_bad_filter_rows, skipped_empty_parts\n# ============================================================\n\nimport gc, json, warnings, time, shutil\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\", \"LOG_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n\nART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\n\nCFG = globals().get(\"CFG\", {}) if isinstance(globals().get(\"CFG\", {}), dict) else {}\n\n# ----------------------------\n# 1) Settings\n# ----------------------------\nCHUNKSIZE   = int(CFG.get(\"PHOT_CHUNKSIZE\", 350_000))\nERR_EPS     = float(CFG.get(\"PHOT_ERR_EPS\", 1e-6))\n\nSNR_DET_POS = float(CFG.get(\"SNR_DET_POS\", 3.0))  # detection for MAG branch (positive snr only)\nDET_SIGMA   = float(CFG.get(\"DET_SIGMA\", 3.0))\n\nSNR_CLIP    = float(CFG.get(\"SNR_CLIP\", 30.0))    # for stability\nMIN_FLUX_POS_UJY = float(CFG.get(\"MIN_FLUX_POS_UJY\", 1e-6))\n\nMAG_MIN, MAG_MAX   = float(CFG.get(\"MAG_MIN\", -10.0)), float(CFG.get(\"MAG_MAX\", 50.0))\nMAGERR_FLOOR_DET   = float(CFG.get(\"MAGERR_FLOOR_DET\", 1e-3))\nMAGERR_FLOOR_ND    = float(CFG.get(\"MAGERR_FLOOR_ND\", 0.75))\nMAGERR_CAP         = float(CFG.get(\"MAGERR_CAP\", 10.0))\n\nWRITE_FORMAT = str(CFG.get(\"PHOT_WRITE_FORMAT\", \"parquet\")).lower()  # \"parquet\" or \"csv.gz\"\nONLY_SPLITS  = CFG.get(\"PHOT_ONLY_SPLITS\", None)   # e.g. [\"split_01\"]\nKEEP_FLUX_DEBUG = bool(CFG.get(\"PHOT_KEEP_FLUX_DEBUG\", False))\nDROP_BAD_TIME_ROWS = bool(CFG.get(\"PHOT_DROP_BAD_TIME_ROWS\", True))\nDROP_NAN_FLUX_ROWS  = bool(CFG.get(\"PHOT_DROP_NAN_FLUX_ROWS\", False))  # if True, drop NaN flux instead of zeroing\nREBUILD_MODE = str(CFG.get(\"PHOT_REBUILD_MODE\", \"wipe_all\")).lower()  # \"wipe_all\" | \"wipe_parts_only\"\n\n# v7.1: strict filter or drop invalid\nSTRICT_FILTER = bool(CFG.get(\"PHOT_STRICT_FILTER\", False))\n\n# ----------------------------\n# 2) Extinction coefficients (Rubin DustValues().r_x)\n#    r_x * EBV = A_x  (extinction in mag)\n# ----------------------------\nEXT_RLAMBDA = {\n    \"u\": 4.757217815396922,\n    \"g\": 3.6605664439892625,\n    \"r\": 2.70136780871597,\n    \"i\": 2.0536599130965882,\n    \"z\": 1.5900964472616756,\n    \"y\": 1.3077049588254708,\n}\n\nBAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\nID2BAND = {v: k for k, v in BAND2ID.items()}\nRLAM_BY_ID = np.array([EXT_RLAMBDA[\"u\"], EXT_RLAMBDA[\"g\"], EXT_RLAMBDA[\"r\"], EXT_RLAMBDA[\"i\"], EXT_RLAMBDA[\"z\"], EXT_RLAMBDA[\"y\"]], dtype=np.float32)\n\n# filter aliases\nFILTER_NUM2STR = {\"0\": \"u\", \"1\": \"g\", \"2\": \"r\", \"3\": \"i\", \"4\": \"z\", \"5\": \"y\"}\n\ndef _get_ebv_series(df_meta: pd.DataFrame):\n    # pakai EBV_clip jika ada, fallback EBV, kalau tidak ada -> zeros\n    if \"EBV_clip\" in df_meta.columns:\n        s = df_meta[\"EBV_clip\"]\n    elif \"EBV\" in df_meta.columns:\n        s = df_meta[\"EBV\"]\n    else:\n        s = pd.Series(0.0, index=df_meta.index)\n    # numeric + fill\n    s = pd.to_numeric(s, errors=\"coerce\").fillna(0.0)\n    return s\n\nEBV_TRAIN_SER = _get_ebv_series(df_train_meta)\nEBV_TEST_SER  = _get_ebv_series(df_test_meta)\n\n# flux unit assumed uJy -> AB mag zero point:\nMAG_ZP = float(2.5 * np.log10(3631e6))  # ~23.9 (uJy)\nK_2P5_LN10 = np.float32(2.5 / np.log(10.0))     # 2.5/ln(10)\nK_1P0857 = np.float32(1.0857362)\n\n# ----------------------------\n# 3) Output root + WIPE (with safety guard)\n# ----------------------------\nLC_CLEAN_DIR = ART_DIR / \"lc_clean_mag\"   # keep name for compatibility\n\nart_abs = ART_DIR.resolve()\nlc_abs  = LC_CLEAN_DIR.resolve()\n\ntry:\n    ok_rel = lc_abs.is_relative_to(art_abs)\nexcept AttributeError:\n    ok_rel = str(lc_abs).startswith(str(art_abs) + \"/\") or str(lc_abs).startswith(str(art_abs) + \"\\\\\")\n\nif not ok_rel:\n    raise RuntimeError(f\"Safety guard failed: LC_CLEAN_DIR bukan turunan ART_DIR.\\nART_DIR={art_abs}\\nLC_CLEAN_DIR={lc_abs}\")\n\nif REBUILD_MODE == \"wipe_all\":\n    if LC_CLEAN_DIR.exists():\n        shutil.rmtree(LC_CLEAN_DIR, ignore_errors=True)\n    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\nelif REBUILD_MODE == \"wipe_parts_only\":\n    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\nelse:\n    raise ValueError(\"REBUILD_MODE must be 'wipe_all' or 'wipe_parts_only'\")\n\n# ----------------------------\n# 4) Atomic writer\n# ----------------------------\ndef _atomic_write_parquet(df: pd.DataFrame, out_path: Path):\n    tmp = out_path.with_name(out_path.stem + \".tmp\" + out_path.suffix)\n    try:\n        df.to_parquet(tmp, index=False)\n        tmp.replace(out_path)\n    finally:\n        if tmp.exists() and (not out_path.exists()):\n            try: tmp.unlink()\n            except Exception: pass\n\ndef _atomic_write_csv_gz(df: pd.DataFrame, out_path: Path):\n    final_path = out_path.with_suffix(\".csv.gz\")\n    tmp = final_path.with_name(final_path.stem + \".tmp\" + \"\".join(final_path.suffixes))\n    try:\n        df.to_csv(tmp, index=False, compression=\"gzip\")\n        tmp.replace(final_path)\n    finally:\n        if tmp.exists() and (not final_path.exists()):\n            try: tmp.unlink()\n            except Exception: pass\n    return final_path\n\ndef write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    if fmt == \"parquet\":\n        try:\n            _atomic_write_parquet(df, out_path)\n            return \"parquet\", out_path\n        except Exception as e:\n            alt = _atomic_write_csv_gz(df, out_path)\n            return f\"csv.gz (fallback from parquet: {type(e).__name__})\", alt\n    elif fmt == \"csv.gz\":\n        alt = _atomic_write_csv_gz(df, out_path)\n        return \"csv.gz\", alt\n    else:\n        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n\n# ----------------------------\n# 5) Core cleaning -> MAG + ASINH_MAG + SNR features\n# ----------------------------\ndef _normalize_filter_series(filt_ser: pd.Series) -> pd.Series:\n    f = filt_ser.astype(\"string\").str.strip().str.lower()\n    # common prefixes\n    f = f.str.replace(\"lsst_\", \"\", regex=False)\n    f = f.str.replace(\"rubin_\", \"\", regex=False)\n    f = f.str.replace(\"band_\", \"\", regex=False)\n    # numeric 0..5\n    f = f.replace(FILTER_NUM2STR)\n    # if still long but endswith ugrizy, take last char\n    last = f.str[-1]\n    mask_end = last.isin(list(BAND2ID.keys()))\n    f = f.where(f.isin(list(BAND2ID.keys())), last.where(mask_end, f))\n    return f\n\ndef clean_chunk_to_phot(ch: pd.DataFrame, ebv_ser: pd.Series):\n    # Required columns\n    for c in [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]:\n        if c not in ch.columns:\n            raise ValueError(f\"iter_lightcurve_chunks chunk missing column: {c}. Found={list(ch.columns)}\")\n\n    oid_ser  = ch[\"object_id\"].astype(\"string\").str.strip()\n    filt_ser = _normalize_filter_series(ch[\"filter\"])\n\n    # numeric arrays\n    mjd  = ch[\"mjd\"].to_numpy(copy=False).astype(np.float32, copy=False)\n    flux = ch[\"flux\"].to_numpy(copy=False).astype(np.float32, copy=False)\n    err  = ch[\"flux_err\"].to_numpy(copy=False).astype(np.float32, copy=False)\n\n    # sanitize err\n    err = np.nan_to_num(err, nan=np.float32(ERR_EPS), posinf=np.float32(ERR_EPS), neginf=np.float32(ERR_EPS))\n    err = np.maximum(err, np.float32(ERR_EPS))\n\n    # sanitize flux\n    flux = flux.astype(np.float32, copy=False)\n    flux[~np.isfinite(flux)] = np.float32(np.nan)\n\n    # band id (robust)\n    band_id_s = filt_ser.map(BAND2ID)\n    bad_filter_rows = int(band_id_s.isna().sum())\n    if bad_filter_rows:\n        if STRICT_FILTER:\n            bad = filt_ser[band_id_s.isna()].value_counts().head(10).index.tolist()\n            raise ValueError(f\"Unknown/invalid filter values encountered (top examples): {bad}\")\n        keep_f = (~band_id_s.isna()).to_numpy()\n        oid_ser = oid_ser[keep_f]\n        filt_ser = filt_ser[keep_f]\n        mjd = mjd[keep_f]\n        flux = flux[keep_f]\n        err = err[keep_f]\n        band_id_s = band_id_s[keep_f]\n\n    band_id = band_id_s.to_numpy(copy=False).astype(np.int8, copy=False)\n\n    # EBV map (index=object_id)\n    ebv = oid_ser.map(ebv_ser).fillna(0.0).to_numpy(copy=False).astype(np.float32, copy=False)\n    ebv[~np.isfinite(ebv)] = np.float32(0.0)\n\n    # A_x\n    rlam = RLAM_BY_ID[band_id.astype(np.int32)]\n    A = (rlam * ebv).astype(np.float32, copy=False)\n\n    # de-extinction in flux space: f0 = f * 10^(0.4 A)\n    mul = np.power(np.float32(10.0), (np.float32(0.4) * A)).astype(np.float32, copy=False)\n    flux_deext = (flux * mul).astype(np.float32, copy=False)\n    err_deext  = (err  * mul).astype(np.float32, copy=False)\n\n    ok_flux = np.isfinite(flux_deext)\n    nan_flux_rows = int((~ok_flux).sum())\n\n    if DROP_NAN_FLUX_ROWS and nan_flux_rows:\n        keep_flux = ok_flux\n    else:\n        keep_flux = None\n        if nan_flux_rows:\n            flux_deext[~ok_flux] = np.float32(0.0)\n\n    # SNR\n    denom = np.maximum(err_deext, np.float32(ERR_EPS))\n    snr = (flux_deext / denom).astype(np.float32, copy=False)\n    snr = np.clip(snr, -np.float32(SNR_CLIP), np.float32(SNR_CLIP)).astype(np.float32, copy=False)\n    snr_abs = np.abs(snr).astype(np.float32, copy=False)\n\n    detected_pos = (snr > np.float32(SNR_DET_POS)).astype(np.int8, copy=False)\n\n    # MAG branch (positive-only; non-detect uses DET_SIGMA*err)\n    flux_detlim = (np.float32(DET_SIGMA) * err_deext).astype(np.float32, copy=False)\n    flux_for_mag = np.where(\n        detected_pos == 1,\n        np.maximum(flux_deext, np.float32(MIN_FLUX_POS_UJY)),\n        np.maximum(flux_detlim, np.float32(MIN_FLUX_POS_UJY)),\n    ).astype(np.float32, copy=False)\n\n    mag = (np.float32(MAG_ZP) - np.float32(2.5) * np.log10(flux_for_mag)).astype(np.float32, copy=False)\n    mag = np.clip(mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32, copy=False)\n\n    mag_err = (K_1P0857 * (err_deext / flux_for_mag)).astype(np.float32, copy=False)\n    mag_err = np.clip(mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32, copy=False)\n    if MAGERR_FLOOR_ND is not None and float(MAGERR_FLOOR_ND) > 0:\n        mag_err = np.where(detected_pos == 1, mag_err, np.maximum(mag_err, np.float32(MAGERR_FLOOR_ND))).astype(np.float32, copy=False)\n\n    # ASINH magnitude (Luptitude-like; defined for negative flux)\n    b = np.maximum(np.float32(DET_SIGMA) * err_deext, np.float32(MIN_FLUX_POS_UJY)).astype(np.float32, copy=False)\n    x = (flux_deext / (np.float32(2.0) * b)).astype(np.float32, copy=False)\n    asinh_term = np.arcsinh(x).astype(np.float32, copy=False)\n    ln_b = np.log(b).astype(np.float32, copy=False)\n\n    asinh_mag = (np.float32(MAG_ZP) - K_2P5_LN10 * (asinh_term + ln_b)).astype(np.float32, copy=False)\n    asinh_mag = np.clip(asinh_mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32, copy=False)\n\n    denom_asinh = np.sqrt((flux_deext * flux_deext) + (np.float32(2.0) * b) * (np.float32(2.0) * b)).astype(np.float32, copy=False)\n    asinh_mag_err = (K_2P5_LN10 * (err_deext / np.maximum(denom_asinh, np.float32(ERR_EPS)))).astype(np.float32, copy=False)\n    asinh_mag_err = np.clip(asinh_mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32, copy=False)\n\n    snr_asinh = np.arcsinh(snr).astype(np.float32, copy=False)\n\n    out = pd.DataFrame({\n        \"object_id\": pd.array(oid_ser.to_numpy(copy=False), dtype=\"string\"),\n        \"mjd\": mjd,\n        \"band_id\": band_id,\n        \"mag\": mag,\n        \"mag_err\": mag_err,\n        \"asinh_mag\": asinh_mag,\n        \"asinh_mag_err\": asinh_mag_err,\n        \"snr\": snr,\n        \"snr_abs\": snr_abs,\n        \"snr_asinh\": snr_asinh,\n        \"detected_pos\": detected_pos,\n    })\n\n    if KEEP_FLUX_DEBUG:\n        out[\"A_x\"]        = pd.Series(A, dtype=\"float32\")\n        out[\"flux_deext\"] = pd.Series(flux_deext, dtype=\"float32\")\n        out[\"err_deext\"]  = pd.Series(err_deext, dtype=\"float32\")\n        out[\"b_soft\"]     = pd.Series(b, dtype=\"float32\")\n\n    dropped_flux = 0\n    if keep_flux is not None:\n        keep = keep_flux\n        dropped_flux = int((~keep).sum())\n        out = out[keep]\n\n    dropped_time = 0\n    if DROP_BAD_TIME_ROWS:\n        t = out[\"mjd\"].to_numpy(copy=False).astype(np.float32, copy=False)\n        keep_t = np.isfinite(t)\n        dropped_time = int((~keep_t).sum())\n        if dropped_time:\n            out = out[keep_t]\n\n    return out, int(dropped_time), int(nan_flux_rows), int(dropped_flux), int(bad_filter_rows)\n\n# ----------------------------\n# 6) Process split-wise\n# ----------------------------\nsplits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else list(SPLIT_LIST)\nsplits_to_use = list(splits_to_use)\n\nsummary_rows, manifest_rows = [], []\n\ndef _wipe_parts_dir(out_dir: Path):\n    if out_dir.exists():\n        for pat in [\"part_*.parquet\", \"part_*.csv.gz\", \"*.tmp\", \"*.tmp.parquet\", \"*.tmp.csv.gz\"]:\n            for f in out_dir.glob(pat):\n                try: f.unlink()\n                except Exception: pass\n\ndef process_split(split_name: str, which: str):\n    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n    out_dir = LC_CLEAN_DIR / split_name / which\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    if REBUILD_MODE == \"wipe_parts_only\":\n        _wipe_parts_dir(out_dir)\n\n    t0 = time.time()\n    part_idx = 0\n    n_rows_total = 0\n    n_det_pos = 0\n    dropped_time_total = 0\n    nan_flux_total = 0\n    dropped_flux_total = 0\n    dropped_bad_filter_total = 0\n    skipped_empty_parts = 0\n\n    mag_min = np.inf\n    mag_max = -np.inf\n    asinh_mag_min = np.inf\n    asinh_mag_max = -np.inf\n\n    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n        cleaned, dropped_time, nan_flux, dropped_flux, bad_filter_rows = clean_chunk_to_phot(ch, ebv_ser)\n\n        dropped_time_total += int(dropped_time)\n        nan_flux_total += int(nan_flux)\n        dropped_flux_total += int(dropped_flux)\n        dropped_bad_filter_total += int(bad_filter_rows)\n\n        if cleaned is None or len(cleaned) == 0:\n            skipped_empty_parts += 1\n            continue\n\n        n_rows = int(len(cleaned))\n        n_rows_total += n_rows\n        n_det_pos += int(cleaned[\"detected_pos\"].to_numpy(copy=False).astype(np.int8).sum())\n\n        mag_arr = cleaned[\"mag\"].to_numpy(copy=False).astype(np.float32, copy=False)\n        fin = np.isfinite(mag_arr)\n        if fin.any():\n            mag_min = float(min(mag_min, float(np.min(mag_arr[fin]))))\n            mag_max = float(max(mag_max, float(np.max(mag_arr[fin]))))\n\n        am_arr = cleaned[\"asinh_mag\"].to_numpy(copy=False).astype(np.float32, copy=False)\n        fin2 = np.isfinite(am_arr)\n        if fin2.any():\n            asinh_mag_min = float(min(asinh_mag_min, float(np.min(am_arr[fin2]))))\n            asinh_mag_max = float(max(asinh_mag_max, float(np.max(am_arr[fin2]))))\n\n        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n        used_fmt, final_path = write_part(cleaned, out_path, WRITE_FORMAT)\n\n        manifest_rows.append({\n            \"split\": split_name,\n            \"which\": which,\n            \"part\": int(part_idx),\n            \"path\": str(final_path),\n            \"rows\": int(n_rows),\n            \"format\": str(used_fmt),\n        })\n\n        part_idx += 1\n        del cleaned, ch\n        if part_idx % 10 == 0:\n            gc.collect()\n\n    dt = time.time() - t0\n    summary_rows.append({\n        \"split\": split_name,\n        \"which\": which,\n        \"parts\": int(part_idx),\n        \"rows\": int(n_rows_total),\n        \"det_pos_frac\": float(n_det_pos / max(n_rows_total, 1)),\n        \"mag_min\": (mag_min if np.isfinite(mag_min) else np.nan),\n        \"mag_max\": (mag_max if np.isfinite(mag_max) else np.nan),\n        \"asinh_mag_min\": (asinh_mag_min if np.isfinite(asinh_mag_min) else np.nan),\n        \"asinh_mag_max\": (asinh_mag_max if np.isfinite(asinh_mag_max) else np.nan),\n        \"dropped_time_rows\": int(dropped_time_total),\n        \"nan_flux_rows\": int(nan_flux_total),\n        \"dropped_nan_flux_rows\": int(dropped_flux_total),\n        \"dropped_bad_filter_rows\": int(dropped_bad_filter_total),\n        \"skipped_empty_parts\": int(skipped_empty_parts),\n        \"sec\": float(dt),\n    })\n\n    print(\n        f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | \"\n        f\"det_pos%={100*(n_det_pos/max(n_rows_total,1)):.2f}% | \"\n        f\"bad_filter_drop={dropped_bad_filter_total:,} | nan_flux={nan_flux_total:,} | drop_nan_flux={dropped_flux_total:,} | drop_time={dropped_time_total:,} | \"\n        f\"mag=[{(mag_min if np.isfinite(mag_min) else np.nan):.2f},{(mag_max if np.isfinite(mag_max) else np.nan):.2f}] | \"\n        f\"asinh_mag=[{(asinh_mag_min if np.isfinite(asinh_mag_min) else np.nan):.2f},{(asinh_mag_max if np.isfinite(asinh_mag_max) else np.nan):.2f}] | \"\n        f\"time={dt:.1f}s\"\n    )\n\nprint(f\"[Stage 4] REBUILD_MODE={REBUILD_MODE} | Writing to: {LC_CLEAN_DIR}\")\nprint(f\"[Stage 4] WRITE_FORMAT={WRITE_FORMAT} | CHUNKSIZE={CHUNKSIZE:,} | DROP_NAN_FLUX_ROWS={DROP_NAN_FLUX_ROWS} | STRICT_FILTER={STRICT_FILTER}\")\n\nfor s in splits_to_use:\n    process_split(s, \"train\")\n    process_split(s, \"test\")\n\ndf_parts_manifest = pd.DataFrame(manifest_rows)\ndf_summary  = pd.DataFrame(summary_rows)\n\nmanifest_path = LC_CLEAN_DIR / \"lc_clean_mag_manifest.csv\"\nsummary_path  = LC_CLEAN_DIR / \"lc_clean_mag_summary.csv\"\ndf_parts_manifest.to_csv(manifest_path, index=False)\ndf_summary.to_csv(summary_path, index=False)\n\ncfg_path = LC_CLEAN_DIR / \"photometric_config_mag.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\n        \"STAGE\": \"stage4\",\n        \"VERSION\": \"v7.1\",\n        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n        \"SNR_DET_POS\": float(SNR_DET_POS),\n        \"DET_SIGMA\": float(DET_SIGMA),\n        \"ERR_EPS\": float(ERR_EPS),\n        \"SNR_CLIP\": float(SNR_CLIP),\n        \"MAG_ZP\": float(MAG_ZP),\n        \"MAG_MIN\": float(MAG_MIN),\n        \"MAG_MAX\": float(MAG_MAX),\n        \"CHUNKSIZE\": int(CHUNKSIZE),\n        \"WRITE_FORMAT\": str(WRITE_FORMAT),\n        \"ONLY_SPLITS\": list(splits_to_use),\n        \"KEEP_FLUX_DEBUG\": bool(KEEP_FLUX_DEBUG),\n        \"DROP_BAD_TIME_ROWS\": bool(DROP_BAD_TIME_ROWS),\n        \"DROP_NAN_FLUX_ROWS\": bool(DROP_NAN_FLUX_ROWS),\n        \"REBUILD_MODE\": str(REBUILD_MODE),\n        \"STRICT_FILTER\": bool(STRICT_FILTER),\n        \"SCHEMA\": \"mag + asinh_mag + snr_asinh\",\n        \"COLUMNS\": [\n            \"object_id\",\"mjd\",\"band_id\",\n            \"mag\",\"mag_err\",\n            \"asinh_mag\",\"asinh_mag_err\",\n            \"snr\",\"snr_abs\",\"snr_asinh\",\n            \"detected_pos\",\n        ] + ([\"A_x\",\"flux_deext\",\"err_deext\",\"b_soft\"] if KEEP_FLUX_DEBUG else []),\n    }, f, indent=2)\n\n# sanity: manifest coverage\nif len(df_parts_manifest) == 0:\n    raise RuntimeError(\"Stage 4 produced empty manifest. Check iter_lightcurve_chunks and split routing.\")\nif (df_parts_manifest[\"rows\"] <= 0).any():\n    bad = df_parts_manifest.loc[df_parts_manifest[\"rows\"] <= 0].head(5).to_dict(\"records\")\n    raise RuntimeError(f\"Found empty part files (rows<=0). Examples: {bad}\")\n\nprint(\"\\n[Stage 4] Done.\")\nprint(f\"- LC_CLEAN_DIR  : {LC_CLEAN_DIR}\")\nprint(f\"- Saved manifest: {manifest_path}\")\nprint(f\"- Saved summary : {summary_path}\")\nprint(f\"- Saved config  : {cfg_path}\")\n\ndef get_clean_parts(split_name: str, which: str):\n    m = df_parts_manifest[(df_parts_manifest[\"split\"] == split_name) & (df_parts_manifest[\"which\"] == which)].sort_values(\"part\")\n    return m[\"path\"].astype(str).tolist()\n\nglobals().update({\n    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n    \"BAND2ID\": BAND2ID,\n    \"ID2BAND\": ID2BAND,\n    \"MAG_ZP\": MAG_ZP,\n    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n    \"lc_clean_mag_manifest\": df_parts_manifest,\n    \"lc_clean_mag_summary\": df_summary,\n    \"get_clean_parts\": get_clean_parts,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sequence Tokenization (Event-based Tokens)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL)\n# REVISI FULL v6.1 (FORCE ASINH + FEATURESET v2 + STAGE4 v7.1 compatible + robust meta + safer reuse)\n#\n# Compatible STAGE 4 schemas:\n# - v7+: columns include: asinh_mag, asinh_mag_err, detected_pos, snr (and/or snr_asinh), band_id\n# - legacy: columns include: flux_asinh, err_log1p, detected, snr, band_id\n#\n# Output:\n# - ART_DIR/seq_tokens/<split>/<train|test>/shard_*.npz\n# - ART_DIR/seq_tokens/seq_manifest_train.csv\n# - ART_DIR/seq_tokens/seq_manifest_test.csv\n# - ART_DIR/seq_tokens/seq_build_stats.csv\n# - ART_DIR/seq_tokens/seq_config.json\n#\n# NPZ shard arrays:\n# - object_id: (n_obj,) bytes\n# - x       : (total_tokens, feature_dim) float32\n# - band    : (total_tokens,) int8   (band_id per token)\n# - offsets : (n_obj+1,) int64       (start offsets for each object)\n# ============================================================\n\nimport gc, json, warnings, time, shutil, os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nfor need in [\"ART_DIR\", \"df_train_meta\", \"df_test_meta\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 0 + STAGE 2 dulu (ART_DIR + meta).\")\n\nART_DIR = Path(ART_DIR)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nCFG = globals().get(\"CFG\", {})\nCFG = CFG if isinstance(CFG, dict) else {}\nSEED = int(globals().get(\"SEED\", 2025))\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\ndef _safe_string_series(s: pd.Series) -> pd.Series:\n    try:\n        return s.astype(\"string\").str.strip()\n    except Exception:\n        return s.astype(str).str.strip()\n\ndef _find_stage4_manifest(art_dir: Path):\n    cand = art_dir / \"lc_clean_mag\" / \"lc_clean_mag_manifest.csv\"\n    if cand.exists():\n        return cand\n    root = Path(\"/kaggle/working/mallorn_run\")\n    if not root.exists():\n        return None\n    cands = list(root.glob(\"run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\"))\n    if not cands:\n        cands = list(root.glob(\"run_*/**/lc_clean_mag_manifest.csv\"))\n    if not cands:\n        return None\n    cands = sorted(cands, key=lambda p: p.stat().st_mtime, reverse=True)\n    return cands[0]\n\ndef _sync_dirs_from_manifest(manifest_csv: Path):\n    lc_clean_dir = manifest_csv.parent\n    art_dir_new  = lc_clean_dir.parent\n    run_dir_new  = art_dir_new.parent\n    return run_dir_new, art_dir_new, lc_clean_dir\n\ndef _read_meta_file(art_dir_synced: Path, which: str) -> pd.DataFrame:\n    pq = art_dir_synced / f\"{which}_meta.parquet\"\n    csv = art_dir_synced / f\"{which}_meta.csv\"\n    if pq.exists():\n        df = pd.read_parquet(pq)\n    elif csv.exists():\n        df = pd.read_csv(csv)\n    else:\n        return None\n    if isinstance(df.index, pd.RangeIndex) and (\"object_id\" in df.columns):\n        df = df.set_index(\"object_id\", drop=True)\n    elif (\"object_id\" in df.columns) and (df.index.name != \"object_id\"):\n        df = df.set_index(\"object_id\", drop=True)\n    if isinstance(df.index, pd.RangeIndex):\n        for c in [\"Unnamed: 0\", \"index\"]:\n            if c in df.columns:\n                df = df.set_index(c, drop=True)\n                break\n    df.index = df.index.astype(\"string\")\n    return df\n\ndef _load_meta_if_needed(art_dir_synced: Path):\n    global df_train_meta, df_test_meta\n    cand_train = _read_meta_file(art_dir_synced, \"train\")\n    cand_test  = _read_meta_file(art_dir_synced, \"test\")\n    if cand_train is None or cand_test is None:\n        return False, \"meta file not found in synced ART_DIR; keep in-memory\"\n    try:\n        if (len(df_train_meta) != len(cand_train)) or (len(df_test_meta) != len(cand_test)):\n            df_train_meta = cand_train\n            df_test_meta  = cand_test\n            return True, \"reloaded meta due to size mismatch\"\n        sample_ids = df_train_meta.index[:5].astype(str).tolist()\n        if not all((sid in cand_train.index) for sid in sample_ids):\n            df_train_meta = cand_train\n            df_test_meta  = cand_test\n            return True, \"reloaded meta due to id mismatch\"\n        return False, \"meta already consistent\"\n    except Exception as e:\n        return False, f\"meta reload skipped ({type(e).__name__}: {e})\"\n\ndef _ensure_meta_features(meta_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Ensure columns needed for per-token meta exist: EBV_clip, log1pZ, zerr_rel, is_photoz.\"\"\"\n    df = meta_df.copy(deep=False)\n\n    # EBV_clip\n    if \"EBV_clip\" not in df.columns:\n        if \"EBV\" in df.columns:\n            df[\"EBV_clip\"] = pd.to_numeric(df[\"EBV\"], errors=\"coerce\")\n        else:\n            df[\"EBV_clip\"] = 0.0\n    df[\"EBV_clip\"] = pd.to_numeric(df[\"EBV_clip\"], errors=\"coerce\").fillna(0.0).astype(np.float32)\n\n    # choose Z source\n    if \"Z\" in df.columns:\n        z = pd.to_numeric(df[\"Z\"], errors=\"coerce\")\n    elif \"Z_clip\" in df.columns:\n        z = pd.to_numeric(df[\"Z_clip\"], errors=\"coerce\")\n    elif \"photoz\" in df.columns:\n        z = pd.to_numeric(df[\"photoz\"], errors=\"coerce\")\n    else:\n        z = pd.Series(0.0, index=df.index)\n\n    z = z.fillna(0.0)\n    z_pos = np.maximum(z.to_numpy(dtype=np.float32, copy=False), 0.0).astype(np.float32)\n    df[\"log1pZ\"] = np.log1p(z_pos).astype(np.float32)\n\n    # Z_err relative\n    if \"Z_err\" in df.columns:\n        zerr = pd.to_numeric(df[\"Z_err\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float32, copy=False)\n    elif \"Z_err_clip\" in df.columns:\n        zerr = pd.to_numeric(df[\"Z_err_clip\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float32, copy=False)\n    else:\n        zerr = np.zeros((len(df),), dtype=np.float32)\n\n    denom = np.maximum(z_pos, np.float32(1e-3))\n    df[\"zerr_rel\"] = (np.asarray(zerr, dtype=np.float32) / denom).astype(np.float32)\n\n    # is_photoz\n    if \"is_photoz\" in df.columns:\n        ip = pd.to_numeric(df[\"is_photoz\"], errors=\"coerce\").fillna(0).astype(np.int8)\n    elif \"photoz\" in df.columns:\n        ip = pd.to_numeric(df[\"photoz\"], errors=\"coerce\").notna().astype(np.int8)\n    else:\n        ip = pd.Series(0, index=df.index, dtype=np.int8)\n    df[\"is_photoz\"] = ip.astype(np.int8)\n\n    return df\n\n# ----------------------------\n# 2) Locate STAGE 4 output\n# ----------------------------\nmanifest_csv = _find_stage4_manifest(ART_DIR)\nif manifest_csv is None:\n    root = Path(\"/kaggle/working/mallorn_run\")\n    runs = sorted([p.name for p in root.glob(\"run_*\") if p.is_dir()])[-15:] if root.exists() else []\n    raise RuntimeError(\n        \"Output STAGE 4 (lc_clean_mag_manifest.csv) tidak ditemukan.\\n\"\n        f\"- ART_DIR saat ini: {ART_DIR}\\n\"\n        f\"- Expected: {ART_DIR/'lc_clean_mag'}\\n\"\n        f\"- Runs available (last 15): {runs}\\n\"\n        \"Solusi: pastikan STAGE 4 selesai dan menulis artifacts/lc_clean_mag.\"\n    )\n\nRUN_DIR, ART_DIR, LC_CLEAN_DIR = _sync_dirs_from_manifest(manifest_csv)\nART_DIR = Path(ART_DIR); LC_CLEAN_DIR = Path(LC_CLEAN_DIR)\n\nprint(\"STAGE 5 ROUTING SYNC OK\")\nprint(f\"- RUN_DIR      : {RUN_DIR}\")\nprint(f\"- ART_DIR      : {ART_DIR}\")\nprint(f\"- LC_CLEAN_DIR : {LC_CLEAN_DIR}\")\nprint(f\"- manifest_csv : {manifest_csv}\")\n\nreloaded, msg = _load_meta_if_needed(ART_DIR)\nprint(f\"- meta_sync    : {msg}\")\n\n# normalize meta index\ndf_train_meta = df_train_meta.copy(deep=False)\ndf_test_meta  = df_test_meta.copy(deep=False)\ndf_train_meta.index = df_train_meta.index.astype(\"string\")\ndf_test_meta.index  = df_test_meta.index.astype(\"string\")\n\n# ensure meta derived features exist\ndf_train_meta = _ensure_meta_features(df_train_meta)\ndf_test_meta  = _ensure_meta_features(df_test_meta)\n\n# ----------------------------\n# 3) Load & validate Stage4 manifest\n# ----------------------------\n_df_clean_manifest = pd.read_csv(manifest_csv)\n_df_clean_manifest.columns = [c.strip() for c in _df_clean_manifest.columns]\n\nneed_cols = {\"split\", \"which\", \"part\", \"path\"}\nmiss = sorted(list(need_cols - set(_df_clean_manifest.columns)))\nif miss:\n    raise RuntimeError(f\"Manifest STAGE 4 missing columns: {miss} | cols={list(_df_clean_manifest.columns)}\")\n\npaths = _df_clean_manifest[\"path\"].astype(str).tolist()\nmissing_paths = [p for p in paths if not Path(p).exists()]\nif missing_paths:\n    raise RuntimeError(\n        \"Ada file part STAGE 4 yang hilang.\\n\"\n        f\"Missing count={len(missing_paths)} | contoh={missing_paths[:10]}\\n\"\n        \"Solusi: rerun STAGE 4 (wipe_all) untuk regenerasi cache.\"\n    )\n\ndef get_clean_parts(split_name: str, which: str):\n    m = _df_clean_manifest[(_df_clean_manifest[\"split\"] == split_name) & (_df_clean_manifest[\"which\"] == which)]\n    if m.empty:\n        return []\n    return m.sort_values(\"part\")[\"path\"].astype(str).tolist()\n\n# ----------------------------\n# 4) Recover SPLIT_LIST + routing ids\n# ----------------------------\nif \"split\" not in df_train_meta.columns or \"split\" not in df_test_meta.columns:\n    raise RuntimeError(\"Kolom `split` tidak ada di meta. Pastikan STAGE 2/3 membuat routing split di meta.\")\n\nsplits_meta = sorted(set(df_train_meta[\"split\"].astype(str).tolist()) | set(df_test_meta[\"split\"].astype(str).tolist()))\nsplits_in_manifest = sorted(set(_df_clean_manifest[\"split\"].astype(str).tolist()))\nSPLIT_LIST = sorted(set(splits_in_manifest) | set(splits_meta))\nSPLITS_TO_CONSIDER = [s for s in SPLIT_LIST if s in splits_in_manifest]\n\ntrain_ids_by_split = {s: [] for s in SPLIT_LIST}\nfor oid, sp in df_train_meta[\"split\"].astype(str).items():\n    if sp in train_ids_by_split:\n        train_ids_by_split[sp].append(str(oid))\n\ntest_ids_by_split = {s: [] for s in SPLIT_LIST}\nfor oid, sp in df_test_meta[\"split\"].astype(str).items():\n    if sp in test_ids_by_split:\n        test_ids_by_split[sp].append(str(oid))\n\n# ----------------------------\n# 5) Settings (recommended defaults)\n# ----------------------------\nONLY_SPLITS = CFG.get(\"STAGE5_ONLY_SPLITS\", None)\n\nREBUILD_MODE = str(CFG.get(\"STAGE5_REBUILD_MODE\", \"wipe_all\")).lower()   # \"wipe_all\" or \"reuse_if_exists\"\nif REBUILD_MODE not in (\"wipe_all\", \"reuse_if_exists\"):\n    REBUILD_MODE = \"wipe_all\"\n\nCOMPRESS_NPZ = bool(CFG.get(\"STAGE5_COMPRESS_NPZ\", False))\nSHARD_MAX_OBJECTS = int(CFG.get(\"STAGE5_SHARD_MAX_OBJECTS\", 1500))\n\nSNR_TANH_SCALE = float(CFG.get(\"STAGE5_SNR_TANH_SCALE\", 10.0))\nTIME_CLIP_MAX_DAYS = CFG.get(\"STAGE5_TIME_CLIP_MAX_DAYS\", None)\nTIME_CLIP_MAX_DAYS = None if TIME_CLIP_MAX_DAYS in [None, \"None\", \"none\", \"\"] else float(TIME_CLIP_MAX_DAYS)\nDROP_BAD_TIME_ROWS = bool(CFG.get(\"STAGE5_DROP_BAD_TIME_ROWS\", True))\n\nL_MAX = int(CFG.get(\"L_MAX\", 256))\nTRUNC_POLICY = str(CFG.get(\"TRUNC_POLICY\", \"smart\")).lower()      # smart/head/none\nKEEP_DET_FRAC = float(CFG.get(\"KEEP_DET_FRAC\", 0.70))\nKEEP_EDGE = bool(CFG.get(\"KEEP_EDGE\", True))\nUSE_RESTFRAME_TIME = bool(CFG.get(\"USE_RESTFRAME_TIME\", True))\n\nNUM_BUCKETS = int(CFG.get(\"SEQ_NUM_BUCKETS\", 256))\n\n# FORCE ASINH\nTOKEN_MODE_FORCE = \"asinh\"\nFEATURE_SET = str(CFG.get(\"SEQ_FEATURE_SET\", \"v2\")).lower()   # v2 recommended\nADD_OBJ_META_PER_TOKEN = bool(CFG.get(\"SEQ_ADD_META_PER_TOKEN\", True))\n\nOBJ_META_COLS = [\"EBV_clip\", \"log1pZ\", \"zerr_rel\", \"is_photoz\"]\n\nSEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\nSEQ_DIR.mkdir(parents=True, exist_ok=True)\n\nif FEATURE_SET == \"v2\":\n    CORE_FEATURES = [\"t_rel_log\", \"dt_log\", \"dt_band_log\", \"signal\", \"err_log\", \"snr_tanh\", \"detected\", \"band_change\", \"delta_signal\"]\nelse:\n    CORE_FEATURES = [\"t_rel_log\", \"dt_log\", \"signal\", \"err_log\", \"snr_tanh\", \"detected\"]\n\nMETA_FEATURES = [f\"meta_{c}\" for c in OBJ_META_COLS] if ADD_OBJ_META_PER_TOKEN else []\nFEATURE_NAMES = CORE_FEATURES + META_FEATURES\nFEATURE_DIM = len(FEATURE_NAMES)\n\n# ----------------------------\n# 6) Reader for cleaned parts (schema auto; force-asinh)\n# ----------------------------\nBASE_COLS_MIN = {\"object_id\", \"mjd\", \"band_id\", \"snr\"}\n\ndef _read_clean_part(path: str) -> pd.DataFrame:\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(f\"Clean part missing: {p}\")\n\n    if p.suffix == \".parquet\":\n        df = pd.read_parquet(p)\n    elif p.name.endswith(\".csv.gz\"):\n        df = pd.read_csv(p, compression=\"gzip\")\n    else:\n        df = pd.read_csv(p)\n\n    df.columns = [c.strip() for c in df.columns]\n    cols = set(df.columns)\n\n    if not BASE_COLS_MIN.issubset(cols):\n        raise RuntimeError(f\"Clean part missing base cols {sorted(list(BASE_COLS_MIN - cols))} | file={p}\")\n\n    # unify detection column -> detected (int8)\n    if \"detected\" in cols:\n        df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n    elif \"detected_pos\" in cols:\n        df[\"detected\"] = pd.to_numeric(df[\"detected_pos\"], errors=\"coerce\").fillna(0).astype(np.int8)\n    else:\n        df[\"detected\"] = (pd.to_numeric(df[\"snr\"], errors=\"coerce\").fillna(0.0).astype(np.float32) > 0).astype(np.int8)\n\n    # FORCE ASINH signal/err\n    if (\"asinh_mag\" in cols) and (\"asinh_mag_err\" in cols):\n        df[\"signal\"] = pd.to_numeric(df[\"asinh_mag\"], errors=\"coerce\").astype(np.float32)\n        err_lin = pd.to_numeric(df[\"asinh_mag_err\"], errors=\"coerce\").astype(np.float32).to_numpy(copy=False)\n        err_lin = np.nan_to_num(err_lin, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n        err_lin = np.maximum(err_lin, 0.0).astype(np.float32)\n        df[\"err_log\"] = np.log1p(err_lin).astype(np.float32)\n    elif (\"flux_asinh\" in cols) and (\"err_log1p\" in cols):\n        df[\"signal\"] = pd.to_numeric(df[\"flux_asinh\"], errors=\"coerce\").astype(np.float32)\n        df[\"err_log\"] = pd.to_numeric(df[\"err_log1p\"], errors=\"coerce\").astype(np.float32)\n    else:\n        raise RuntimeError(\n            \"FORCE ASINH gagal: tidak menemukan pasangan kolom asinh_mag/asinh_mag_err \"\n            \"atau flux_asinh/err_log1p.\\n\"\n            f\"Found cols sample={list(df.columns)[:40]} | file={p}\"\n        )\n\n    df[\"object_id\"] = _safe_string_series(df[\"object_id\"])\n    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n    df[\"snr\"] = pd.to_numeric(df[\"snr\"], errors=\"coerce\").astype(np.float32)\n    df[\"signal\"] = pd.to_numeric(df[\"signal\"], errors=\"coerce\").astype(np.float32)\n    df[\"err_log\"] = pd.to_numeric(df[\"err_log\"], errors=\"coerce\").astype(np.float32)\n\n    # band sanity -> 0..5\n    b = df[\"band_id\"].to_numpy(copy=False)\n    okb = (b >= 0) & (b <= 5)\n    if not np.all(okb):\n        df = df[okb]\n\n    if DROP_BAD_TIME_ROWS:\n        df = df[np.isfinite(df[\"mjd\"].to_numpy(copy=False))]\n\n    keep = [\"object_id\", \"mjd\", \"band_id\", \"snr\", \"detected\", \"signal\", \"err_log\"]\n    return df[keep]\n\n# ----------------------------\n# 7) Truncation (smart)\n# ----------------------------\ndef _smart_truncate(mjd, det, snr, Lmax: int):\n    n = len(mjd)\n    if n <= Lmax:\n        return np.arange(n, dtype=np.int64)\n\n    idx_all = np.arange(n, dtype=np.int64)\n    keep = set()\n    if KEEP_EDGE and n >= 2:\n        keep.add(0); keep.add(n - 1)\n\n    det_idx = idx_all[det.astype(bool)]\n    k_det = int(max(0, min(len(det_idx), int(np.floor(Lmax * KEEP_DET_FRAC)))))\n    if k_det > 0 and len(det_idx) > 0:\n        score = np.abs(snr[det_idx])\n        top = det_idx[np.argsort(-score)[:k_det]]\n        for i in top.tolist():\n            keep.add(int(i))\n\n    if len(keep) < Lmax:\n        rem = [i for i in idx_all.tolist() if i not in keep]\n        need = Lmax - len(keep)\n        if rem and need > 0:\n            pick = np.linspace(0, len(rem) - 1, num=need, dtype=int)\n            for p in pick.tolist():\n                keep.add(int(rem[p]))\n\n    out = np.array(sorted(keep), dtype=np.int64)\n    if len(out) > Lmax:\n        pos = np.linspace(0, len(out) - 1, num=Lmax, dtype=int)\n        out = out[pos]\n    return out\n\n# ----------------------------\n# 8) Meta per-token helper\n# ----------------------------\ndef _get_obj_meta_vec(meta_df: pd.DataFrame, oid: str) -> np.ndarray:\n    if (not ADD_OBJ_META_PER_TOKEN) or (oid not in meta_df.index):\n        return np.zeros((len(OBJ_META_COLS),), dtype=np.float32)\n    vals = []\n    for c in OBJ_META_COLS:\n        if c in meta_df.columns:\n            v = meta_df.loc[oid, c]\n            v = float(v) if (v is not None and np.isfinite(v)) else 0.0\n        else:\n            v = 0.0\n        vals.append(v)\n    return np.asarray(vals, dtype=np.float32)\n\ndef build_empty_tokens():\n    X = np.zeros((1, int(FEATURE_DIM)), dtype=np.float32)\n    B = np.full((1,), -1, dtype=np.int8)\n    return X, B, 0, 1\n\n# ----------------------------\n# 9) Build tokens per object (v2)\n# ----------------------------\ndef build_object_tokens(df_obj: pd.DataFrame, meta_df: pd.DataFrame, z_val: float = 0.0):\n    if df_obj is None or df_obj.empty:\n        return build_empty_tokens()\n\n    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n    sig  = df_obj[\"signal\"].to_numpy(dtype=np.float32, copy=False)\n    err_log = df_obj[\"err_log\"].to_numpy(dtype=np.float32, copy=False)\n\n    order = np.lexsort((band, mjd))\n    mjd = mjd[order]; band = band[order]; snr = snr[order]; det = det[order]\n    sig = sig[order]; err_log = err_log[order]\n\n    z = float(z_val) if (z_val is not None and np.isfinite(z_val)) else 0.0\n    z = max(z, 0.0)\n    denom = (1.0 + z) if USE_RESTFRAME_TIME else 1.0\n\n    t0 = float(mjd[0])\n    t_rel = ((mjd - np.float32(t0)) / np.float32(denom)).astype(np.float32)\n    if TIME_CLIP_MAX_DAYS is not None:\n        mx = np.float32(TIME_CLIP_MAX_DAYS)\n        t_rel = np.clip(t_rel, 0.0, mx)\n\n    dt = np.empty_like(t_rel, dtype=np.float32)\n    dt[0] = np.float32(0.0)\n    if len(t_rel) > 1:\n        dt[1:] = np.maximum(t_rel[1:] - t_rel[:-1], 0.0).astype(np.float32)\n\n    dt_band = np.zeros_like(dt, dtype=np.float32)\n    last_mjd = {}\n    for i in range(len(mjd)):\n        b = int(band[i])\n        if b in last_mjd:\n            dt_band[i] = max(float((mjd[i] - last_mjd[b]) / denom), 0.0)\n        last_mjd[b] = float(mjd[i])\n    if TIME_CLIP_MAX_DAYS is not None:\n        mx = np.float32(TIME_CLIP_MAX_DAYS)\n        dt = np.clip(dt, 0.0, mx)\n        dt_band = np.clip(dt_band, 0.0, mx)\n\n    t_rel_log = np.log1p(t_rel).astype(np.float32)\n    dt_log = np.log1p(dt).astype(np.float32)\n    dt_band_log = np.log1p(dt_band).astype(np.float32)\n\n    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n    det_f = det.astype(np.float32)\n\n    band_change = np.zeros((len(band),), dtype=np.float32)\n    delta_signal = np.zeros((len(sig),), dtype=np.float32)\n    if len(band) > 1:\n        band_change[1:] = (band[1:] != band[:-1]).astype(np.float32)\n        delta_signal[1:] = (sig[1:] - sig[:-1]).astype(np.float32)\n\n    if FEATURE_SET == \"v2\":\n        X = np.stack(\n            [t_rel_log, dt_log, dt_band_log, sig, err_log, snr_tanh, det_f, band_change, delta_signal],\n            axis=1\n        ).astype(np.float32)\n    else:\n        X = np.stack([t_rel_log, dt_log, sig, err_log, snr_tanh, det_f], axis=1).astype(np.float32)\n\n    if ADD_OBJ_META_PER_TOKEN:\n        oid = str(df_obj[\"object_id\"].iloc[0])\n        mv = _get_obj_meta_vec(meta_df, oid)\n        mv_rep = np.repeat(mv[None, :], repeats=X.shape[0], axis=0).astype(np.float32)\n        X = np.concatenate([X, mv_rep], axis=1).astype(np.float32)\n\n    L0 = int(X.shape[0])\n\n    if L_MAX and int(L_MAX) > 0 and X.shape[0] > int(L_MAX):\n        if TRUNC_POLICY == \"smart\":\n            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n        elif TRUNC_POLICY == \"head\":\n            keep = np.arange(int(L_MAX), dtype=np.int64)\n        elif TRUNC_POLICY in (\"none\", \"full\"):\n            keep = np.arange(X.shape[0], dtype=np.int64)\n        else:\n            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n\n        if len(keep) != X.shape[0]:\n            keep = keep.astype(np.int64)\n\n            mjd2 = mjd[keep]\n            band2 = band[keep].astype(np.int16)\n            snr2 = snr[keep].astype(np.float32)\n            det2 = det[keep].astype(np.int8)\n            sig2 = sig[keep].astype(np.float32)\n            err_log2 = err_log[keep].astype(np.float32)\n\n            t0 = float(mjd2[0])\n            t_rel2 = ((mjd2 - np.float32(t0)) / np.float32(denom)).astype(np.float32)\n            if TIME_CLIP_MAX_DAYS is not None:\n                mx = np.float32(TIME_CLIP_MAX_DAYS)\n                t_rel2 = np.clip(t_rel2, 0.0, mx)\n\n            dt2 = np.empty_like(t_rel2, dtype=np.float32)\n            dt2[0] = np.float32(0.0)\n            if len(t_rel2) > 1:\n                dt2[1:] = np.maximum(t_rel2[1:] - t_rel2[:-1], 0.0).astype(np.float32)\n\n            dtb2 = np.zeros_like(dt2, dtype=np.float32)\n            last_mjd2 = {}\n            for i in range(len(mjd2)):\n                b = int(band2[i])\n                if b in last_mjd2:\n                    dtb2[i] = max(float((mjd2[i] - last_mjd2[b]) / denom), 0.0)\n                last_mjd2[b] = float(mjd2[i])\n\n            if TIME_CLIP_MAX_DAYS is not None:\n                mx = np.float32(TIME_CLIP_MAX_DAYS)\n                dt2 = np.clip(dt2, 0.0, mx)\n                dtb2 = np.clip(dtb2, 0.0, mx)\n\n            t_rel_log2 = np.log1p(t_rel2).astype(np.float32)\n            dt_log2 = np.log1p(dt2).astype(np.float32)\n            dtb_log2 = np.log1p(dtb2).astype(np.float32)\n\n            snr_tanh2 = np.tanh(snr2 / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n            det_f2 = det2.astype(np.float32)\n\n            band_change2 = np.zeros((len(band2),), dtype=np.float32)\n            delta_signal2 = np.zeros((len(sig2),), dtype=np.float32)\n            if len(band2) > 1:\n                band_change2[1:] = (band2[1:] != band2[:-1]).astype(np.float32)\n                delta_signal2[1:] = (sig2[1:] - sig2[:-1]).astype(np.float32)\n\n            if FEATURE_SET == \"v2\":\n                Xcore2 = np.stack(\n                    [t_rel_log2, dt_log2, dtb_log2, sig2, err_log2, snr_tanh2, det_f2, band_change2, delta_signal2],\n                    axis=1\n                ).astype(np.float32)\n            else:\n                Xcore2 = np.stack([t_rel_log2, dt_log2, sig2, err_log2, snr_tanh2, det_f2], axis=1).astype(np.float32)\n\n            if ADD_OBJ_META_PER_TOKEN:\n                oid = str(df_obj[\"object_id\"].iloc[0])\n                mv = _get_obj_meta_vec(meta_df, oid)\n                mv_rep = np.repeat(mv[None, :], repeats=Xcore2.shape[0], axis=0).astype(np.float32)\n                X2 = np.concatenate([Xcore2, mv_rep], axis=1).astype(np.float32)\n            else:\n                X2 = Xcore2\n\n            return X2, band2.astype(np.int8), L0, int(X2.shape[0])\n\n    return X, band.astype(np.int8), L0, int(X.shape[0])\n\n# ----------------------------\n# 10) Shard writer + reuse manifest\n# ----------------------------\ndef save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    obj_arr = np.asarray(object_ids, dtype=\"S\")\n    if COMPRESS_NPZ:\n        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n    else:\n        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n\ndef reconstruct_manifest_from_shards(split_name: str, which: str, out_dir: Path):\n    rows = []\n    for sp in sorted(out_dir.glob(\"shard_*.npz\")):\n        data = np.load(sp, allow_pickle=False)\n        obj = data[\"object_id\"]\n        offsets = data[\"offsets\"].astype(np.int64)\n        if offsets.ndim != 1 or offsets.size < 2:\n            continue\n        if len(obj) != (offsets.size - 1):\n            raise RuntimeError(f\"Bad shard (object_id len != offsets-1): {sp}\")\n        lengths = offsets[1:] - offsets[:-1]\n        for i in range(len(lengths)):\n            oid = obj[i]\n            oid = oid.decode(\"utf-8\", errors=\"ignore\") if isinstance(oid, (bytes, np.bytes_)) else str(oid)\n            rows.append({\n                \"object_id\": oid,\n                \"split\": split_name,\n                \"which\": which,\n                \"shard\": str(sp),\n                \"start\": int(offsets[i]),\n                \"length\": int(lengths[i]),\n            })\n    return rows\n\n# ----------------------------\n# 11) Bucket builder (IO hemat)\n# ----------------------------\ndef build_sequences_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 256):\n    try:\n        import pyarrow as pa\n        import pyarrow.parquet as pq\n    except Exception as e:\n        raise RuntimeError(\"pyarrow tidak tersedia. Di Kaggle biasanya ada.\") from e\n\n    parts = get_clean_parts(split_name, which)\n    if not parts:\n        raise RuntimeError(f\"Tidak ada cleaned parts untuk {split_name}/{which}. Cek STAGE 4 output.\")\n\n    tmp_dir = Path(ART_DIR) / \"tmp_seq_buckets\" / split_name / which\n    if tmp_dir.exists():\n        shutil.rmtree(tmp_dir, ignore_errors=True)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n\n    writers = {}\n    kept_rows = 0\n    t0 = time.time()\n\n    def bucket_idx(series_objid: pd.Series) -> np.ndarray:\n        h = pd.util.hash_pandas_object(series_objid, index=False).to_numpy(dtype=np.uint64, copy=False)\n        return (h % np.uint64(num_buckets)).astype(np.int16)\n\n    try:\n        for p in parts:\n            df = _read_clean_part(p)\n            if df.empty:\n                continue\n\n            df = df[df[\"object_id\"].isin(expected_ids)]\n            if df.empty:\n                continue\n\n            kept_rows += int(len(df))\n            bidx = bucket_idx(df[\"object_id\"])\n            df[\"_b\"] = bidx\n\n            for b, sub in df.groupby(\"_b\", sort=False):\n                sub = sub.drop(columns=[\"_b\"])\n                if sub.empty:\n                    continue\n                fp = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n                table = pa.Table.from_pandas(sub, preserve_index=False)\n                if int(b) not in writers:\n                    writers[int(b)] = pq.ParquetWriter(fp, table.schema, compression=\"snappy\")\n                writers[int(b)].write_table(table)\n\n            del df\n            gc.collect()\n\n    finally:\n        for w in list(writers.values()):\n            try: w.close()\n            except Exception: pass\n\n    meta = df_train_meta if which == \"train\" else df_test_meta\n\n    manifest_rows = []\n    shard_idx = 0\n    batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n    built_ids = set()\n    len_before, len_after = [], []\n\n    def flush_shard_local():\n        nonlocal shard_idx, batch_obj_ids, batch_X, batch_B, batch_len, manifest_rows\n        if not batch_obj_ids:\n            return\n        lengths = np.asarray(batch_len, dtype=np.int64)\n        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n        offsets[1:] = np.cumsum(lengths)\n\n        Xc = np.concatenate(batch_X, axis=0).astype(np.float32)\n        Bc = np.concatenate(batch_B, axis=0).astype(np.int8)\n\n        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n        save_shard(shard_path, batch_obj_ids, Xc, Bc, offsets)\n\n        for i, oid in enumerate(batch_obj_ids):\n            manifest_rows.append({\n                \"object_id\": oid,\n                \"split\": split_name,\n                \"which\": which,\n                \"shard\": str(shard_path),\n                \"start\": int(offsets[i]),\n                \"length\": int(lengths[i]),\n            })\n\n        shard_idx += 1\n        batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n        gc.collect()\n\n    for bf in sorted(tmp_dir.glob(\"bucket_*.parquet\")):\n        dfb = pd.read_parquet(bf)\n        if dfb.empty:\n            continue\n\n        for oid, g in dfb.groupby(\"object_id\", sort=False):\n            oid = str(oid)\n            if oid in built_ids:\n                continue\n\n            z_val = 0.0\n            if USE_RESTFRAME_TIME and (oid in meta.index):\n                if \"Z\" in meta.columns:\n                    z_val = float(meta.loc[oid, \"Z\"])\n                elif \"Z_clip\" in meta.columns:\n                    z_val = float(meta.loc[oid, \"Z_clip\"])\n                elif \"photoz\" in meta.columns:\n                    z_val = float(meta.loc[oid, \"photoz\"])\n\n            X, B, lb, la = build_object_tokens(g, meta_df=meta, z_val=z_val)\n            len_before.append(lb); len_after.append(la)\n\n            batch_obj_ids.append(oid)\n            batch_X.append(X)\n            batch_B.append(B)\n            batch_len.append(int(X.shape[0]))\n            built_ids.add(oid)\n\n            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                flush_shard_local()\n\n        del dfb\n        gc.collect()\n\n    # fill missing objects with empty token\n    missing_ids = list(expected_ids - built_ids)\n    if missing_ids:\n        for oid in missing_ids:\n            oid = str(oid)\n            X, B, lb, la = build_empty_tokens()\n            len_before.append(lb); len_after.append(la)\n\n            batch_obj_ids.append(oid)\n            batch_X.append(X)\n            batch_B.append(B)\n            batch_len.append(int(X.shape[0]))\n            built_ids.add(oid)\n\n            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                flush_shard_local()\n\n    flush_shard_local()\n    shutil.rmtree(tmp_dir, ignore_errors=True)\n\n    st = {\n        \"kept_rows\": int(kept_rows),\n        \"built_objects\": int(len(built_ids)),\n        \"missing_filled\": int(len(missing_ids)),\n        \"len_before_mean\": float(np.mean(len_before)) if len_before else 0.0,\n        \"len_before_p95\": float(np.quantile(len_before, 0.95)) if len_before else 0.0,\n        \"len_after_mean\": float(np.mean(len_after)) if len_after else 0.0,\n        \"len_after_p95\": float(np.quantile(len_after, 0.95)) if len_after else 0.0,\n        \"truncated_frac\": float(np.mean([a < b for a, b in zip(len_after, len_before)])) if len_before else 0.0,\n        \"time_s\": float(time.time() - t0),\n    }\n    return manifest_rows, st\n\n# ----------------------------\n# 12) RUN\n# ----------------------------\nsplits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLITS_TO_CONSIDER\nsplits_to_run = list(splits_to_run)\n\nprint(\"\\n[Stage 5] SETTINGS\")\nprint(f\"- TOKEN_MODE_FORCE: {TOKEN_MODE_FORCE}\")\nprint(f\"- FEATURE_SET     : {FEATURE_SET} | dim={FEATURE_DIM}\")\nprint(f\"- ADD_META        : {ADD_OBJ_META_PER_TOKEN} | meta_cols={OBJ_META_COLS}\")\nprint(f\"- L_MAX/TRUNC     : {L_MAX} / {TRUNC_POLICY} (KEEP_DET_FRAC={KEEP_DET_FRAC})\")\nprint(f\"- NUM_BUCKETS     : {NUM_BUCKETS} | SHARD_MAX_OBJECTS={SHARD_MAX_OBJECTS}\")\nprint(f\"- REBUILD_MODE    : {REBUILD_MODE} | COMPRESS_NPZ={COMPRESS_NPZ}\")\n\nall_manifest_train, all_manifest_test, split_run_stats = [], [], []\n\ndef expected_set_for(split_name: str, which: str) -> set:\n    return set(train_ids_by_split.get(split_name, [])) if which == \"train\" else set(test_ids_by_split.get(split_name, []))\n\nfor split_name in splits_to_run:\n    for which in [\"train\", \"test\"]:\n        out_dir = SEQ_DIR / split_name / which\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        expected_ids = expected_set_for(split_name, which)\n        if len(expected_ids) == 0:\n            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}.\")\n\n        shard_exists = any(out_dir.glob(\"shard_*.npz\"))\n        if REBUILD_MODE == \"reuse_if_exists\" and shard_exists:\n            print(f\"\\n[Stage 5] REUSE (exists): {split_name}/{which}\")\n            man_rows = reconstruct_manifest_from_shards(split_name, which, out_dir)\n            if not man_rows:\n                raise RuntimeError(f\"REUSE mode aktif tapi gagal rekonstruksi manifest: {out_dir}\")\n\n            got_ids = set([r[\"object_id\"] for r in man_rows])\n            miss_ids = expected_ids - got_ids\n            if miss_ids:\n                raise RuntimeError(\n                    f\"REUSE shard tidak cover semua expected ids untuk {split_name}/{which}. \"\n                    f\"missing={len(miss_ids)} (contoh={list(sorted(miss_ids))[:5]}). \"\n                    \"Solusi: set STAGE5_REBUILD_MODE='wipe_all' untuk rebuild bersih.\"\n                )\n\n            if which == \"train\":\n                all_manifest_train.extend(man_rows)\n            else:\n                all_manifest_test.extend(man_rows)\n\n            split_run_stats.append({\n                \"split\": split_name, \"which\": which,\n                \"kept_rows\": 0,\n                \"built_objects\": len(got_ids),\n                \"missing_filled\": 0,\n                \"len_before_mean\": 0.0,\n                \"len_before_p95\": 0.0,\n                \"len_after_mean\": 0.0,\n                \"len_after_p95\": 0.0,\n                \"truncated_frac\": 0.0,\n                \"time_s\": 0.0,\n            })\n            continue\n        else:\n            for f in out_dir.glob(\"shard_*.npz\"):\n                try: f.unlink()\n                except Exception: pass\n\n        print(f\"\\n[Stage 5] {split_name}/{which} | expected={len(expected_ids):,}\")\n\n        manifest_rows, st = build_sequences_bucket(\n            split_name=split_name,\n            which=which,\n            expected_ids=expected_ids,\n            out_dir=out_dir,\n            num_buckets=NUM_BUCKETS\n        )\n\n        print(f\"[Stage 5] OK: built={st['built_objects']:,} (missing_filled={st['missing_filled']:,}) | \"\n              f\"kept_rows={st['kept_rows']:,} | \"\n              f\"len_mean {st['len_before_mean']:.1f}->{st['len_after_mean']:.1f} | \"\n              f\"p95 {st['len_before_p95']:.1f}->{st['len_after_p95']:.1f} | \"\n              f\"trunc%={st['truncated_frac']*100:.1f}% | \"\n              f\"time={st['time_s']:.2f}s\")\n\n        split_run_stats.append({\"split\": split_name, \"which\": which, **st})\n\n        if which == \"train\":\n            all_manifest_train.extend(manifest_rows)\n        else:\n            all_manifest_test.extend(manifest_rows)\n\n        gc.collect()\n\n# ----------------------------\n# 13) Save manifests + stats + config\n# ----------------------------\ndf_m_train = pd.DataFrame(all_manifest_train)\ndf_m_test  = pd.DataFrame(all_manifest_test)\n\nif not df_m_train.empty:\n    df_m_train = df_m_train.sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\nif not df_m_test.empty:\n    df_m_test = df_m_test.sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n\nmtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\nmtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\ndf_m_train.to_csv(mtrain_path, index=False)\ndf_m_test.to_csv(mtest_path, index=False)\n\ndf_stats = pd.DataFrame(split_run_stats)\nstats_path = SEQ_DIR / \"seq_build_stats.csv\"\ndf_stats.to_csv(stats_path, index=False)\n\ncfg_out = {\n    \"stage\": \"stage5\",\n    \"version\": \"v6.1\",\n    \"token_mode\": \"asinh\",\n    \"token_mode_force\": \"asinh\",\n    \"feature_set\": FEATURE_SET,\n    \"feature_names\": FEATURE_NAMES,\n    \"feature_dim\": int(FEATURE_DIM),\n    \"obj_meta_cols\": OBJ_META_COLS,\n    \"add_meta_per_token\": bool(ADD_OBJ_META_PER_TOKEN),\n    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n    \"compress_npz\": bool(COMPRESS_NPZ),\n    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n    \"num_buckets\": int(NUM_BUCKETS),\n    \"L_MAX\": int(L_MAX),\n    \"TRUNC_POLICY\": str(TRUNC_POLICY),\n    \"KEEP_DET_FRAC\": float(KEEP_DET_FRAC),\n    \"KEEP_EDGE\": bool(KEEP_EDGE),\n    \"USE_RESTFRAME_TIME\": bool(USE_RESTFRAME_TIME),\n    \"REBUILD_MODE\": str(REBUILD_MODE),\n    \"RUN_DIR_USED\": str(RUN_DIR),\n    \"ART_DIR_USED\": str(ART_DIR),\n    \"LC_CLEAN_DIR_USED\": str(LC_CLEAN_DIR),\n    \"manifest_csv\": str(manifest_csv),\n    \"stage4_schema_hint\": \"v7(asinh_mag/asinh_mag_err, detected_pos) or legacy(flux_asinh/err_log1p, detected)\",\n}\ncfg_path = SEQ_DIR / \"seq_config.json\"\ncfg_path.write_text(json.dumps(cfg_out, indent=2))\n\nprint(\"\\n[Stage 5] DONE\")\nprint(f\"- token_mode : asinh (forced)\")\nprint(f\"- feature_set: {FEATURE_SET} | dim={FEATURE_DIM}\")\nprint(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\nprint(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\nprint(f\"- Saved: {stats_path}\")\nprint(f\"- Saved: {cfg_path}\")\n\n# ----------------------------\n# 14) Smoke test\n# ----------------------------\ndef load_sequence(object_id: str, which: str):\n    object_id = str(object_id).strip()\n    m = df_m_train if which == \"train\" else df_m_test\n    row = m[m[\"object_id\"] == object_id]\n    if row.empty:\n        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n    r = row.iloc[0]\n    data = np.load(r[\"shard\"], allow_pickle=False)\n    start = int(r[\"start\"]); length = int(r[\"length\"])\n    X = data[\"x\"][start:start+length]\n    B = data[\"band\"][start:start+length]\n    return X, B\n\n_smoke_oid = str(df_train_meta.index[0])\nX_sm, B_sm = load_sequence(_smoke_oid, \"train\")\nprint(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\nprint(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n\nglobals().update({\n    \"RUN_DIR\": RUN_DIR,\n    \"ART_DIR\": ART_DIR,\n    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n    \"SEQ_DIR\": SEQ_DIR,\n    \"seq_manifest_train\": df_m_train,\n    \"seq_manifest_test\": df_m_test,\n    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n    \"SEQ_FEATURE_DIM\": int(FEATURE_DIM),\n    \"SEQ_TOKEN_MODE\": \"asinh\",\n    \"get_clean_parts\": get_clean_parts,\n    \"load_sequence\": load_sequence,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sequence Length Policy (Padding, Truncation, Windowing)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n# ONE CELL, Kaggle CPU-SAFE — REVISI FULL v3.1\n#\n# Upgrade v3.1:\n# - Tetap 100% kompatibel STAGE 5 v6+ (signal/err_log) & legacy\n# - Manifest safety: auto-filter ke expected ids, anti-duplicate object_id\n# - Target column discovery lebih robust (meta -> fallback train.csv jika PATHS ada)\n# - REUSE mode: strict file-set + strict coverage\n# ============================================================\n\nimport gc, json, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nfor need in [\"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(\"Missing `ART_DIR`. Jalankan STAGE 0 dulu (setup run dir).\")\n\nART_DIR = Path(ART_DIR)\nCFG = globals().get(\"CFG\", {})\nCFG = CFG if isinstance(CFG, dict) else {}\nSEED = int(globals().get(\"SEED\", 2025))\n\n# ----------------------------\n# 0a) Locate STAGE 5 outputs (seq_tokens)\n# ----------------------------\ndef _find_seq_tokens_dir(art_dir: Path) -> Path:\n    cand = art_dir / \"seq_tokens\"\n    if cand.exists() and cand.is_dir() and (cand / \"seq_manifest_train.csv\").exists() and (cand / \"seq_manifest_test.csv\").exists():\n        return cand\n    for p in art_dir.glob(\"*\"):\n        if p.is_dir() and (p / \"seq_manifest_train.csv\").exists() and (p / \"seq_manifest_test.csv\").exists():\n            return p\n    raise FileNotFoundError(\n        \"Cannot find STAGE 5 seq_tokens directory under ART_DIR.\\n\"\n        f\"ART_DIR={art_dir}\\n\"\n        \"Expected: ART_DIR/seq_tokens/seq_manifest_train.csv and seq_manifest_test.csv\"\n    )\n\nSEQ_TOKENS_DIR = Path(globals().get(\"SEQ_TOKENS_DIR\", None) or _find_seq_tokens_dir(ART_DIR))\n\np_mtr = SEQ_TOKENS_DIR / \"seq_manifest_train.csv\"\np_mte = SEQ_TOKENS_DIR / \"seq_manifest_test.csv\"\np_cfg = SEQ_TOKENS_DIR / \"seq_config.json\"\n\nif not p_mtr.exists(): raise FileNotFoundError(f\"Missing: {p_mtr}\")\nif not p_mte.exists(): raise FileNotFoundError(f\"Missing: {p_mte}\")\nif not p_cfg.exists(): raise FileNotFoundError(f\"Missing: {p_cfg}\")\n\nseq_manifest_train = pd.read_csv(p_mtr)\nseq_manifest_test  = pd.read_csv(p_mte)\n\nwith open(p_cfg, \"r\", encoding=\"utf-8\") as f:\n    seq_cfg = json.load(f) if p_cfg.exists() else {}\n\nSEQ_FEATURE_NAMES = (\n    seq_cfg.get(\"feature_names\", None)\n    or seq_cfg.get(\"SEQ_FEATURE_NAMES\", None)\n    or globals().get(\"SEQ_FEATURE_NAMES\", None)\n)\nif SEQ_FEATURE_NAMES is None:\n    raise RuntimeError(\"SEQ_FEATURE_NAMES not found in seq_config.json and not present in globals.\")\nSEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\nfeat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n\nSEQ_TOKEN_MODE_IN = seq_cfg.get(\"token_mode\", None) or globals().get(\"SEQ_TOKEN_MODE\", None)\n\nprint(f\"[Stage 6] Using SEQ_TOKENS_DIR: {SEQ_TOKENS_DIR}\")\nprint(f\"[Stage 6] Loaded manifests: train_rows={len(seq_manifest_train):,} | test_rows={len(seq_manifest_test):,}\")\nprint(f\"[Stage 6] token_mode(prefer)={SEQ_TOKEN_MODE_IN} | F={len(SEQ_FEATURE_NAMES)}\")\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _resolve_shard_path(x, base_dir: Path) -> str:\n    p = Path(str(x))\n    if p.exists():\n        return str(p)\n    p2 = base_dir / p\n    if p2.exists():\n        return str(p2)\n    p3 = base_dir / p.name\n    if p3.exists():\n        return str(p3)\n    return str(p)\n\ndef _pick_first_existing(keys, d):\n    for k in keys:\n        if k in d:\n            return k\n    return None\n\ndef _pick_value_feat(feat_dict, prefer_mode=None):\n    value_exact = [\n        \"signal\", \"value\", \"flux\",\n        \"flux_asinh\", \"asinh_flux\", \"asinh_mag\",\n        \"flux_asinh_clip\", \"flux_asinh_norm\", \"flux_asinh_scaled\",\n    ]\n    mag_exact = [\"mag\", \"mag_norm\", \"mag_clip\", \"mag_scaled\"]\n\n    value_feat = _pick_first_existing(value_exact, feat_dict)\n    mag_feat   = _pick_first_existing(mag_exact, feat_dict)\n\n    if value_feat is None:\n        value_fuzzy = [k for k in feat_dict.keys() if any(t in k for t in [\"signal\", \"flux\", \"value\", \"asinh\"])]\n        value_feat = sorted(value_fuzzy)[0] if value_fuzzy else None\n    if mag_feat is None:\n        mag_fuzzy = [k for k in feat_dict.keys() if \"mag\" in k]\n        mag_feat = sorted(mag_fuzzy)[0] if mag_fuzzy else None\n\n    pm = (str(prefer_mode).lower().strip() if prefer_mode is not None else None)\n    if pm == \"mag\" and mag_feat is not None:\n        return \"mag\", mag_feat\n    if pm == \"asinh\":\n        if value_feat is not None:\n            return \"asinh\", value_feat\n        if mag_feat is not None:\n            print(\"[WARN] prefer asinh but no value-like feature found; fallback to mag.\")\n            return \"mag\", mag_feat\n\n    if value_feat is not None:\n        return \"asinh\", value_feat\n    if mag_feat is not None:\n        return \"mag\", mag_feat\n\n    return None, None\n\n# required for downstream model\nREQ_FOR_MODEL = [\"t_rel_log\", \"dt_log\"]\nmissing_req = [k for k in REQ_FOR_MODEL if k not in feat]\nif missing_req:\n    raise ValueError(f\"SEQ_FEATURE_NAMES missing required feats for model: {missing_req}. Head={SEQ_FEATURE_NAMES[:40]}\")\n\nSNR_FEAT = _pick_first_existing([\"snr_tanh\", \"snr\"], feat)\nif SNR_FEAT is None:\n    raise ValueError(\"SEQ_FEATURE_NAMES missing snr_tanh or snr.\")\n\nDET_FEAT = _pick_first_existing([\"detected\", \"detected_pos\"], feat)\nif DET_FEAT is None:\n    raise ValueError(\"SEQ_FEATURE_NAMES missing detected or detected_pos.\")\n\nSEQ_TOKEN_MODE, SCORE_VALUE_FEAT = _pick_value_feat(feat, prefer_mode=SEQ_TOKEN_MODE_IN)\nif SEQ_TOKEN_MODE is None or SCORE_VALUE_FEAT is None:\n    raise ValueError(\n        \"Cannot infer token mode/value feature from SEQ_FEATURE_NAMES.\\n\"\n        f\"SEQ_FEATURE_NAMES={SEQ_FEATURE_NAMES}\"\n    )\n\nprint(f\"[Stage 6] token_mode={SEQ_TOKEN_MODE} | score_value_feat={SCORE_VALUE_FEAT} | snr_feat={SNR_FEAT} | det_feat={DET_FEAT}\")\n\n# ----------------------------\n# 0c) Ensure df_train_meta / df_test_meta exist (rebuild if missing)\n# ----------------------------\nif \"df_train_meta\" not in globals() or not isinstance(globals()[\"df_train_meta\"], pd.DataFrame):\n    if \"df_train_log\" in globals() and isinstance(globals()[\"df_train_log\"], pd.DataFrame):\n        df_train_meta = globals()[\"df_train_log\"].copy()\n        df_train_meta[\"object_id\"] = df_train_meta[\"object_id\"].astype(str).apply(_norm_id)\n        df_train_meta = df_train_meta.set_index(\"object_id\", drop=True)\n        print(\"[Stage 6] df_train_meta missing -> rebuilt from df_train_log.\")\n    else:\n        raise RuntimeError(\"Missing df_train_meta and df_train_log. Jalankan STAGE 0/1 dulu.\")\nelse:\n    df_train_meta = globals()[\"df_train_meta\"].copy()\n\nif \"df_test_meta\" not in globals() or not isinstance(globals()[\"df_test_meta\"], pd.DataFrame):\n    if \"df_test_log\" in globals() and isinstance(globals()[\"df_test_log\"], pd.DataFrame):\n        df_test_meta = globals()[\"df_test_log\"].copy()\n        df_test_meta[\"object_id\"] = df_test_meta[\"object_id\"].astype(str).apply(_norm_id)\n        df_test_meta = df_test_meta.set_index(\"object_id\", drop=True)\n        print(\"[Stage 6] df_test_meta missing -> rebuilt from df_test_log.\")\n    else:\n        raise RuntimeError(\"Missing df_test_meta and df_test_log. Jalankan STAGE 0/1 dulu.\")\nelse:\n    df_test_meta = globals()[\"df_test_meta\"].copy()\n\ndf_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\ndf_test_meta.index  = pd.Index([_norm_id(z) for z in df_test_meta.index],  name=df_test_meta.index.name)\n\n# ----------------------------\n# 1) Settings\n# ----------------------------\nFORCE_MAX_LEN = CFG.get(\"STAGE6_FORCE_MAX_LEN\", None)\nMAXLEN_CAPS = (256, 384, 512)\n\nif SEQ_TOKEN_MODE == \"asinh\":\n    W_SNR = float(CFG.get(\"STAGE6_W_SNR\", 1.00))\n    W_VAL = float(CFG.get(\"STAGE6_W_VAL\", 0.05))\n    W_DET = float(CFG.get(\"STAGE6_W_DET\", 0.05))\nelse:\n    W_SNR = float(CFG.get(\"STAGE6_W_SNR\", 1.00))\n    W_VAL = float(CFG.get(\"STAGE6_W_VAL\", 0.35))\n    W_DET = float(CFG.get(\"STAGE6_W_DET\", 0.25))\n\nPAD_BAND_ID = int(CFG.get(\"STAGE6_PAD_BAND_ID\", 0))\nDUMMY_TOKEN_FOR_EMPTY = bool(CFG.get(\"STAGE6_DUMMY_TOKEN_FOR_EMPTY\", True))\nSHIFT_BAND_IDS = bool(CFG.get(\"STAGE6_SHIFT_BAND_IDS\", True))\n\nDTYPE_X = np.float32\nREBUILD_MODE = str(CFG.get(\"STAGE6_REBUILD_MODE\", \"wipe_all\")).lower()  # wipe_all / reuse_if_exists\n\nSNR_RAW_TO_TANH_SCALE = float(CFG.get(\"STAGE6_SNR_RAW_TO_TANH_SCALE\", 10.0))  # only if snr(raw)\n\n# ----------------------------\n# 2) Length distribution -> choose MAX_LEN\n# ----------------------------\ndef describe_lengths(m: pd.DataFrame, name: str):\n    if \"length\" not in m.columns:\n        raise RuntimeError(f\"Manifest {name} missing 'length' column.\")\n    L = pd.to_numeric(m[\"length\"], errors=\"coerce\").fillna(0).to_numpy(dtype=np.int32, copy=False)\n    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n    print(f\"\\n{name} length stats\")\n    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[7])} | p95={int(q[8])} | p99={int(q[10])} | max={int(q[-1])}\")\n    return q\n\nq_tr = describe_lengths(seq_manifest_train, \"TRAIN\")\nq_te = describe_lengths(seq_manifest_test,  \"TEST\")\n\np95 = int(max(q_tr[8], q_te[8]))\nif FORCE_MAX_LEN is not None and str(FORCE_MAX_LEN) not in [\"None\", \"none\", \"\"]:\n    MAX_LEN = int(FORCE_MAX_LEN)\nelse:\n    if p95 <= 256:\n        MAX_LEN = 256\n    elif p95 <= 384:\n        MAX_LEN = 384\n    else:\n        MAX_LEN = 512\n    if MAX_LEN not in MAXLEN_CAPS:\n        MAX_LEN = int(min(MAXLEN_CAPS, key=lambda x: abs(x - MAX_LEN)))\n\nprint(f\"\\n[Stage 6] MAX_LEN={MAX_LEN} (based on p95={p95})\")\nprint(f\"[Stage 6] Weights: W_SNR={W_SNR} | W_VAL={W_VAL} | W_DET={W_DET} | SHIFT_BAND_IDS={SHIFT_BAND_IDS} | DUMMY_TOKEN_FOR_EMPTY={DUMMY_TOKEN_FOR_EMPTY}\")\n\n# ----------------------------\n# 3) Window scoring + padding\n# ----------------------------\ndef _brightness_proxy_from_mag(mag: np.ndarray) -> np.ndarray:\n    mag = np.nan_to_num(mag, nan=np.float32(0.0), posinf=np.float32(0.0), neginf=np.float32(0.0)).astype(np.float32, copy=False)\n    if mag.size == 0:\n        return np.zeros_like(mag, dtype=np.float32)\n    med = np.float32(np.median(mag))\n    br = np.maximum(med - mag, np.float32(0.0))\n    br = np.log1p(br).astype(np.float32, copy=False)\n    return br\n\ndef _get_snr_tanh_from_X(X: np.ndarray) -> np.ndarray:\n    s = X[:, feat[SNR_FEAT]].astype(np.float32, copy=False)\n    s = np.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n    if SNR_FEAT == \"snr_tanh\":\n        return np.abs(s).astype(np.float32, copy=False)\n    return np.abs(np.tanh(s / np.float32(SNR_RAW_TO_TANH_SCALE))).astype(np.float32, copy=False)\n\ndef _score_tokens(X: np.ndarray) -> np.ndarray:\n    snr = _get_snr_tanh_from_X(X)\n    det = X[:, feat[DET_FEAT]].astype(np.float32, copy=False)\n    det = np.nan_to_num(det, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n\n    if SEQ_TOKEN_MODE == \"mag\":\n        mag = X[:, feat[SCORE_VALUE_FEAT]].astype(np.float32, copy=False)\n        val = _brightness_proxy_from_mag(mag)\n    else:\n        val = np.abs(X[:, feat[SCORE_VALUE_FEAT]]).astype(np.float32, copy=False)\n        val = np.nan_to_num(val, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n\n    score = (np.float32(W_SNR) * snr) + (np.float32(W_VAL) * val) + (np.float32(W_DET) * det)\n    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n\ndef select_best_window(score: np.ndarray, max_len: int) -> tuple[int, int]:\n    L = int(score.shape[0])\n    if L <= max_len:\n        return 0, L\n    cs = np.empty(L + 1, dtype=np.float32)\n    cs[0] = 0.0\n    np.cumsum(score.astype(np.float32, copy=False), out=cs[1:])\n    ws = cs[max_len:] - cs[:-max_len]\n    if ws.size <= 0 or (not np.isfinite(ws).any()):\n        start = int((L - max_len) // 2)\n    else:\n        start = int(np.argmax(ws))\n    return start, start + max_len\n\ndef _is_empty_stub(X: np.ndarray, B: np.ndarray) -> bool:\n    try:\n        if X is None or B is None:\n            return True\n        if int(X.shape[0]) != 1 or int(B.shape[0]) != 1:\n            return False\n        if int(B[0]) >= 0:\n            return False\n        return bool(np.all(np.abs(X.astype(np.float32, copy=False)) <= np.float32(1e-12)))\n    except Exception:\n        return False\n\ndef pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n    F = int(X.shape[1]) if (X is not None and getattr(X, \"ndim\", 0) == 2) else int(len(SEQ_FEATURE_NAMES))\n    Xp = np.zeros((max_len, F), dtype=DTYPE_X)\n    Bp = np.full((max_len,), PAD_BAND_ID, dtype=np.int8)\n    Mp = np.zeros((max_len,), dtype=np.int8)\n\n    empty_like = (\n        X is None or B is None\n        or getattr(X, \"size\", 0) == 0\n        or getattr(B, \"size\", 0) == 0\n        or _is_empty_stub(X, B)\n    )\n\n    if empty_like:\n        if DUMMY_TOKEN_FOR_EMPTY:\n            Mp[0] = 1\n            Bp[0] = np.int8(PAD_BAND_ID)\n            return Xp, Bp, Mp, 0, 0, 1\n        else:\n            return Xp, Bp, Mp, 0, 0, 0\n\n    L0 = int(X.shape[0])\n    if L0 <= max_len:\n        ws, we = 0, L0\n        Xw, Bw = X, B\n    else:\n        sc = _score_tokens(X)\n        ws, we = select_best_window(sc, max_len=max_len)\n        Xw, Bw = X[ws:we], B[ws:we]\n\n    lw = int(Xw.shape[0])\n    Xp[:lw] = Xw.astype(DTYPE_X, copy=False)\n\n    if SHIFT_BAND_IDS:\n        Bw16 = Bw.astype(np.int16, copy=False)\n        Bp[:lw] = np.clip(Bw16 + 1, 0, 127).astype(np.int8, copy=False)\n    else:\n        Bp[:lw] = Bw.astype(np.int8, copy=False)\n\n    Mp[:lw] = 1\n    return Xp, Bp, Mp, int(L0), int(ws), int(we)\n\n# ----------------------------\n# 4) Fixed cache builder setup\n# ----------------------------\nFIX_DIR = ART_DIR / \"fixed_seq\"\nFIX_DIR.mkdir(parents=True, exist_ok=True)\n\ntrain_ids = df_train_meta.index.astype(str).tolist()\n\ndef _try_load_sample_sub_ids():\n    if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in globals()[\"df_sub\"].columns:\n        return globals()[\"df_sub\"][\"object_id\"].astype(str).str.strip().apply(_norm_id).to_list()\n    if \"PATHS\" in globals() and isinstance(globals()[\"PATHS\"], dict):\n        p = globals()[\"PATHS\"].get(\"SAMPLE_SUB\", None)\n        if p and Path(p).exists():\n            df = pd.read_csv(p)\n            if \"object_id\" in df.columns:\n                return df[\"object_id\"].astype(str).str.strip().apply(_norm_id).to_list()\n    return None\n\ntest_ids = _try_load_sample_sub_ids()\nif test_ids is None:\n    test_ids = df_test_meta.index.astype(str).tolist()\n\n# find y column (meta -> fallback train.csv if exists in PATHS)\ndef _find_target_col(df: pd.DataFrame):\n    for cand in [\"target\", \"y\", \"label\", \"class\", \"target_id\", \"binary_target\", \"is_tde\", \"is_event\", \"event\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\n_y_col = _find_target_col(df_train_meta)\nif _y_col is None and \"PATHS\" in globals() and isinstance(globals()[\"PATHS\"], dict):\n    p_train = globals()[\"PATHS\"].get(\"TRAIN_CSV\", None) or globals()[\"PATHS\"].get(\"TRAIN\", None)\n    if p_train and Path(p_train).exists():\n        dft = pd.read_csv(p_train)\n        if \"object_id\" in dft.columns:\n            dft[\"object_id\"] = dft[\"object_id\"].astype(str).apply(_norm_id)\n            ycol = _find_target_col(dft)\n            if ycol is not None:\n                # join back to meta (safe)\n                tmp = dft.set_index(\"object_id\")[ycol]\n                df_train_meta[ycol] = pd.to_numeric(df_train_meta.index.map(tmp), errors=\"coerce\")\n                _y_col = ycol\n                print(f\"[Stage 6] target col loaded from TRAIN_CSV: {_y_col}\")\n\nif _y_col is None:\n    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols_head={list(df_train_meta.columns)[:40]}\")\n\ny_train = pd.to_numeric(df_train_meta[_y_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n\n# dedup checks\nif len(set(train_ids)) != len(train_ids):\n    s = pd.Series(train_ids); dup = s[s.duplicated()].head(10).tolist()\n    raise RuntimeError(f\"train_ids has duplicates. ex={dup}\")\nif len(set(test_ids)) != len(test_ids):\n    s = pd.Series(test_ids); dup = s[s.duplicated()].head(10).tolist()\n    raise RuntimeError(f\"test_ids has duplicates. ex={dup}\")\n\ntrain_row = {oid: i for i, oid in enumerate(train_ids)}\ntest_row  = {oid: i for i, oid in enumerate(test_ids)}\n\nNTR, NTE, F = len(train_ids), len(test_ids), len(SEQ_FEATURE_NAMES)\n\ndef _gb(nbytes): return float(nbytes) / (1024**3)\nprint(f\"\\n[Stage 6] Memmap X sizes approx: train={_gb(NTR*MAX_LEN*F*np.dtype(DTYPE_X).itemsize):.3f} GB | \"\n      f\"test={_gb(NTE*MAX_LEN*F*np.dtype(DTYPE_X).itemsize):.3f} GB | dtype={DTYPE_X}\")\n\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\ntest_X_path  = FIX_DIR / \"test_X.dat\"\ntest_B_path  = FIX_DIR / \"test_B.dat\"\ntest_M_path  = FIX_DIR / \"test_M.dat\"\n\ntrain_len_path = FIX_DIR / \"train_origlen.npy\"\ntrain_ws_path  = FIX_DIR / \"train_winstart.npy\"\ntrain_we_path  = FIX_DIR / \"train_winend.npy\"\ntest_len_path  = FIX_DIR / \"test_origlen.npy\"\ntest_ws_path   = FIX_DIR / \"test_winstart.npy\"\ntest_we_path   = FIX_DIR / \"test_winend.npy\"\n\ndef _all_exist(paths):\n    return all(Path(p).exists() for p in paths)\n\nreuse_paths = [\n    train_X_path, train_B_path, train_M_path,\n    test_X_path, test_B_path, test_M_path,\n    FIX_DIR / \"train_ids.npy\", FIX_DIR / \"test_ids.npy\", FIX_DIR / \"train_y.npy\",\n    train_len_path, train_ws_path, train_we_path,\n    test_len_path, test_ws_path, test_we_path,\n    FIX_DIR / \"length_policy_config.json\"\n]\n\nif REBUILD_MODE == \"wipe_all\":\n    for p in reuse_paths:\n        try:\n            Path(p).unlink(missing_ok=True)\n        except Exception:\n            pass\n\nif REBUILD_MODE == \"reuse_if_exists\" and _all_exist(reuse_paths):\n    print(\"[Stage 6] REUSE: fixed_seq cache already present.\")\n    globals().update({\n        \"SEQ_TOKENS_DIR\": SEQ_TOKENS_DIR,\n        \"FIX_DIR\": FIX_DIR, \"MAX_LEN\": MAX_LEN,\n        \"FIX_TRAIN_X_PATH\": train_X_path, \"FIX_TRAIN_B_PATH\": train_B_path, \"FIX_TRAIN_M_PATH\": train_M_path,\n        \"FIX_TEST_X_PATH\": test_X_path,  \"FIX_TEST_B_PATH\": test_B_path,  \"FIX_TEST_M_PATH\": test_M_path,\n        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n        \"FIX_POLICY_CFG_PATH\": FIX_DIR / \"length_policy_config.json\",\n        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n        \"SCORE_VALUE_FEAT\": SCORE_VALUE_FEAT,\n        \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n        \"PAD_BAND_ID\": PAD_BAND_ID,\n        \"df_train_meta\": df_train_meta,\n        \"df_test_meta\": df_test_meta,\n    })\n    gc.collect()\n\nelse:\n    # memmaps\n    Xtr = np.memmap(train_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTR, MAX_LEN, F))\n    Btr = np.memmap(train_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n    Mtr = np.memmap(train_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n\n    Xte = np.memmap(test_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTE, MAX_LEN, F))\n    Bte = np.memmap(test_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n    Mte = np.memmap(test_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n\n    origlen_tr  = np.zeros((NTR,), dtype=np.int32)\n    winstart_tr = np.zeros((NTR,), dtype=np.int32)\n    winend_tr   = np.zeros((NTR,), dtype=np.int32)\n\n    origlen_te  = np.zeros((NTE,), dtype=np.int32)\n    winstart_te = np.zeros((NTE,), dtype=np.int32)\n    winend_te   = np.zeros((NTE,), dtype=np.int32)\n\n    filled_tr = np.zeros((NTR,), dtype=np.uint8)\n    filled_te = np.zeros((NTE,), dtype=np.uint8)\n\n    def _prep_manifest(m: pd.DataFrame, expected_set: set, which: str) -> pd.DataFrame:\n        m2 = m.copy()\n        for c in [\"object_id\", \"shard\", \"start\", \"length\"]:\n            if c not in m2.columns:\n                raise RuntimeError(f\"Manifest missing '{c}'. cols={list(m2.columns)}\")\n\n        m2[\"object_id\"] = m2[\"object_id\"].astype(str).apply(_norm_id)\n        m2 = m2[m2[\"object_id\"].isin(expected_set)].copy()\n\n        # drop duplicate objects (keep first)\n        if m2[\"object_id\"].duplicated().any():\n            dups = m2.loc[m2[\"object_id\"].duplicated(), \"object_id\"].head(10).tolist()\n            print(f\"[WARN] {which} manifest has duplicate object_id; dropping duplicates. ex={dups}\")\n            m2 = m2.drop_duplicates(\"object_id\", keep=\"first\")\n\n        m2[\"shard\"] = m2[\"shard\"].astype(str).apply(lambda s: _resolve_shard_path(s, SEQ_TOKENS_DIR))\n        m2[\"start\"] = pd.to_numeric(m2[\"start\"], errors=\"coerce\").fillna(-1).astype(np.int64)\n        m2[\"length\"] = pd.to_numeric(m2[\"length\"], errors=\"coerce\").fillna(0).astype(np.int64)\n        m2 = m2[(m2[\"start\"] >= 0) & (m2[\"length\"] >= 0)].copy()\n\n        return m2\n\n    exp_train = set(train_ids)\n    exp_test  = set(test_ids)\n    m_train2 = _prep_manifest(seq_manifest_train, exp_train, \"train\")\n    m_test2  = _prep_manifest(seq_manifest_test,  exp_test,  \"test\")\n\n    def process_manifest_into_memmap(m2: pd.DataFrame, which: str):\n        if which == \"train\":\n            row_map = train_row\n            Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n            origlen, ws_arr, we_arr = origlen_tr, winstart_tr, winend_tr\n            filled_mask = filled_tr\n            expected_n = NTR\n        else:\n            row_map = test_row\n            Xmm, Bmm, Mmm = Xte, Bte, Mte\n            origlen, ws_arr, we_arr = origlen_te, winstart_te, winend_te\n            filled_mask = filled_te\n            expected_n = NTE\n\n        shard_paths = m2[\"shard\"].unique().tolist()\n        miss_sh = [p for p in shard_paths if not Path(p).exists()]\n        if miss_sh:\n            raise RuntimeError(f\"Missing shard files ({which}): count={len(miss_sh)} | ex={miss_sh[:5]}\")\n\n        filled = dup = empty = dropped_bad = 0\n        t0 = time.time()\n        get = row_map.get\n\n        for si, (shard_path, g) in enumerate(m2.groupby(\"shard\", sort=True), start=1):\n            data = np.load(shard_path, allow_pickle=False)\n            if \"x\" not in data or \"band\" not in data:\n                raise RuntimeError(f\"Shard missing keys ['x','band']. Got={list(data.keys())} | shard={shard_path}\")\n\n            x_all = data[\"x\"]\n            b_all = data[\"band\"]\n\n            oids = g[\"object_id\"].to_numpy(copy=False)\n            starts = g[\"start\"].to_numpy(dtype=np.int64, copy=False)\n            lens   = g[\"length\"].to_numpy(dtype=np.int64, copy=False)\n\n            for oid, st, ln in zip(oids, starts, lens):\n                idx = get(str(oid), -1)\n                if idx < 0:\n                    continue\n                if ln <= 0:\n                    empty += 1\n                    continue\n                if filled_mask[idx]:\n                    dup += 1\n                    continue\n\n                end = int(st + ln)\n                if st < 0 or end > x_all.shape[0] or end > b_all.shape[0]:\n                    dropped_bad += 1\n                    continue\n\n                X = x_all[st:end]\n                B = b_all[st:end]\n\n                Xp, Bp, Mp, L0, ws, we = pad_to_fixed(X, B, max_len=MAX_LEN)\n\n                Xmm[idx, :, :] = Xp\n                Bmm[idx, :] = Bp\n                Mmm[idx, :] = Mp\n                origlen[idx] = int(L0)\n                ws_arr[idx] = int(ws)\n                we_arr[idx] = int(we)\n                filled_mask[idx] = 1\n                filled += 1\n\n                if filled % 2000 == 0:\n                    gc.collect()\n\n            if si % 25 == 0:\n                print(f\"[Stage 6][{which}] shards_processed={si:,}/{len(shard_paths):,} | filled={filled:,}\")\n\n        elapsed = time.time() - t0\n        return {\n            \"filled\": int(filled),\n            \"dup_skipped\": int(dup),\n            \"empty_len\": int(empty),\n            \"dropped_bad_slices\": int(dropped_bad),\n            \"time_s\": float(elapsed),\n            \"expected\": int(expected_n)\n        }\n\n    print(\"\\n[Stage 6] Building fixed cache (TRAIN) from STAGE 5 manifests...\")\n    st_tr = process_manifest_into_memmap(m_train2, \"train\")\n    print(f\"[Stage 6] TRAIN filled={st_tr['filled']:,}/{st_tr['expected']:,} | dup={st_tr['dup_skipped']:,} | empty={st_tr['empty_len']:,} | dropped_bad={st_tr['dropped_bad_slices']:,} | time={st_tr['time_s']:.2f}s\")\n\n    print(\"\\n[Stage 6] Building fixed cache (TEST) from STAGE 5 manifests...\")\n    st_te = process_manifest_into_memmap(m_test2, \"test\")\n    print(f\"[Stage 6] TEST  filled={st_te['filled']:,}/{st_te['expected']:,} | dup={st_te['dup_skipped']:,} | empty={st_te['empty_len']:,} | dropped_bad={st_te['dropped_bad_slices']:,} | time={st_te['time_s']:.2f}s\")\n\n    Xtr.flush(); Btr.flush(); Mtr.flush()\n    Xte.flush(); Bte.flush(); Mte.flush()\n\n    miss_tr = np.where(filled_tr == 0)[0]\n    miss_te = np.where(filled_te == 0)[0]\n    if len(miss_tr) > 0:\n        ex = [train_ids[i] for i in miss_tr[:10]]\n        raise RuntimeError(f\"[Stage 6] TRAIN missing filled rows: {len(miss_tr):,}/{NTR:,} | ex={ex}\")\n    if len(miss_te) > 0:\n        ex = [test_ids[i] for i in miss_te[:10]]\n        raise RuntimeError(f\"[Stage 6] TEST missing filled rows: {len(miss_te):,}/{NTE:,} | ex={ex}\")\n\n    np.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n    np.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids,  dtype=\"S\"))\n    np.save(FIX_DIR / \"train_y.npy\",   y_train)\n\n    np.save(train_len_path, origlen_tr)\n    np.save(train_ws_path,  winstart_tr)\n    np.save(train_we_path,  winend_tr)\n\n    np.save(test_len_path, origlen_te)\n    np.save(test_ws_path,  winstart_te)\n    np.save(test_we_path,  winend_te)\n\n    policy_cfg = {\n        \"stage5_inputs\": {\n            \"SEQ_TOKENS_DIR\": str(SEQ_TOKENS_DIR),\n            \"seq_manifest_train\": str(p_mtr),\n            \"seq_manifest_test\": str(p_mte),\n            \"seq_config\": str(p_cfg),\n        },\n        \"token_mode\": SEQ_TOKEN_MODE,\n        \"score_value_feat\": SCORE_VALUE_FEAT,\n        \"snr_feat\": SNR_FEAT,\n        \"det_feat\": DET_FEAT,\n        \"max_len\": int(MAX_LEN),\n        \"feature_names\": list(SEQ_FEATURE_NAMES),\n        \"score_weights\": {\"W_SNR\": float(W_SNR), \"W_VAL\": float(W_VAL), \"W_DET\": float(W_DET)},\n        \"window_policy\": \"best_contiguous_window_by_max_sum(score)\",\n        \"padding\": {\n            \"PAD_BAND_ID\": int(PAD_BAND_ID),\n            \"SHIFT_BAND_IDS\": bool(SHIFT_BAND_IDS),\n            \"DUMMY_TOKEN_FOR_EMPTY\": bool(DUMMY_TOKEN_FOR_EMPTY),\n        },\n        \"dtype_X\": \"float32\",\n        \"order\": {\"train\": \"df_train_meta.index\", \"test\": \"df_sub.object_id if exists else df_test_meta.index\", \"y_col\": str(_y_col)},\n        \"stats\": {\"train\": st_tr, \"test\": st_te},\n    }\n\n    cfg_path = FIX_DIR / \"length_policy_config.json\"\n    with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(policy_cfg, f, indent=2)\n\n    print(\"\\n[Stage 6] DONE\")\n    print(f\"- FIX_DIR: {FIX_DIR}\")\n    print(f\"- Saved config: {cfg_path}\")\n\n    globals().update({\n        \"SEQ_TOKENS_DIR\": SEQ_TOKENS_DIR,\n        \"SEQ_FEATURE_NAMES\": SEQ_FEATURE_NAMES,\n        \"FIX_DIR\": FIX_DIR,\n        \"MAX_LEN\": MAX_LEN,\n        \"FIX_TRAIN_X_PATH\": train_X_path,\n        \"FIX_TRAIN_B_PATH\": train_B_path,\n        \"FIX_TRAIN_M_PATH\": train_M_path,\n        \"FIX_TEST_X_PATH\": test_X_path,\n        \"FIX_TEST_B_PATH\": test_B_path,\n        \"FIX_TEST_M_PATH\": test_M_path,\n        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n        \"FIX_POLICY_CFG_PATH\": cfg_path,\n        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n        \"SCORE_VALUE_FEAT\": SCORE_VALUE_FEAT,\n        \"SNR_FEAT\": SNR_FEAT,\n        \"DET_FEAT\": DET_FEAT,\n        \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n        \"PAD_BAND_ID\": PAD_BAND_ID,\n        \"df_train_meta\": df_train_meta,\n        \"df_test_meta\": df_test_meta,\n    })\n\n    gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CV Split (Object-Level, Stratified)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL)\n# REVISI FULL v2.7 (UPGRADE: stronger auto-k, per-k group fallback, fold_stats csv, cv_train_ids.npy)\n#\n# Output:\n# - artifacts/cv/cv_folds.csv\n# - artifacts/cv/cv_folds.npz   (train_idx_f + val_idx_f + optional holdout_val_mask)\n# - artifacts/cv/cv_report.txt\n# - artifacts/cv/cv_config.json\n# - artifacts/cv/fold_stats.csv\n# - artifacts/cv/cv_train_ids.npy\n# - (optional) artifacts/cv/cv_holdout_val_mask.npy\n# - globals: fold_assign, folds, n_splits, CV_DIR\n# ============================================================\n\nimport gc, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nfor need in [\"df_train_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 2 dulu (df_train_meta & ART_DIR).\")\n\nSEED = int(globals().get(\"SEED\", 2025))\nART_DIR = Path(ART_DIR)\n\n# ----------------------------\n# 1) CV Settings\n# ----------------------------\nDEFAULT_SPLITS = 5\nFORCE_N_SPLITS = None              # set int kalau mau paksa (mis. 3), else None\n\nMIN_POS_PER_FOLD = 3               # umum 2–10 (imbalance -> kecilkan)\nENFORCE_MIN_POS_PER_FOLD = True    # kalau True: n_splits turun otomatis sampai min_pos>=MIN_POS_PER_FOLD (atau fallback holdout)\n\nMIN_NEG_PER_FOLD = 1               # biasanya aman 1 (neg biasanya banyak). Boleh dinaikkan kalau perlu.\nENFORCE_MIN_NEG_PER_FOLD = True\n\nUSE_GROUP_BY_SPLIT = False         # True => prefer StratifiedGroupKFold (groups=df_train_meta[\"split\"])\nAUTO_FALLBACK_GROUP = True         # True => kalau group-cv gagal, fallback ke StratifiedKFold\n\nHOLDOUT_FALLBACK = True            # True => kalau CV tidak mungkin, pakai holdout\nHOLDOUT_FRAC = 0.20                # target val fraction untuk holdout (stratified)\n\nSAVE_PARQUET_FOLDS = False         # opsional: save cv_folds.parquet jika pyarrow tersedia\n\nprint(\n    f\"[Stage 7] seed={SEED} | default_splits={DEFAULT_SPLITS} | \"\n    f\"MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} enforce_pos={ENFORCE_MIN_POS_PER_FOLD} | \"\n    f\"MIN_NEG_PER_FOLD={MIN_NEG_PER_FOLD} enforce_neg={ENFORCE_MIN_NEG_PER_FOLD} | \"\n    f\"group_by_split={USE_GROUP_BY_SPLIT} fallback_group={AUTO_FALLBACK_GROUP} | \"\n    f\"holdout_fallback={HOLDOUT_FALLBACK} holdout_frac={HOLDOUT_FRAC}\"\n)\n\n# ----------------------------\n# 2) Helpers\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, bytearray, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _decode_ids(arr) -> list:\n    return [_norm_id(x) for x in arr.tolist()]\n\ndef _safe_str_list(idx) -> list:\n    return pd.Index(idx).astype(\"string\").str.strip().astype(str).tolist()\n\ndef _find_train_ids_npy(art_dir: Path):\n    # priority 1: FIX_DIR (Stage 6)\n    if \"FIX_DIR\" in globals():\n        p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n        if p.exists():\n            return p\n    # priority 2: ART_DIR/fixed_seq\n    p = art_dir / \"fixed_seq\" / \"train_ids.npy\"\n    if p.exists():\n        return p\n    # priority 3: scan mallorn_run runs (latest mtime)\n    root = Path(\"/kaggle/working/mallorn_run\")\n    if root.exists():\n        cands = list(root.glob(\"run_*/artifacts/fixed_seq/train_ids.npy\"))\n        if cands:\n            cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n            return cands[0]\n    return None\n\ndef _pick_target_col(df: pd.DataFrame):\n    for cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\n# ----------------------------\n# 3) Determine train_ids ordering (prefer fixed cache from STAGE 6)\n# ----------------------------\np_ids = _find_train_ids_npy(ART_DIR)\nif p_ids is not None:\n    raw = np.load(p_ids, allow_pickle=False)\n    train_ids = _decode_ids(raw)\n    order_source = str(p_ids)\nelse:\n    train_ids = _safe_str_list(df_train_meta.index)\n    order_source = \"df_train_meta.index\"\n\nif len(train_ids) != len(set(train_ids)):\n    s = pd.Series(train_ids)\n    dup = s[s.duplicated()].iloc[:10].tolist()\n    raise RuntimeError(f\"[Stage 7] train_ids has duplicates (examples): {dup}\")\n\nN = int(len(train_ids))\n\n# ----------------------------\n# 4) Normalize meta index (string+strip) + fast mapping\n# ----------------------------\nmeta = df_train_meta.copy()\nmeta_ids = _safe_str_list(meta.index)\n\nif len(meta_ids) != len(set(meta_ids)):\n    vc = pd.Series(meta_ids).value_counts()\n    dup = vc[vc > 1].index.tolist()[:10]\n    raise RuntimeError(f\"[Stage 7] df_train_meta index has duplicates after str/strip (examples): {dup}\")\n\nmeta.index = pd.Index(meta_ids, name=\"object_id\")\n\npos_idx = meta.index.get_indexer(train_ids)\nmissing_mask = (pos_idx < 0)\nif missing_mask.any():\n    ex = [train_ids[i] for i in np.where(missing_mask)[0][:10]]\n    raise RuntimeError(\n        \"[Stage 7] Some train_ids not found in df_train_meta (after str/strip index).\\n\"\n        f\"Missing count={int(missing_mask.sum())} | ex={ex}\\n\"\n        \"Solusi: pastikan df_train_meta index adalah object_id dan konsisten dengan fixed_seq train_ids (kalau pakai Stage 6).\"\n    )\n\n# ----------------------------\n# 5) Robust target -> y (ordered by train_ids)\n# ----------------------------\ntarget_col = _pick_target_col(meta)\nif target_col is None:\n    raise RuntimeError(f\"[Stage 7] Cannot find target column in df_train_meta. cols(sample)={list(meta.columns)[:40]}\")\n\ny_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).to_numpy(copy=False)\ny = y_all[pos_idx]\ny = (y > 0).astype(np.int8)\n\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\nif pos == 0 or neg == 0:\n    raise RuntimeError(f\"[Stage 7] Invalid class distribution: pos={pos}, neg={neg}. Cannot do stratified split.\")\n\npos_rate = pos / max(N, 1)\npos_weight = float(neg / max(pos, 1))\nprint(f\"[Stage 7] N={N:,} pos={pos:,} neg={neg:,} pos%={pos_rate*100:.6f}% | pos_weight~{pos_weight:.4f} | order_source={order_source}\")\n\n# ----------------------------\n# 6) Optional groups (by split)\n# ----------------------------\ngroups = None\ngroup_col = None\nif USE_GROUP_BY_SPLIT:\n    for cand in [\"split\", \"split_id\", \"split_name\", \"split_idx\"]:\n        if cand in meta.columns:\n            group_col = cand\n            break\n    if group_col is None:\n        if not AUTO_FALLBACK_GROUP:\n            raise RuntimeError(\"[Stage 7] USE_GROUP_BY_SPLIT=True but no split column found in df_train_meta.\")\n        print(\"[Stage 7] WARN: split column not found; fallback to StratifiedKFold.\")\n        USE_GROUP_BY_SPLIT = False\n    else:\n        g_all = meta[group_col].astype(\"string\").str.strip().astype(str).to_numpy(copy=False)\n        groups = g_all[pos_idx]\n\n# ----------------------------\n# 7) Choose n_splits safely + auto-adjust\n# ----------------------------\nmax_splits_by_pos = pos\nmax_splits_by_neg = neg\nmax_splits_by_minpos = max(1, pos // max(int(MIN_POS_PER_FOLD), 1))\nmax_splits_by_minneg = max(1, neg // max(int(MIN_NEG_PER_FOLD), 1))\n\nn0 = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg, max_splits_by_minpos, max_splits_by_minneg)\nif FORCE_N_SPLITS is not None:\n    n0 = int(FORCE_N_SPLITS)\n\nprint(f\"[Stage 7] Candidate n_splits={n0} | enforce_pos={ENFORCE_MIN_POS_PER_FOLD} enforce_neg={ENFORCE_MIN_NEG_PER_FOLD}\")\n\n# ----------------------------\n# 8) Build folds with robust fallback (group->non-group per-k)\n# ----------------------------\ntry:\n    from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n    try:\n        from sklearn.model_selection import StratifiedGroupKFold\n    except Exception:\n        StratifiedGroupKFold = None\nexcept Exception as e:\n    raise RuntimeError(\"scikit-learn is not available in this environment.\") from e\n\ndef _try_split_kfold(k: int, use_group: bool):\n    fold_assign = np.full(N, -1, dtype=np.int16)\n    folds = []\n    per = []\n\n    if use_group:\n        if StratifiedGroupKFold is None:\n            return (False, \"StratifiedGroupKFold(unavailable)\", None, None, None)\n        if groups is None:\n            return (False, \"StratifiedGroupKFold(groups=None)\", None, None, None)\n        splitter = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(np.zeros(N), y, groups=groups)\n        cv_type = f\"StratifiedGroupKFold({group_col})\"\n    else:\n        splitter = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n        split_iter = splitter.split(np.zeros(N), y)\n        cv_type = \"StratifiedKFold\"\n\n    try:\n        for fold, (tr_idx, val_idx) in enumerate(split_iter):\n            fold_assign[val_idx] = fold\n            yf = y[val_idx]\n            pf = int((yf == 1).sum())\n            nf = int((yf == 0).sum())\n            per.append((len(val_idx), pf, nf))\n            folds.append({\n                \"fold\": int(fold),\n                \"train_idx\": tr_idx.astype(np.int32, copy=False),\n                \"val_idx\": val_idx.astype(np.int32, copy=False),\n            })\n    except Exception as e:\n        return (False, f\"{cv_type} (error: {type(e).__name__}: {e})\", None, None, None)\n\n    if (fold_assign < 0).any():\n        return (False, f\"{cv_type} (unassigned)\", None, None, None)\n\n    # strict: no fold with empty class\n    for (_, pf, nf) in per:\n        if pf == 0 or nf == 0:\n            return (False, f\"{cv_type} (empty class in fold)\", None, None, None)\n\n    return (True, cv_type, fold_assign, folds, per)\n\ndef _passes_min_constraints(per):\n    if not per:\n        return False, 0, 0\n    min_pos_seen = min(pf for (_, pf, _) in per)\n    min_neg_seen = min(nf for (_, _, nf) in per)\n    if ENFORCE_MIN_POS_PER_FOLD and (min_pos_seen < int(MIN_POS_PER_FOLD)) and (FORCE_N_SPLITS is None):\n        return False, min_pos_seen, min_neg_seen\n    if ENFORCE_MIN_NEG_PER_FOLD and (min_neg_seen < int(MIN_NEG_PER_FOLD)) and (FORCE_N_SPLITS is None):\n        return False, min_pos_seen, min_neg_seen\n    return True, min_pos_seen, min_neg_seen\n\ndef _make_holdout():\n    if pos < 2 or neg < 2:\n        raise RuntimeError(f\"[Stage 7] Cannot build holdout safely. Need pos>=2 and neg>=2. Got pos={pos}, neg={neg}.\")\n\n    val_n = int(round(N * float(HOLDOUT_FRAC)))\n    val_n = max(val_n, 2)\n    val_n = min(val_n, N - 2)\n\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=(val_n / N), random_state=SEED)\n    tr_idx, val_idx = next(splitter.split(np.zeros(N), y))\n\n    # ensure val has both classes\n    pf = int((y[val_idx] == 1).sum())\n    nf = int((y[val_idx] == 0).sum())\n    if pf == 0 or nf == 0:\n        raise RuntimeError(f\"[Stage 7] Holdout produced empty class in val. pos_val={pf} neg_val={nf}. Try different seed/frac.\")\n\n    fold_assign = np.zeros(N, dtype=np.int16)  # no -1 (downstream safe)\n    val_mask = np.zeros(N, dtype=np.uint8)\n    val_mask[val_idx] = 1\n\n    folds = [{\n        \"fold\": 0,\n        \"train_idx\": tr_idx.astype(np.int32, copy=False),\n        \"val_idx\": val_idx.astype(np.int32, copy=False),\n    }]\n    per = [(len(val_idx), pf, nf)]\n    return 1, \"Holdout(StratifiedShuffleSplit)\", fold_assign, folds, per, val_mask\n\nbest = None\nval_mask_holdout = None\n\nif n0 >= 2:\n    for k in range(n0, 1, -1):\n        # try group first (if requested), else plain\n        tried = []\n        if USE_GROUP_BY_SPLIT:\n            tried.append(True)\n        tried.append(False)  # always allow non-group attempt\n\n        for use_group in tried:\n            ok, cv_type, fa, folds_k, per = _try_split_kfold(k, use_group=use_group)\n            if not ok:\n                continue\n\n            passed, min_pos_seen, min_neg_seen = _passes_min_constraints(per)\n            if not passed:\n                continue\n\n            best = (k, cv_type, fa, folds_k, per, min_pos_seen, min_neg_seen)\n            break\n\n        if best is not None:\n            break\n\n# if strict constraints failed, pick first valid anyway (still must have both classes per fold)\nif best is None and n0 >= 2:\n    for k in range(n0, 1, -1):\n        ok, cv_type, fa, folds_k, per = _try_split_kfold(k, use_group=False)\n        if ok:\n            min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n            min_neg_seen = min(nf for (_, _, nf) in per) if per else 0\n            best = (k, cv_type, fa, folds_k, per, min_pos_seen, min_neg_seen)\n            print(f\"[Stage 7] NOTE: Could not satisfy min constraints. Using k={k} with min_pos={min_pos_seen}, min_neg={min_neg_seen}.\")\n            break\n\n# fallback to holdout\nif best is None:\n    if HOLDOUT_FALLBACK:\n        n_splits, cv_type, fold_assign, folds, per, val_mask_holdout = _make_holdout()\n        min_pos_seen = per[0][1] if per else 0\n        min_neg_seen = per[0][2] if per else 0\n        best = (n_splits, cv_type, fold_assign, folds, per, min_pos_seen, min_neg_seen)\n        print(f\"[Stage 7] FALLBACK -> {cv_type} | val_pos={min_pos_seen} val_neg={min_neg_seen}\")\n    else:\n        raise RuntimeError(\"[Stage 7] Failed to build a valid CV split. Try smaller DEFAULT_SPLITS / FORCE_N_SPLITS, or enable HOLDOUT_FALLBACK.\")\n\n# unpack\nn_splits, cv_type, fold_assign, folds, per, min_pos_seen, min_neg_seen = best\nprint(f\"[Stage 7] FINAL: n_splits={n_splits} | cv_type={cv_type} | min_pos_in_fold={min_pos_seen} | min_neg_in_fold={min_neg_seen}\")\n\n# ----------------------------\n# 9) Report + fold stats\n# ----------------------------\nlines = []\nlines.append(f\"CV={cv_type} n_splits={n_splits} seed={SEED}\")\nlines.append(f\"Order source: {order_source}\")\nlines.append(f\"Target column: {target_col}\")\nlines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos_rate*100:.6f}% | pos_weight~{pos_weight:.6f}\")\nif USE_GROUP_BY_SPLIT:\n    lines.append(f\"Group requested: {group_col} | used_group={('Group' in cv_type)}\")\n\nlines.append(\"Per-fold distribution (val):\")\nfold_rows = []\nif n_splits >= 2:\n    for f in range(n_splits):\n        idx = np.where(fold_assign == f)[0]\n        yf = y[idx]\n        pf = int((yf == 1).sum())\n        nf = int((yf == 0).sum())\n        lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:9.6f}%\")\n        fold_rows.append({\"fold\": f, \"n_val\": len(idx), \"pos_val\": pf, \"neg_val\": nf, \"pos_frac_val\": pf/max(len(idx),1)})\nelse:\n    vidx = folds[0][\"val_idx\"]\n    yf = y[vidx]\n    pf = int((yf == 1).sum())\n    nf = int((yf == 0).sum())\n    lines.append(f\"- holdout val: n={len(vidx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(vidx),1))*100:9.6f}%\")\n    lines.append(\"NOTE: holdout mode uses folds[0].train_idx / folds[0].val_idx; fold_assign is all zeros (no -1).\")\n    fold_rows.append({\"fold\": 0, \"n_val\": len(vidx), \"pos_val\": pf, \"neg_val\": nf, \"pos_frac_val\": pf/max(len(vidx),1)})\n\ndf_fold_stats = pd.DataFrame(fold_rows)\n\n# ----------------------------\n# 10) Save artifacts\n# ----------------------------\nCV_DIR = ART_DIR / \"cv\"\nCV_DIR.mkdir(parents=True, exist_ok=True)\n\ndf_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\n\nfolds_csv = CV_DIR / \"cv_folds.csv\"\ndf_folds.to_csv(folds_csv, index=False)\n\nif SAVE_PARQUET_FOLDS:\n    try:\n        df_folds.to_parquet(CV_DIR / \"cv_folds.parquet\", index=False)\n    except Exception as e:\n        print(f\"[Stage 7] WARN: parquet save skipped ({type(e).__name__}: {e})\")\n\n# save ids for downstream consistency (even without Stage 6)\nnp.save(CV_DIR / \"cv_train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n\nnpz_path = CV_DIR / \"cv_folds.npz\"\nnpz_kwargs = {}\nfor fd in folds:\n    f = int(fd[\"fold\"])\n    npz_kwargs[f\"train_idx_{f}\"] = fd[\"train_idx\"].astype(np.int32, copy=False)\n    npz_kwargs[f\"val_idx_{f}\"]   = fd[\"val_idx\"].astype(np.int32, copy=False)\n\nif val_mask_holdout is not None:\n    npz_kwargs[\"holdout_val_mask\"] = val_mask_holdout.astype(np.uint8, copy=False)\n    np.save(CV_DIR / \"cv_holdout_val_mask.npy\", val_mask_holdout.astype(np.uint8, copy=False))\n\nnp.savez(npz_path, **npz_kwargs)\n\nreport_path = CV_DIR / \"cv_report.txt\"\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\nfold_stats_path = CV_DIR / \"fold_stats.csv\"\ndf_fold_stats.to_csv(fold_stats_path, index=False)\n\ncfg_path = CV_DIR / \"cv_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"seed\": SEED,\n            \"n_splits\": int(n_splits),\n            \"cv_type\": cv_type,\n            \"min_pos_per_fold\": int(MIN_POS_PER_FOLD),\n            \"min_neg_per_fold\": int(MIN_NEG_PER_FOLD),\n            \"enforce_min_pos_per_fold\": bool(ENFORCE_MIN_POS_PER_FOLD),\n            \"enforce_min_neg_per_fold\": bool(ENFORCE_MIN_NEG_PER_FOLD),\n            \"use_group_by_split_requested\": bool(USE_GROUP_BY_SPLIT),\n            \"auto_fallback_group\": bool(AUTO_FALLBACK_GROUP),\n            \"holdout_fallback\": bool(HOLDOUT_FALLBACK),\n            \"holdout_frac\": float(HOLDOUT_FRAC),\n            \"order_source\": order_source,\n            \"target_col\": target_col,\n            \"group_col\": group_col,\n            \"pos_weight_hint\": float(pos_weight),\n            \"summary\": {\n                \"N\": int(N), \"pos\": int(pos), \"neg\": int(neg), \"pos_rate\": float(pos_rate),\n                \"min_pos_in_fold\": int(min_pos_seen), \"min_neg_in_fold\": int(min_neg_seen),\n            },\n            \"artifacts\": {\n                \"folds_csv\": str(folds_csv),\n                \"folds_npz\": str(npz_path),\n                \"report_txt\": str(report_path),\n                \"fold_stats_csv\": str(fold_stats_path),\n                \"cv_train_ids_npy\": str(CV_DIR / \"cv_train_ids.npy\"),\n                \"holdout_val_mask_npy\": (str(CV_DIR / \"cv_holdout_val_mask.npy\") if val_mask_holdout is not None else None),\n            },\n        },\n        f,\n        indent=2,\n    )\n\nprint(\"\\n[Stage 7] CV split OK\")\nprint(f\"- Saved: {folds_csv}\")\nprint(f\"- Saved: {npz_path}\")\nprint(f\"- Saved: {report_path}\")\nprint(f\"- Saved: {fold_stats_path}\")\nprint(f\"- Saved: {cfg_path}\")\n\ntail_n = min(len(lines), 12)\nprint(\"\\n\".join(lines[-tail_n:]))\n\n# ----------------------------\n# 11) Export globals for next stage\n# ----------------------------\nglobals().update({\n    \"CV_DIR\": CV_DIR,\n    \"n_splits\": int(n_splits),\n    \"train_ids_ordered\": train_ids,\n    \"y_ordered\": y,\n    \"fold_assign\": fold_assign,\n    \"folds\": folds,\n    \"CV_FOLDS_CSV\": folds_csv,\n    \"CV_FOLDS_NPZ\": npz_path,\n    \"CV_CFG_PATH\": cfg_path,\n    \"CV_TYPE\": cv_type,\n    \"CV_ORDER_SOURCE\": order_source,\n    \"POS_WEIGHT_HINT\": float(pos_weight),\n    \"HOLDOUT_VAL_MASK\": (val_mask_holdout if val_mask_holdout is not None else None),\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Model (CPU-Safe Configuration)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 8 — Train Multiband Event Transformer\n# REVISI FULL v4.5 (IMPROVED: PR-AUC primary metric + optional focal + safer agg chunk + better logging)\n#\n# Key upgrades v4.5:\n# - Primary metric default = PR-AUC (AveragePrecision) (lebih cocok imbalance) + tie-break val_loss\n# - Optional focal loss (CFG[\"focal_gamma\"] > 0) for extreme imbalance\n# - Auto chunk size for AGG features to avoid RAM spikes\n# - Per-fold training history saved to LOG_DIR/fold_{fold}_history.csv\n# - Stabilize encoder input with LayerNorm\n# - Keeps: global_features_raw.npy caching + EMA + pos_weight default\n# ============================================================\n\nimport os, gc, json, math, time, warnings, hashlib\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require minimal previous stages\n# ----------------------------\nneed_min = [\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_train_meta\",\"n_splits\",\"folds\"]\nfor k in need_min:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n\n# ----------------------------\n# 0a) Resolve train_ids ordering + labels (FAST + robust)\n# ----------------------------\ndef _decode_ids(arr):\n    out = []\n    for x in arr.tolist():\n        if isinstance(x, (bytes, bytearray, np.bytes_)):\n            s = x.decode(\"utf-8\", errors=\"ignore\")\n        else:\n            s = str(x)\n        out.append(s.strip())\n    return out\n\nif \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n    train_ids = [str(x).strip() for x in list(globals()[\"train_ids_ordered\"])]\nelse:\n    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n    if p.exists():\n        raw = np.load(p, allow_pickle=False)\n        train_ids = _decode_ids(raw)\n    else:\n        train_ids = pd.Index(globals()[\"df_train_meta\"].index).astype(\"string\").str.strip().astype(str).tolist()\n\ndf_train_meta = globals()[\"df_train_meta\"]\ntarget_col = None\nfor cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n    if cand in df_train_meta.columns:\n        target_col = cand\n        break\nif target_col is None:\n    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols(sample)={list(df_train_meta.columns)[:40]}\")\n\nmeta = df_train_meta.copy()\nmeta.index = pd.Index(pd.Index(meta.index).astype(\"string\").str.strip().astype(str), name=\"object_id\")\n\npos_idx = meta.index.get_indexer(train_ids)\nif (pos_idx < 0).any():\n    miss = [train_ids[i] for i in np.where(pos_idx < 0)[0][:10]]\n    raise RuntimeError(f\"Some train_ids not found in df_train_meta.index (after str/strip). ex={miss}\")\n\ny_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\ny = y_all[pos_idx]\ny = (y > 0).astype(np.int8)\n\n# ----------------------------\n# 0b) Ensure output dirs exist\n# ----------------------------\nif \"RUN_DIR\" in globals() and globals()[\"RUN_DIR\"] is not None:\n    RUN_DIR = Path(globals()[\"RUN_DIR\"])\nelse:\n    if \"ART_DIR\" in globals() and globals()[\"ART_DIR\"] is not None:\n        RUN_DIR = Path(globals()[\"ART_DIR\"]).parent\n    else:\n        RUN_DIR = Path(\"/kaggle/working/mallorn_run\")\n\nART_DIR = Path(globals().get(\"ART_DIR\", RUN_DIR / \"artifacts\"))\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nCKPT_DIR = Path(globals().get(\"CKPT_DIR\", RUN_DIR / \"checkpoints\"))\nOOF_DIR  = Path(globals().get(\"OOF_DIR\",  RUN_DIR / \"oof\"))\nLOG_DIR  = Path(globals().get(\"LOG_DIR\",  RUN_DIR / \"logs\"))\nCKPT_DIR.mkdir(parents=True, exist_ok=True)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nglobals().update({\"RUN_DIR\": RUN_DIR, \"ART_DIR\": ART_DIR, \"CKPT_DIR\": CKPT_DIR, \"OOF_DIR\": OOF_DIR, \"LOG_DIR\": LOG_DIR})\n\n# ----------------------------\n# 1) Torch imports + CPU safety\n# ----------------------------\ntry:\n    import torch\n    import torch.nn as nn\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\ndevice = torch.device(\"cpu\")\n\n# threads (safe)\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\n# metrics\ntry:\n    from sklearn.metrics import roc_auc_score, average_precision_score\n    _HAS_AP = True\nexcept Exception as e:\n    raise RuntimeError(\"scikit-learn metrics tidak tersedia (roc_auc_score/average_precision_score).\") from e\n\n# ----------------------------\n# 2) Open memmaps (fixed seq)\n# ----------------------------\nFIX_DIR = Path(globals()[\"FIX_DIR\"])\nN = len(train_ids)\nL = int(globals()[\"MAX_LEN\"])\nSEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\nFdim = len(SEQ_FEATURE_NAMES)\nfeat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\n\nfor p in [train_X_path, train_B_path, train_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\n# ----------------------------\n# 2b) Read Stage6 policy (SHIFT_BAND_IDS + dtype_X + score_value_feat + token_mode)\n# ----------------------------\nSHIFT_BAND_IDS = False\nPAD_BAND_ID = 0\nDTYPE_X_MEMMAP = np.float32\nSEQ_TOKEN_MODE = None\nSCORE_VALUE_FEAT = globals().get(\"SCORE_VALUE_FEAT\", None)\n\npolicy_path = FIX_DIR / \"length_policy_config.json\"\nif policy_path.exists():\n    try:\n        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n            pol = json.load(f)\n        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n        dt = str(pol.get(\"dtype_X\", \"float32\")).lower()\n        DTYPE_X_MEMMAP = np.float16 if ((\"float16\" in dt) or (\"fp16\" in dt)) else np.float32\n        if SCORE_VALUE_FEAT is None:\n            SCORE_VALUE_FEAT = pol.get(\"score_value_feat\", None)\n        if SEQ_TOKEN_MODE is None:\n            SEQ_TOKEN_MODE = pol.get(\"token_mode\", None)\n    except Exception:\n        pass\n\nX_mm = np.memmap(train_X_path, dtype=DTYPE_X_MEMMAP, mode=\"r\", shape=(N, L, Fdim))\nB_mm = np.memmap(train_B_path, dtype=np.int8,        mode=\"r\", shape=(N, L))\nM_mm = np.memmap(train_M_path, dtype=np.int8,        mode=\"r\", shape=(N, L))\n\n# ----------------------------\n# 2c) Robust value feature resolution\n# ----------------------------\ndef _pick_val_feat(feat_map, prefer=None):\n    cand = []\n    if prefer is not None:\n        cand.append(str(prefer))\n    cand += [\n        \"signal\", \"value\", \"flux\",\n        \"flux_asinh\", \"flux_asinh_clip\", \"flux_asinh_norm\", \"flux_asinh_scaled\",\n        \"mag\", \"mag_norm\", \"mag_clip\", \"mag_scaled\",\n        \"delta_signal\", \"delta_flux\", \"signal_clip\", \"signal_norm\",\n    ]\n    for c in cand:\n        if c and (c in feat_map):\n            return c\n    keys = list(feat_map.keys())\n    fuzzy = [k for k in keys if any(t in k for t in [\"signal\", \"flux\", \"value\", \"asinh\", \"mag\"])]\n    if fuzzy:\n        return sorted(fuzzy)[0]\n    return None\n\nif SEQ_TOKEN_MODE is None:\n    SEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\nif SEQ_TOKEN_MODE is None:\n    SEQ_TOKEN_MODE = \"mag\" if any((k == \"mag\") or k.startswith(\"mag\") for k in feat.keys()) else \"asinh\"\nSEQ_TOKEN_MODE = str(SEQ_TOKEN_MODE).lower().strip()\n\nVAL_FEAT = _pick_val_feat(feat, prefer=SCORE_VALUE_FEAT)\nif VAL_FEAT is None:\n    raise RuntimeError(\"Cannot resolve VAL_FEAT from SEQ_FEATURE_NAMES.\")\n\nfor k in [\"snr_tanh\",\"detected\"]:\n    if k not in feat:\n        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n\nVAL_IS_MAG = (SEQ_TOKEN_MODE == \"mag\") and (\"mag\" in VAL_FEAT)\n\nprint(f\"[Stage 8] token_mode={SEQ_TOKEN_MODE} | VAL_FEAT={VAL_FEAT} | VAL_IS_MAG={VAL_IS_MAG}\")\nprint(f\"[Stage 8] SHIFT_BAND_IDS(from stage6)={SHIFT_BAND_IDS} | PAD_BAND_ID={PAD_BAND_ID} | X_dtype={DTYPE_X_MEMMAP}\")\n\n# ----------------------------\n# 3) Build RAW meta global features (no leak)\n# ----------------------------\nif (\"EBV_clip\" in meta.columns):\n    EBV_used = pd.to_numeric(meta[\"EBV_clip\"], errors=\"coerce\")\nelif (\"EBV\" in meta.columns):\n    EBV_used = pd.to_numeric(meta[\"EBV\"], errors=\"coerce\")\nelse:\n    EBV_used = pd.Series(np.zeros((len(meta),), dtype=np.float32), index=meta.index)\n\nBASE_G_COLS = [\"Z\",\"Z_err\",\"EBV_used\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\n\ntmp_meta = meta.copy()\ntmp_meta[\"EBV_used\"] = EBV_used\nfor c in BASE_G_COLS:\n    if c not in tmp_meta.columns:\n        tmp_meta[c] = 0.0\n\nG_meta = tmp_meta.iloc[pos_idx][BASE_G_COLS].copy()\nfor c in BASE_G_COLS:\n    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\nG_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n\nwith open(Path(LOG_DIR)/\"global_meta_cols.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"cols\": BASE_G_COLS}, f, indent=2)\n\n# ----------------------------\n# 3b) Sequence aggregate features (global + per-band) + CACHE\n# ----------------------------\nUSE_AGG_SEQ_FEATURES = True\nN_BANDS = 6\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1.0)\n\ndef _auto_chunk(L, F, target_mb=220, min_chunk=64, max_chunk=2048):\n    # approx bytes = chunk * L * F * 4\n    target_bytes = float(target_mb) * (1024**2)\n    denom = float(L) * float(F) * 4.0\n    if denom <= 0:\n        return min_chunk\n    c = int(target_bytes // denom)\n    c = max(min_chunk, min(max_chunk, c))\n    return int(c)\n\ndef build_agg_seq_features(X_mm, B_mm, M_mm, chunk=None):\n    snr_i = feat[\"snr_tanh\"]\n    det_i = feat[\"detected\"]\n    val_i = feat[VAL_FEAT]\n\n    if chunk is None:\n        chunk = _auto_chunk(L, Fdim, target_mb=220, min_chunk=64, max_chunk=1024)\n\n    out_chunks = []\n    for s in range(0, N, chunk):\n        e = min(N, s + chunk)\n\n        # NOTE: slicing memmap will read chunk; keep chunk modest to avoid RAM spikes\n        Xc = np.asarray(X_mm[s:e])  # (B,L,F)\n        Bc = np.asarray(B_mm[s:e])  # (B,L)\n        Mc = np.asarray(M_mm[s:e])  # (B,L)\n\n        real = (Mc == 1)\n        tok_count = real.sum(axis=1).astype(np.float32)\n\n        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32, copy=False)\n        det = (Xc[:, :, det_i] > 0.5).astype(np.float32, copy=False)\n        val = Xc[:, :, val_i].astype(np.float32, copy=False)\n\n        snr_r = snr * real\n        det_r = det * real\n\n        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n        max_abs_snr = np.where(tok_count > 0, snr_r.max(axis=1), 0.0).astype(np.float32)\n\n        if VAL_IS_MAG:\n            val_r = np.where(real, val, np.nan)\n            mean_val = np.nan_to_num(np.nanmean(val_r, axis=1).astype(np.float32), nan=0.0)\n            std_val  = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32),  nan=0.0)\n            min_val  = np.nan_to_num(np.nanmin(val_r, axis=1).astype(np.float32),  nan=0.0)\n            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n        else:\n            aval = np.abs(val).astype(np.float32, copy=False)\n            aval_r = aval * real\n            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n            val_r = np.where(real, val, np.nan)\n            std_val = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32), nan=0.0)\n            max_aval = np.where(tok_count > 0, aval_r.max(axis=1), 0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n\n        # band ids to [0..N_BANDS-1]\n        if SHIFT_BAND_IDS:\n            Badj = Bc.astype(np.int16, copy=False)\n            Badj = np.where(real, np.clip(Badj - 1, 0, N_BANDS - 1), 0).astype(np.int16, copy=False)\n        else:\n            Badj = Bc.astype(np.int16, copy=False)\n\n        per_band = []\n        for b in range(N_BANDS):\n            bm = (Badj == b) & real\n            cnt = bm.sum(axis=1).astype(np.float32)\n\n            detb = (det * bm).sum(axis=1).astype(np.float32)\n            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n\n            det_frac_b = _safe_div(detb, cnt)\n            mean_abs_snr_b = _safe_div(snrb, cnt)\n\n            if VAL_IS_MAG:\n                vb = np.where(bm, val, np.nan)\n                mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n            else:\n                ab = (np.abs(val).astype(np.float32) * bm).sum(axis=1).astype(np.float32)\n                mean_val_b = _safe_div(ab, cnt).astype(np.float32)\n\n            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n\n        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n\n        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n\n        out_chunks.append(agg)\n\n        del Xc, Bc, Mc, Badj\n        if ((s // chunk) % 4) == 0:\n            gc.collect()\n\n    return np.concatenate(out_chunks, axis=0).astype(np.float32)\n\n# ---- cache key for G_seq ----\ndef _hash_cfg(d: dict) -> str:\n    s = json.dumps(d, sort_keys=True, ensure_ascii=True)\n    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:12]\n\nG_CACHE_DIR = FIX_DIR\nG_RAW_CACHE = G_CACHE_DIR / \"global_features_raw.npy\"\nG_RAW_META  = G_CACHE_DIR / \"global_features_raw_meta.json\"\n\nagg_spec = {\n    \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n    \"N\": int(N),\n    \"L\": int(L),\n    \"Fdim\": int(Fdim),\n    \"n_bands\": int(N_BANDS),\n    \"seq_token_mode\": str(SEQ_TOKEN_MODE),\n    \"val_feat\": str(VAL_FEAT),\n    \"val_is_mag\": bool(VAL_IS_MAG),\n    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n    \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n    \"meta_cols\": list(BASE_G_COLS),\n}\nagg_hash = _hash_cfg(agg_spec)\n\nG_raw_np = None\nif G_RAW_CACHE.exists() and G_RAW_META.exists():\n    try:\n        old = json.loads(G_RAW_META.read_text())\n        if old.get(\"agg_hash\", None) == agg_hash and int(old.get(\"N\", -1)) == int(N):\n            G_raw_np = np.load(G_RAW_CACHE, allow_pickle=False).astype(np.float32, copy=False)\n            if G_raw_np.shape[0] != N:\n                G_raw_np = None\n    except Exception:\n        G_raw_np = None\n\nif G_raw_np is None:\n    if USE_AGG_SEQ_FEATURES:\n        chunk0 = _auto_chunk(L, Fdim, target_mb=220, min_chunk=64, max_chunk=1024)\n        print(f\"[Stage 8] Building AGG sequence features (one-time, cached)... chunk={chunk0}\")\n        t0 = time.time()\n        G_seq_np = build_agg_seq_features(X_mm, B_mm, M_mm, chunk=chunk0)\n        print(f\"[Stage 8] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\n    else:\n        G_seq_np = np.zeros((N,0), dtype=np.float32)\n\n    G_raw_np = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\n    np.save(G_RAW_CACHE, G_raw_np.astype(np.float32, copy=False))\n    G_RAW_META.write_text(json.dumps({\"agg_hash\": agg_hash, \"N\": int(N), \"spec\": agg_spec}, indent=2))\n\ng_dim = int(G_raw_np.shape[1])\n\nwith open(Path(LOG_DIR)/\"global_feature_spec.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"agg_hash\": agg_hash,\n            \"meta_cols\": BASE_G_COLS,\n            \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n            \"token_mode\": SEQ_TOKEN_MODE,\n            \"val_feat\": VAL_FEAT,\n            \"val_is_mag\": bool(VAL_IS_MAG),\n            \"total_g_dim\": int(g_dim),\n            \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n            \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n            \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n            \"score_value_feat_from_stage6\": (None if SCORE_VALUE_FEAT is None else str(SCORE_VALUE_FEAT)),\n            \"cache\": {\"path\": str(G_RAW_CACHE), \"meta\": str(G_RAW_META)},\n        },\n        f,\n        indent=2,\n    )\n\n# ----------------------------\n# 4) Dataset / Loader\n# ----------------------------\nclass MemmapSeqDataset(torch.utils.data.Dataset):\n    def __init__(self, idx, X_mm, B_mm, M_mm, G_raw_np, y=None):\n        self.idx = np.asarray(idx, dtype=np.int32)\n        self.X_mm = X_mm\n        self.B_mm = B_mm\n        self.M_mm = M_mm\n        self.G_raw = G_raw_np\n        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n\n    def __len__(self):\n        return len(self.idx)\n\n    def __getitem__(self, i):\n        j = int(self.idx[i])\n        X = np.asarray(self.X_mm[j])                   # (L,F)\n        B = np.asarray(self.B_mm[j]).astype(np.int64)  # (L,)\n        M = np.asarray(self.M_mm[j]).astype(np.int64)  # (L,)\n        G0 = np.asarray(self.G_raw[j], dtype=np.float32)\n\n        Xt = torch.from_numpy(X)   # dtype as memmap; model casts to float32\n        Bt = torch.from_numpy(B)\n        Mt = torch.from_numpy(M)\n        Gt = torch.from_numpy(G0)\n\n        if self.y is None:\n            return Xt, Bt, Mt, Gt\n\n        yy = float(self.y[j])\n        return Xt, Bt, Mt, Gt, torch.tensor(yy, dtype=torch.float32)\n\ndef make_loader(ds, batch_size, shuffle, sampler=None):\n    return torch.utils.data.DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=(sampler is None and shuffle),\n        sampler=sampler,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n# ----------------------------\n# 5) EMA helper\n# ----------------------------\nclass EMA:\n    def __init__(self, model, decay=0.999):\n        self.decay = float(decay)\n        self.shadow = {}\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.detach().clone()\n\n    @torch.no_grad()\n    def update(self, model):\n        d = self.decay\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n\n    def store(self, model):\n        self.backup = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.backup[name] = p.detach().clone()\n\n    def copy_to(self, model):\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                p.data.copy_(self.shadow[name].data)\n\n    def restore(self, model):\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                p.data.copy_(self.backup[name].data)\n        self.backup = {}\n\n# ----------------------------\n# 6) Model\n# ----------------------------\nclass MultibandEventTransformer(nn.Module):\n    def __init__(\n        self,\n        feat_dim, max_len, n_bands=6,\n        d_model=160, n_heads=4, n_layers=3, ff_mult=2,\n        dropout=0.14, g_dim=0,\n        shift_band_ids=False\n    ):\n        super().__init__()\n        self.n_bands = int(n_bands)\n        self.d_model = int(d_model)\n        self.max_len = int(max_len)\n        self.shift_band_ids = bool(shift_band_ids)\n\n        self.x_proj = nn.Sequential(\n            nn.Linear(feat_dim, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n        self.x_ln = nn.LayerNorm(d_model)  # stabilizer\n\n        self.band_emb = nn.Embedding(self.n_bands, d_model)\n\n        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=int(d_model * ff_mult),\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        self.attn = nn.Linear(d_model, 1)\n        self.pool_ln = nn.LayerNorm(d_model)\n\n        g_out = max(32, d_model // 2)\n        self.g_proj = nn.Sequential(\n            nn.Linear(g_dim, g_out),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model + g_out, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def set_global_scaler(self, mean_np, std_np):\n        mean_t = torch.tensor(mean_np, dtype=torch.float32)\n        std_t  = torch.tensor(std_np, dtype=torch.float32)\n        self.register_buffer(\"g_mean_buf\", mean_t, persistent=False)\n        self.register_buffer(\"g_std_buf\", std_t, persistent=False)\n\n    def forward(self, X, band_id, mask, G_raw):\n        X = X.to(torch.float32)\n        band_id = band_id.to(torch.long)\n        mask = mask.to(torch.long)\n\n        if self.shift_band_ids:\n            real = (mask == 1)\n            if real.any():\n                band2 = band_id.clone()\n                band2[real] = (band2[real] - 1).clamp(0, self.n_bands - 1)\n                band2[~real] = 0\n                band_id = band2\n            else:\n                band_id = torch.zeros_like(band_id)\n\n        band_id = band_id.clamp(0, self.n_bands - 1)\n\n        pad_mask = (mask == 0)\n        all_pad = pad_mask.all(dim=1)\n        if all_pad.any():\n            pad_mask = pad_mask.clone()\n            pad_mask[all_pad, 0] = False\n\n        h = self.x_proj(X)\n        h = self.x_ln(h)\n        h = h + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        # attention pooling\n        a = self.attn(h).squeeze(-1)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)\n        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n\n        # mean pooling\n        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n        denom = valid.sum(dim=1).clamp_min(1.0)\n        pooled_mean = (h * valid).sum(dim=1) / denom\n\n        # max pooling\n        h_masked = h.masked_fill(pad_mask.unsqueeze(-1), -1e9)\n        pooled_max = torch.max(h_masked, dim=1).values\n        pooled_max = torch.where(torch.isfinite(pooled_max), pooled_max, torch.zeros_like(pooled_max))\n\n        pooled = (0.50 * pooled_attn) + (0.30 * pooled_mean) + (0.20 * pooled_max)\n        pooled = self.pool_ln(pooled)\n\n        # global\n        G = G_raw.to(torch.float32)\n        G = (G - self.g_mean_buf) / self.g_std_buf\n        g = self.g_proj(G)\n\n        z = torch.cat([pooled, g], dim=1)\n        return self.head(z).squeeze(-1)\n\n# ----------------------------\n# 7) Training config (improved defaults)\n# ----------------------------\nCFG = {\n    \"d_model\": 160,\n    \"n_heads\": 4,\n    \"n_layers\": 3,\n    \"ff_mult\": 2,\n    \"dropout\": 0.14,\n\n    \"batch_size\": 16,\n    \"grad_accum\": 2,\n\n    \"epochs\": 18,\n    \"lr\": 5e-4,\n    \"weight_decay\": 0.02,\n\n    \"patience\": 6,\n    \"max_grad_norm\": 1.0,\n\n    # imbalance:\n    # \"pos_weight\" recommended default; PR-AUC becomes primary metric\n    \"balance_mode\": \"pos_weight\",   # pos_weight / sampler / both\n    \"label_smoothing\": 0.02,\n\n    # optional focal (set >0 if imbalance ekstrem; start 1.0~2.0)\n    \"focal_gamma\": 0.0,             # 0.0 = OFF\n\n    \"primary_metric\": \"ap\",         # \"ap\" (PR-AUC) or \"auc\"\n    \"scheduler\": \"onecycle\",\n\n    \"use_ema\": True,\n    \"ema_decay\": 0.999,\n\n    # augmentation (keep moderate on CPU)\n    \"aug_tokendrop_p\": 0.05,\n    \"aug_value_noise\": 0.010,\n    \"aug_featdrop_p\": 0.00,\n}\n\n# adapt for long sequences\nif L >= 512:\n    CFG[\"d_model\"] = 128\n    CFG[\"n_heads\"] = 4\n    CFG[\"n_layers\"] = 2\n    CFG[\"batch_size\"] = 12\n    CFG[\"grad_accum\"] = 2\n    CFG[\"lr\"] = 4e-4\n    CFG[\"dropout\"] = 0.16\n\ncfg_path = Path(LOG_DIR) / \"train_cfg_stage8.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(CFG, f, indent=2)\n\npos_all = int((y == 1).sum())\nneg_all = int((y == 0).sum())\nprint(\"[Stage 8] TRAIN CONFIG (CPU)\")\nprint(f\"- N={N:,} | pos={pos_all:,} | neg={neg_all:,} | pos%={pos_all/max(N,1)*100:.6f}%\")\nprint(f\"- token_mode={SEQ_TOKEN_MODE} | VAL_FEAT={VAL_FEAT} | val_is_mag={VAL_IS_MAG} | g_dim={g_dim} | use_agg_seq={USE_AGG_SEQ_FEATURES}\")\nprint(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\nprint(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\nprint(f\"- balance_mode={CFG['balance_mode']} | focal_gamma={CFG['focal_gamma']} | primary_metric={CFG['primary_metric']}\")\nprint(f\"- ema={CFG['use_ema']}({CFG['ema_decay']})\")\nprint(f\"- CKPT_DIR={CKPT_DIR}\")\nprint(f\"- OOF_DIR ={OOF_DIR}\")\nprint(f\"- LOG_DIR ={LOG_DIR}\")\n\n# ----------------------------\n# 8) Helpers\n# ----------------------------\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef f1_binary(y_true, y_pred01):\n    y_true = y_true.astype(np.int32)\n    y_pred01 = y_pred01.astype(np.int32)\n    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n    if tp == 0:\n        return 0.0\n    prec = tp / max(tp + fp, 1)\n    rec  = tp / max(tp + fn, 1)\n    if prec + rec == 0:\n        return 0.0\n    return float(2 * prec * rec / (prec + rec))\n\ndef best_f1_threshold(y_true, prob, grid=240):\n    y_true = y_true.astype(np.int8)\n    prob = prob.astype(np.float32)\n    best_thr, best_f1 = 0.5, -1.0\n    for thr in np.linspace(0.01, 0.99, grid, dtype=np.float32):\n        f1 = f1_binary(y_true, (prob >= thr).astype(np.int8))\n        if f1 > best_f1:\n            best_f1, best_thr = float(f1), float(thr)\n    return best_thr, best_f1\n\n@torch.inference_mode()\ndef eval_model(model, loader, criterion_eval, use_ema=False, ema=None):\n    model.eval()\n    if use_ema and (ema is not None):\n        ema.store(model)\n        ema.copy_to(model)\n\n    losses, logits_all, y_all = [], [], []\n    for batch in loader:\n        Xb, Bb, Mb, Gb, yb = batch\n        Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n        logit = model(Xb, Bb, Mb, Gb)\n        loss = criterion_eval(logit, yb)\n        losses.append(float(loss.item()))\n        logits_all.append(logit.detach().cpu().numpy())\n        y_all.append(yb.detach().cpu().numpy())\n\n    if use_ema and (ema is not None):\n        ema.restore(model)\n\n    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n\n    probs = sigmoid_np(logits_all)\n    pred01 = (probs >= 0.5).astype(np.int8)\n\n    f1 = f1_binary(y_all, pred01)\n    if len(np.unique(y_all)) == 2:\n        auc = float(roc_auc_score(y_all, probs))\n        ap  = float(average_precision_score(y_all, probs))\n    else:\n        auc, ap = float(\"nan\"), float(\"nan\")\n\n    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1, auc, ap\n\ndef fit_scaler_fold(G_raw_np, tr_idx):\n    X = G_raw_np[tr_idx]\n    mean = X.mean(axis=0).astype(np.float32)\n    std  = X.std(axis=0).astype(np.float32)\n    std  = np.where(std < 1e-6, 1.0, std).astype(np.float32)\n    return mean, std\n\ndef make_adamw_param_groups(model, weight_decay):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        ln = name.lower()\n        if name.endswith(\".bias\") or (\"norm\" in ln) or (\"ln\" in ln):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n    return [\n        {\"params\": decay, \"weight_decay\": float(weight_decay)},\n        {\"params\": no_decay, \"weight_decay\": 0.0},\n    ]\n\n# focal with logits (optional) + optional pos_weight already inside BCE\ndef make_train_loss(pos_weight_t=None, focal_gamma=0.0):\n    focal_gamma = float(focal_gamma)\n    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t, reduction=\"none\") if pos_weight_t is not None else nn.BCEWithLogitsLoss(reduction=\"none\")\n    def _loss(logit, target):\n        # target in [0,1]\n        target = target.to(logit.dtype)\n        base = bce(logit, target)  # (B,)\n        if focal_gamma <= 0:\n            return base.mean()\n        p = torch.sigmoid(logit)\n        pt = torch.where(target > 0.5, p, 1.0 - p)\n        w = (1.0 - pt).clamp_min(0.0).pow(focal_gamma)\n        return (w * base).mean()\n    return _loss\n\n# ----------------------------\n# 9) Batch augment (safe)\n# ----------------------------\ndef apply_batch_aug(Xb, Bb, Mb, cfg, feat_map, val_feat_name):\n    p_drop  = float(cfg.get(\"aug_tokendrop_p\", 0.0))\n    noise   = float(cfg.get(\"aug_value_noise\", 0.0))\n    p_fdrop = float(cfg.get(\"aug_featdrop_p\", 0.0))\n\n    # token drop\n    if p_drop and p_drop > 0:\n        real = (Mb == 1)\n        if real.any():\n            rnd = torch.rand_like(Mb.float())\n            drop = (rnd < p_drop) & real\n\n            # ensure at least 1 token remains for samples that had any\n            nreal = real.sum(dim=1)\n            ndrop = drop.sum(dim=1)\n            bad = (nreal > 0) & (ndrop >= nreal)\n            if bad.any():\n                bad_idx = torch.where(bad)[0].tolist()\n                for bi in bad_idx:\n                    pos = torch.where(real[bi])[0]\n                    if pos.numel() > 0:\n                        keep_one = pos[int(torch.randint(0, pos.numel(), (1,)).item())]\n                        drop[bi, keep_one] = False\n\n            Mb = Mb.clone()\n            Mb[drop] = 0\n\n    # value noise\n    if noise and noise > 0 and (val_feat_name in feat_map):\n        vi = int(feat_map[val_feat_name])\n        real = (Mb == 1)\n        n = int(real.sum().item())\n        if n > 0:\n            eps = torch.randn((n,), device=Xb.device, dtype=Xb.dtype) * float(noise)\n            Xb = Xb.clone()\n            col = Xb[:, :, vi].clone()\n            col[real] = col[real] + eps\n            Xb[:, :, vi] = col\n\n    # feature dropout (optional): only on key features\n    if p_fdrop and p_fdrop > 0:\n        real = (Mb == 1)\n        if real.any():\n            cand_feats = []\n            for nm in [val_feat_name, \"snr_tanh\"]:\n                if nm in feat_map:\n                    cand_feats.append(int(feat_map[nm]))\n            if cand_feats:\n                Xb = Xb.clone()\n                rnd = torch.rand_like(Mb.float())\n                for fi in cand_feats:\n                    mask_drop = (rnd < p_fdrop) & real\n                    col = Xb[:, :, fi].clone()\n                    col[mask_drop] = 0.0\n                    Xb[:, :, fi] = col\n\n    return Xb, Bb, Mb\n\n# ----------------------------\n# 10) CV Train\n# ----------------------------\noof_prob = np.full((N,), np.nan, dtype=np.float32)\nfold_metrics = []\n\nstart_time = time.time()\nn_splits = int(globals()[\"n_splits\"])\ncv_type = str(globals().get(\"CV_TYPE\", \"\"))\n\nprimary_metric = str(CFG.get(\"primary_metric\", \"ap\")).lower().strip()\nif primary_metric not in (\"ap\", \"auc\"):\n    primary_metric = \"ap\"\n\nfor fold_info in globals()[\"folds\"]:\n    fold = int(fold_info.get(\"fold\", 0))\n    tr_idx = np.asarray(fold_info[\"train_idx\"], dtype=np.int32)\n    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n\n    y_tr = y[tr_idx]\n    pos_f = int((y_tr == 1).sum())\n    neg_f = int((y_tr == 0).sum())\n    if pos_f == 0:\n        raise RuntimeError(f\"[Stage 8] Fold {fold}: no positives in training split.\")\n\n    balance_mode = str(CFG.get(\"balance_mode\", \"pos_weight\")).lower().strip()\n    use_sampler = balance_mode in (\"sampler\", \"both\")\n    use_posw    = balance_mode in (\"pos_weight\", \"both\")\n\n    pos_weight = float(neg_f / max(pos_f, 1))\n    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device) if use_posw else None\n\n    ls = float(CFG.get(\"label_smoothing\", 0.0))\n    def smooth(yb):\n        if ls <= 0:\n            return yb\n        return yb * (1.0 - ls) + 0.5 * ls\n\n    # train loss (weighted + optional focal)\n    focal_gamma = float(CFG.get(\"focal_gamma\", 0.0))\n    loss_train_fn = make_train_loss(pos_weight_t=pos_weight_t, focal_gamma=focal_gamma)\n    # eval loss (unweighted BCE for comparability)\n    criterion_eval  = nn.BCEWithLogitsLoss()\n\n    print(f\"\\n[Stage 8] FOLD {fold} | train={len(tr_idx):,} val={len(val_idx):,} \"\n          f\"| pos={pos_f:,} neg={neg_f:,} | pos_weight={pos_weight:.4f} | balance_mode={balance_mode} | primary={primary_metric}\")\n\n    g_mean, g_std = fit_scaler_fold(G_raw_np, tr_idx)\n\n    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_raw_np, y=y)\n    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_raw_np, y=y)\n\n    sampler = None\n    if use_sampler:\n        ytr_local = y[tr_idx]\n        w = np.ones((len(tr_idx),), dtype=np.float32)\n        w[ytr_local == 1] = float(neg_f / max(pos_f, 1))\n        sampler = torch.utils.data.WeightedRandomSampler(\n            weights=torch.from_numpy(w),\n            num_samples=len(tr_idx),\n            replacement=True\n        )\n\n    dl_tr = make_loader(ds_tr, batch_size=int(CFG[\"batch_size\"]), shuffle=True, sampler=sampler)\n    dl_va = make_loader(ds_va, batch_size=int(CFG[\"batch_size\"]), shuffle=False)\n\n    model = MultibandEventTransformer(\n        feat_dim=Fdim,\n        max_len=L,\n        n_bands=6,\n        d_model=int(CFG[\"d_model\"]),\n        n_heads=int(CFG[\"n_heads\"]),\n        n_layers=int(CFG[\"n_layers\"]),\n        ff_mult=int(CFG[\"ff_mult\"]),\n        dropout=float(CFG[\"dropout\"]),\n        g_dim=g_dim,\n        shift_band_ids=bool(SHIFT_BAND_IDS),\n    ).to(device)\n    model.set_global_scaler(g_mean, g_std)\n\n    param_groups = make_adamw_param_groups(model, weight_decay=float(CFG[\"weight_decay\"]))\n    opt = torch.optim.AdamW(param_groups, lr=float(CFG[\"lr\"]))\n\n    scheduler = None\n    grad_accum = int(CFG[\"grad_accum\"])\n    if str(CFG.get(\"scheduler\",\"\")).lower() == \"onecycle\":\n        steps_per_epoch_opt = int(math.ceil(len(dl_tr) / max(grad_accum, 1)))\n        steps_per_epoch_opt = max(steps_per_epoch_opt, 1)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            opt,\n            max_lr=float(CFG[\"lr\"]),\n            epochs=int(CFG[\"epochs\"]),\n            steps_per_epoch=steps_per_epoch_opt,\n            pct_start=0.1,\n            anneal_strategy=\"cos\",\n            div_factor=10.0,\n            final_div_factor=50.0,\n        )\n\n    use_ema = bool(CFG.get(\"use_ema\", True))\n    ema = EMA(model, decay=float(CFG.get(\"ema_decay\", 0.999))) if use_ema else None\n\n    # best tracking\n    best_score = -1e18\n    best_val_loss = float(\"inf\")\n    best_epoch = -1\n    best_probs = None\n    best_thr = 0.5\n    best_f1_at_bestthr = -1.0\n    patience_left = int(CFG[\"patience\"])\n\n    # history log\n    hist_rows = []\n\n    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        total_loss = 0.0\n        n_batches = 0\n        accum = 0\n        opt_steps = 0\n\n        for batch in dl_tr:\n            Xb, Bb, Mb, Gb, yb = batch\n            Xb = Xb.to(device).to(torch.float32)\n            Bb = Bb.to(device)\n            Mb = Mb.to(device)\n            Gb = Gb.to(device)\n            yb = yb.to(device)\n\n            # aug\n            Xb, Bb, Mb = apply_batch_aug(Xb, Bb, Mb, CFG, feat, VAL_FEAT)\n\n            yb_s = smooth(yb)\n            logit = model(Xb, Bb, Mb, Gb)\n            loss = loss_train_fn(logit, yb_s)\n\n            total_loss += float(loss.item())\n            n_batches += 1\n\n            (loss / float(grad_accum)).backward()\n            accum += 1\n\n            if accum == grad_accum:\n                if CFG[\"max_grad_norm\"] is not None:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n                opt.step()\n                opt.zero_grad(set_to_none=True)\n                opt_steps += 1\n                accum = 0\n                if scheduler is not None:\n                    scheduler.step()\n                if ema is not None:\n                    ema.update(model)\n\n        if accum > 0:\n            if CFG[\"max_grad_norm\"] is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n            opt.step()\n            opt.zero_grad(set_to_none=True)\n            opt_steps += 1\n            if scheduler is not None:\n                scheduler.step()\n            if ema is not None:\n                ema.update(model)\n\n        train_loss = total_loss / max(n_batches, 1)\n\n        val_loss, probs, y_val, f1_05, val_auc, val_ap = eval_model(model, dl_va, criterion_eval, use_ema=use_ema, ema=ema)\n        score = float(val_ap) if primary_metric == \"ap\" else float(val_auc)\n\n        # improve rule: primary metric first, tie-break val_loss\n        improved = (score > best_score + 1e-7) or (math.isnan(best_score) and not math.isnan(score))\n        if (not improved) and (abs(score - best_score) <= 1e-7) and (val_loss < best_val_loss - 1e-6):\n            improved = True\n\n        if improved:\n            best_score = float(score)\n            best_val_loss = float(val_loss)\n            best_epoch = int(epoch)\n            best_probs = probs.copy()\n            best_thr, best_f1_at_bestthr = best_f1_threshold(y_val, best_probs, grid=240)\n\n            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n            payload = {\n                \"fold\": fold,\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"cfg\": CFG,\n                \"seq_feature_names\": SEQ_FEATURE_NAMES,\n                \"max_len\": L,\n                \"token_mode\": SEQ_TOKEN_MODE,\n                \"val_feat\": VAL_FEAT,\n                \"val_is_mag\": bool(VAL_IS_MAG),\n                \"global_meta_cols\": BASE_G_COLS,\n                \"use_agg_seq_features\": bool(USE_AGG_SEQ_FEATURES),\n                \"global_feature_cache\": {\"path\": str(G_RAW_CACHE), \"meta\": str(G_RAW_META), \"agg_hash\": agg_hash},\n                \"global_scaler\": {\"mean\": g_mean.astype(np.float32), \"std\": g_std.astype(np.float32)},\n                \"pos_weight_train\": float(pos_weight),\n                \"balance_mode\": balance_mode,\n                \"focal_gamma\": float(focal_gamma),\n                \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n                \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n                \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n                \"cv_type\": str(cv_type),\n                \"score_value_feat_from_stage6\": (None if SCORE_VALUE_FEAT is None else str(SCORE_VALUE_FEAT)),\n                \"primary_metric\": primary_metric,\n                \"best_thr_val_f1\": float(best_thr),\n                \"best_f1_val\": float(best_f1_at_bestthr),\n                \"best_val_auc\": float(val_auc),\n                \"best_val_ap\": float(val_ap),\n                \"best_val_loss\": float(best_val_loss),\n            }\n            if ema is not None:\n                payload[\"ema_shadow\"] = {k: v.detach().cpu() for k, v in ema.shadow.items()}\n                payload[\"ema_decay\"] = float(ema.decay)\n\n            torch.save(payload, ckpt_path)\n            patience_left = int(CFG[\"patience\"])\n        else:\n            patience_left -= 1\n\n        lr_now = opt.param_groups[0][\"lr\"]\n        hist_rows.append({\n            \"epoch\": int(epoch),\n            \"lr\": float(lr_now),\n            \"opt_steps\": int(opt_steps),\n            \"train_loss\": float(train_loss),\n            \"val_loss\": float(val_loss),\n            \"val_auc\": float(val_auc),\n            \"val_ap\": float(val_ap),\n            \"f1_at_0p5\": float(f1_05),\n            \"score_primary\": float(score),\n            \"best_epoch\": int(best_epoch),\n            \"patience_left\": int(patience_left),\n        })\n\n        print(f\"  epoch {epoch:02d} | lr={lr_now:.2e} | opt_steps={opt_steps:4d} | \"\n              f\"train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | auc={val_auc:.5f} | ap={val_ap:.5f} | \"\n              f\"f1@0.5={f1_05:.4f} | best_ep={best_epoch} | pat={patience_left}\")\n\n        if patience_left <= 0:\n            break\n\n    # save history\n    try:\n        pd.DataFrame(hist_rows).to_csv(Path(LOG_DIR)/f\"fold_{fold}_history.csv\", index=False)\n    except Exception:\n        pass\n\n    if best_probs is None:\n        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n\n    oof_prob[val_idx] = best_probs.astype(np.float32)\n\n    pred01 = (best_probs >= 0.5).astype(np.int8)\n    best_f1_05 = f1_binary(y[val_idx], pred01)\n\n    fold_metrics.append({\n        \"fold\": fold,\n        \"val_size\": int(len(val_idx)),\n        \"best_epoch\": int(best_epoch),\n        \"primary_metric\": primary_metric,\n        \"best_primary_score\": float(best_score),\n        \"best_val_loss\": float(best_val_loss),\n        \"val_auc_at_best\": float(val_auc),\n        \"val_ap_at_best\": float(val_ap),\n        \"f1_at_0p5\": float(best_f1_05),\n        \"best_thr_val_f1\": float(best_thr),\n        \"best_f1_val\": float(best_f1_at_bestthr),\n        \"pos_weight_train\": float(pos_weight),\n        \"focal_gamma\": float(focal_gamma),\n        \"g_dim\": int(g_dim),\n        \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n        \"balance_mode\": balance_mode,\n        \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n        \"ema_used\": bool(use_ema),\n        \"val_feat\": str(VAL_FEAT),\n        \"val_is_mag\": bool(VAL_IS_MAG),\n    })\n\n    del model, opt, ds_tr, ds_va, dl_tr, dl_va, ema\n    gc.collect()\n\nelapsed = time.time() - start_time\n\n# ----------------------------\n# 11) Save OOF artifacts + summary\n# ----------------------------\noof_path_npy = OOF_DIR / \"oof_prob.npy\"\nnp.save(oof_path_npy, oof_prob)\n\ndf_oof = pd.DataFrame({\"object_id\": train_ids, \"target\": y.astype(int), \"oof_prob\": oof_prob.astype(np.float32)})\noof_path_csv = OOF_DIR / \"oof_prob.csv\"\ndf_oof.to_csv(oof_path_csv, index=False)\n\nmetrics_path = OOF_DIR / \"fold_metrics.json\"\nwith open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed), \"cv_type\": str(cv_type)}, f, indent=2)\n\nvalid = np.isfinite(oof_prob)\nif valid.any() and len(np.unique(y[valid])) == 2:\n    oof_auc = float(roc_auc_score(y[valid], oof_prob[valid]))\n    oof_ap  = float(average_precision_score(y[valid], oof_prob[valid]))\nelse:\n    oof_auc = float(\"nan\")\n    oof_ap  = float(\"nan\")\n\nif valid.any():\n    oof_pred01 = (oof_prob[valid] >= 0.5).astype(np.int8)\n    oof_f1_05 = f1_binary(y[valid], oof_pred01)\n    oof_thr, oof_bestf1 = best_f1_threshold(y[valid], oof_prob[valid], grid=300)\nelse:\n    oof_f1_05 = float(\"nan\")\n    oof_thr, oof_bestf1 = float(\"nan\"), float(\"nan\")\n\nprint(\"\\n[Stage 8] TRAIN DONE\")\nprint(f\"- elapsed: {elapsed/60:.2f} min\")\nprint(f\"- OOF saved: {oof_path_npy}\")\nprint(f\"- OOF saved: {oof_path_csv}\")\nprint(f\"- fold metrics: {metrics_path}\")\nprint(f\"- OOF rows valid: {int(valid.sum()):,}/{N:,}\")\nprint(f\"- OOF AUC (valid-only): {oof_auc:.5f}\")\nprint(f\"- OOF AP  (valid-only): {oof_ap:.5f}\")\nprint(f\"- OOF F1@0.5 (valid-only): {oof_f1_05:.4f}\")\nprint(f\"- OOF best F1={oof_bestf1:.4f} @ thr={oof_thr:.4f}\")\n\nglobals().update({\n    \"oof_prob\": oof_prob,\n    \"OOF_PROB_PATH\": oof_path_npy,\n    \"OOF_CSV_PATH\": oof_path_csv,\n    \"FOLD_METRICS_PATH\": metrics_path,\n    \"TRAIN_CFG_PATH\": cfg_path,\n    \"VAL_FEAT\": VAL_FEAT,\n    \"VAL_IS_MAG\": VAL_IS_MAG,\n    \"OOF_BEST_THR_F1\": float(oof_thr) if np.isfinite(oof_thr) else None,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OOF Prediction + Threshold Tuning","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL)\n# REVISI FULL v4.4 (EXACT unique-level sweep + add F-beta + constraints + better tie-break)\n#\n# Upgrades v4.4:\n# - Exact sweep on UNIQUE prob levels (more accurate than mixed candidate grids)\n# - Still supports rule=\"ge\" (>=) and rule=\"gt\" (>)\n# - Provides ge_equiv thresholds for gt (via nextafter) for downstream that uses >=\n# - Adds F0.5 and F2 (precision/recall emphasis)\n# - Optional constraints: best thr with min_precision / min_recall\n# - Improved tie-break: after metric ties, prefer smaller |pos_pred - pos| gap\n#\n# Output:\n# - OOF_DIR/threshold_tuning.json\n# - OOF_DIR/threshold_report.txt\n# - OOF_DIR/threshold_table_top1000.csv\n# - globals: BEST_THR, BEST_THR_GE_F1, BEST_THR_GE_MCC, ... + tables\n# ============================================================\n\nimport gc, json, math, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\"OOF_DIR\", \"df_train_meta\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n\nOOF_DIR = Path(OOF_DIR)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Helper: robust stringify id\n# ----------------------------\ndef _to_str_list(ids):\n    out = []\n    for x in ids:\n        if isinstance(x, (bytes, np.bytes_, bytearray)):\n            out.append(x.decode(\"utf-8\", errors=\"ignore\").strip())\n        else:\n            out.append(str(x).strip())\n    return out\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a.reshape(1)\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\n# ----------------------------\n# Detect target column in df_train_meta\n# ----------------------------\ndef _detect_target_col(df):\n    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\nTARGET_COL = _detect_target_col(df_train_meta)\nif TARGET_COL is None:\n    raise RuntimeError(\n        \"Cannot detect target column in df_train_meta. \"\n        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n    )\n\n# normalize meta index to string+strip once\nmeta = df_train_meta.copy()\nmeta.index = pd.Index(pd.Index(meta.index).astype(\"string\").str.strip().astype(str), name=\"object_id\")\n\n# ----------------------------\n# Load OOF (prefer CSV)\n# ----------------------------\ndef _load_oof():\n    pcsv = OOF_DIR / \"oof_prob.csv\"\n    if pcsv.exists():\n        df = pd.read_csv(pcsv)\n        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n            ids = df[\"object_id\"].astype(str).str.strip().tolist()\n            prob = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n            return ids, prob, \"csv(oof_prob.csv)\"\n\n    if \"oof_prob\" in globals():\n        prob = _as_1d_float32(globals()[\"oof_prob\"])\n        if isinstance(prob, np.ndarray) and prob.ndim == 1 and len(prob) > 0:\n            if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n                ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n                return ids, prob, \"globals(oof_prob + train_ids_ordered)\"\n            if len(prob) == len(meta):\n                ids = _to_str_list(meta.index.tolist())\n                return ids, prob, \"globals(oof_prob + df_train_meta.index)\"\n\n    pnpy = OOF_DIR / \"oof_prob.npy\"\n    if pnpy.exists():\n        prob = _as_1d_float32(np.load(pnpy, allow_pickle=False))\n        if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n            ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n            return ids, prob, \"npy(oof_prob.npy + train_ids_ordered)\"\n        if len(prob) == len(meta):\n            ids = _to_str_list(meta.index.tolist())\n            return ids, prob, \"npy(oof_prob.npy + df_train_meta.index)\"\n\n    raise FileNotFoundError(\"OOF prob not found (csv/globals/npy). Jalankan STAGE 8 dulu.\")\n\ntrain_ids, oof_prob, src = _load_oof()\noof_prob = _as_1d_float32(oof_prob).astype(np.float32)\n\nif len(train_ids) != len(oof_prob):\n    raise RuntimeError(f\"OOF length mismatch: len(train_ids)={len(train_ids)} vs len(oof_prob)={len(oof_prob)}\")\n\n# IMPORTANT: keep NaN for holdout-safe (do NOT nan_to_num->0)\nvalid = np.isfinite(oof_prob)\nif not valid.any():\n    raise RuntimeError(\"All oof_prob are non-finite (NaN/inf). Check STAGE 8 output.\")\n\n# align y by train_ids via fast indexer\nidx = meta.index.get_indexer(train_ids)\nif (idx < 0).any():\n    bad = [train_ids[i] for i in np.where(idx < 0)[0][:10]]\n    raise KeyError(\n        f\"OOF ids not found in df_train_meta.index (string-normalized). ex={bad} | missing_n={int((idx<0).sum())}\"\n    )\n\ny_raw = pd.to_numeric(meta[TARGET_COL], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\ny_all = (y_raw[idx] > 0).astype(np.int8)\n\n# filter valid rows\ntrain_ids_v = [train_ids[i] for i in np.where(valid)[0]]\np_v = np.clip(oof_prob[valid].astype(np.float32), 0.0, 1.0)\ny_v = y_all[valid].astype(np.int8)\n\nN_all = int(len(y_all))\nN = int(len(y_v))\npos = int((y_v == 1).sum())\nneg = int((y_v == 0).sum())\n\nprint(f\"[Stage 9] Loaded OOF from: {src}\")\nprint(f\"[Stage 9] Valid rows: {N:,}/{N_all:,} (holdout mode => valid << total)\")\nprint(f\"[Stage 9] pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}% | target_col={TARGET_COL}\")\n\nuy = set(np.unique(y_v).tolist())\nif not uy.issubset({0, 1}):\n    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n\n# threshold-free sanity\ntry:\n    from sklearn.metrics import roc_auc_score, average_precision_score\n    auc_oof = float(roc_auc_score(y_v, p_v)) if (len(uy) == 2 and N > 1) else float(\"nan\")\n    ap_oof  = float(average_precision_score(y_v, p_v)) if (len(uy) == 2 and N > 1) else float(\"nan\")\nexcept Exception:\n    auc_oof = float(\"nan\")\n    ap_oof  = float(\"nan\")\n\n# ----------------------------\n# 1) Metric helpers (vectorized)\n# ----------------------------\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1e-12)\n\ndef _fbeta(prec, rec, beta):\n    beta2 = float(beta) ** 2\n    return _safe_div((1.0 + beta2) * prec * rec, beta2 * prec + rec)\n\ndef _metrics_from_counts(tp, fp, fn, tn):\n    tp = tp.astype(np.float64); fp = fp.astype(np.float64)\n    fn = fn.astype(np.float64); tn = tn.astype(np.float64)\n\n    prec = _safe_div(tp, tp + fp)\n    rec  = _safe_div(tp, tp + fn)\n\n    f1   = _safe_div(2 * prec * rec, prec + rec)\n    f05  = _fbeta(prec, rec, beta=0.5)\n    f2   = _fbeta(prec, rec, beta=2.0)\n\n    acc  = _safe_div(tp + tn, tp + fp + fn + tn)\n\n    tpr  = _safe_div(tp, tp + fn)\n    tnr  = _safe_div(tn, tn + fp)\n    bacc = 0.5 * (tpr + tnr)\n\n    num = tp * tn - fp * fn\n    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n\n    return f1, f05, f2, prec, rec, acc, bacc, mcc\n\n# ----------------------------\n# 2) Exact sweep on UNIQUE probability levels (fast + exact)\n# ----------------------------\np = p_v.astype(np.float32)\ny = y_v.astype(np.int8)\n\nord_desc = np.argsort(-p, kind=\"mergesort\")  # stable\np_sorted = p[ord_desc]\ny_sorted = y[ord_desc]\n\npos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\nneg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\n\npos_total = int(pos_prefix[-1]) if N > 0 else 0\nneg_total = int(neg_prefix[-1]) if N > 0 else 0\n\n# unique groups in descending sorted probs\n# group boundaries where p changes\nif N == 0:\n    raise RuntimeError(\"No valid rows to tune thresholds.\")\n\nchg = np.ones((N,), dtype=bool)\nchg[1:] = (p_sorted[1:] != p_sorted[:-1])\n\ngrp_starts = np.where(chg)[0].astype(np.int64)\ngrp_vals = p_sorted[grp_starts].astype(np.float32)\n\n# ends are starts next -1, last ends at N-1\ngrp_ends = np.empty_like(grp_starts)\ngrp_ends[:-1] = grp_starts[1:] - 1\ngrp_ends[-1] = N - 1\n\n# for ge: pred positive includes current group => k = end+1\nk_ge = (grp_ends + 1).astype(np.int64)\n# for gt: pred positive excludes equals => k = start (count of > thr)\nk_gt = grp_starts.astype(np.int64)\n\n# also add edge thresholds:\n# - for ge: thr just above max => k=0\n# - for gt: thr below min => k=N\nthr_ge_edge = np.nextafter(np.float32(grp_vals[0]), np.float32(1.0))  # >max (if max<1)\nthr_gt_edge = np.float32(0.0)  # <=0 means almost all > thr depending (gt uses >)\n\n# build table function from k and thr\ndef _table_from_k(thr_vals, k_vals, rule):\n    k = np.clip(k_vals, 0, N).astype(np.int64)\n    tp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\n    fp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\n    fn = (pos_total - tp).astype(np.int64)\n    tn = (neg_total - fp).astype(np.int64)\n\n    f1, f05, f2, prec, rec, acc, bacc, mcc = _metrics_from_counts(tp, fp, fn, tn)\n    pos_pred = k.astype(np.int64)\n    gap = np.abs(pos_pred - pos_total).astype(np.int64)\n\n    return pd.DataFrame({\n        \"thr\": thr_vals.astype(np.float32),\n        \"rule\": rule,\n        \"f1\": f1.astype(np.float32),\n        \"f05\": f05.astype(np.float32),\n        \"f2\": f2.astype(np.float32),\n        \"precision\": prec.astype(np.float32),\n        \"recall\": rec.astype(np.float32),\n        \"accuracy\": acc.astype(np.float32),\n        \"balanced_accuracy\": bacc.astype(np.float32),\n        \"mcc\": mcc.astype(np.float32),\n        \"tp\": tp.astype(np.int64),\n        \"fp\": fp.astype(np.int64),\n        \"fn\": fn.astype(np.int64),\n        \"tn\": tn.astype(np.int64),\n        \"pos_pred\": pos_pred.astype(np.int64),\n        \"pos_pred_gap\": gap.astype(np.int64),\n    })\n\n# base tables (unique)\nthr_table_ge = _table_from_k(grp_vals, k_ge, \"ge\")\nthr_table_gt = _table_from_k(grp_vals, k_gt, \"gt\")\n\n# add edge rows\n# ge edge: thr > max => k=0 (if thr_ge_edge==max when max==1, still ok)\nedge_ge = _table_from_k(np.array([thr_ge_edge], np.float32), np.array([0], np.int64), \"ge\")\nthr_table_ge = pd.concat([edge_ge, thr_table_ge], axis=0, ignore_index=True)\n\n# gt edge: thr < min => k=N (use thr=-eps as 0 with >0 rule; safest use thr=-1e-7 then clip? keep 0 with separate row)\n# better: use thr = np.nextafter(0, -1) to include probs==0 in >thr; but keep in [0,1] by storing 0 and k computed separately.\nedge_gt = _table_from_k(np.array([np.float32(0.0)], np.float32), np.array([N], np.int64), \"gt\")\nthr_table_gt = pd.concat([thr_table_gt, edge_gt], axis=0, ignore_index=True)\n\n# de-dup thresholds per rule (keep best by pos_pred count uniqueness)\nthr_table_ge = thr_table_ge.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\nthr_table_gt = thr_table_gt.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\n\n# ----------------------------\n# 3) Add extra candidate thresholds (prevalence & Stage8 best)\n# ----------------------------\n# prevalence-match (roughly pos_pred ~ pos for ge)\nif pos_total > 0:\n    thr_prev = float(p_sorted[min(pos_total - 1, N - 1)])\nelse:\n    thr_prev = 1.0\n\nextra_thr = [0.5, thr_prev, 0.0, 1.0]\nif \"OOF_BEST_THR_F1\" in globals() and globals()[\"OOF_BEST_THR_F1\"] is not None:\n    try:\n        extra_thr.append(float(globals()[\"OOF_BEST_THR_F1\"]))\n    except Exception:\n        pass\n\ndef _eval_specific(thr, rule):\n    thr = float(thr)\n    # use sorted probs and rule to find k\n    if rule == \"ge\":\n        # count(prob >= thr)\n        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"right\"))\n    else:\n        # count(prob > thr)\n        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"left\"))\n    k0 = max(0, min(k0, N))\n\n    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n    fn0 = int(pos_total - tp0)\n    tn0 = int(neg_total - fp0)\n\n    prec0 = tp0 / max(tp0 + fp0, 1)\n    rec0  = tp0 / max(tp0 + fn0, 1)\n    f10 = 0.0 if (tp0 == 0 or (prec0 + rec0) == 0) else (2 * prec0 * rec0 / (prec0 + rec0))\n    f05 = 0.0 if (prec0 == 0 and rec0 == 0) else ((1.25 * prec0 * rec0) / max(0.25 * prec0 + rec0, 1e-12))\n    f2  = 0.0 if (prec0 == 0 and rec0 == 0) else ((5.00 * prec0 * rec0) / max(4.00 * prec0 + rec0, 1e-12))\n    acc0 = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n    gap0 = abs(k0 - pos_total)\n\n    return {\n        \"thr\": thr, \"rule\": rule,\n        \"f1\": float(f10), \"f05\": float(f05), \"f2\": float(f2),\n        \"precision\": float(prec0), \"recall\": float(rec0),\n        \"accuracy\": float(acc0), \"balanced_accuracy\": float(bacc0), \"mcc\": float(mcc0),\n        \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0,\n        \"pos_pred\": int(k0), \"pos_pred_gap\": int(gap0),\n    }\n\nextra_rows = []\nfor t in extra_thr:\n    extra_rows.append(_eval_specific(t, \"ge\"))\n    extra_rows.append(_eval_specific(t, \"gt\"))\n\nextra_df = pd.DataFrame(extra_rows)\n# merge (prefer keeping best metric rows if duplicates)\nthr_table_ge = pd.concat([thr_table_ge, extra_df[extra_df[\"rule\"]==\"ge\"]], ignore_index=True)\nthr_table_gt = pd.concat([thr_table_gt, extra_df[extra_df[\"rule\"]==\"gt\"]], ignore_index=True)\nthr_table_ge = thr_table_ge.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\nthr_table_gt = thr_table_gt.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\n\n# ----------------------------\n# 4) Best pickers + constraints\n# ----------------------------\ndef _pick_best(df, primary, tie_cols=None):\n    # primary desc, tie cols desc, last: pos_pred_gap asc (prefer prevalence match)\n    tie_cols = tie_cols or []\n    sort_cols = [primary] + tie_cols + [\"pos_pred_gap\"]\n    asc = [False] * (1 + len(tie_cols)) + [True]\n    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n\ndef _pick_best_constrained(df, primary, min_precision=None, min_recall=None):\n    dd = df.copy()\n    if min_precision is not None:\n        dd = dd[dd[\"precision\"] >= float(min_precision)]\n    if min_recall is not None:\n        dd = dd[dd[\"recall\"] >= float(min_recall)]\n    if len(dd) == 0:\n        return None\n    return _pick_best(dd, primary, tie_cols=[\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\n\n# baseline\nbase05_ge = _eval_specific(0.5, \"ge\")\nbase05_gt = _eval_specific(0.5, \"gt\")\n\n# best per rule\nbest_ge_f1   = _pick_best(thr_table_ge, \"f1\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\nbest_ge_f05  = _pick_best(thr_table_ge, \"f05\", [\"mcc\",\"balanced_accuracy\",\"precision\",\"recall\"])\nbest_ge_f2   = _pick_best(thr_table_ge, \"f2\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\nbest_ge_mcc  = _pick_best(thr_table_ge, \"mcc\", [\"f1\",\"balanced_accuracy\",\"accuracy\"])\nbest_ge_bacc = _pick_best(thr_table_ge, \"balanced_accuracy\", [\"mcc\",\"accuracy\",\"f1\"])\n\nbest_gt_f1   = _pick_best(thr_table_gt, \"f1\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\nbest_gt_f05  = _pick_best(thr_table_gt, \"f05\", [\"mcc\",\"balanced_accuracy\",\"precision\",\"recall\"])\nbest_gt_f2   = _pick_best(thr_table_gt, \"f2\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\nbest_gt_mcc  = _pick_best(thr_table_gt, \"mcc\", [\"f1\",\"balanced_accuracy\",\"accuracy\"])\nbest_gt_bacc = _pick_best(thr_table_gt, \"balanced_accuracy\", [\"mcc\",\"accuracy\",\"f1\"])\n\n# optional constraint examples (edit if you want)\nMIN_PREC = None   # e.g. 0.80\nMIN_REC  = None   # e.g. 0.30\nbest_ge_f1_con = _pick_best_constrained(thr_table_ge, \"f1\", min_precision=MIN_PREC, min_recall=MIN_REC)\n\n# gt -> ge equivalent threshold (so downstream can still do prob >= thr)\ndef _gt_to_ge_equiv(thr_gt):\n    thr_gt = float(thr_gt)\n    thr_ge = float(np.nextafter(np.float32(thr_gt), np.float32(1.0)))\n    return float(min(max(thr_ge, 0.0), 1.0))\n\nBEST_THR_GE_F1   = float(best_ge_f1[\"thr\"])\nBEST_THR_GE_F05  = float(best_ge_f05[\"thr\"])\nBEST_THR_GE_F2   = float(best_ge_f2[\"thr\"])\nBEST_THR_GE_MCC  = float(best_ge_mcc[\"thr\"])\nBEST_THR_GE_BACC = float(best_ge_bacc[\"thr\"])\n\nBEST_THR_GT_F1   = float(best_gt_f1[\"thr\"])\nBEST_THR_GT_F05  = float(best_gt_f05[\"thr\"])\nBEST_THR_GT_F2   = float(best_gt_f2[\"thr\"])\nBEST_THR_GT_MCC  = float(best_gt_mcc[\"thr\"])\nBEST_THR_GT_BACC = float(best_gt_bacc[\"thr\"])\n\nBEST_THR_GT_F1_GE_EQUIV   = _gt_to_ge_equiv(BEST_THR_GT_F1)\nBEST_THR_GT_MCC_GE_EQUIV  = _gt_to_ge_equiv(BEST_THR_GT_MCC)\nBEST_THR_GT_BACC_GE_EQUIV = _gt_to_ge_equiv(BEST_THR_GT_BACC)\n\n# default threshold choice (keep F1 + GE)\nBEST_THR = BEST_THR_GE_F1\n\n# ----------------------------\n# 5) Save artifacts\n# ----------------------------\nthr_table_all = pd.concat([thr_table_ge, thr_table_gt], axis=0, ignore_index=True)\n\n# save top1000 by F1 primary\ntop1000 = thr_table_all.sort_values(\n    [\"f1\",\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\",\"pos_pred_gap\"],\n    ascending=[False, False, False, False, False, True]\n).head(1000).reset_index(drop=True)\n\nout_json = OOF_DIR / \"threshold_tuning.json\"\nout_txt  = OOF_DIR / \"threshold_report.txt\"\nout_csv  = OOF_DIR / \"threshold_table_top1000.csv\"\ntop1000.to_csv(out_csv, index=False)\n\npayload = {\n    \"version\": \"v4.4\",\n    \"source\": src,\n    \"target_col\": TARGET_COL,\n    \"n_total_rows\": int(N_all),\n    \"n_valid_rows\": int(N),\n    \"pos_valid\": int(pos),\n    \"neg_valid\": int(neg),\n    \"pos_rate_valid\": float(pos / max(N, 1)),\n    \"oof_auc_valid_only\": float(auc_oof),\n    \"oof_ap_valid_only\": float(ap_oof),\n    \"prevalence_match_thr_ge\": float(thr_prev),\n    \"baseline_thr_0p5\": {\"ge\": base05_ge, \"gt\": base05_gt},\n    \"best_ge\": {\n        \"best_thr_f1\":   best_ge_f1.to_dict(),\n        \"best_thr_f05\":  best_ge_f05.to_dict(),\n        \"best_thr_f2\":   best_ge_f2.to_dict(),\n        \"best_thr_mcc\":  best_ge_mcc.to_dict(),\n        \"best_thr_bacc\": best_ge_bacc.to_dict(),\n        \"best_thr_f1_constrained\": (None if best_ge_f1_con is None else best_ge_f1_con.to_dict()),\n        \"constraints\": {\"min_precision\": MIN_PREC, \"min_recall\": MIN_REC},\n    },\n    \"best_gt\": {\n        \"best_thr_f1\":   best_gt_f1.to_dict(),\n        \"best_thr_f05\":  best_gt_f05.to_dict(),\n        \"best_thr_f2\":   best_gt_f2.to_dict(),\n        \"best_thr_mcc\":  best_gt_mcc.to_dict(),\n        \"best_thr_bacc\": best_gt_bacc.to_dict(),\n        \"ge_equiv_for_downstream_using_ge\": {\n            \"f1\": float(BEST_THR_GT_F1_GE_EQUIV),\n            \"mcc\": float(BEST_THR_GT_MCC_GE_EQUIV),\n            \"bacc\": float(BEST_THR_GT_BACC_GE_EQUIV),\n        },\n    },\n    \"default_best_thr\": {\"metric\": \"f1\", \"rule\": \"ge\", \"thr\": float(BEST_THR)},\n}\n\nwith open(out_json, \"w\", encoding=\"utf-8\") as f:\n    json.dump(payload, f, indent=2)\n\ndef _fmt_row(d):\n    return (f\"thr={d['thr']:.6f} | F1={d['f1']:.6f} | F0.5={d['f05']:.6f} | F2={d['f2']:.6f} | \"\n            f\"P={d['precision']:.6f} R={d['recall']:.6f} | BACC={d['balanced_accuracy']:.6f} | MCC={d['mcc']:.6f} | \"\n            f\"pos_pred={int(d['pos_pred'])} (gap={int(d['pos_pred_gap'])})\")\n\nlines = []\nlines.append(\"OOF Threshold Tuning Report (v4.4)\")\nlines.append(f\"- source={src}\")\nlines.append(f\"- target_col={TARGET_COL}\")\nlines.append(f\"- total_rows={N_all} | valid_rows={N} | pos_valid={pos} | neg_valid={neg} | pos%={pos/max(N,1)*100:.6f}%\")\nlines.append(f\"- OOF AUC (valid-only) = {auc_oof:.6f}\")\nlines.append(f\"- OOF AP  (valid-only) = {ap_oof:.6f}\")\nlines.append(f\"- prevalence-match thr (ge) ~ {thr_prev:.6f}\")\nlines.append(\"\")\nlines.append(\"Baseline @ thr=0.5\")\nlines.append(f\"- rule=ge: {_fmt_row(base05_ge)}\")\nlines.append(f\"- rule=gt: {_fmt_row(base05_gt)}\")\nlines.append(\"\")\nlines.append(\"BEST (rule=ge)  [downstream default: pred = prob >= thr]\")\nlines.append(f\"- BEST-F1   : {_fmt_row(best_ge_f1.to_dict())}\")\nlines.append(f\"- BEST-F0.5 : {_fmt_row(best_ge_f05.to_dict())}\")\nlines.append(f\"- BEST-F2   : {_fmt_row(best_ge_f2.to_dict())}\")\nlines.append(f\"- BEST-MCC  : {_fmt_row(best_ge_mcc.to_dict())}\")\nlines.append(f\"- BEST-BACC : {_fmt_row(best_ge_bacc.to_dict())}\")\nif best_ge_f1_con is not None:\n    lines.append(\"\")\n    lines.append(f\"BEST-F1 constrained (minP={MIN_PREC}, minR={MIN_REC}): {_fmt_row(best_ge_f1_con.to_dict())}\")\nlines.append(\"\")\nlines.append(\"BEST (rule=gt)  [strict '>' boundary]\")\nlines.append(f\"- BEST-F1   : {_fmt_row(best_gt_f1.to_dict())} | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.6f}\")\nlines.append(f\"- BEST-MCC  : {_fmt_row(best_gt_mcc.to_dict())} | ge_equiv={BEST_THR_GT_MCC_GE_EQUIV:.6f}\")\nlines.append(f\"- BEST-BACC : {_fmt_row(best_gt_bacc.to_dict())} | ge_equiv={BEST_THR_GT_BACC_GE_EQUIV:.6f}\")\nlines.append(\"\")\nlines.append(\"Top 12 (by F1) overall:\")\nfor i in range(min(12, len(top1000))):\n    r = top1000.iloc[i].to_dict()\n    lines.append(f\"{i+1:02d}. rule={r['rule']} | {_fmt_row(r)}\")\n\nwith open(out_txt, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\nprint(\"[Stage 9] DONE\")\nprint(f\"- Saved: {out_json}\")\nprint(f\"- Saved: {out_txt}\")\nprint(f\"- Saved: {out_csv}\")\nprint(f\"- OOF AUC (valid-only): {auc_oof:.6f} | AP: {ap_oof:.6f}\")\nprint(f\"- DEFAULT BEST_THR (ge/F1) = {BEST_THR:.6f} | F1={float(best_ge_f1['f1']):.6f} (P={float(best_ge_f1['precision']):.6f} R={float(best_ge_f1['recall']):.6f})\")\nprint(f\"- BEST_THR_GE_MCC          = {BEST_THR_GE_MCC:.6f} | MCC={float(best_ge_mcc['mcc']):.6f}\")\nprint(f\"- BEST_THR_GE_BACC         = {BEST_THR_GE_BACC:.6f} | BACC={float(best_ge_bacc['balanced_accuracy']):.6f}\")\nprint(f\"- BEST_THR_GT_F1           = {BEST_THR_GT_F1:.6f} (gt) | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.6f}\")\n\nglobals().update({\n    \"train_ids_oof_all\": train_ids,\n    \"train_ids_oof_valid\": train_ids_v,\n    \"oof_prob_all\": oof_prob,\n    \"oof_prob_valid\": p_v,\n    \"y_oof_valid\": y_v,\n\n    \"BEST_THR\": float(BEST_THR),                 # default: rule=ge, metric=F1\n    \"BEST_THR_GE_F1\": float(BEST_THR_GE_F1),\n    \"BEST_THR_GE_F05\": float(BEST_THR_GE_F05),\n    \"BEST_THR_GE_F2\": float(BEST_THR_GE_F2),\n    \"BEST_THR_GE_MCC\": float(BEST_THR_GE_MCC),\n    \"BEST_THR_GE_BACC\": float(BEST_THR_GE_BACC),\n\n    \"BEST_THR_GT_F1\": float(BEST_THR_GT_F1),\n    \"BEST_THR_GT_F05\": float(BEST_THR_GT_F05),\n    \"BEST_THR_GT_F2\": float(BEST_THR_GT_F2),\n    \"BEST_THR_GT_MCC\": float(BEST_THR_GT_MCC),\n    \"BEST_THR_GT_BACC\": float(BEST_THR_GT_BACC),\n\n    # use these if downstream always uses >= but you want strict \">\"\n    \"BEST_THR_GT_F1_GE_EQUIV\": float(BEST_THR_GT_F1_GE_EQUIV),\n    \"BEST_THR_GT_MCC_GE_EQUIV\": float(BEST_THR_GT_MCC_GE_EQUIV),\n    \"BEST_THR_GT_BACC_GE_EQUIV\": float(BEST_THR_GT_BACC_GE_EQUIV),\n\n    \"thr_table_ge\": thr_table_ge,\n    \"thr_table_gt\": thr_table_gt,\n    \"thr_table_top1000\": top1000,\n    \"THR_JSON_PATH\": out_json,\n    \"THR_REPORT_PATH\": out_txt,\n    \"THR_TABLE_CSV_PATH\": out_csv,\n    \"OOF_AUC_VALID_ONLY\": float(auc_oof),\n    \"OOF_AP_VALID_ONLY\": float(ap_oof),\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Inference (Fold Ensemble)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL)\n# REVISI FULL v4.5 (TEST G-FEAT CACHE + STRICT GDIM + BAND RANGE CHECK + EMA INFER + DEBUG EXPORTS)\n#\n# Upgrades v4.5:\n# - Cache TEST global features: FIX_DIR/global_features_test_raw.npy (+ meta json)\n# - Strict g_dim: if computed G_raw < g_dim expected by ckpt => raise (avoid silent padding)\n# - Band range sanity check for SHIFT_BAND_IDS & n_bands (fail-fast on mismatch)\n# - EMA inference: apply EMA shadow safely (shape match only) + report hits\n# - Optional debug exports: test_prob_folds.csv\n# ============================================================\n\nimport os, gc, json, re, math, time, warnings, hashlib\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True.*\")\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\"ART_DIR\",\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_test_meta\",\"CKPT_DIR\",\"n_splits\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n\n# Torch\ntry:\n    import torch\n    import torch.nn as nn\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\ndevice = torch.device(\"cpu\")\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# Thread guard (CPU)\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\nFIX_DIR = Path(globals()[\"FIX_DIR\"])\nART_DIR = Path(globals()[\"ART_DIR\"]); ART_DIR.mkdir(parents=True, exist_ok=True)\nCKPT_DIR = Path(globals()[\"CKPT_DIR\"])\n\nOUT_DIR = ART_DIR / \"preds\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Settings\n# ----------------------------\nUSE_EMA_WEIGHTS_FOR_INFER = True      # use EMA if ckpt has ema_shadow\nEXPORT_TEST_PRED_01 = True            # export 0/1 csv using BEST_THR if available\nDEFAULT_THR_IF_MISSING = 0.5\nEXPORT_TEST_PROB_FOLDS_CSV = False    # debug: wide CSV per fold\nSTRICT_GDIM = True                   # if computed G_raw < g_dim expected -> raise\n\n# ----------------------------\n# helper: normalize id robustly\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_, bytearray)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _load_ids_npy(path: Path):\n    arr = np.load(path, allow_pickle=False)\n    xs = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n    return [_norm_id(z) for z in xs]\n\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\n# ----------------------------\n# 0b) Read Stage6 policy (SHIFT_BAND_IDS, PAD_BAND_ID, dtype_X, token/value hints)\n# ----------------------------\nSHIFT_BAND_IDS = False\nPAD_BAND_ID = 0\nDTYPE_X_MEMMAP = np.float32\nPOL_TOKEN_MODE = None\nPOL_SCORE_VALUE_FEAT = None\n\npolicy_path = FIX_DIR / \"length_policy_config.json\"\nif policy_path.exists():\n    try:\n        pol = json.loads(policy_path.read_text())\n        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n        dt = str(pol.get(\"dtype_X\", \"float32\")).lower()\n        DTYPE_X_MEMMAP = np.float16 if ((\"float16\" in dt) or (\"fp16\" in dt)) else np.float32\n        POL_TOKEN_MODE = pol.get(\"token_mode\", None)\n        POL_SCORE_VALUE_FEAT = pol.get(\"score_value_feat\", None)\n    except Exception:\n        pass\n\n# ----------------------------\n# 1) Load TEST ordering (must match STAGE 6)\n# ----------------------------\ntest_ids_path = FIX_DIR / \"test_ids.npy\"\nif not test_ids_path.exists():\n    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n\ntest_ids = _load_ids_npy(test_ids_path)\nNTE = len(test_ids)\nif NTE <= 0:\n    raise RuntimeError(\"test_ids kosong (NTE=0). Pastikan STAGE 6 sukses membuat test_ids.npy.\")\n\n# Align df_test_meta.index via string-map (HARD)\ndf_test_meta = globals()[\"df_test_meta\"].copy(deep=False)\ndf_test_meta.index = pd.Index([_norm_id(z) for z in df_test_meta.index.tolist()], name=df_test_meta.index.name)\n\npos_idx = df_test_meta.index.get_indexer(test_ids)\nif (pos_idx < 0).any():\n    bad = [test_ids[i] for i in np.where(pos_idx < 0)[0][:10]]\n    raise KeyError(f\"Some test_ids not found in df_test_meta.index. ex={bad} | missing_n={int((pos_idx<0).sum())}\")\npos_idx = pos_idx.astype(np.int32)\n\nif len(set(test_ids)) != len(test_ids):\n    s = pd.Series(test_ids)\n    dup = s[s.duplicated()].head(10).tolist()\n    raise ValueError(f\"Duplicate object_id in test_ids ordering (examples): {dup}\")\n\n# ----------------------------\n# 2) Open fixed-length TEST memmaps (dtype from Stage6 policy)\n# ----------------------------\nSEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\nFdim = len(SEQ_FEATURE_NAMES)\nL = int(globals()[\"MAX_LEN\"])\n\ntest_X_path = FIX_DIR / \"test_X.dat\"\ntest_B_path = FIX_DIR / \"test_B.dat\"\ntest_M_path = FIX_DIR / \"test_M.dat\"\nfor p in [test_X_path, test_B_path, test_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\nXte = np.memmap(test_X_path, dtype=DTYPE_X_MEMMAP, mode=\"r\", shape=(NTE, L, Fdim))\nBte = np.memmap(test_B_path, dtype=np.int8,        mode=\"r\", shape=(NTE, L))\nMte = np.memmap(test_M_path, dtype=np.int8,        mode=\"r\", shape=(NTE, L))\n\nfeat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\nfor k in [\"snr_tanh\",\"detected\"]:\n    if k not in feat:\n        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n\n# ----------------------------\n# 3) Checkpoints (fold_*.pt)\n# ----------------------------\nn_splits = int(globals()[\"n_splits\"])\nckpts = []\nfor f in range(n_splits):\n    p = CKPT_DIR / f\"fold_{f}.pt\"\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n    ckpts.append(p)\n\n# ----------------------------\n# 4) Safe/compat checkpoint loader\n# ----------------------------\ndef torch_load_compat(path: Path):\n    try:\n        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n        if isinstance(obj, dict) and (\"model_state\" in obj or \"global_scaler\" in obj or \"cfg\" in obj):\n            return obj\n        return torch.load(path, map_location=\"cpu\", weights_only=False)\n    except TypeError:\n        return torch.load(path, map_location=\"cpu\")\n    except Exception:\n        return torch.load(path, map_location=\"cpu\", weights_only=False)\n\ndef extract_state_and_meta(ckpt_obj):\n    if isinstance(ckpt_obj, dict) and \"model_state\" in ckpt_obj and isinstance(ckpt_obj[\"model_state\"], dict):\n        return ckpt_obj[\"model_state\"], ckpt_obj\n    if isinstance(ckpt_obj, dict):\n        any_tensor = any(torch.is_tensor(v) for v in ckpt_obj.values())\n        if any_tensor:\n            return ckpt_obj, {}\n        return ckpt_obj, ckpt_obj\n    raise RuntimeError(f\"Unsupported ckpt object type: {type(ckpt_obj)}\")\n\n# ----------------------------\n# 5) Infer architecture from state_dict\n# ----------------------------\ndef infer_from_state(sd: dict):\n    keys = set(sd.keys())\n\n    if \"band_emb.weight\" not in sd:\n        raise RuntimeError(\"state_dict missing band_emb.weight.\")\n    n_bands = int(sd[\"band_emb.weight\"].shape[0])\n    d_model = int(sd[\"band_emb.weight\"].shape[1])\n\n    if \"pos_emb\" not in sd:\n        raise RuntimeError(\"state_dict missing pos_emb.\")\n    max_len_ckpt = int(sd[\"pos_emb\"].shape[1])\n\n    if \"x_proj.0.weight\" in keys:\n        feat_dim = int(sd[\"x_proj.0.weight\"].shape[1])\n    elif \"x_proj.weight\" in keys:\n        feat_dim = int(sd[\"x_proj.weight\"].shape[1])\n    else:\n        raise RuntimeError(\"state_dict missing x_proj.*.weight\")\n\n    if \"g_proj.0.weight\" in keys:\n        g_dim = int(sd[\"g_proj.0.weight\"].shape[1])\n        g_hidden = int(sd[\"g_proj.0.weight\"].shape[0])\n    else:\n        g_dim = 0\n        g_hidden = 0\n\n    layer_ids = set()\n    for k in keys:\n        m = re.match(r\"encoder\\.layers\\.(\\d+)\\.\", k)\n        if m:\n            layer_ids.add(int(m.group(1)))\n    n_layers = (max(layer_ids) + 1) if layer_ids else 0\n    if n_layers <= 0:\n        raise RuntimeError(\"Cannot infer n_layers (encoder.layers.* not found).\")\n\n    k_lin1 = \"encoder.layers.0.linear1.weight\"\n    if k_lin1 in sd:\n        dim_ff = int(sd[k_lin1].shape[0])\n    else:\n        lin1_keys = [k for k in keys if k.endswith(\"linear1.weight\")]\n        if not lin1_keys:\n            raise RuntimeError(\"Cannot infer dim_feedforward (linear1.weight not found).\")\n        dim_ff = int(sd[sorted(lin1_keys)[0]].shape[0])\n\n    has_pool_ln = (\"pool_ln.weight\" in keys and \"pool_ln.bias\" in keys)\n\n    head_w_idx = []\n    for k in keys:\n        m = re.match(r\"head\\.(\\d+)\\.weight\", k)\n        if m:\n            head_w_idx.append(int(m.group(1)))\n    if not head_w_idx:\n        raise RuntimeError(\"Cannot infer head structure (head.*.weight not found).\")\n    head_final_idx = max(sorted(set(head_w_idx)))\n\n    return {\n        \"n_bands\": n_bands,\n        \"d_model\": d_model,\n        \"max_len_ckpt\": max_len_ckpt,\n        \"feat_dim\": feat_dim,\n        \"g_dim\": g_dim,\n        \"g_hidden\": g_hidden,\n        \"n_layers\": n_layers,\n        \"dim_ff\": dim_ff,\n        \"has_pool_ln\": has_pool_ln,\n        \"head_final_idx\": head_final_idx,\n    }\n\n# ----------------------------\n# 6) Resolve token/value feature from CKPT meta + fallback\n# ----------------------------\ndef _pick_val_feat(feat_map, prefer=None):\n    cand = []\n    if prefer is not None:\n        cand.append(str(prefer))\n    cand += [\n        \"signal\", \"value\", \"flux\",\n        \"flux_asinh\", \"flux_asinh_clip\", \"flux_asinh_norm\", \"flux_asinh_scaled\",\n        \"mag\", \"mag_norm\", \"mag_clip\", \"mag_scaled\",\n        \"delta_signal\", \"delta_flux\", \"signal_clip\", \"signal_norm\",\n    ]\n    for c in cand:\n        if c and (c in feat_map):\n            return c\n    keys = list(feat_map.keys())\n    fuzzy = [k for k in keys if any(t in k for t in [\"signal\", \"flux\", \"value\", \"asinh\", \"mag\"])]\n    if fuzzy:\n        return sorted(fuzzy)[0]\n    return None\n\nfirst_obj = torch_load_compat(ckpts[0])\n_, first_meta = extract_state_and_meta(first_obj)\n\nCKPT_TOKEN_MODE = None\nCKPT_VAL_FEAT = None\nCKPT_VAL_IS_MAG = None\nCKPT_SHIFT_BAND_IDS = None\nCKPT_META_COLS = None\nCKPT_GCACHE = None\n\nif isinstance(first_meta, dict):\n    CKPT_TOKEN_MODE = first_meta.get(\"token_mode\", None)\n    CKPT_VAL_FEAT = first_meta.get(\"val_feat\", None)\n    CKPT_VAL_IS_MAG = first_meta.get(\"val_is_mag\", None)\n    CKPT_SHIFT_BAND_IDS = first_meta.get(\"shift_band_ids_from_stage6\", None)\n    CKPT_META_COLS = first_meta.get(\"global_meta_cols\", None)\n    CKPT_GCACHE = first_meta.get(\"global_feature_cache\", None)\n\n# HARD guard: SHIFT_BAND_IDS must match ckpt if present\nif CKPT_SHIFT_BAND_IDS is not None and bool(CKPT_SHIFT_BAND_IDS) != bool(SHIFT_BAND_IDS):\n    raise RuntimeError(\n        \"[Stage 10] SHIFT_BAND_IDS mismatch between Stage6 policy and ckpt meta.\\n\"\n        f\"- Stage6 policy SHIFT_BAND_IDS={SHIFT_BAND_IDS}\\n\"\n        f\"- ckpt meta shift_band_ids_from_stage6={CKPT_SHIFT_BAND_IDS}\\n\"\n        \"Solusi: inference harus pakai FIX_DIR yang sama dengan training ckpt.\"\n    )\n\nSEQ_TOKEN_MODE = CKPT_TOKEN_MODE if CKPT_TOKEN_MODE is not None else POL_TOKEN_MODE\nif SEQ_TOKEN_MODE is None:\n    SEQ_TOKEN_MODE = \"mag\" if any(k.startswith(\"mag\") for k in feat.keys()) else \"generic\"\nSEQ_TOKEN_MODE = str(SEQ_TOKEN_MODE).lower().strip()\n\nVAL_FEAT = _pick_val_feat(feat, prefer=(CKPT_VAL_FEAT or POL_SCORE_VALUE_FEAT))\nif VAL_FEAT is None:\n    raise RuntimeError(\"Cannot resolve VAL_FEAT from SEQ_FEATURE_NAMES.\")\nVAL_IS_MAG = bool(CKPT_VAL_IS_MAG) if CKPT_VAL_IS_MAG is not None else (SEQ_TOKEN_MODE == \"mag\" and \"mag\" in VAL_FEAT)\n\n# ----------------------------\n# 7) Band range sanity (fail-fast)\n# ----------------------------\ndef _band_sanity_check(Bmm, Mmm, n_bands, shift_flag, sample_n=512):\n    s = min(int(sample_n), int(Bmm.shape[0]))\n    if s <= 0:\n        return\n    Bc = np.asarray(Bmm[:s])\n    Mc = np.asarray(Mmm[:s])\n    real = (Mc == 1)\n    if not real.any():\n        return\n    br = Bc[real].astype(np.int64, copy=False)\n    bmin = int(br.min())\n    bmax = int(br.max())\n\n    # accepted sets:\n    # - if shift_flag True: real bands expected in [1..n_bands] (pad=0)\n    # - if shift_flag False: real bands expected in [0..n_bands-1]\n    if shift_flag:\n        ok = (bmin >= 0) and (bmax <= n_bands)  # allow 0 if some weirdness, but real typically 1..n_bands\n        if not ok:\n            raise RuntimeError(\n                \"[Stage 10] Band-id range looks incompatible with SHIFT_BAND_IDS=True.\\n\"\n                f\"- observed real band min={bmin} max={bmax}\\n\"\n                f\"- expected roughly 1..{n_bands} (pad=0)\\n\"\n                \"Solusi: pastikan test_B.dat dibuat dengan Stage6 policy yang sama seperti training.\"\n            )\n    else:\n        ok = (bmin >= 0) and (bmax <= (n_bands - 1))\n        if not ok:\n            raise RuntimeError(\n                \"[Stage 10] Band-id range looks incompatible with SHIFT_BAND_IDS=False.\\n\"\n                f\"- observed real band min={bmin} max={bmax}\\n\"\n                f\"- expected 0..{n_bands-1}\\n\"\n                \"Solusi: kemungkinan Stage6 menghasilkan band_id 1..n_bands tapi SHIFT_BAND_IDS tidak diset.\\n\"\n                \"Rebuild Stage6 / gunakan FIX_DIR yang benar.\"\n            )\n\n# we'll run sanity after we know n_bands from ckpt arch below\n\n# ----------------------------\n# 8) Determine META_COLS and whether need agg\n# ----------------------------\nDEFAULT_META_COLS = [\"Z\",\"Z_err\",\"EBV_used\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\nMETA_COLS = DEFAULT_META_COLS\nif isinstance(CKPT_META_COLS, (list, tuple)) and len(CKPT_META_COLS) > 0:\n    META_COLS = [str(c) for c in CKPT_META_COLS]\n\n# infer all folds arch + decide need_agg\nfold_arch = []\nfold_meta_summary = []\nneed_agg = False\narch_used = None\n\nfor fold, p in enumerate(ckpts):\n    obj = torch_load_compat(p)\n    sd, meta = extract_state_and_meta(obj)\n    arch = infer_from_state(sd)\n    fold_arch.append(arch)\n    if arch_used is None:\n        arch_used = dict(arch)\n\n    if int(arch.get(\"g_dim\", 0)) > len(META_COLS):\n        need_agg = True\n    if isinstance(meta, dict) and bool(meta.get(\"use_agg_seq_features\", False)):\n        need_agg = True\n\n    # consistency guard across folds\n    if isinstance(meta, dict):\n        vf = meta.get(\"val_feat\", None)\n        vim = meta.get(\"val_is_mag\", None)\n        sb = meta.get(\"shift_band_ids_from_stage6\", None)\n        if vf is not None and str(vf) != str(VAL_FEAT):\n            raise RuntimeError(f\"[Stage 10] Fold {fold}: val_feat mismatch. fold={vf} vs resolved={VAL_FEAT}\")\n        if vim is not None and bool(vim) != bool(VAL_IS_MAG):\n            raise RuntimeError(f\"[Stage 10] Fold {fold}: val_is_mag mismatch. fold={vim} vs resolved={VAL_IS_MAG}\")\n        if sb is not None and bool(sb) != bool(SHIFT_BAND_IDS):\n            raise RuntimeError(f\"[Stage 10] Fold {fold}: SHIFT_BAND_IDS mismatch. fold={sb} vs policy={SHIFT_BAND_IDS}\")\n\n    fold_meta_summary.append({\n        \"fold\": fold,\n        \"g_dim\": int(arch.get(\"g_dim\", 0)),\n        \"d_model\": int(arch.get(\"d_model\", 0)),\n        \"n_layers\": int(arch.get(\"n_layers\", 0)),\n    })\n\n# now we can sanity-check band range using ckpt n_bands\nN_BANDS = int(arch_used[\"n_bands\"])\n_band_sanity_check(Bte, Mte, n_bands=N_BANDS, shift_flag=SHIFT_BAND_IDS, sample_n=512)\n\n# ----------------------------\n# 9) Build TEST global features (meta + optional agg seq feats) + CACHE\n# ----------------------------\ndef _hash_cfg(d: dict) -> str:\n    s = json.dumps(d, sort_keys=True, ensure_ascii=True)\n    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:12]\n\ndef _prepare_meta_cols(df):\n    df = df.copy(deep=False)\n\n    if \"EBV_used\" in META_COLS and \"EBV_used\" not in df.columns:\n        if (\"EBV_clip\" in df.columns):\n            df[\"EBV_used\"] = df[\"EBV_clip\"]\n        elif \"EBV\" in df.columns:\n            df[\"EBV_used\"] = df[\"EBV\"]\n        else:\n            df[\"EBV_used\"] = 0.0\n\n    for c in META_COLS:\n        if c not in df.columns:\n            df[c] = 0.0\n\n    Gm = df.iloc[pos_idx][META_COLS].copy()\n    for c in META_COLS:\n        Gm[c] = pd.to_numeric(Gm[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n    return Gm.to_numpy(dtype=np.float32, copy=False)\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1.0)\n\ndef build_agg_seq_features_memmap(Xmm, Bmm, Mmm, chunk=512):\n    snr_i = feat[\"snr_tanh\"]\n    det_i = feat[\"detected\"]\n    val_i = feat[VAL_FEAT]\n\n    # dim = glob(4) + global_val_feats(3) + per_band(N_BANDS*4)\n    agg_dim = 4 + 3 + (N_BANDS * 4)\n    out = np.zeros((NTE, agg_dim), dtype=np.float32)\n\n    for start in range(0, NTE, int(chunk)):\n        end = min(NTE, start + int(chunk))\n        Xc = np.asarray(Xmm[start:end])  # (B,L,F)\n        Bc = np.asarray(Bmm[start:end])  # (B,L)\n        Mc = np.asarray(Mmm[start:end])  # (B,L)\n\n        real = (Mc == 1)\n\n        # band mapping (same as Stage8 agg builder)\n        if SHIFT_BAND_IDS:\n            Bc2 = Bc.astype(np.int16, copy=True)\n            if real.any():\n                Bc2[real] = np.clip(Bc2[real] - 1, 0, N_BANDS - 1)\n            Bc2[~real] = 0\n            Bc = Bc2.astype(np.int8, copy=False)\n\n        tok_count = real.sum(axis=1).astype(np.float32)\n\n        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32, copy=False)\n        det = (Xc[:, :, det_i] > 0.5).astype(np.float32, copy=False)\n        val = Xc[:, :, val_i].astype(np.float32, copy=False)\n\n        snr_r = snr * real\n        det_r = det * real\n\n        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n        max_abs_snr = np.where(tok_count > 0, snr_r.max(axis=1), 0.0).astype(np.float32)\n\n        if VAL_IS_MAG:\n            val_r = np.where(real, val, np.nan)\n            mean_val = np.nan_to_num(np.nanmean(val_r, axis=1).astype(np.float32), nan=0.0)\n            std_val  = np.nan_to_num(np.nanstd(val_r,  axis=1).astype(np.float32), nan=0.0)\n            min_val  = np.nan_to_num(np.nanmin(val_r,  axis=1).astype(np.float32), nan=0.0)\n            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n        else:\n            aval = np.abs(val).astype(np.float32, copy=False)\n            aval_r = aval * real\n            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n            val_r = np.where(real, val, np.nan)\n            std_val = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32), nan=0.0)\n            max_aval = np.where(tok_count > 0, aval_r.max(axis=1), 0.0).astype(np.float32)\n            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n\n        per_band = []\n        for b in range(N_BANDS):\n            bm = (Bc == b) & real\n            cnt = bm.sum(axis=1).astype(np.float32)\n\n            detb = (det * bm).sum(axis=1).astype(np.float32)\n            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n\n            det_frac_b = _safe_div(detb, cnt)\n            mean_abs_snr_b = _safe_div(snrb, cnt)\n\n            if VAL_IS_MAG:\n                vb = np.where(bm, val, np.nan)\n                mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n            else:\n                ab = (np.abs(val).astype(np.float32, copy=False) * bm).sum(axis=1).astype(np.float32)\n                mean_val_b = _safe_div(ab, cnt).astype(np.float32)\n\n            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n\n        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n\n        out[start:end] = agg\n\n        del Xc, Bc, Mc\n        if (start // int(chunk)) % 4 == 0:\n            gc.collect()\n\n    return out\n\n# --- TEST cache paths ---\nG_TEST_CACHE = FIX_DIR / \"global_features_test_raw.npy\"\nG_TEST_META  = FIX_DIR / \"global_features_test_raw_meta.json\"\n\nagg_spec = {\n    \"NTE\": int(NTE),\n    \"L\": int(L),\n    \"Fdim\": int(Fdim),\n    \"n_bands\": int(N_BANDS),\n    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n    \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n    \"meta_cols\": list(META_COLS),\n    \"val_feat\": str(VAL_FEAT),\n    \"val_is_mag\": bool(VAL_IS_MAG),\n    \"need_agg\": bool(need_agg),\n}\nagg_hash = _hash_cfg(agg_spec)\n\nprint(f\"[Stage 10] token_mode={SEQ_TOKEN_MODE} | VAL_FEAT={VAL_FEAT} | VAL_IS_MAG={VAL_IS_MAG} | X_dtype={DTYPE_X_MEMMAP}\")\nprint(f\"[Stage 10] SHIFT_BAND_IDS={SHIFT_BAND_IDS} | PAD_BAND_ID={PAD_BAND_ID} | N_BANDS={N_BANDS}\")\nprint(f\"[Stage 10] META_COLS={META_COLS} | need_agg={need_agg}\")\nprint(f\"[Stage 10] USE_EMA_WEIGHTS_FOR_INFER={USE_EMA_WEIGHTS_FOR_INFER} | STRICT_GDIM={STRICT_GDIM}\")\n\nG_raw_default = None\nif G_TEST_CACHE.exists() and G_TEST_META.exists():\n    try:\n        old = json.loads(G_TEST_META.read_text())\n        if old.get(\"agg_hash\") == agg_hash and int(old.get(\"NTE\", -1)) == int(NTE):\n            G_raw_default = np.load(G_TEST_CACHE, allow_pickle=False).astype(np.float32, copy=False)\n            if G_raw_default.shape[0] != NTE:\n                G_raw_default = None\n    except Exception:\n        G_raw_default = None\n\nif G_raw_default is None:\n    print(\"[Stage 10] Building TEST global features (then cached)...\")\n    t0 = time.time()\n    G_meta_np = _prepare_meta_cols(df_test_meta)\n\n    if need_agg:\n        G_seq_np = build_agg_seq_features_memmap(Xte, Bte, Mte, chunk=512)\n        G_raw_default = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\n        agg_dim = int(G_seq_np.shape[1])\n    else:\n        G_raw_default = G_meta_np.astype(np.float32, copy=False)\n        agg_dim = 0\n\n    np.save(G_TEST_CACHE, G_raw_default.astype(np.float32, copy=False))\n    G_TEST_META.write_text(json.dumps(\n        {\"agg_hash\": agg_hash, \"NTE\": int(NTE), \"spec\": agg_spec, \"agg_dim\": int(agg_dim), \"g_dim\": int(G_raw_default.shape[1])},\n        indent=2\n    ))\n    print(f\"[Stage 10] TEST G built: shape={G_raw_default.shape} | time={time.time()-t0:.1f}s | cached={G_TEST_CACHE}\")\n\n# ----------------------------\n# 10) Model definition (pooling matches Stage 8)\n# ----------------------------\nclass FlexMultibandEventTransformer(nn.Module):\n    def __init__(self, feat_dim, max_len, n_bands, d_model, n_heads, n_layers, dim_ff, dropout,\n                 g_dim, g_hidden, has_pool_ln=True, head_final_idx=3):\n        super().__init__()\n        self.n_bands = int(n_bands)\n        self.max_len = int(max_len)\n        self.d_model = int(d_model)\n\n        self.x_proj = nn.Sequential(\n            nn.Linear(int(feat_dim), int(d_model)),\n            nn.GELU(),\n            nn.Dropout(float(dropout)),\n        )\n        self.band_emb = nn.Embedding(int(n_bands), int(d_model))\n\n        self.pos_emb = nn.Parameter(torch.zeros(1, int(max_len), int(d_model)))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=int(d_model),\n            nhead=int(n_heads),\n            dim_feedforward=int(dim_ff),\n            dropout=float(dropout),\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=int(n_layers))\n\n        self.attn = nn.Linear(int(d_model), 1)\n\n        self.has_pool_ln = bool(has_pool_ln)\n        if self.has_pool_ln:\n            self.pool_ln = nn.LayerNorm(int(d_model))\n\n        self.g_dim = int(g_dim)\n        self.g_hidden = int(g_hidden)\n        if self.g_dim > 0 and self.g_hidden > 0:\n            self.g_proj = nn.Sequential(\n                nn.Linear(int(g_dim), int(g_hidden)),\n                nn.GELU(),\n                nn.Dropout(float(dropout)),\n            )\n        else:\n            self.g_proj = None\n\n        in_head = int(d_model + (g_hidden if (self.g_proj is not None) else 0))\n        if head_final_idx == 3:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.GELU(),\n                nn.Dropout(float(dropout)),\n                nn.Linear(int(d_model), 1),\n            )\n        elif head_final_idx == 2:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.GELU(),\n                nn.Linear(int(d_model), 1),\n            )\n        else:\n            self.head = nn.Sequential(\n                nn.Linear(in_head, int(d_model)),\n                nn.Linear(int(d_model), 1),\n            )\n\n    def forward(self, X, band_id, mask, G):\n        X = X.to(torch.float32)\n        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n        mask = mask.to(torch.long)\n\n        pad_mask = (mask == 0)\n        all_pad = pad_mask.all(dim=1)\n        if all_pad.any():\n            pad_mask = pad_mask.clone()\n            pad_mask[all_pad, 0] = False\n\n        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        a = self.attn(h).squeeze(-1)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)\n        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n\n        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n        denom = valid.sum(dim=1).clamp_min(1.0)\n        pooled_mean = (h * valid).sum(dim=1) / denom\n\n        h_masked = h.masked_fill(pad_mask.unsqueeze(-1), -1e9)\n        pooled_max = torch.max(h_masked, dim=1).values\n        pooled_max = torch.where(torch.isfinite(pooled_max), pooled_max, torch.zeros_like(pooled_max))\n\n        pooled = (0.50 * pooled_attn) + (0.30 * pooled_mean) + (0.20 * pooled_max)\n        if self.has_pool_ln:\n            pooled = self.pool_ln(pooled)\n\n        if self.g_proj is not None:\n            g = self.g_proj(G.to(torch.float32))\n            z = torch.cat([pooled, g], dim=1)\n        else:\n            z = pooled\n\n        return self.head(z).squeeze(-1)\n\n@torch.inference_mode()\ndef predict_logits_batchwise(model, Xmm, Bmm, Mmm, G_raw, mean, std, batch_size=64):\n    model.eval()\n    out = np.zeros((Xmm.shape[0],), dtype=np.float32)\n    N0 = int(Xmm.shape[0])\n\n    for i in range(0, N0, int(batch_size)):\n        j = min(N0, i + int(batch_size))\n        Xb_np = np.asarray(Xmm[i:j]).astype(np.float32, copy=False)\n        Bb_np = np.asarray(Bmm[i:j])\n        Mb_np = np.asarray(Mmm[i:j])\n\n        real = (Mb_np == 1)\n\n        # Apply band shift EXACTLY ONCE if needed\n        if SHIFT_BAND_IDS:\n            Bb_np2 = Bb_np.astype(np.int16, copy=True)\n            if real.any():\n                Bb_np2[real] = np.clip(Bb_np2[real] - 1, 0, N_BANDS - 1)\n            Bb_np2[~real] = 0\n            Bb_np = Bb_np2.astype(np.int64, copy=False)\n        else:\n            Bb_np = Bb_np.astype(np.int64, copy=False)\n\n        Gb_np = G_raw[i:j]\n        Gb_np = ((Gb_np - mean) / std).astype(np.float32, copy=False)\n\n        Xb = torch.from_numpy(Xb_np)\n        Bb = torch.from_numpy(Bb_np)\n        Mb = torch.from_numpy(Mb_np.astype(np.int64, copy=False))\n        Gb = torch.from_numpy(Gb_np)\n\n        logit = model(Xb.to(device), Bb.to(device), Mb.to(device), Gb.to(device))\n        out[i:j] = logit.detach().cpu().numpy().astype(np.float32, copy=False)\n\n    return out\n\n# Batch size heuristic\nBATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"64\"))\nif L >= 512:\n    BATCH_SIZE = min(BATCH_SIZE, 32)\nBATCH_SIZE = max(4, BATCH_SIZE)\n\ntest_logit_folds = np.zeros((NTE, n_splits), dtype=np.float32)\n\nprint(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} | ensemble=mean_logits_then_sigmoid\")\n\n# ----------------------------\n# 11) Fold inference\n# ----------------------------\nfor fold, ckpt_path in enumerate(ckpts):\n    ckpt_obj = torch_load_compat(ckpt_path)\n    sd, meta = extract_state_and_meta(ckpt_obj)\n\n    arch = fold_arch[fold]\n    cfg = meta.get(\"cfg\", {}) if isinstance(meta, dict) else {}\n    dropout = float(cfg.get(\"dropout\", 0.0)) if isinstance(cfg, dict) else 0.0\n\n    n_heads = int(cfg.get(\"n_heads\", 4)) if isinstance(cfg, dict) else 4\n    if n_heads <= 0:\n        n_heads = 4\n    if (arch[\"d_model\"] % n_heads) != 0:\n        for h in [4, 8, 2, 1, 16, 32]:\n            if h > 0 and (arch[\"d_model\"] % h) == 0:\n                n_heads = h\n                break\n        if (arch[\"d_model\"] % n_heads) != 0:\n            raise RuntimeError(f\"Fold {fold}: cannot choose valid n_heads for d_model={arch['d_model']}\")\n\n    # HARD checks\n    if arch[\"feat_dim\"] != Fdim:\n        raise RuntimeError(\n            f\"Fold {fold}: feature_dim mismatch.\\n\"\n            f\"- ckpt expects feat_dim={arch['feat_dim']}\\n\"\n            f\"- memmap has Fdim={Fdim}\\n\"\n            \"Solusi: pastikan STAGE 6 feature list sama saat training ckpt dibuat.\"\n        )\n    if arch[\"max_len_ckpt\"] != L:\n        raise RuntimeError(\n            f\"Fold {fold}: max_len mismatch.\\n\"\n            f\"- ckpt max_len={arch['max_len_ckpt']}\\n\"\n            f\"- memmap MAX_LEN={L}\\n\"\n        )\n\n    g_dim = int(arch[\"g_dim\"])\n    if g_dim <= 0:\n        G_raw = np.zeros((NTE, 0), dtype=np.float32)\n        g_mean = np.zeros((0,), dtype=np.float32)\n        g_std  = np.ones((0,), dtype=np.float32)\n    else:\n        if G_raw_default.shape[1] < g_dim:\n            msg = (\n                f\"[Stage 10] Fold {fold}: computed TEST G_raw dim is smaller than ckpt expects.\\n\"\n                f\"- G_raw_default_dim={G_raw_default.shape[1]}\\n\"\n                f\"- ckpt g_dim={g_dim}\\n\"\n                \"Ini biasanya artinya kamu tidak membangun global features yang sama seperti saat training.\\n\"\n                \"Solusi: pakai FIX_DIR yang sama, pastikan need_agg benar, dan META_COLS sama.\\n\"\n            )\n            if STRICT_GDIM:\n                raise RuntimeError(msg)\n            else:\n                # legacy unsafe fallback\n                pad = np.zeros((NTE, g_dim - G_raw_default.shape[1]), dtype=np.float32)\n                G_raw = np.concatenate([G_raw_default, pad], axis=1).astype(np.float32, copy=False)\n        elif G_raw_default.shape[1] > g_dim:\n            G_raw = G_raw_default[:, :g_dim]\n        else:\n            G_raw = G_raw_default\n\n        scaler = meta.get(\"global_scaler\", None) if isinstance(meta, dict) else None\n        if scaler is None or not isinstance(scaler, dict) or (\"mean\" not in scaler) or (\"std\" not in scaler):\n            raise RuntimeError(f\"[Stage 10] Fold {fold}: missing global_scaler in checkpoint meta.\")\n        g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32).reshape(-1)\n        g_std  = np.asarray(scaler[\"std\"],  dtype=np.float32).reshape(-1)\n\n        if g_mean.shape[0] != g_dim or g_std.shape[0] != g_dim:\n            raise RuntimeError(\n                f\"[Stage 10] Fold {fold}: global_scaler shape mismatch.\\n\"\n                f\"- mean/std len: {g_mean.shape[0]}/{g_std.shape[0]}\\n\"\n                f\"- g_dim: {g_dim}\\n\"\n                \"Solusi: ckpt tidak konsisten atau fitur global tidak sama.\"\n            )\n        g_std = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n\n    model = FlexMultibandEventTransformer(\n        feat_dim=arch[\"feat_dim\"],\n        max_len=arch[\"max_len_ckpt\"],\n        n_bands=arch[\"n_bands\"],\n        d_model=arch[\"d_model\"],\n        n_heads=n_heads,\n        n_layers=arch[\"n_layers\"],\n        dim_ff=arch[\"dim_ff\"],\n        dropout=dropout,\n        g_dim=g_dim,\n        g_hidden=arch[\"g_hidden\"],\n        has_pool_ln=arch[\"has_pool_ln\"],\n        head_final_idx=arch[\"head_final_idx\"],\n    ).to(device)\n\n    model.load_state_dict(sd, strict=True)\n\n    # OPTIONAL: apply EMA weights for inference (if present)\n    used_ema = False\n    ema_hits = 0\n    if USE_EMA_WEIGHTS_FOR_INFER and isinstance(meta, dict) and isinstance(meta.get(\"ema_shadow\", None), dict):\n        ema_shadow = meta[\"ema_shadow\"]\n        st = model.state_dict()\n        for k, v in ema_shadow.items():\n            if k in st and torch.is_tensor(v) and st[k].shape == v.shape:\n                st[k] = v.to(dtype=st[k].dtype, device=st[k].device)\n                ema_hits += 1\n        if ema_hits > 0:\n            model.load_state_dict(st, strict=True)\n            used_ema = True\n\n    logits = predict_logits_batchwise(\n        model, Xte, Bte, Mte, G_raw, mean=g_mean, std=g_std, batch_size=BATCH_SIZE\n    )\n    if not np.isfinite(logits).all():\n        raise RuntimeError(f\"[Stage 10] Fold {fold}: logits has NaN/inf. Check inputs/scaler.\")\n\n    test_logit_folds[:, fold] = logits\n    probs_tmp = sigmoid_np(logits)\n\n    print(\n        f\"  fold {fold}: d_model={arch['d_model']} n_heads={n_heads} g_dim={g_dim} | \"\n        f\"ema={used_ema} (hits={ema_hits}) | \"\n        f\"logit_mean={float(logits.mean()):.6f} | prob_mean={float(probs_tmp.mean()):.6f} | prob_std={float(probs_tmp.std()):.6f}\"\n    )\n\n    del model, logits, probs_tmp\n    gc.collect()\n\n# ensemble on logits\ntest_logit_ens = test_logit_folds.mean(axis=1).astype(np.float32)\ntest_prob_folds = sigmoid_np(test_logit_folds).astype(np.float32)\ntest_prob_ens   = sigmoid_np(test_logit_ens).astype(np.float32)\n\nif not np.isfinite(test_prob_ens).all():\n    raise RuntimeError(\"[Stage 10] test_prob_ens contains NaN/inf (unexpected).\")\n\n# ----------------------------\n# 12) Save artifacts\n# ----------------------------\nlogit_fold_path = OUT_DIR / \"test_logit_folds.npy\"\nlogit_ens_path  = OUT_DIR / \"test_logit_ens.npy\"\nprob_fold_path  = OUT_DIR / \"test_prob_folds.npy\"\nprob_ens_path   = OUT_DIR / \"test_prob_ens.npy\"\ncsv_path        = OUT_DIR / \"test_prob_ens.csv\"\ncfg_path        = OUT_DIR / \"test_infer_config.json\"\n\nnp.save(logit_fold_path, test_logit_folds)\nnp.save(logit_ens_path,  test_logit_ens)\nnp.save(prob_fold_path,  test_prob_folds)\nnp.save(prob_ens_path,   test_prob_ens)\n\npd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens}).to_csv(csv_path, index=False)\n\nif EXPORT_TEST_PROB_FOLDS_CSV:\n    df_f = pd.DataFrame({\"object_id\": test_ids})\n    for f in range(n_splits):\n        df_f[f\"prob_fold{f}\"] = test_prob_folds[:, f]\n    (OUT_DIR / \"test_prob_folds.csv\").write_text(df_f.to_csv(index=False))\n\n# Optional: binary predictions file (0/1) using BEST_THR from Stage 9 if available\nthr_used = None\npred01_path = None\nif EXPORT_TEST_PRED_01:\n    thr_used = float(globals().get(\"BEST_THR\", DEFAULT_THR_IF_MISSING))\n    thr_used = min(max(thr_used, 0.0), 1.0)\n    test_pred01 = (test_prob_ens >= thr_used).astype(np.int8)\n    pred01_path = OUT_DIR / \"test_pred_01.csv\"\n    pd.DataFrame({\"object_id\": test_ids, \"prediction\": test_pred01.astype(int)}).to_csv(pred01_path, index=False)\n\ninfer_cfg = {\n    \"seed\": int(SEED),\n    \"n_splits\": int(n_splits),\n    \"ensemble\": \"mean_logits_then_sigmoid\",\n    \"batch_size\": int(BATCH_SIZE),\n    \"max_len\": int(L),\n    \"feature_dim\": int(Fdim),\n    \"token_mode\": str(SEQ_TOKEN_MODE),\n    \"val_feat\": str(VAL_FEAT),\n    \"val_is_mag\": bool(VAL_IS_MAG),\n    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n    \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n    \"global_meta_cols\": META_COLS,\n    \"need_agg_seq\": bool(need_agg),\n    \"global_default_dim\": int(G_raw_default.shape[1]),\n    \"use_ema_weights_for_infer\": bool(USE_EMA_WEIGHTS_FOR_INFER),\n    \"export_test_pred_01\": bool(EXPORT_TEST_PRED_01),\n    \"thr_used_for_pred01\": (None if thr_used is None else float(thr_used)),\n    \"ckpt_dir\": str(CKPT_DIR),\n    \"ckpts\": [str(p) for p in ckpts],\n    \"arch_inferred_from_first_fold\": arch_used,\n    \"fold_meta_summary\": fold_meta_summary,\n    \"test_global_cache\": {\"path\": str(G_TEST_CACHE), \"meta\": str(G_TEST_META), \"agg_hash\": agg_hash},\n    \"outputs\": {\n        \"test_logit_folds\": str(logit_fold_path),\n        \"test_logit_ens\": str(logit_ens_path),\n        \"test_prob_folds\": str(prob_fold_path),\n        \"test_prob_ens\": str(prob_ens_path),\n        \"test_prob_ens_csv\": str(csv_path),\n        \"test_pred_01_csv\": (None if pred01_path is None else str(pred01_path)),\n    }\n}\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(infer_cfg, f, indent=2)\n\nprint(\"\\n[Stage 10] DONE\")\nprint(f\"- Saved logits folds: {logit_fold_path}\")\nprint(f\"- Saved logits ens  : {logit_ens_path}\")\nprint(f\"- Saved probs folds : {prob_fold_path}\")\nprint(f\"- Saved probs ens   : {prob_ens_path}\")\nprint(f\"- Saved csv         : {csv_path}\")\nif pred01_path is not None:\n    print(f\"- Saved pred 0/1    : {pred01_path} (thr={thr_used:.6f})\")\nprint(f\"- Saved config      : {cfg_path}\")\nprint(f\"- ens prob mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | \"\n      f\"min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n\nglobals().update({\n    \"test_ids\": test_ids,\n    \"test_logit_folds\": test_logit_folds,\n    \"test_logit_ens\": test_logit_ens,\n    \"test_prob_folds\": test_prob_folds,\n    \"test_prob_ens\": test_prob_ens,\n    \"TEST_LOGIT_FOLDS_PATH\": logit_fold_path,\n    \"TEST_LOGIT_ENS_PATH\": logit_ens_path,\n    \"TEST_PROB_FOLDS_PATH\": prob_fold_path,\n    \"TEST_PROB_ENS_PATH\": prob_ens_path,\n    \"TEST_PROB_CSV_PATH\": csv_path,\n    \"TEST_INFER_CFG_PATH\": cfg_path,\n    \"STAGE10_VAL_FEAT\": VAL_FEAT,\n    \"STAGE10_VAL_IS_MAG\": VAL_IS_MAG,\n    \"STAGE10_USE_EMA_INFER\": bool(USE_EMA_WEIGHTS_FOR_INFER),\n    \"TEST_PRED01_PATH\": (pred01_path if pred01_path is not None else None),\n    \"TEST_PRED01_THR_USED\": (thr_used if thr_used is not None else None),\n    \"TEST_GLOBAL_FEAT_CACHE_PATH\": G_TEST_CACHE,\n    \"TEST_GLOBAL_FEAT_META_PATH\": G_TEST_META,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation ","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ONE CELL — EVALUATION (Precision / Recall / F1) + Threshold Sweep (OOF)\n# REVISI FULL v3.4 (HOLDOUT-SAFE + GE/GT + PREVALENCE THR + EXTRA CANDS + GT->GE_EQUIV)\n#\n# Default behavior:\n# - HOLDOUT_SAFE=True: drop non-finite probs (NaN/inf) from tuning\n# - Evaluate BOTH rules: ge (>=) and gt (>)\n# - Default BEST_THR uses rule=ge, metric=F1\n# ============================================================\n\nimport gc, json, math, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require minimal\n# ----------------------------\nif \"df_train_meta\" not in globals():\n    raise RuntimeError(\"Missing df_train_meta. Jalankan stage meta dulu.\")\n\nART_DIR = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\nOOF_DIR = Path(globals().get(\"OOF_DIR\", ART_DIR / \"oof\"))\nOOF_DIR.mkdir(parents=True, exist_ok=True)\n\n# Switches\nHOLDOUT_SAFE = True            # drop non-finite oof probs from tuning (recommended)\nDO_BOTH_RULES = True           # compute ge and gt tables\nADD_NEXTAFTER_CANDIDATES = True  # add nextafter(unique_probs) as thr candidates (recommended)\n\n# ----------------------------\n# Utils\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a.reshape(1)\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\ndef _safe_div(a, b):\n    return a / np.maximum(b, 1e-12)\n\ndef _to_np_bool(x):\n    if isinstance(x, np.ndarray):\n        return x.astype(bool, copy=False)\n    if hasattr(x, \"to_numpy\"):\n        return x.to_numpy(dtype=bool, copy=False)\n    return np.asarray(x, dtype=bool)\n\n# HOLDOUT_SAFE: keep NaN as invalid (do not nan_to_num -> 0 for tuning)\ndef _sanitize_prob(p, holdout_safe=True):\n    p = np.asarray(p, dtype=np.float32)\n    if holdout_safe:\n        # keep non-finite for filtering later\n        p = np.clip(p, 0.0, 1.0, out=p, where=np.isfinite(p))\n        return p.astype(np.float32, copy=False)\n    # legacy: force finite\n    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n    p = np.clip(p, 0.0, 1.0)\n    return p.astype(np.float32)\n\n# ----------------------------\n# 0a) Normalize meta index\n# ----------------------------\ndf_train_meta = df_train_meta.copy(deep=False)\ndf_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\n\n# ----------------------------\n# 0b) Detect target column\n# ----------------------------\ndef _detect_target_col(df):\n    for cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n        if cand in df.columns:\n            return cand\n    return None\n\nTARGET_COL = _detect_target_col(df_train_meta)\nif TARGET_COL is None:\n    raise RuntimeError(\n        \"Cannot detect target column in df_train_meta. \"\n        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n    )\n\ny_series = pd.to_numeric(df_train_meta[TARGET_COL], errors=\"coerce\").fillna(0.0)\ny_bin = (y_series.to_numpy(dtype=np.float32) > 0).astype(np.int8)\ny_map = pd.Series(y_bin, index=df_train_meta.index)\nif y_map.index.has_duplicates:\n    y_map = y_map.groupby(level=0).max()\n\n# ----------------------------\n# 1) Load oof_prob (prefer csv)\n# ----------------------------\ndef load_oof():\n    pcsv = OOF_DIR / \"oof_prob.csv\"\n    if pcsv.exists():\n        df = pd.read_csv(pcsv)\n        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n            df = df[[\"object_id\", \"oof_prob\"]].copy()\n            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n            p = _sanitize_prob(_as_1d_float32(df[\"oof_prob\"].to_numpy()), holdout_safe=HOLDOUT_SAFE)\n            if len(p) != len(df):\n                raise RuntimeError(\"oof_prob.csv: length mismatch after parsing.\")\n            df[\"oof_prob\"] = p\n            return p, df, \"csv(oof_prob.csv)\"\n\n    if \"oof_prob\" in globals():\n        p = _sanitize_prob(_as_1d_float32(globals()[\"oof_prob\"]), holdout_safe=HOLDOUT_SAFE)\n        return p, None, \"globals(oof_prob)\"\n\n    pnpy = OOF_DIR / \"oof_prob.npy\"\n    if pnpy.exists():\n        p = _sanitize_prob(_as_1d_float32(np.load(pnpy, allow_pickle=False)), holdout_safe=HOLDOUT_SAFE)\n        return p, None, \"npy(oof_prob.npy)\"\n\n    raise FileNotFoundError(\"OOF prob tidak ditemukan (oof_prob.csv / globals oof_prob / oof_prob.npy).\")\n\noof_prob_all, df_oof_csv, oof_src = load_oof()\nif not isinstance(oof_prob_all, np.ndarray) or oof_prob_all.ndim != 1:\n    raise TypeError(f\"Invalid oof_prob. type={type(oof_prob_all)} ndim={getattr(oof_prob_all,'ndim',None)}\")\n\n# ----------------------------\n# 2) Align y to OOF order\n# ----------------------------\nif df_oof_csv is not None:\n    # de-dup ids in oof: mean per id but preserve first order\n    ids_first = pd.unique(df_oof_csv[\"object_id\"].to_numpy())\n    if len(ids_first) != len(df_oof_csv):\n        df_mean = df_oof_csv.groupby(\"object_id\", as_index=True)[\"oof_prob\"].mean()\n        df_oof_csv = pd.DataFrame({\"object_id\": ids_first})\n        df_oof_csv[\"oof_prob\"] = df_mean.reindex(ids_first).to_numpy(dtype=np.float32)\n        oof_prob_all = df_oof_csv[\"oof_prob\"].to_numpy(dtype=np.float32, copy=False)\n\n    train_ids_all = df_oof_csv[\"object_id\"].tolist()\n\n    ok_raw = pd.Index(train_ids_all).isin(y_map.index)\n    ok = _to_np_bool(ok_raw)\n    if not ok.all():\n        bad = [train_ids_all[i] for i in np.where(~ok)[0][:10]]\n        print(f\"[WARN] oof ids not in df_train_meta: missing_n={int((~ok).sum())} examples={bad}\")\n        df_oof_csv = df_oof_csv.loc[ok].reset_index(drop=True)\n        train_ids_all = df_oof_csv[\"object_id\"].tolist()\n        oof_prob_all = df_oof_csv[\"oof_prob\"].to_numpy(dtype=np.float32, copy=False)\n\n    y_all = y_map.reindex(train_ids_all).to_numpy(dtype=np.int8, copy=True)\n\nelif \"train_ids_ordered\" in globals():\n    ids = [_norm_id(z) for z in list(globals()[\"train_ids_ordered\"])]\n    if len(ids) != len(oof_prob_all):\n        raise RuntimeError(\"train_ids_ordered length mismatch with oof_prob. Gunakan oof_prob.csv agar alignment aman.\")\n    missing = [oid for oid in ids if oid not in y_map.index]\n    if missing:\n        raise KeyError(f\"train_ids_ordered contains ids not in df_train_meta. ex={missing[:10]} missing_n={len(missing)}\")\n    train_ids_all = ids\n    y_all = y_map.reindex(train_ids_all).to_numpy(dtype=np.int8, copy=True)\n\nelse:\n    if len(oof_prob_all) != len(df_train_meta):\n        raise RuntimeError(\n            f\"Tidak bisa align y. len(oof_prob)={len(oof_prob_all)} != len(df_train_meta)={len(df_train_meta)} \"\n            \"dan tidak ada oof_prob.csv atau train_ids_ordered.\"\n        )\n    if df_train_meta.index.has_duplicates:\n        raise RuntimeError(\n            \"df_train_meta.index has duplicates, tapi oof source tidak punya object_id ordering. \"\n            \"Solusi: simpan oof_prob.csv (object_id + oof_prob) atau sediakan train_ids_ordered.\"\n        )\n    train_ids_all = df_train_meta.index.astype(str).tolist()\n    y_all = y_map.reindex(train_ids_all).to_numpy(dtype=np.int8, copy=True)\n\nif len(y_all) != len(oof_prob_all):\n    raise RuntimeError(f\"Length mismatch: y={len(y_all)} vs oof_prob={len(oof_prob_all)}\")\n\nuy = set(np.unique(y_all).tolist())\nif not uy.issubset({0, 1}):\n    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n\n# ----------------------------\n# 2b) HOLDOUT_SAFE filtering (valid only)\n# ----------------------------\nvalid = np.isfinite(oof_prob_all)\nif HOLDOUT_SAFE:\n    train_ids = [train_ids_all[i] for i in np.where(valid)[0]]\n    oof_prob = np.clip(oof_prob_all[valid].astype(np.float32), 0.0, 1.0)\n    y = y_all[valid].astype(np.int8)\nelse:\n    train_ids = train_ids_all\n    oof_prob = np.clip(np.nan_to_num(oof_prob_all, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32), 0.0, 1.0)\n    y = y_all.astype(np.int8)\n\nN_all = int(len(y_all))\nN = int(len(y))\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\n\nprint(f\"[Eval] OOF source={oof_src} | target_col={TARGET_COL}\")\nprint(f\"[Eval] rows: valid={N:,} / total={N_all:,} | pos={pos:,} neg={neg:,} pos%={pos/max(N,1)*100:.6f}%\")\nif HOLDOUT_SAFE and (N < N_all):\n    print(f\"[Eval] HOLDOUT_SAFE dropped non-finite rows: {N_all - N} rows\")\n\n# ----------------------------\n# 3) Ranking metrics (threshold-free)\n# ----------------------------\nroc_auc = None\npr_auc = None\ntry:\n    from sklearn.metrics import roc_auc_score, average_precision_score\n    if (y.max() == 1) and (y.min() == 0) and N > 1:\n        roc_auc = float(roc_auc_score(y, oof_prob))\n        pr_auc  = float(average_precision_score(y, oof_prob))\nexcept Exception:\n    pass\n\n# ----------------------------\n# 4) Threshold candidates (grid + quantiles + unique + nextafter + extras)\n# ----------------------------\ngrid = np.concatenate([\n    np.linspace(0.00, 0.10, 41, dtype=np.float32),\n    np.linspace(0.10, 0.90, 161, dtype=np.float32),\n    np.linspace(0.90, 1.00, 41, dtype=np.float32),\n]).astype(np.float32)\n\nqs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\ntry:\n    quant_thr = np.quantile(oof_prob, qs).astype(np.float32) if N > 0 else np.array([], dtype=np.float32)\nexcept Exception:\n    quant_thr = np.array([], dtype=np.float32)\n\nuniq = np.unique(oof_prob.astype(np.float32))\nif len(uniq) > 8000:\n    take = np.linspace(0, len(uniq) - 1, 8000, dtype=int)\n    uniq = uniq[take].astype(np.float32)\n\nuniq_up = np.nextafter(uniq, np.float32(1.0)).astype(np.float32) if (ADD_NEXTAFTER_CANDIDATES and len(uniq) > 0) else np.array([], dtype=np.float32)\n\n# prevalence-match threshold (for ge): choose thr so predicted positives roughly == pos\nif pos > 0:\n    p_sorted_tmp = np.sort(oof_prob)[::-1]\n    thr_prev = float(p_sorted_tmp[min(pos - 1, N - 1)])\nelse:\n    thr_prev = 1.0\n\nextra = [0.0, 0.5, 1.0, float(thr_prev)]\nfor cand_name in [\"BEST_THR\", \"OOF_BEST_THR_F1\", \"BEST_THR_F1\"]:\n    if cand_name in globals() and globals()[cand_name] is not None:\n        try:\n            extra.append(float(globals()[cand_name]))\n        except Exception:\n            pass\n\nthr_candidates = np.unique(\n    np.clip(np.concatenate([grid, quant_thr, uniq, uniq_up, np.array(extra, dtype=np.float32)]), 0.0, 1.0)\n).astype(np.float32)\n\nif len(thr_candidates) > 20000:\n    take = np.linspace(0, len(thr_candidates) - 1, 20000, dtype=int)\n    thr_candidates = thr_candidates[take].astype(np.float32)\n\n# ----------------------------\n# 5) FAST sweep using sorted probabilities\n# ----------------------------\nord_desc = np.argsort(-oof_prob)\np_sorted = oof_prob[ord_desc]\ny_sorted = y[ord_desc].astype(np.int8)\n\npos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\nneg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\npos_total = int(pos_prefix[-1]) if N > 0 else 0\nneg_total = int(neg_prefix[-1]) if N > 0 else 0\n\ndef _metrics_from_counts(tp, fp, fn, tn):\n    tp = tp.astype(np.float64); fp = fp.astype(np.float64)\n    fn = fn.astype(np.float64); tn = tn.astype(np.float64)\n\n    prec = _safe_div(tp, tp + fp)\n    rec  = _safe_div(tp, tp + fn)\n    f1   = _safe_div(2 * prec * rec, prec + rec)\n\n    def fbeta(prec, rec, beta):\n        b2 = beta * beta\n        return _safe_div((1.0 + b2) * prec * rec, b2 * prec + rec)\n\n    f05 = fbeta(prec, rec, 0.5)\n    f2  = fbeta(prec, rec, 2.0)\n\n    acc  = _safe_div(tp + tn, tp + fp + fn + tn)\n    tpr  = _safe_div(tp, tp + fn)\n    tnr  = _safe_div(tn, tn + fp)\n    bacc = 0.5 * (tpr + tnr)\n\n    num = tp * tn - fp * fn\n    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n\n    return f1, f05, f2, prec, rec, acc, bacc, mcc\n\ndef _sweep(rule: str):\n    # k = number of predicted positives\n    if rule == \"ge\":\n        k = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"right\").astype(np.int64)\n    elif rule == \"gt\":\n        k = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"left\").astype(np.int64)\n    else:\n        raise ValueError(\"rule must be 'ge' or 'gt'\")\n\n    k = np.clip(k, 0, N).astype(np.int64)\n\n    tp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\n    fp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\n    fn = (pos_total - tp).astype(np.int64)\n    tn = (neg_total - fp).astype(np.int64)\n\n    f1, f05, f2, prec, rec, acc, bacc, mcc = _metrics_from_counts(tp, fp, fn, tn)\n\n    return pd.DataFrame({\n        \"thr\": thr_candidates.astype(np.float32),\n        \"rule\": rule,\n        \"f1\": f1.astype(np.float32),\n        \"f0.5\": f05.astype(np.float32),\n        \"f2\": f2.astype(np.float32),\n        \"precision\": prec.astype(np.float32),\n        \"recall\": rec.astype(np.float32),\n        \"acc\": acc.astype(np.float32),\n        \"bacc\": bacc.astype(np.float32),\n        \"mcc\": mcc.astype(np.float32),\n        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n        \"pos_pred\": k.astype(np.int64),\n    })\n\nthr_ge = _sweep(\"ge\")\nthr_gt = _sweep(\"gt\") if DO_BOTH_RULES else None\n\ndef _pick_best(df, primary, tie_cols):\n    sort_cols = [primary] + tie_cols\n    asc = [False] * len(sort_cols)\n    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n\ndef _eval_at(thr, rule):\n    thr = float(thr)\n    if rule == \"ge\":\n        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"right\"))\n    else:\n        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"left\"))\n    k0 = max(0, min(k0, N))\n\n    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n    fn0 = int(pos_total - tp0)\n    tn0 = int(neg_total - fp0)\n\n    p0 = tp0 / max(tp0 + fp0, 1)\n    r0 = tp0 / max(tp0 + fn0, 1)\n    f10 = 0.0 if (tp0 == 0 or (p0 + r0) == 0) else (2 * p0 * r0 / (p0 + r0))\n    f05 = 0.0 if (0.25 * p0 + r0) == 0 else ((1.25) * p0 * r0 / (0.25 * p0 + r0))\n    f2  = 0.0 if (4.0 * p0 + r0) == 0 else ((5.0) * p0 * r0 / (4.0 * p0 + r0))\n\n    acc0  = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n\n    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n\n    return {\n        \"thr\": thr, \"rule\": rule,\n        \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0, \"pos_pred\": k0,\n        \"precision\": float(p0), \"recall\": float(r0),\n        \"f1\": float(f10), \"f0.5\": float(f05), \"f2\": float(f2),\n        \"acc\": float(acc0), \"bacc\": float(bacc0), \"mcc\": float(mcc0),\n    }\n\nbase_ge = _eval_at(0.5, \"ge\")\nbase_gt = _eval_at(0.5, \"gt\") if DO_BOTH_RULES else None\n\n# Bests (rule=ge)\nbest_f1_ge  = _pick_best(thr_ge, \"f1\",   [\"mcc\",\"bacc\",\"recall\",\"precision\",\"acc\"])\nbest_f05_ge = _pick_best(thr_ge, \"f0.5\", [\"precision\",\"mcc\",\"f1\",\"acc\"])\nbest_f2_ge  = _pick_best(thr_ge, \"f2\",   [\"recall\",\"mcc\",\"f1\",\"bacc\",\"acc\"])\n\nBEST_THR_GE_F1  = float(best_f1_ge[\"thr\"])\nBEST_THR_GE_F05 = float(best_f05_ge[\"thr\"])\nBEST_THR_GE_F2  = float(best_f2_ge[\"thr\"])\n\nbest_ge_f1  = _eval_at(BEST_THR_GE_F1, \"ge\")\nbest_ge_f05 = _eval_at(BEST_THR_GE_F05, \"ge\")\nbest_ge_f2  = _eval_at(BEST_THR_GE_F2, \"ge\")\n\n# Bests (rule=gt)\nif DO_BOTH_RULES:\n    best_f1_gt  = _pick_best(thr_gt, \"f1\",   [\"mcc\",\"bacc\",\"recall\",\"precision\",\"acc\"])\n    BEST_THR_GT_F1 = float(best_f1_gt[\"thr\"])\n    best_gt_f1 = _eval_at(BEST_THR_GT_F1, \"gt\")\n\n    # gt -> ge equivalent (so downstream can still use prob >= thr)\n    BEST_THR_GT_F1_GE_EQUIV = float(np.nextafter(np.float32(BEST_THR_GT_F1), np.float32(1.0)))\nelse:\n    BEST_THR_GT_F1 = None\n    BEST_THR_GT_F1_GE_EQUIV = None\n    best_gt_f1 = None\n\n# Default export for downstream (keep old variable names)\nBEST_THR_F1  = BEST_THR_GE_F1\nBEST_THR_F05 = BEST_THR_GE_F05\nBEST_THR_F2  = BEST_THR_GE_F2\nBEST_THR     = BEST_THR_GE_F1\n\n# ----------------------------\n# 6) Report + tables\n# ----------------------------\nprint(\"\\nEVALUATION (OOF) — Precision/Recall/F-scores (+BACC/MCC)\")\nif roc_auc is not None:\n    print(f\"- ROC-AUC={roc_auc:.6f} | PR-AUC={pr_auc:.6f}\")\nprint(f\"- prevalence-match thr (ge) ~ {thr_prev:.6f}\")\n\nprint(\"\\nBaseline @ thr=0.5\")\nprint(f\"- rule=ge: F1={base_ge['f1']:.6f} | P={base_ge['precision']:.6f} | R={base_ge['recall']:.6f} | \"\n      f\"ACC={base_ge['acc']:.6f} | BACC={base_ge['bacc']:.6f} | MCC={base_ge['mcc']:.6f} | pos_pred={base_ge['pos_pred']}\")\nif DO_BOTH_RULES:\n    print(f\"- rule=gt: F1={base_gt['f1']:.6f} | P={base_gt['precision']:.6f} | R={base_gt['recall']:.6f} | \"\n          f\"ACC={base_gt['acc']:.6f} | BACC={base_gt['bacc']:.6f} | MCC={base_gt['mcc']:.6f} | pos_pred={base_gt['pos_pred']}\")\n\nprint(f\"\\nBEST (rule=ge) — default downstream: pred = prob >= thr\")\nprint(f\"- BEST-F1   @ thr={best_ge_f1['thr']:.6f} | F1={best_ge_f1['f1']:.6f} | P={best_ge_f1['precision']:.6f} | R={best_ge_f1['recall']:.6f} | pos_pred={best_ge_f1['pos_pred']}\")\nprint(f\"- BEST-F0.5 @ thr={best_ge_f05['thr']:.6f} | F0.5={best_ge_f05['f0.5']:.6f} | P={best_ge_f05['precision']:.6f} | R={best_ge_f05['recall']:.6f}\")\nprint(f\"- BEST-F2   @ thr={best_ge_f2['thr']:.6f} | F2={best_ge_f2['f2']:.6f} | P={best_ge_f2['precision']:.6f} | R={best_ge_f2['recall']:.6f}\")\n\nif DO_BOTH_RULES:\n    print(f\"\\nBEST (rule=gt) — strict boundary (>)\")\n    print(f\"- BEST-F1 @ thr={best_gt_f1['thr']:.6f} (gt) | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.6f} | \"\n          f\"F1={best_gt_f1['f1']:.6f} | pos_pred={best_gt_f1['pos_pred']}\")\n\n# Combine + sort for top view\nthr_all = pd.concat([thr_ge, thr_gt], ignore_index=True) if DO_BOTH_RULES else thr_ge.copy()\nthr_sorted = thr_all.sort_values(\n    [\"f1\",\"mcc\",\"bacc\",\"recall\",\"precision\"],\n    ascending=[False, False, False, False, False]\n).reset_index(drop=True)\n\nprint(\"\\nTop 10 by F1 (mixed rules):\")\nfor i in range(min(10, len(thr_sorted))):\n    r = thr_sorted.iloc[i]\n    print(f\"{i+1:02d}. rule={r['rule']} thr={float(r['thr']):.6f} | f1={float(r['f1']):.6f} | \"\n          f\"P={float(r['precision']):.6f} R={float(r['recall']):.6f} | mcc={float(r['mcc']):.6f} bacc={float(r['bacc']):.6f} | \"\n          f\"pos_pred={int(r['pos_pred'])}\")\n\n# ----------------------------\n# 7) Save artifacts\n# ----------------------------\nout_txt   = OOF_DIR / \"eval_report.txt\"\nout_csv   = OOF_DIR / \"eval_threshold_table.csv\"\nout_csv_t = OOF_DIR / \"eval_threshold_table_top500.csv\"\nout_json  = OOF_DIR / \"eval_summary.json\"\n\nthr_sorted.to_csv(out_csv, index=False)\nthr_sorted.head(500).to_csv(out_csv_t, index=False)\n\npayload = {\n    \"version\": \"v3.4\",\n    \"source\": oof_src,\n    \"target_col\": TARGET_COL,\n    \"n_total_rows\": int(N_all),\n    \"n_valid_rows\": int(N),\n    \"pos_valid\": int(pos),\n    \"neg_valid\": int(neg),\n    \"roc_auc_valid_only\": roc_auc,\n    \"pr_auc_valid_only\": pr_auc,\n    \"prevalence_match_thr_ge\": float(thr_prev),\n    \"baseline_thr_0p5\": {\"ge\": base_ge, \"gt\": base_gt},\n    \"best_ge\": {\"f1\": best_ge_f1, \"f0.5\": best_ge_f05, \"f2\": best_ge_f2},\n    \"best_gt\": {\"f1\": best_gt_f1, \"ge_equiv_for_downstream_using_ge\": {\"f1\": BEST_THR_GT_F1_GE_EQUIV}},\n    \"default_best_thr\": {\"metric\": \"f1\", \"rule\": \"ge\", \"thr\": float(BEST_THR)},\n    \"switches\": {\n        \"HOLDOUT_SAFE\": bool(HOLDOUT_SAFE),\n        \"DO_BOTH_RULES\": bool(DO_BOTH_RULES),\n        \"ADD_NEXTAFTER_CANDIDATES\": bool(ADD_NEXTAFTER_CANDIDATES),\n    },\n    \"paths\": {\"report\": str(out_txt), \"table\": str(out_csv), \"table_top500\": str(out_csv_t), \"summary\": str(out_json)},\n}\n\n# text report (ringkas tapi jelas)\nlines = []\nlines.append(\"OOF Evaluation Report (v3.4)\")\nlines.append(f\"source={oof_src} | target_col={TARGET_COL}\")\nlines.append(f\"valid_rows={N} / total_rows={N_all} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.10f}%\")\nif roc_auc is not None:\n    lines.append(f\"ROC-AUC(valid-only)={roc_auc:.10f} | PR-AUC(valid-only)={pr_auc:.10f}\")\nlines.append(f\"prevalence_match_thr_ge={thr_prev:.10f}\")\nlines.append(\"\")\nlines.append(\"Baseline @ thr=0.5\")\nlines.append(f\"ge: F1={base_ge['f1']:.10f} P={base_ge['precision']:.10f} R={base_ge['recall']:.10f} \"\n             f\"ACC={base_ge['acc']:.10f} BACC={base_ge['bacc']:.10f} MCC={base_ge['mcc']:.10f} pos_pred={base_ge['pos_pred']}\")\nif DO_BOTH_RULES and base_gt is not None:\n    lines.append(f\"gt: F1={base_gt['f1']:.10f} P={base_gt['precision']:.10f} R={base_gt['recall']:.10f} \"\n                 f\"ACC={base_gt['acc']:.10f} BACC={base_gt['bacc']:.10f} MCC={base_gt['mcc']:.10f} pos_pred={base_gt['pos_pred']}\")\nlines.append(\"\")\nlines.append(f\"BEST (ge/F1) thr={BEST_THR_GE_F1:.10f} | F1={best_ge_f1['f1']:.10f} P={best_ge_f1['precision']:.10f} R={best_ge_f1['recall']:.10f}\")\nif DO_BOTH_RULES and best_gt_f1 is not None:\n    lines.append(f\"BEST (gt/F1) thr={BEST_THR_GT_F1:.10f} | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.10f} | F1={best_gt_f1['f1']:.10f}\")\nlines.append(\"\")\nlines.append(\"Top 10 by F1 (mixed rules):\")\nfor i in range(min(10, len(thr_sorted))):\n    r = thr_sorted.iloc[i]\n    lines.append(f\"{i+1:02d}. rule={r['rule']} thr={float(r['thr']):.10f} f1={float(r['f1']):.10f} \"\n                 f\"P={float(r['precision']):.10f} R={float(r['recall']):.10f} \"\n                 f\"mcc={float(r['mcc']):.10f} bacc={float(r['bacc']):.10f} pos_pred={int(r['pos_pred'])}\")\n\nout_txt.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\nout_json.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n\nprint(\"\\nSaved:\")\nprint(f\"- {out_txt}\")\nprint(f\"- {out_csv}\")\nprint(f\"- {out_csv_t}\")\nprint(f\"- {out_json}\")\n\nglobals().update({\n    \"BEST_THR\": float(BEST_THR),\n    \"BEST_THR_F1\": float(BEST_THR_F1),\n    \"BEST_THR_F05\": float(BEST_THR_F05),\n    \"BEST_THR_F2\": float(BEST_THR_F2),\n\n    \"BEST_THR_GE_F1\": float(BEST_THR_GE_F1),\n    \"BEST_THR_GE_F05\": float(BEST_THR_GE_F05),\n    \"BEST_THR_GE_F2\": float(BEST_THR_GE_F2),\n\n    \"BEST_THR_GT_F1\": (None if BEST_THR_GT_F1 is None else float(BEST_THR_GT_F1)),\n    \"BEST_THR_GT_F1_GE_EQUIV\": (None if BEST_THR_GT_F1_GE_EQUIV is None else float(BEST_THR_GT_F1_GE_EQUIV)),\n\n    \"thr_table_eval\": thr_sorted,\n    \"EVAL_REPORT_PATH\": out_txt,\n    \"EVAL_TABLE_PATH\": out_csv,\n    \"EVAL_TABLE_TOP500_PATH\": out_csv_t,\n    \"EVAL_SUMMARY_PATH\": out_json,\n    \"OOF_AUC_VALID_ONLY\": roc_auc,\n    \"OOF_AP_VALID_ONLY\": pr_auc,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission Build","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 11 — Submission Build (ONE CELL) — REVISI FULL v3.5\n#\n# Upgrade v3.5:\n# - Threshold JSON fallback FIX (supports Stage9 threshold_tuning.json & Eval v3.4 summary)\n# - Auto-handle if pred file already 0/1 (test_pred_01.csv)\n# - Stronger diagnostics + strict order = sample_submission\n#\n# Output:\n# - /kaggle/working/submission.csv\n# - SUB_DIR/submission.csv (copy)\n# ============================================================\n\nimport gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 0 globals\n# ----------------------------\nfor need in [\"PATHS\", \"SUB_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n\nsample_path = Path(PATHS[\"SAMPLE_SUB\"])\nif not sample_path.exists():\n    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n\n# IMPORTANT: dtype object_id=str to prevent ID corruption\ndf_sub = pd.read_csv(sample_path, dtype={\"object_id\": str})\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef _norm_id(x):\n    if isinstance(x, (bytes, np.bytes_)):\n        try:\n            x = x.decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            x = str(x)\n    s = str(x).strip()\n    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n        s = s[2:-1]\n    return s.strip()\n\ndef _as_1d_float32(arr):\n    a = np.asarray(arr)\n    if a.dtype == object and a.ndim == 0:\n        try:\n            a = np.asarray(a.item())\n        except Exception:\n            pass\n    a = np.asarray(a, dtype=np.float32)\n    if a.ndim == 0:\n        return a.reshape(1)          # IMPORTANT: always 1D\n    if a.ndim > 1:\n        a = a.reshape(-1)\n    return a\n\ndef _sanitize_prob(p):\n    p = np.asarray(p, dtype=np.float32)\n    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n    p = np.clip(p, 0.0, 1.0)\n    return p.astype(np.float32)\n\ndef _load_ids_npy(path: Path):\n    arr = np.load(path, allow_pickle=False)\n    ids = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n    return [_norm_id(x) for x in ids]\n\ndef _try_load_json(p):\n    try:\n        p = Path(p)\n        if not p.exists():\n            return None\n        with open(p, \"r\", encoding=\"utf-8\") as f:\n            obj = json.load(f)\n        return obj if isinstance(obj, dict) else None\n    except Exception:\n        return None\n\ndef _try_load_stage10_cfg():\n    p = globals().get(\"TEST_INFER_CFG_PATH\", None)\n    if p is None:\n        return None\n    return _try_load_json(p)\n\ndef _detect_prob_col(df):\n    # Prefer explicit names (probability)\n    for c in [\"prob\", \"proba\", \"oof_prob\", \"pred\", \"p\"]:\n        if c in df.columns:\n            return c\n    # Some users store probability in \"prediction\"\n    if \"prediction\" in df.columns:\n        # if it looks float-ish (not only 0/1), treat as prob\n        s = pd.to_numeric(df[\"prediction\"], errors=\"coerce\")\n        if s.notna().mean() > 0.95:\n            u = set(np.unique(s.dropna().astype(float).to_numpy()).tolist())\n            if not u.issubset({0.0, 1.0}):\n                return \"prediction\"\n\n    # else: pick a single mostly-numeric column besides object_id\n    cand = [c for c in df.columns if c != \"object_id\"]\n    floatish = []\n    for c in cand:\n        s = pd.to_numeric(df[c], errors=\"coerce\")\n        if s.notna().mean() > 0.95:\n            floatish.append(c)\n    if len(floatish) == 1:\n        return floatish[0]\n    return None\n\ndef _detect_binary_col(df):\n    # Prefer explicit binary prediction column\n    for c in [\"prediction\", \"pred\", \"label\", \"y\"]:\n        if c in df.columns:\n            s = pd.to_numeric(df[c], errors=\"coerce\")\n            if s.notna().mean() > 0.95:\n                u = set(np.unique(s.dropna().astype(int).to_numpy()).tolist())\n                if u.issubset({0, 1}):\n                    return c\n    return None\n\ndef _load_pred_df():\n    \"\"\"\n    Return (df_pred, mode, src_str)\n    mode:\n      - \"prob\": df_pred has columns object_id, prob (float [0,1])\n      - \"bin\" : df_pred has columns object_id, prediction (int 0/1)\n    Priority:\n      A) globals: test_ids + test_prob_ens\n      B) STAGE10 config json -> outputs.test_pred_01_csv (bin) OR outputs.test_prob_ens_csv (prob)\n      C) csv fallbacks\n      D) npy fallback: FIX_DIR/test_ids.npy + test_prob_ens.npy\n    \"\"\"\n    # ---- A) globals ----\n    if (\"test_prob_ens\" in globals()) and (globals()[\"test_prob_ens\"] is not None) and \\\n       (\"test_ids\" in globals()) and (globals()[\"test_ids\"] is not None):\n        ids = [_norm_id(x) for x in list(globals()[\"test_ids\"])]\n        prob = _sanitize_prob(_as_1d_float32(globals()[\"test_prob_ens\"]))\n        if len(ids) == len(prob) and len(ids) > 0:\n            return pd.DataFrame({\"object_id\": ids, \"prob\": prob}), \"prob\", \"globals(test_ids + test_prob_ens)\"\n\n    # ---- B) STAGE 10 config json ----\n    cfg = _try_load_stage10_cfg()\n    if isinstance(cfg, dict):\n        out = cfg.get(\"outputs\", {}) if isinstance(cfg.get(\"outputs\", {}), dict) else {}\n        # prefer binary if exists\n        pred01 = out.get(\"test_pred_01_csv\", None)\n        if pred01:\n            p = Path(pred01)\n            if p.exists():\n                df = pd.read_csv(p, dtype={\"object_id\": str})\n                if \"object_id\" in df.columns:\n                    colb = _detect_binary_col(df)\n                    if colb is None:\n                        raise RuntimeError(f\"Cannot detect binary column in: {p} | cols={list(df.columns)}\")\n                    df = df.copy()\n                    df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n                    y = pd.to_numeric(df[colb], errors=\"coerce\").fillna(0).astype(int).clip(0,1).to_numpy()\n                    return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prediction\": y}), \"bin\", f\"stage10_cfg_bin({p})\"\n\n        csvp = out.get(\"test_prob_ens_csv\", None)\n        if csvp:\n            p = Path(csvp)\n            if p.exists():\n                df = pd.read_csv(p, dtype={\"object_id\": str})\n                if \"object_id\" in df.columns:\n                    colp = _detect_prob_col(df)\n                    if colp is None:\n                        raise RuntimeError(f\"Cannot detect prob column in: {p} | cols={list(df.columns)}\")\n                    df = df.copy()\n                    df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n                    prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n                    if len(prob) != len(df):\n                        raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n                    return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob}), \"prob\", f\"stage10_cfg_csv({p})\"\n\n    # ---- C) csv fallback ----\n    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n    preds_dir = art_dir / \"preds\"\n\n    cand_csv = []\n    if \"TEST_PRED01_PATH\" in globals() and globals()[\"TEST_PRED01_PATH\"] is not None:\n        cand_csv.append(Path(globals()[\"TEST_PRED01_PATH\"]))\n    if \"TEST_PROB_CSV_PATH\" in globals() and globals()[\"TEST_PROB_CSV_PATH\"] is not None:\n        cand_csv.append(Path(globals()[\"TEST_PROB_CSV_PATH\"]))\n\n    cand_csv += [\n        preds_dir / \"test_pred_01.csv\",\n        preds_dir / \"test_prob_ens.csv\",\n        art_dir / \"test_pred_01.csv\",\n        art_dir / \"test_prob_ens.csv\",\n    ]\n\n    for p in cand_csv:\n        if p.exists():\n            df = pd.read_csv(p, dtype={\"object_id\": str})\n            if \"object_id\" not in df.columns:\n                continue\n\n            # If binary file\n            colb = _detect_binary_col(df)\n            if colb is not None:\n                df = df.copy()\n                df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n                y = pd.to_numeric(df[colb], errors=\"coerce\").fillna(0).astype(int).clip(0,1).to_numpy()\n                return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prediction\": y}), \"bin\", f\"csv_bin({p})\"\n\n            # Else probability file\n            colp = _detect_prob_col(df)\n            if colp is None:\n                raise RuntimeError(f\"Cannot detect prob/binary column in: {p} | cols={list(df.columns)}\")\n            df = df.copy()\n            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n            prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n            if len(prob) != len(df):\n                raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n            return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob}), \"prob\", f\"csv_prob({p})\"\n\n    # ---- D) npy fallback ----\n    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n    p_ids = fix_dir / \"test_ids.npy\"\n    if not p_ids.exists():\n        raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat fixed_seq/test_ids.npy atau STAGE 10 export test_ids.\")\n    ids = _load_ids_npy(p_ids)\n    if len(ids) == 0:\n        raise RuntimeError(\"test_ids.npy kosong.\")\n\n    cand_npy = []\n    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n        cand_npy.append(Path(globals()[\"TEST_PROB_ENS_PATH\"]))\n    cand_npy += [preds_dir / \"test_prob_ens.npy\", art_dir / \"test_prob_ens.npy\"]\n\n    prob = None\n    used = None\n    for p in cand_npy:\n        if p.exists():\n            prob = _sanitize_prob(_as_1d_float32(np.load(p, allow_pickle=False)))\n            used = p\n            break\n    if prob is None:\n        raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n    if len(prob) != len(ids):\n        raise RuntimeError(f\"Length mismatch (NPY): test_prob={len(prob)} vs test_ids={len(ids)}\")\n\n    return pd.DataFrame({\"object_id\": ids, \"prob\": prob}), \"prob\", f\"npy({used}) + ids({p_ids})\"\n\ndef _load_best_threshold_fallback():\n    \"\"\"\n    Correctly parse thresholds from:\n      - Stage 9: OOF_DIR/threshold_tuning.json\n          * payload[\"default_best_thr\"][\"thr\"]\n          * payload[\"best_ge\"][\"best_thr_f1\"][\"thr\"]  (if exists)\n      - Eval v3.4/v3.5 summary: OOF_DIR/eval_summary.json\n          * payload[\"default_best_thr\"][\"thr\"]\n          * payload[\"best_ge\"][\"f1\"][\"thr\"]\n    \"\"\"\n    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n    oof_dir = Path(globals().get(\"OOF_DIR\", art_dir / \"oof\"))\n\n    cand = []\n    for k in [\"THR_JSON_PATH\", \"EVAL_SUMMARY_PATH\"]:\n        if k in globals() and globals()[k] is not None:\n            cand.append(Path(globals()[k]))\n    cand += [oof_dir / \"threshold_tuning.json\", oof_dir / \"eval_summary.json\"]\n\n    def _dig(obj, path_list):\n        cur = obj\n        for key in path_list:\n            if not isinstance(cur, dict) or key not in cur:\n                return None\n            cur = cur[key]\n        return cur\n\n    for p in cand:\n        obj = _try_load_json(p)\n        if not isinstance(obj, dict):\n            continue\n        name = Path(p).name\n\n        # generic: default_best_thr.thr\n        v = _dig(obj, [\"default_best_thr\", \"thr\"])\n        if v is not None:\n            try:\n                return float(v)\n            except Exception:\n                pass\n\n        if name == \"threshold_tuning.json\":\n            v = _dig(obj, [\"best_ge\", \"best_thr_f1\", \"thr\"])\n            if v is not None:\n                try:\n                    return float(v)\n                except Exception:\n                    pass\n\n        if name == \"eval_summary.json\":\n            v = _dig(obj, [\"best_ge\", \"f1\", \"thr\"])\n            if v is not None:\n                try:\n                    return float(v)\n                except Exception:\n                    pass\n\n        # last resort: scan known keys if user saved them\n        for k in [\"BEST_THR\", \"BEST_THR_F1\", \"BEST_THR_GE_F1\"]:\n            if k in obj:\n                try:\n                    return float(obj[k])\n                except Exception:\n                    pass\n\n    return None\n\n# ----------------------------\n# 1) Load prediction df\n# ----------------------------\ndf_pred, pred_mode, pred_src = _load_pred_df()\nif df_pred is None or df_pred.empty:\n    raise RuntimeError(\"df_pred empty (unexpected).\")\n\ndf_pred = df_pred.copy()\ndf_pred[\"object_id\"] = df_pred[\"object_id\"].apply(_norm_id)\n\n# strict: no duplicate ids\nif df_pred[\"object_id\"].duplicated().any():\n    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n\nif pred_mode == \"prob\":\n    p = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n    if not np.isfinite(p).all():\n        bad = int((~np.isfinite(p)).sum())\n        raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\n    df_pred[\"prob\"] = _sanitize_prob(p)\n\n    p2 = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n    print(\"[Stage 11] Loaded test predictions (PROB)\")\n    print(f\"- source: {pred_src}\")\n    print(f\"- N_pred={len(df_pred):,} | prob_mean={float(p2.mean()):.6f} | std={float(p2.std()):.6f} | min={float(p2.min()):.6f} | max={float(p2.max()):.6f}\")\nelse:\n    yb = pd.to_numeric(df_pred[\"prediction\"], errors=\"coerce\").fillna(0).astype(int).clip(0,1).to_numpy()\n    df_pred[\"prediction\"] = yb.astype(np.int8)\n    print(\"[Stage 11] Loaded test predictions (BINARY 0/1)\")\n    print(f\"- source: {pred_src}\")\n    print(f\"- N_pred={len(df_pred):,} | pos_pred={int(df_pred['prediction'].sum()):,} ({float(df_pred['prediction'].mean())*100:.6f}%)\")\n\n# ----------------------------\n# 2) Threshold selection (only if needed)\n# ----------------------------\nFORCE_THR = None  # set manual if you want, e.g. 0.37\n\nthr_src = None\nthr = None\n\nif pred_mode == \"prob\":\n    if FORCE_THR is not None:\n        thr = float(FORCE_THR); thr_src = \"FORCE_THR\"\n    elif \"BEST_THR_F1\" in globals() and globals()[\"BEST_THR_F1\"] is not None:\n        thr = float(globals()[\"BEST_THR_F1\"]); thr_src = \"globals(BEST_THR_F1)\"\n    elif \"BEST_THR\" in globals() and globals()[\"BEST_THR\"] is not None:\n        thr = float(globals()[\"BEST_THR\"]); thr_src = \"globals(BEST_THR)\"\n    else:\n        fb = _load_best_threshold_fallback()\n        if fb is not None:\n            thr = float(fb); thr_src = \"json_fallback(threshold_tuning/eval_summary)\"\n        else:\n            thr = 0.5; thr_src = \"default(0.5)\"\n    thr = float(np.clip(thr, 0.0, 1.0))\n\n# ----------------------------\n# 3) Align to sample_submission order + build output\n# ----------------------------\ndf_sub = df_sub.copy()\ndf_sub[\"object_id\"] = df_sub[\"object_id\"].apply(_norm_id)\n\nif df_sub[\"object_id\"].duplicated().any():\n    dup = df_sub.loc[df_sub[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n    raise ValueError(f\"sample_submission has duplicate object_id (unexpected). examples={dup}\")\n\nsample_ids = pd.Index(df_sub[\"object_id\"].tolist())\npred_ids = pd.Index(df_pred[\"object_id\"].tolist())\n\nin_pred = sample_ids.isin(pred_ids)\nin_sample = pred_ids.isin(sample_ids)\n\nprint(f\"[Stage 11] ID coverage check:\")\nprint(f\"- sample_in_pred = {int(in_pred.sum()):,} / {len(sample_ids):,}\")\nprint(f\"- pred_in_sample = {int(in_sample.sum()):,} / {len(pred_ids):,}\")\n\nif (~in_sample).any():\n    extra = pred_ids[~in_sample][:10].tolist()\n    print(f\"[Stage 11] WARN: predictions contain extra ids not in sample (show 10): {extra}\")\nif (~in_pred).any():\n    miss = sample_ids[~in_pred][:10].tolist()\n    print(f\"[Stage 11] WARN: sample contains ids missing in predictions (show 10): {miss}\")\n\n# Merge in sample order (STRICT)\ndf_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\", sort=False)\n\nif pred_mode == \"prob\":\n    if df_out[\"prob\"].isna().any():\n        missing_n = int(df_out[\"prob\"].isna().sum())\n        miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:10].tolist()\n        raise ValueError(\n            f\"Some sample_submission object_id have no prediction: missing_n={missing_n}. Examples: {miss_ids}\\n\"\n            \"Penyebab umum: object_id kebaca numeric (leading zero hilang) atau pred tidak lengkap.\"\n        )\n    df_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\nelse:\n    if df_out[\"prediction\"].isna().any():\n        missing_n = int(df_out[\"prediction\"].isna().sum())\n        miss_ids = df_out.loc[df_out[\"prediction\"].isna(), \"object_id\"].iloc[:10].tolist()\n        raise ValueError(\n            f\"Some sample_submission object_id have no binary prediction: missing_n={missing_n}. Examples: {miss_ids}\"\n        )\n    df_out[\"prediction\"] = pd.to_numeric(df_out[\"prediction\"], errors=\"coerce\").fillna(0).astype(int).clip(0,1).astype(np.int8)\n\ndf_out = df_out[[\"object_id\", \"prediction\"]]\n\n# Strict checks\nif len(df_out) != len(df_sub):\n    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\nif not df_out[\"object_id\"].equals(df_sub[\"object_id\"]):\n    raise RuntimeError(\"submission order mismatch with sample_submission (must be identical).\")\n\nu = set(np.unique(df_out[\"prediction\"].to_numpy()).tolist())\nif not u.issubset({0, 1}):\n    raise RuntimeError(f\"submission prediction contains values outside {{0,1}}: {sorted(list(u))}\")\n\npos_pred = int(df_out[\"prediction\"].sum())\nprint(\"\\n[Stage 11] SUBMISSION READY (BINARY 0/1)\")\nif pred_mode == \"prob\":\n    print(f\"- threshold_used={thr:.6f} | thr_source={thr_src}\")\nprint(f\"- rows={len(df_out):,} | pos_pred={pos_pred:,} ({pos_pred/max(len(df_out),1)*100:.6f}%)\")\n\n# ----------------------------\n# 4) Write files\n# ----------------------------\nSUB_DIR = Path(SUB_DIR)\nSUB_DIR.mkdir(parents=True, exist_ok=True)\n\nout_main = Path(\"/kaggle/working/submission.csv\")\nout_copy = SUB_DIR / \"submission.csv\"\n\ndf_out.to_csv(out_main, index=False)\ndf_out.to_csv(out_copy, index=False)\n\nprint(f\"- wrote: {out_main}\")\nprint(f\"- copy : {out_copy}\")\nprint(\"\\nPreview:\")\nprint(df_out.head(8).to_string(index=False))\n\nglobals().update({\n    \"SUBMISSION_PATH\": out_main,\n    \"SUBMISSION_COPY_PATH\": out_copy,\n    \"SUBMISSION_MODE\": \"binary\",\n    \"SUBMISSION_THRESHOLD\": (None if pred_mode != \"prob\" else float(thr)),\n    \"SUBMISSION_THRESHOLD_SOURCE\": (None if pred_mode != \"prob\" else thr_src),\n    \"SUBMISSION_PRED_SOURCE\": pred_src,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}