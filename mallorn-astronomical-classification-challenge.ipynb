{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14010596,"sourceType":"datasetVersion","datasetId":8925232}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:15.314932Z","iopub.execute_input":"2026-01-01T15:21:15.315201Z","iopub.status.idle":"2026-01-01T15:21:17.310517Z","shell.execute_reply.started":"2026-01-01T15:21:15.315174Z","shell.execute_reply":"2026-01-01T15:21:17.309491Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mallorn-dataset/sample_submission.csv\n/kaggle/input/mallorn-dataset/test_log.csv\n/kaggle/input/mallorn-dataset/train_log.csv\n/kaggle/input/mallorn-dataset/split_17/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_17/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_01/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_01/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_02/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_02/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_08/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_08/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_04/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_04/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_07/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_07/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_15/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_15/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_20/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_20/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_06/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_06/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_19/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_19/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_09/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_09/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_12/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_12/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_16/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_16/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_10/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_10/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_18/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_18/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_03/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_03/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_11/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_11/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_14/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_14/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_05/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_05/test_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_13/train_full_lightcurves.csv\n/kaggle/input/mallorn-dataset/split_13/test_full_lightcurves.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":" # Kaggle CPU Environment Setup","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 0 — Kaggle CPU Environment Setup (ONE CELL, SAFE + COHESIVE)\n# - Kaggle Web Notebook (CPU only)\n# - No heavy loading (no full lightcurve concat)\n# - Hard guards: paths exist, splits exist, required files exist\n# - Safe thread limits to avoid freeze/oversubscription\n# ============================================================\n\nimport os, sys, gc, random, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Quiet + deterministic (reduce noisy warnings, keep critical)\n# ----------------------------\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\nSEED = 2025\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\n# ----------------------------\n# 1) CPU thread limits (anti-freeze on Kaggle CPU)\n# ----------------------------\n# Prevent BLAS/OMP oversubscription which can make Kaggle CPU notebooks crawl/hang.\nos.environ.setdefault(\"OMP_NUM_THREADS\", \"2\")\nos.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"2\")\nos.environ.setdefault(\"MKL_NUM_THREADS\", \"2\")\nos.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"2\")\nos.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"2\")\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\ntry:\n    import torch\n    torch.manual_seed(SEED)\n    torch.set_num_threads(2)\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    torch = None  # torch may be unavailable in some environments\n\n# ----------------------------\n# 2) Paths (as you listed)\n# ----------------------------\nDATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\n\nPATHS = {\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n}\n\n# ----------------------------\n# 3) Working directories (writeable on Kaggle)\n# ----------------------------\nWORKDIR = Path(\"/kaggle/working\")\nRUN_DIR = WORKDIR / \"mallorn_run\"\nART_DIR = RUN_DIR / \"artifacts\"\nCKPT_DIR = RUN_DIR / \"checkpoints\"\nOOF_DIR = RUN_DIR / \"oof\"\nSUB_DIR = RUN_DIR / \"submissions\"\nLOG_DIR = RUN_DIR / \"logs\"\n\nfor d in [RUN_DIR, ART_DIR, CKPT_DIR, OOF_DIR, SUB_DIR, LOG_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 4) Hard guards: files must exist\n# ----------------------------\ndef _must_exist(p: Path, what: str):\n    if not p.exists():\n        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n\n_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n_must_exist(PATHS[\"TRAIN_LOG\"], \"train_log.csv\")\n_must_exist(PATHS[\"TEST_LOG\"],  \"test_log.csv\")\n\n# Validate split folders + key files inside\nmissing_splits = [s for s in PATHS[\"SPLITS\"] if not s.exists()]\nif missing_splits:\n    # Show first few to help debug, then fail hard\n    sample = \"\\n\".join(str(x) for x in missing_splits[:5])\n    raise FileNotFoundError(f\"Some split folders are missing (showing up to 5):\\n{sample}\")\n\n# Verify presence of lightcurve csvs per split (train/test)\nbad = []\nfor sd in PATHS[\"SPLITS\"]:\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    if (not tr.exists()) or (not te.exists()):\n        bad.append((sd.name, tr.exists(), te.exists()))\nif bad:\n    msg = \"\\n\".join([f\"- {name}: train={tr_ok}, test={te_ok}\" for name, tr_ok, te_ok in bad[:10]])\n    raise FileNotFoundError(\n        \"Some split lightcurve files are missing (showing up to 10):\\n\"\n        f\"{msg}\"\n    )\n\n# ----------------------------\n# 5) Load small metadata only (safe on CPU)\n# ----------------------------\n# Use dtype hints to reduce parsing warnings and memory.\ndf_sub = pd.read_csv(PATHS[\"SAMPLE_SUB\"])\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission columns must include object_id,prediction. Found: {list(df_sub.columns)}\")\n\ndf_train_log = pd.read_csv(PATHS[\"TRAIN_LOG\"])\ndf_test_log  = pd.read_csv(PATHS[\"TEST_LOG\"])\n\n# Basic column sanity (do not assume perfect casing)\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndf_train_log = _norm_cols(df_train_log)\ndf_test_log  = _norm_cols(df_test_log)\n\nneed_train = {\"object_id\", \"EBV\", \"Z\", \"split\", \"target\"}\nneed_test  = {\"object_id\", \"EBV\", \"Z\", \"split\"}  # Z_err may or may not exist; handle later\nmissing_train = sorted(list(need_train - set(df_train_log.columns)))\nmissing_test  = sorted(list(need_test - set(df_test_log.columns)))\n\nif missing_train:\n    raise ValueError(f\"train_log.csv missing required columns: {missing_train}\")\nif missing_test:\n    raise ValueError(f\"test_log.csv missing required columns: {missing_test}\")\n\n# Ensure split names align with folders you have (split_01..split_20)\n# (Keep as string; later we use it for routing.)\ndf_train_log[\"split\"] = df_train_log[\"split\"].astype(str).str.strip()\ndf_test_log[\"split\"]  = df_test_log[\"split\"].astype(str).str.strip()\n\n# Object_id uniqueness check\nif df_train_log[\"object_id\"].duplicated().any():\n    dup_n = int(df_train_log[\"object_id\"].duplicated().sum())\n    raise ValueError(f\"train_log.csv has duplicated object_id rows: {dup_n} duplicates found.\")\nif df_test_log[\"object_id\"].duplicated().any():\n    dup_n = int(df_test_log[\"object_id\"].duplicated().sum())\n    raise ValueError(f\"test_log.csv has duplicated object_id rows: {dup_n} duplicates found.\")\n\n# Quick target sanity\nif not set(pd.unique(df_train_log[\"target\"])).issubset({0, 1}):\n    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(pd.unique(df_train_log['target']).tolist())}\")\n\npos = int((df_train_log[\"target\"] == 1).sum())\nneg = int((df_train_log[\"target\"] == 0).sum())\ntot = int(len(df_train_log))\n\n# Verify all submission object_id exist in test_log (or warn)\nsub_missing = set(df_sub[\"object_id\"]) - set(df_test_log[\"object_id\"])\nif sub_missing:\n    # do not fail hard; Kaggle sample_submission sometimes includes all test ids, so this is serious\n    # We'll fail hard to avoid silent mismatch.\n    sample = list(sub_missing)[:5]\n    raise ValueError(f\"sample_submission has object_id not found in test_log (showing up to 5): {sample}\")\n\n# ----------------------------\n# 6) Summarize environment (minimal, helpful)\n# ----------------------------\nprint(\"ENV OK (Kaggle CPU)\")\nprint(f\"- Python: {sys.version.split()[0]}\")\nprint(f\"- Numpy:  {np.__version__}\")\nprint(f\"- Pandas: {pd.__version__}\")\nif torch is not None:\n    print(f\"- Torch:  {torch.__version__} | CUDA available: {torch.cuda.is_available()}\")\nelse:\n    print(\"- Torch:  not available\")\n\nprint(\"\\nDATA OK\")\nprint(f\"- train_log: {len(df_train_log):,} objects | pos(TDE)={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.2f}%\")\nprint(f\"- test_log:  {len(df_test_log):,} objects\")\nprint(f\"- submission template rows: {len(df_sub):,}\")\nprint(f\"- splits detected: {len(PATHS['SPLITS'])} folders (split_01..split_20)\")\n\n# Optional: save a tiny config snapshot for reproducibility\ncfg_path = RUN_DIR / \"env_config.txt\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(f\"SEED={SEED}\\n\")\n    f.write(f\"DATA_ROOT={DATA_ROOT}\\n\")\n    f.write(f\"WORKDIR={WORKDIR}\\n\")\n    f.write(\"THREADS:\\n\")\n    for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n        f.write(f\"  {k}={os.environ.get(k,'')}\\n\")\n\n# Keep objects in globals for next stages\nglobals().update({\n    \"SEED\": SEED,\n    \"PATHS\": PATHS,\n    \"RUN_DIR\": RUN_DIR,\n    \"ART_DIR\": ART_DIR,\n    \"CKPT_DIR\": CKPT_DIR,\n    \"OOF_DIR\": OOF_DIR,\n    \"SUB_DIR\": SUB_DIR,\n    \"LOG_DIR\": LOG_DIR,\n    \"df_sub\": df_sub,\n    \"df_train_log\": df_train_log,\n    \"df_test_log\": df_test_log,\n})\n\ngc.collect()\nprint(f\"\\nSaved env snapshot: {cfg_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:17.312124Z","iopub.execute_input":"2026-01-01T15:21:17.312695Z","iopub.status.idle":"2026-01-01T15:21:23.587778Z","shell.execute_reply.started":"2026-01-01T15:21:17.312661Z","shell.execute_reply":"2026-01-01T15:21:23.586642Z"}},"outputs":[{"name":"stdout","text":"ENV OK (Kaggle CPU)\n- Python: 3.12.12\n- Numpy:  2.0.2\n- Pandas: 2.2.2\n- Torch:  2.8.0+cu126 | CUDA available: False\n\nDATA OK\n- train_log: 3,043 objects | pos(TDE)=148 | neg=2,895 | pos%=4.86%\n- test_log:  7,135 objects\n- submission template rows: 7,135\n- splits detected: 20 folders (split_01..split_20)\n\nSaved env snapshot: /kaggle/working/mallorn_run/env_config.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Verify Dataset Paths & Split Discovery","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — Verify Dataset Paths & Split Discovery (ONE CELL, CPU-SAFE)\n# - Uses globals from STAGE 0: PATHS, df_train_log, df_test_log\n# - Normalizes split names -> \"split_XX\"\n# - Verifies: split folders + required files + lightcurve column sanity (nrows only)\n# - Summarizes: object counts per split + file sizes\n# ============================================================\n\nimport os, re, gc\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require STAGE 0 globals\n# ----------------------------\nfor need in [\"PATHS\", \"df_train_log\", \"df_test_log\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (Kaggle CPU Environment Setup).\")\n\nDATA_ROOT = PATHS[\"DATA_ROOT\"]\nSPLIT_DIRS = {p.name: p for p in PATHS[\"SPLITS\"]}  # split_01..split_20 -> Path\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\ndef normalize_split_name(x: str) -> str:\n    \"\"\"\n    Convert various split formats to canonical 'split_XX'.\n    Accepts: 'split_01', '01', '1', 'split_1', 'SPLIT_01', etc.\n    \"\"\"\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip().lower()\n    # already canonical?\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s)\n    if m:\n        k = int(m.group(1))\n        return f\"split_{k:02d}\"\n    # maybe just digits\n    m = re.fullmatch(r\"(\\d{1,2})\", s)\n    if m:\n        k = int(m.group(1))\n        return f\"split_{k:02d}\"\n    # maybe splitXX without underscore\n    m = re.fullmatch(r\"split(\\d{1,2})\", s)\n    if m:\n        k = int(m.group(1))\n        return f\"split_{k:02d}\"\n    # fallback: keep raw, but mark as unknown\n    return s\n\ndef must_exist(p: Path, what: str):\n    if not p.exists():\n        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n\ndef sizeof_mb(p: Path) -> float:\n    try:\n        return p.stat().st_size / (1024**2)\n    except Exception:\n        return float(\"nan\")\n\ndef quick_check_lightcurve_csv(p: Path, nrows: int = 5):\n    \"\"\"\n    Read tiny head to validate columns without heavy IO.\n    \"\"\"\n    df = pd.read_csv(p, nrows=nrows)\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\nREQ_LC_COLS = {\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"}\n\n# ----------------------------\n# 2) Normalize split columns in logs (in-place for later stages)\n# ----------------------------\nfor df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n    if \"split\" not in df.columns:\n        raise ValueError(f\"{name} missing 'split' column.\")\n    df[\"split\"] = df[\"split\"].astype(str).str.strip()\n    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n\n# ----------------------------\n# 3) Verify split values referenced by logs exist on disk\n# ----------------------------\ntrain_splits = set(df_train_log[\"split\"].unique())\ntest_splits  = set(df_test_log[\"split\"].unique())\ndisk_splits  = set(SPLIT_DIRS.keys())\n\nbad_train = sorted([s for s in train_splits if s not in disk_splits])\nbad_test  = sorted([s for s in test_splits  if s not in disk_splits])\n\nif bad_train:\n    raise FileNotFoundError(f\"train_log references split(s) not found on disk: {bad_train[:10]}\")\nif bad_test:\n    raise FileNotFoundError(f\"test_log references split(s) not found on disk: {bad_test[:10]}\")\n\n# Also verify we actually have 20 splits (as expected)\nif len(disk_splits) != 20:\n    # not always fatal, but better to fail early for this competition\n    raise RuntimeError(f\"Expected 20 split folders, found {len(disk_splits)}: {sorted(list(disk_splits))}\")\n\n# ----------------------------\n# 4) Verify required files per split exist\n# ----------------------------\nmissing_files = []\nsplit_file_info = []\n\nfor split_name in sorted(disk_splits):\n    sd = SPLIT_DIRS[split_name]\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    if not tr.exists():\n        missing_files.append(str(tr))\n    if not te.exists():\n        missing_files.append(str(te))\n    split_file_info.append((split_name, sizeof_mb(tr), sizeof_mb(te)))\n\nif missing_files:\n    sample = \"\\n\".join(missing_files[:10])\n    raise FileNotFoundError(f\"Some lightcurve files missing (showing up to 10):\\n{sample}\")\n\n# ----------------------------\n# 5) Lightweight column sanity check (read only a few rows per split)\n# ----------------------------\ncol_issues = []\nfilter_issues = []\n\nfor split_name in sorted(disk_splits):\n    sd = SPLIT_DIRS[split_name]\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n\n    dtr = quick_check_lightcurve_csv(tr, nrows=5)\n    dte = quick_check_lightcurve_csv(te, nrows=5)\n\n    # Columns present?\n    miss_tr = sorted(list(REQ_LC_COLS - set(dtr.columns)))\n    miss_te = sorted(list(REQ_LC_COLS - set(dte.columns)))\n    if miss_tr or miss_te:\n        col_issues.append((split_name, miss_tr, miss_te, list(dtr.columns), list(dte.columns)))\n\n    # Filter values sanity (tiny sample)\n    if \"Filter\" in dtr.columns:\n        vals = set(dtr[\"Filter\"].astype(str).str.strip().str.lower().unique())\n        badf = sorted([v for v in vals if v not in {\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"}])\n        if badf:\n            filter_issues.append((split_name, \"train\", badf, sorted(list(vals))))\n    if \"Filter\" in dte.columns:\n        vals = set(dte[\"Filter\"].astype(str).str.strip().str.lower().unique())\n        badf = sorted([v for v in vals if v not in {\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"}])\n        if badf:\n            filter_issues.append((split_name, \"test\", badf, sorted(list(vals))))\n\nif col_issues:\n    # Print one detailed example then fail hard (structure mismatch)\n    s, miss_tr, miss_te, cols_tr, cols_te = col_issues[0]\n    raise ValueError(\n        \"Lightcurve column mismatch detected.\\n\"\n        f\"Example split: {s}\\n\"\n        f\"Missing in train_full_lightcurves.csv: {miss_tr}\\n\"\n        f\"Missing in test_full_lightcurves.csv : {miss_te}\\n\"\n        f\"Train columns: {cols_tr}\\n\"\n        f\"Test columns : {cols_te}\\n\"\n    )\n\nif filter_issues:\n    # Not always fatal, but usually indicates whitespace/format issues. Fail early.\n    ex = filter_issues[0]\n    raise ValueError(\n        \"Unexpected Filter values detected (example):\\n\"\n        f\"split={ex[0]} file={ex[1]} bad={ex[2]} all_sampled={ex[3]}\\n\"\n        \"Fix by stripping/lowercasing Filter during preprocessing.\"\n    )\n\n# ----------------------------\n# 6) Summaries (counts per split, file sizes)\n# ----------------------------\ntrain_counts = df_train_log[\"split\"].value_counts().to_dict()\ntest_counts  = df_test_log[\"split\"].value_counts().to_dict()\n\nprint(\"SPLIT DISCOVERY OK\")\nprint(f\"- DATA_ROOT: {DATA_ROOT}\")\nprint(f\"- Splits on disk: {len(disk_splits)} (split_01..split_20)\")\n\nprint(\"\\nOBJECT COUNTS PER SPLIT (from logs)\")\nfor s in sorted(disk_splits):\n    print(f\"- {s}: train_objects={train_counts.get(s,0):,} | test_objects={test_counts.get(s,0):,}\")\n\nprint(\"\\nLIGHTCURVE FILE SIZES (MB)\")\nfor s, mb_tr, mb_te in split_file_info:\n    print(f\"- {s}: train_full={mb_tr:8.1f} MB | test_full={mb_te:8.1f} MB\")\n\n# ----------------------------\n# 7) Export split index for later stages (routing + loops)\n# ----------------------------\n# Useful for downstream: stable split list + mapping\nSPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\nglobals().update({\n    \"DATA_ROOT\": DATA_ROOT,\n    \"SPLIT_DIRS\": SPLIT_DIRS,\n    \"SPLIT_LIST\": SPLIT_LIST,\n})\n\ngc.collect()\nprint(\"\\nStage 1 complete: splits ready for split-wise preprocessing.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:23.590555Z","iopub.execute_input":"2026-01-01T15:21:23.590894Z","iopub.status.idle":"2026-01-01T15:21:24.360858Z","shell.execute_reply.started":"2026-01-01T15:21:23.590864Z","shell.execute_reply":"2026-01-01T15:21:24.359574Z"}},"outputs":[{"name":"stdout","text":"SPLIT DISCOVERY OK\n- DATA_ROOT: /kaggle/input/mallorn-dataset\n- Splits on disk: 20 (split_01..split_20)\n\nOBJECT COUNTS PER SPLIT (from logs)\n- split_01: train_objects=155 | test_objects=364\n- split_02: train_objects=170 | test_objects=414\n- split_03: train_objects=138 | test_objects=338\n- split_04: train_objects=145 | test_objects=332\n- split_05: train_objects=165 | test_objects=375\n- split_06: train_objects=155 | test_objects=374\n- split_07: train_objects=165 | test_objects=398\n- split_08: train_objects=162 | test_objects=387\n- split_09: train_objects=128 | test_objects=289\n- split_10: train_objects=144 | test_objects=331\n- split_11: train_objects=146 | test_objects=325\n- split_12: train_objects=155 | test_objects=353\n- split_13: train_objects=143 | test_objects=379\n- split_14: train_objects=154 | test_objects=351\n- split_15: train_objects=158 | test_objects=342\n- split_16: train_objects=155 | test_objects=354\n- split_17: train_objects=153 | test_objects=351\n- split_18: train_objects=152 | test_objects=345\n- split_19: train_objects=147 | test_objects=375\n- split_20: train_objects=153 | test_objects=358\n\nLIGHTCURVE FILE SIZES (MB)\n- split_01: train_full=     1.4 MB | test_full=     3.1 MB\n- split_02: train_full=     1.3 MB | test_full=     3.7 MB\n- split_03: train_full=     1.1 MB | test_full=     2.8 MB\n- split_04: train_full=     1.2 MB | test_full=     2.7 MB\n- split_05: train_full=     1.3 MB | test_full=     3.1 MB\n- split_06: train_full=     1.3 MB | test_full=     2.9 MB\n- split_07: train_full=     1.3 MB | test_full=     3.4 MB\n- split_08: train_full=     1.3 MB | test_full=     3.2 MB\n- split_09: train_full=     1.0 MB | test_full=     2.4 MB\n- split_10: train_full=     1.3 MB | test_full=     2.6 MB\n- split_11: train_full=     1.2 MB | test_full=     2.6 MB\n- split_12: train_full=     1.3 MB | test_full=     2.8 MB\n- split_13: train_full=     1.2 MB | test_full=     3.3 MB\n- split_14: train_full=     1.3 MB | test_full=     3.0 MB\n- split_15: train_full=     1.2 MB | test_full=     2.7 MB\n- split_16: train_full=     1.3 MB | test_full=     3.0 MB\n- split_17: train_full=     1.2 MB | test_full=     3.1 MB\n- split_18: train_full=     1.1 MB | test_full=     2.7 MB\n- split_19: train_full=     1.1 MB | test_full=     2.9 MB\n- split_20: train_full=     1.2 MB | test_full=     3.0 MB\n\nStage 1 complete: splits ready for split-wise preprocessing.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Load and Validate Train/Test Logs","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — Load and Validate Train/Test Logs (ONE CELL, CPU-SAFE)\n# - Kaggle CPU: ringan, tanpa load full lightcurves\n# - Output:\n#   * df_train_meta, df_test_meta  (index=object_id, bersih & siap dipakai)\n#   * id2split_train, id2split_test (routing cepat ke split folder)\n#   * artifacts/train_log_clean.parquet, artifacts/test_log_clean.parquet\n# ============================================================\n\nimport os, re, gc, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 0/1 globals\n# ----------------------------\nfor need in [\"PATHS\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\nif \"SPLIT_DIRS\" not in globals():\n    raise RuntimeError(\"Missing `SPLIT_DIRS`. Jalankan STAGE 1 (Verify Dataset Paths & Split Discovery) dulu.\")\n\nTRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\nTEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n\ndef _normalize_split_name(x: str) -> str:\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    s = str(x).strip().lower()\n    m = re.fullmatch(r\"split_(\\d{1,2})\", s)\n    if m:\n        k = int(m.group(1))\n        return f\"split_{k:02d}\"\n    m = re.fullmatch(r\"(\\d{1,2})\", s)\n    if m:\n        k = int(m.group(1))\n        return f\"split_{k:02d}\"\n    m = re.fullmatch(r\"split(\\d{1,2})\", s)\n    if m:\n        k = int(m.group(1))\n        return f\"split_{k:02d}\"\n    return s\n\ndef _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef _coerce_numeric(df: pd.DataFrame, col: str) -> None:\n    if col not in df.columns:\n        return\n    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# ----------------------------\n# 1) Load logs (fresh read untuk konsistensi)\n# ----------------------------\ndf_train = pd.read_csv(TRAIN_LOG_PATH)\ndf_test  = pd.read_csv(TEST_LOG_PATH)\n\ndf_train = _norm_cols(df_train)\ndf_test  = _norm_cols(df_test)\n\n# ----------------------------\n# 2) Required columns check\n# ----------------------------\nreq_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\nreq_train  = req_common | {\"target\"}\nreq_test   = req_common\n\nmiss_train = sorted(list(req_train - set(df_train.columns)))\nmiss_test  = sorted(list(req_test  - set(df_test.columns)))\n\nif miss_train:\n    raise ValueError(f\"train_log.csv missing required columns: {miss_train} | found={list(df_train.columns)}\")\nif miss_test:\n    raise ValueError(f\"test_log.csv missing required columns: {miss_test} | found={list(df_test.columns)}\")\n\n# ----------------------------\n# 3) Basic cleaning\n# ----------------------------\n# object_id string\ndf_train[\"object_id\"] = df_train[\"object_id\"].astype(str).str.strip()\ndf_test[\"object_id\"]  = df_test[\"object_id\"].astype(str).str.strip()\n\n# split canonical\ndf_train[\"split\"] = df_train[\"split\"].astype(str).str.strip().map(_normalize_split_name)\ndf_test[\"split\"]  = df_test[\"split\"].astype(str).str.strip().map(_normalize_split_name)\n\n# numeric coercion\nfor c in [\"EBV\", \"Z\", \"Z_err\"]:\n    _coerce_numeric(df_train, c)\n    _coerce_numeric(df_test, c)\n\n# target binary int\ndf_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\nif df_train[\"target\"].isna().any():\n    bad_n = int(df_train[\"target\"].isna().sum())\n    raise ValueError(f\"train_log target has NaN after coercion: {bad_n} rows. Cek isi target di train_log.csv.\")\ndf_train[\"target\"] = df_train[\"target\"].astype(int)\nuniq_t = set(df_train[\"target\"].unique().tolist())\nif not uniq_t.issubset({0, 1}):\n    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n\n# ----------------------------\n# 4) Duplicates check (hard fail to avoid silent leakage)\n# ----------------------------\nif df_train[\"object_id\"].duplicated().any():\n    dup = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].iloc[:5].tolist()\n    raise ValueError(f\"Duplicated object_id in train_log (examples): {dup}\")\nif df_test[\"object_id\"].duplicated().any():\n    dup = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].iloc[:5].tolist()\n    raise ValueError(f\"Duplicated object_id in test_log (examples): {dup}\")\n\n# ----------------------------\n# 5) Missing values handling (anti-error downstream)\n# ----------------------------\n# Buat flag missing agar tidak hilang informasi\nfor df in [df_train, df_test]:\n    for c in [\"EBV\", \"Z\"]:\n        df[f\"{c}_missing\"] = df[c].isna().astype(np.int8)\n\n# Isi EBV NaN -> 0.0 (fisiknya bisa dianggap minimal dust bila tidak ada)\nif df_train[\"EBV\"].isna().any():\n    df_train[\"EBV\"] = df_train[\"EBV\"].fillna(0.0)\nif df_test[\"EBV\"].isna().any():\n    df_test[\"EBV\"] = df_test[\"EBV\"].fillna(0.0)\n\n# Isi Z NaN -> median per split (lebih aman daripada global median)\n# (Kalau tidak ada NaN, ini tidak mengubah apa-apa)\ndef _fill_z_by_split(df: pd.DataFrame):\n    if not df[\"Z\"].isna().any():\n        return df\n    df = df.copy()\n    med = df.groupby(\"split\")[\"Z\"].median()\n    global_med = float(df[\"Z\"].median()) if df[\"Z\"].notna().any() else 0.0\n    def _fill_row(row):\n        if pd.isna(row[\"Z\"]):\n            m = med.get(row[\"split\"], np.nan)\n            return float(m) if not pd.isna(m) else global_med\n        return row[\"Z\"]\n    df[\"Z\"] = df.apply(_fill_row, axis=1)\n    return df\n\ndf_train = _fill_z_by_split(df_train)\ndf_test  = _fill_z_by_split(df_test)\n\n# Untuk train: kalau Z_err tidak ada, buat kolomnya (konsisten untuk model)\nif \"Z_err\" not in df_train.columns:\n    df_train[\"Z_err\"] = np.nan\n# Untuk test: kalau tidak ada Z_err, tetap buat agar pipeline tidak error\nif \"Z_err\" not in df_test.columns:\n    df_test[\"Z_err\"] = np.nan\n\n# Flag photometric redshift: train spec-z (anggap 0), test photo-z (anggap 1)\ndf_train[\"is_photoz\"] = np.int8(0)\ndf_test[\"is_photoz\"]  = np.int8(1)\n\n# Z_err_missing flag + fill NaN Z_err -> 0.0 agar numeric stabil\nfor df in [df_train, df_test]:\n    df[\"Z_err_missing\"] = df[\"Z_err\"].isna().astype(np.int8)\n    df[\"Z_err\"] = df[\"Z_err\"].fillna(0.0)\n\n# ----------------------------\n# 6) Split validity check against disk splits\n# ----------------------------\ndisk_splits = set(SPLIT_DIRS.keys())\nbad_train_s = sorted([s for s in set(df_train[\"split\"].unique()) if s not in disk_splits])\nbad_test_s  = sorted([s for s in set(df_test[\"split\"].unique())  if s not in disk_splits])\nif bad_train_s:\n    raise FileNotFoundError(f\"train_log references unknown split(s): {bad_train_s[:10]}\")\nif bad_test_s:\n    raise FileNotFoundError(f\"test_log references unknown split(s): {bad_test_s[:10]}\")\n\n# ----------------------------\n# 7) Build meta tables (index=object_id) + routing dicts\n# ----------------------------\n# Simpan hanya kolom yang relevan untuk tahap berikutnya (lebih ringan)\nkeep_train = [\"object_id\",\"split\",\"EBV\",\"Z\",\"Z_err\",\"EBV_missing\",\"Z_missing\",\"Z_err_missing\",\"is_photoz\",\"target\"]\nkeep_test  = [\"object_id\",\"split\",\"EBV\",\"Z\",\"Z_err\",\"EBV_missing\",\"Z_missing\",\"Z_err_missing\",\"is_photoz\"]\n\n# Jika SpecType ada, simpan juga untuk analisis (tidak wajib untuk model biner)\nif \"SpecType\" in df_train.columns:\n    keep_train.append(\"SpecType\")\n\ndf_train_meta = df_train[keep_train].copy()\ndf_test_meta  = df_test[keep_test].copy()\n\ndf_train_meta = df_train_meta.set_index(\"object_id\", drop=True).sort_index()\ndf_test_meta  = df_test_meta.set_index(\"object_id\", drop=True).sort_index()\n\nid2split_train = df_train_meta[\"split\"].to_dict()\nid2split_test  = df_test_meta[\"split\"].to_dict()\n\n# ----------------------------\n# 8) Save cleaned logs (fast reuse)\n# ----------------------------\ntrain_out = Path(ART_DIR) / \"train_log_clean.parquet\"\ntest_out  = Path(ART_DIR) / \"test_log_clean.parquet\"\ndf_train_meta.to_parquet(train_out, index=True)\ndf_test_meta.to_parquet(test_out, index=True)\n\n# Save split stats (useful debugging)\nsplit_stats = pd.DataFrame({\n    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(sorted(disk_splits)).fillna(0).astype(int),\n    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(sorted(disk_splits)).fillna(0).astype(int),\n})\nsplit_stats.index.name = \"split\"\nsplit_stats_path = Path(ART_DIR) / \"split_stats.csv\"\nsplit_stats.to_csv(split_stats_path)\n\n# ----------------------------\n# 9) Print summary (minimal but informative)\n# ----------------------------\npos = int((df_train_meta[\"target\"] == 1).sum())\nneg = int((df_train_meta[\"target\"] == 0).sum())\ntot = int(len(df_train_meta))\nprint(\"LOGS OK (clean + validated)\")\nprint(f\"- train objects: {tot:,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.3f}%\")\nprint(f\"- test objects : {len(df_test_meta):,}\")\nprint(f\"- saved: {train_out}\")\nprint(f\"- saved: {test_out}\")\nprint(f\"- saved: {split_stats_path}\")\n\n# Keep in globals for next stages\nglobals().update({\n    \"df_train_meta\": df_train_meta,\n    \"df_test_meta\": df_test_meta,\n    \"id2split_train\": id2split_train,\n    \"id2split_test\": id2split_test,\n    \"split_stats\": split_stats\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:24.362433Z","iopub.execute_input":"2026-01-01T15:21:24.362753Z","iopub.status.idle":"2026-01-01T15:21:24.880556Z","shell.execute_reply.started":"2026-01-01T15:21:24.362725Z","shell.execute_reply":"2026-01-01T15:21:24.879332Z"}},"outputs":[{"name":"stdout","text":"LOGS OK (clean + validated)\n- train objects: 3,043 | pos(TDE)=148 | neg=2,895 | pos%=4.864%\n- test objects : 7,135\n- saved: /kaggle/working/mallorn_run/artifacts/train_log_clean.parquet\n- saved: /kaggle/working/mallorn_run/artifacts/test_log_clean.parquet\n- saved: /kaggle/working/mallorn_run/artifacts/split_stats.csv\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Lightcurve Loading Strategy","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — Lightcurve Loading Strategy (ONE CELL, Kaggle CPU-SAFE)\n# - Split-wise file mapping + chunked reader utilities (no full concat)\n# - Builds:\n#   * SPLIT_FILES: {split_XX: {\"train\": Path, \"test\": Path}}\n#   * train_ids_by_split / test_ids_by_split: routing object_ids per split\n#   * iter_lightcurve_chunks(): generator read_csv(chunksize=...)\n#   * load_object_lightcurve(): debug-safe per-object extraction (streaming)\n# - Saves:\n#   * artifacts/split_file_manifest.csv\n#   * artifacts/object_counts_by_split.csv\n# ============================================================\n\nimport gc, re\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n\n# ----------------------------\n# 1) Build split file mapping (train/test lightcurves)\n# ----------------------------\nSPLIT_FILES = {}\nfor s in SPLIT_LIST:\n    sd = SPLIT_DIRS[s]\n    tr = sd / \"train_full_lightcurves.csv\"\n    te = sd / \"test_full_lightcurves.csv\"\n    if (not tr.exists()) or (not te.exists()):\n        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n\n# Save split file manifest (helps debug path issues)\nmanifest = []\nfor s in SPLIT_LIST:\n    manifest.append({\n        \"split\": s,\n        \"train_path\": str(SPLIT_FILES[s][\"train\"]),\n        \"test_path\": str(SPLIT_FILES[s][\"test\"]),\n        \"train_mb\": SPLIT_FILES[s][\"train\"].stat().st_size / (1024**2),\n        \"test_mb\":  SPLIT_FILES[s][\"test\"].stat().st_size / (1024**2),\n    })\ndf_manifest = pd.DataFrame(manifest).sort_values(\"split\")\nmanifest_path = Path(ART_DIR) / \"split_file_manifest.csv\"\ndf_manifest.to_csv(manifest_path, index=False)\n\n# ----------------------------\n# 2) Build object routing by split (VERY important for split-wise processing)\n# ----------------------------\ntrain_ids_by_split = {s: [] for s in SPLIT_LIST}\ntest_ids_by_split  = {s: [] for s in SPLIT_LIST}\n\n# df_train_meta/df_test_meta index is object_id (from STAGE 2)\nfor oid, row in df_train_meta[[\"split\"]].itertuples():\n    train_ids_by_split[row].append(oid)\nfor oid, row in df_test_meta[[\"split\"]].itertuples():\n    test_ids_by_split[row].append(oid)\n\n# Object counts per split (save)\ndf_counts = pd.DataFrame({\n    \"split\": SPLIT_LIST,\n    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n})\ncounts_path = Path(ART_DIR) / \"object_counts_by_split.csv\"\ndf_counts.to_csv(counts_path, index=False)\n\n# ----------------------------\n# 3) Column normalization & dtypes (memory safe)\n# ----------------------------\n# We canonicalize to: object_id, mjd, flux, flux_err, filter\nLC_RENAME = {\n    \"Time (MJD)\": \"mjd\",\n    \"Time(MJD)\": \"mjd\",\n    \"Time\": \"mjd\",\n    \"Flux\": \"flux\",\n    \"Flux_err\": \"flux_err\",\n    \"FluxErr\": \"flux_err\",\n    \"Filter\": \"filter\",\n    \"object_id\": \"object_id\",\n    \"ObjectID\": \"object_id\",\n}\n# Read only columns we need (safe). If dataset has extra columns, we ignore them.\nUSECOLS_RAW = [\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"]\n# Dtypes (object_id str, filter category-like but keep as string to avoid pandas category pitfalls in chunks)\nDTYPES = {\n    \"object_id\": \"string\",\n    \"Flux\": \"float32\",\n    \"Flux_err\": \"float32\",\n    \"Filter\": \"string\",\n    # Time (MJD) sometimes float64; float32 is usually ok for ML. Keep float32 for memory.\n    \"Time (MJD)\": \"float32\",\n}\n\ndef _normalize_lc_chunk(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = [c.strip() for c in df.columns]\n    # rename known columns\n    df = df.rename(columns={c: LC_RENAME.get(c, c) for c in df.columns})\n    # enforce required canonical columns exist\n    need = {\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"}\n    missing = sorted(list(need - set(df.columns)))\n    if missing:\n        raise ValueError(f\"Lightcurve chunk missing required columns after rename: {missing}. Found: {list(df.columns)}\")\n    # trim/normalize filter values\n    df[\"filter\"] = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n    # object_id cleanup\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n    return df[[\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]]\n\n# ----------------------------\n# 4) Chunked readers (core strategy on Kaggle CPU)\n# ----------------------------\ndef iter_lightcurve_chunks(split_name: str, which: str, chunksize: int = 400_000):\n    \"\"\"\n    Stream read a split lightcurve CSV in chunks.\n    which: 'train' or 'test'\n    yields normalized chunks with canonical columns:\n      object_id, mjd, flux, flux_err, filter\n    \"\"\"\n    if split_name not in SPLIT_FILES:\n        raise KeyError(f\"Unknown split_name={split_name}.\")\n    if which not in (\"train\", \"test\"):\n        raise ValueError(\"which must be 'train' or 'test'\")\n    p = SPLIT_FILES[split_name][which]\n\n    # Use usecols only if they exist; handle header differences robustly:\n    # Read header once (cheap), then decide usecols/dtypes mapping.\n    header = pd.read_csv(p, nrows=0)\n    cols = [c.strip() for c in header.columns]\n    # map raw names we expect\n    raw_usecols = [c for c in USECOLS_RAW if c in cols]\n    # also accept alternative time column names\n    if \"Time (MJD)\" not in raw_usecols:\n        for alt in [\"Time(MJD)\", \"Time\"]:\n            if alt in cols:\n                raw_usecols = [c if c != \"Time (MJD)\" else alt for c in raw_usecols]\n                break\n\n    if set(raw_usecols) != set([c for c in raw_usecols]):  # no-op guard\n        pass\n\n    # dtypes must match chosen time column key\n    dtypes = {}\n    for k, v in DTYPES.items():\n        if k in cols:\n            dtypes[k] = v\n    # if time column is alt, set dtype\n    for tcol in [\"Time(MJD)\", \"Time\"]:\n        if tcol in cols and \"Time (MJD)\" not in cols:\n            dtypes[tcol] = \"float32\"\n\n    reader = pd.read_csv(\n        p,\n        usecols=raw_usecols if raw_usecols else None,\n        dtype=dtypes if dtypes else None,\n        chunksize=int(chunksize),\n    )\n    for chunk in reader:\n        yield _normalize_lc_chunk(chunk)\n\ndef load_split_lightcurves(split_name: str, which: str):\n    \"\"\"\n    Convenience: load entire split file (NOT recommended for huge files).\n    Use only for quick debugging on small splits.\n    \"\"\"\n    parts = []\n    for ch in iter_lightcurve_chunks(split_name, which, chunksize=400_000):\n        parts.append(ch)\n    return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"])\n\ndef load_object_lightcurve(object_id: str, which: str, chunksize: int = 400_000, sort_time: bool = True):\n    \"\"\"\n    Debug-safe per-object extraction by streaming the relevant split file.\n    This scans the split CSV in chunks (OK for occasional use; do NOT do this for all objects).\n    \"\"\"\n    object_id = str(object_id).strip()\n    if which == \"train\":\n        if object_id not in df_train_meta.index:\n            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n        split_name = df_train_meta.loc[object_id, \"split\"]\n    elif which == \"test\":\n        if object_id not in df_test_meta.index:\n            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n        split_name = df_test_meta.loc[object_id, \"split\"]\n    else:\n        raise ValueError(\"which must be 'train' or 'test'\")\n\n    pieces = []\n    for ch in iter_lightcurve_chunks(split_name, which, chunksize=chunksize):\n        sub = ch[ch[\"object_id\"] == object_id]\n        if not sub.empty:\n            pieces.append(sub)\n    if not pieces:\n        out = pd.DataFrame(columns=[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"])\n    else:\n        out = pd.concat(pieces, ignore_index=True)\n        if sort_time and len(out) > 1:\n            out = out.sort_values([\"mjd\", \"filter\"], kind=\"mergesort\").reset_index(drop=True)\n    return out\n\n# ----------------------------\n# 5) Quick smoke test (lightweight, no heavy IO)\n# ----------------------------\n# Take 1 object from a few splits and ensure extraction works.\ntest_splits = [\"split_01\", \"split_08\", \"split_17\"]\nfor s in test_splits:\n    if len(train_ids_by_split[s]) == 0 or len(test_ids_by_split[s]) == 0:\n        raise RuntimeError(f\"Split {s} has 0 objects in train/test log (unexpected).\")\n    oid_tr = train_ids_by_split[s][0]\n    oid_te = test_ids_by_split[s][0]\n    df_tr_obj = load_object_lightcurve(oid_tr, \"train\", chunksize=250_000)\n    df_te_obj = load_object_lightcurve(oid_te, \"test\",  chunksize=250_000)\n\n    if df_tr_obj.empty:\n        raise RuntimeError(f\"Smoke test failed: empty train lightcurve for {oid_tr} in {s}\")\n    if df_te_obj.empty:\n        raise RuntimeError(f\"Smoke test failed: empty test lightcurve for {oid_te} in {s}\")\n\n    # filter sanity (allow only u,g,r,i,z,y)\n    badf_tr = sorted(set(df_tr_obj[\"filter\"].unique()) - set([\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]))\n    badf_te = sorted(set(df_te_obj[\"filter\"].unique()) - set([\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]))\n    if badf_tr or badf_te:\n        raise ValueError(f\"Unexpected filter values in smoke test split={s}: train_bad={badf_tr} test_bad={badf_te}\")\n\nprint(\"LIGHTCURVE LOADING STRATEGY OK (split-wise + chunked)\")\nprint(f\"- Saved: {manifest_path}\")\nprint(f\"- Saved: {counts_path}\")\nprint(\"- Ready for next stage: photometric preprocessing + sequence building (split-wise loop).\")\n\n# Export to globals for next stages\nglobals().update({\n    \"SPLIT_FILES\": SPLIT_FILES,\n    \"train_ids_by_split\": train_ids_by_split,\n    \"test_ids_by_split\": test_ids_by_split,\n    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n    \"load_object_lightcurve\": load_object_lightcurve,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:24.882121Z","iopub.execute_input":"2026-01-01T15:21:24.882850Z","iopub.status.idle":"2026-01-01T15:21:25.971537Z","shell.execute_reply.started":"2026-01-01T15:21:24.882805Z","shell.execute_reply":"2026-01-01T15:21:25.970183Z"}},"outputs":[{"name":"stdout","text":"LIGHTCURVE LOADING STRATEGY OK (split-wise + chunked)\n- Saved: /kaggle/working/mallorn_run/artifacts/split_file_manifest.csv\n- Saved: /kaggle/working/mallorn_run/artifacts/object_counts_by_split.csv\n- Ready for next stage: photometric preprocessing + sequence building (split-wise loop).\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"348"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — Photometric Cleaning (De-extinction + Negative Flux Safe Transform)\n# ONE CELL, Kaggle CPU-SAFE, split-wise + chunked\n#\n# Prasyarat (sudah ada dari stage sebelumnya):\n# - iter_lightcurve_chunks (STAGE 3)\n# - df_train_meta, df_test_meta (STAGE 2)\n# - ART_DIR, SPLIT_LIST (STAGE 0/1)\n#\n# Output:\n# - /kaggle/working/mallorn_run/artifacts/lc_clean/split_XX/{train|test}/part_*.parquet (atau .csv.gz fallback)\n# - manifest CSV per split + summary stats\n# ============================================================\n\nimport gc, json, math, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n\n# ----------------------------\n# 1) Settings (CPU-safe defaults)\n# ----------------------------\nCHUNKSIZE = 350_000          # adjust if needed (bigger = faster but more RAM)\nSNR_DET = 3.0                # simple detection proxy\nERR_EPS = 1e-6               # avoid div-by-zero\nSCALE_SAMPLE_ROWS_TOTAL = 900_000  # sampling to estimate per-band scales (lightweight)\nSCALE_SAMPLES_PER_BAND = 60_000    # cap samples stored per band for median\nWRITE_FORMAT = \"parquet\"     # \"parquet\" recommended; auto-fallback to \"csv.gz\" if parquet fails\n\n# For debugging: set a subset of splits to process, e.g. [\"split_01\",\"split_02\"]\nONLY_SPLITS = None  # None = process all 20 splits\n\n# ----------------------------\n# 2) Extinction coefficients (R_lambda for LSST-like bands)\n#    NOTE: If kamu punya koefisien resmi dari notebook Using_the_Data, ganti nilai di sini.\n#    A_lambda = R_lambda * EBV\n#    flux_deext = flux * 10^(0.4 * A_lambda)\n# ----------------------------\nEXT_RLAMBDA = {\n    \"u\": 4.8,\n    \"g\": 3.6,\n    \"r\": 2.7,\n    \"i\": 2.1,\n    \"z\": 1.6,\n    \"y\": 1.3,\n}\n\nBAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\nID2BAND = {v: k for k, v in BAND2ID.items()}\n\n# EBV mapping (Series with index object_id) for fast vectorized map\nEBV_TRAIN_SER = df_train_meta[\"EBV\"]\nEBV_TEST_SER  = df_test_meta[\"EBV\"]\n\n# ----------------------------\n# 3) Estimate per-band flux scale (robust, sample-based) for safe transforms\n#    We estimate on RAW flux (not de-extincted) to keep this stage fast.\n# ----------------------------\ndef estimate_band_scale_from_sample(which: str, splits, total_rows=SCALE_SAMPLE_ROWS_TOTAL, per_band_cap=SCALE_SAMPLES_PER_BAND):\n    \"\"\"\n    Stream sample abs(flux) from split files to estimate median abs flux per band.\n    Returns dict: {band: scale} with positive float values.\n    \"\"\"\n    rng = np.random.default_rng(2025)\n    samples = {b: [] for b in BAND2ID.keys()}\n    seen = 0\n\n    for s in splits:\n        if seen >= total_rows:\n            break\n        for ch in iter_lightcurve_chunks(s, which, chunksize=CHUNKSIZE):\n            # ch columns: object_id,mjd,flux,flux_err,filter\n            f = ch[\"filter\"].to_numpy(dtype=\"U1\", copy=False)\n            x = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n            ax = np.abs(x)\n\n            # sample a small fraction to keep memory low\n            n = len(ch)\n            if n == 0:\n                continue\n            take = int(min(5000, n))  # small per-chunk sample\n            idx = rng.choice(n, size=take, replace=False)\n            f_s = f[idx]\n            ax_s = ax[idx]\n\n            for b in BAND2ID.keys():\n                mask = (f_s == b)\n                if not np.any(mask):\n                    continue\n                vals = ax_s[mask]\n                if vals.size == 0:\n                    continue\n\n                # append with cap\n                cur = samples[b]\n                if len(cur) < per_band_cap:\n                    need = per_band_cap - len(cur)\n                    if vals.size > need:\n                        vals = vals[:need]\n                    cur.extend(vals.tolist())\n                # else already full; keep\n\n            seen += n\n            if seen >= total_rows:\n                break\n\n    band_scale = {}\n    for b in BAND2ID.keys():\n        arr = np.asarray(samples[b], dtype=np.float32)\n        arr = arr[np.isfinite(arr)]\n        arr = arr[arr > 0]\n        if arr.size == 0:\n            band_scale[b] = 1.0\n        else:\n            med = float(np.median(arr))\n            # clamp to avoid too small\n            band_scale[b] = max(med, 1e-3)\n    return band_scale\n\nsplits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\nprint(f\"[Stage 4] Estimating per-band flux scales from sample ({SCALE_SAMPLE_ROWS_TOTAL:,} rows max) ...\")\nBAND_SCALE = estimate_band_scale_from_sample(\"train\", splits_to_use)\nprint(\"[Stage 4] BAND_SCALE (median |flux| per band):\")\nfor b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n    print(f\"  - {b}: {BAND_SCALE[b]:.6f}\")\n\n# ----------------------------\n# 4) Chunk photometric cleaning (de-extinction + safe transforms)\n# ----------------------------\ndef clean_photometric_chunk(ch: pd.DataFrame, ebv_ser: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Input chunk columns: object_id,mjd,flux,flux_err,filter (from iter_lightcurve_chunks)\n    Output columns (float32/int8):\n      object_id, mjd, band_id, flux_deext, err_deext, flux_asinh, err_log1p, snr, detected\n    \"\"\"\n    # Map EBV and extinction coefficient\n    ebv = ch[\"object_id\"].map(ebv_ser).fillna(0.0).to_numpy(dtype=np.float32)\n    filt = ch[\"filter\"].to_numpy(dtype=\"U1\", copy=False)\n\n    r = pd.Series(filt).map(EXT_RLAMBDA).fillna(0.0).to_numpy(dtype=np.float32)\n    A = r * ebv  # A_lambda\n\n    # de-extinction multiplier: 10^(0.4*A)\n    mul = np.power(10.0, (0.4 * A).astype(np.float32)).astype(np.float32)\n\n    flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n    err  = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)\n\n    # clamp err\n    err = np.maximum(err, np.float32(ERR_EPS))\n\n    flux_deext = (flux * mul).astype(np.float32)\n    err_deext  = (err  * mul).astype(np.float32)\n\n    # per-band scale\n    scale = pd.Series(filt).map(BAND_SCALE).fillna(1.0).to_numpy(dtype=np.float32)\n    scale = np.maximum(scale, np.float32(1e-3))\n\n    # safe transforms\n    flux_asinh = np.arcsinh(flux_deext / scale).astype(np.float32)\n    err_scaled = (err_deext / scale).astype(np.float32)\n    err_log1p  = np.log1p(err_scaled).astype(np.float32)\n\n    snr = (flux_deext / np.maximum(err_deext, np.float32(ERR_EPS))).astype(np.float32)\n    detected = (snr > np.float32(SNR_DET)).astype(np.int8)\n\n    # band_id\n    band_id = pd.Series(filt).map(BAND2ID).fillna(-1).astype(np.int16).to_numpy()\n    if np.any(band_id < 0):\n        bad = sorted(set(pd.Series(filt)[band_id < 0].tolist()))\n        raise ValueError(f\"Unknown filter values encountered: {bad}\")\n\n    out = pd.DataFrame({\n        \"object_id\": ch[\"object_id\"].astype(\"string\").to_numpy(),\n        \"mjd\": ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False),\n        \"band_id\": band_id.astype(np.int16),\n        \"flux_deext\": flux_deext,\n        \"err_deext\": err_deext,\n        \"flux_asinh\": flux_asinh,\n        \"err_log1p\": err_log1p,\n        \"snr\": snr,\n        \"detected\": detected,\n    })\n    return out\n\n# ----------------------------\n# 5) Writer (parquet preferred; fallback csv.gz)\n# ----------------------------\ndef write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    if fmt == \"parquet\":\n        try:\n            df.to_parquet(out_path, index=False)\n            return \"parquet\"\n        except Exception as e:\n            # fallback\n            alt = out_path.with_suffix(\".csv.gz\")\n            df.to_csv(alt, index=False, compression=\"gzip\")\n            return f\"csv.gz (fallback from parquet: {type(e).__name__})\"\n    elif fmt == \"csv.gz\":\n        df.to_csv(out_path.with_suffix(\".csv.gz\"), index=False, compression=\"gzip\")\n        return \"csv.gz\"\n    else:\n        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n\n# ----------------------------\n# 6) Process all splits split-wise (stream -> clean -> write parts)\n# ----------------------------\nLC_CLEAN_DIR = Path(ART_DIR) / \"lc_clean\"\nLC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n\nsummary_rows = []\nmanifest_rows = []\n\ndef process_split(split_name: str, which: str):\n    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n    out_dir = LC_CLEAN_DIR / split_name / which\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    part_idx = 0\n    n_rows_total = 0\n    n_neg_flux = 0\n    n_det = 0\n\n    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n        # clean\n        cleaned = clean_photometric_chunk(ch, ebv_ser)\n\n        # simple stats\n        n_rows = int(len(cleaned))\n        n_rows_total += n_rows\n        n_neg_flux += int((cleaned[\"flux_deext\"].to_numpy() < 0).sum())\n        n_det += int(cleaned[\"detected\"].to_numpy().sum())\n\n        # write part\n        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n        used_fmt = write_part(cleaned, out_path, WRITE_FORMAT)\n\n        manifest_rows.append({\n            \"split\": split_name,\n            \"which\": which,\n            \"part\": part_idx,\n            \"path\": str(out_path if used_fmt.startswith(\"parquet\") else out_path.with_suffix(\".csv.gz\")),\n            \"rows\": n_rows,\n            \"format\": used_fmt,\n        })\n\n        part_idx += 1\n\n        # free memory\n        del cleaned, ch\n        if part_idx % 10 == 0:\n            gc.collect()\n\n    summary_rows.append({\n        \"split\": split_name,\n        \"which\": which,\n        \"parts\": part_idx,\n        \"rows\": n_rows_total,\n        \"neg_flux_frac\": (n_neg_flux / max(n_rows_total, 1)),\n        \"det_frac_snr_gt_3\": (n_det / max(n_rows_total, 1)),\n    })\n\n    print(f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | neg%={100*(n_neg_flux/max(n_rows_total,1)):.2f}% | det%={100*(n_det/max(n_rows_total,1)):.2f}%\")\n\nprint(\"[Stage 4] Building cleaned lightcurve cache (split-wise) ...\")\nfor s in splits_to_use:\n    process_split(s, \"train\")\n    process_split(s, \"test\")\n\n# ----------------------------\n# 7) Save manifests + summary\n# ----------------------------\ndf_manifest = pd.DataFrame(manifest_rows)\ndf_summary  = pd.DataFrame(summary_rows)\n\nmanifest_path = LC_CLEAN_DIR / \"lc_clean_manifest.csv\"\nsummary_path  = LC_CLEAN_DIR / \"lc_clean_summary.csv\"\n\ndf_manifest.to_csv(manifest_path, index=False)\ndf_summary.to_csv(summary_path, index=False)\n\n# Save coefficients & scales for reproducibility\ncfg_path = LC_CLEAN_DIR / \"photometric_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\n        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n        \"BAND_SCALE\": BAND_SCALE,\n        \"SNR_DET\": SNR_DET,\n        \"ERR_EPS\": ERR_EPS,\n        \"CHUNKSIZE\": CHUNKSIZE,\n        \"WRITE_FORMAT\": WRITE_FORMAT,\n        \"ONLY_SPLITS\": splits_to_use,\n    }, f, indent=2)\n\nprint(\"\\n[Stage 4] Done.\")\nprint(f\"- Saved manifest: {manifest_path}\")\nprint(f\"- Saved summary : {summary_path}\")\nprint(f\"- Saved config  : {cfg_path}\")\n\n# ----------------------------\n# 8) Export globals for next stages\n# ----------------------------\ndef get_clean_parts(split_name: str, which: str):\n    \"\"\"Return list of part file paths for cleaned split.\"\"\"\n    m = df_manifest[(df_manifest[\"split\"] == split_name) & (df_manifest[\"which\"] == which)].sort_values(\"part\")\n    return m[\"path\"].tolist()\n\nglobals().update({\n    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n    \"BAND2ID\": BAND2ID,\n    \"ID2BAND\": ID2BAND,\n    \"BAND_SCALE\": BAND_SCALE,\n    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n    \"lc_clean_manifest\": df_manifest,\n    \"lc_clean_summary\": df_summary,\n    \"get_clean_parts\": get_clean_parts,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:25.973212Z","iopub.execute_input":"2026-01-01T15:21:25.973798Z","iopub.status.idle":"2026-01-01T15:21:34.667821Z","shell.execute_reply.started":"2026-01-01T15:21:25.973755Z","shell.execute_reply":"2026-01-01T15:21:34.666810Z"}},"outputs":[{"name":"stdout","text":"[Stage 4] Estimating per-band flux scales from sample (900,000 rows max) ...\n[Stage 4] BAND_SCALE (median |flux| per band):\n  - u: 0.336883\n  - g: 0.249368\n  - r: 0.377052\n  - i: 0.451290\n  - z: 0.576561\n  - y: 0.996014\n[Stage 4] Building cleaned lightcurve cache (split-wise) ...\n[Stage 4] split_01/train: parts=1 | rows=26,324 | neg%=38.95% | det%=19.34%\n[Stage 4] split_01/test: parts=1 | rows=59,235 | neg%=37.74% | det%=23.02%\n[Stage 4] split_02/train: parts=1 | rows=25,609 | neg%=34.02% | det%=24.45%\n[Stage 4] split_02/test: parts=1 | rows=71,229 | neg%=36.48% | det%=21.69%\n[Stage 4] split_03/train: parts=1 | rows=21,676 | neg%=36.82% | det%=21.65%\n[Stage 4] split_03/test: parts=1 | rows=53,751 | neg%=36.70% | det%=21.90%\n[Stage 4] split_04/train: parts=1 | rows=22,898 | neg%=38.36% | det%=21.11%\n[Stage 4] split_04/test: parts=1 | rows=51,408 | neg%=38.16% | det%=21.70%\n[Stage 4] split_05/train: parts=1 | rows=25,934 | neg%=39.19% | det%=18.33%\n[Stage 4] split_05/test: parts=1 | rows=61,179 | neg%=38.41% | det%=18.21%\n[Stage 4] split_06/train: parts=1 | rows=25,684 | neg%=38.07% | det%=18.85%\n[Stage 4] split_06/test: parts=1 | rows=57,620 | neg%=37.39% | det%=19.94%\n[Stage 4] split_07/train: parts=1 | rows=24,473 | neg%=36.28% | det%=21.44%\n[Stage 4] split_07/test: parts=1 | rows=65,101 | neg%=36.96% | det%=19.10%\n[Stage 4] split_08/train: parts=1 | rows=25,571 | neg%=39.54% | det%=22.80%\n[Stage 4] split_08/test: parts=1 | rows=61,498 | neg%=37.88% | det%=24.50%\n[Stage 4] split_09/train: parts=1 | rows=19,690 | neg%=36.18% | det%=21.13%\n[Stage 4] split_09/test: parts=1 | rows=47,239 | neg%=35.22% | det%=22.70%\n[Stage 4] split_10/train: parts=1 | rows=25,151 | neg%=40.38% | det%=20.86%\n[Stage 4] split_10/test: parts=1 | rows=51,056 | neg%=39.39% | det%=21.21%\n[Stage 4] split_11/train: parts=1 | rows=22,927 | neg%=39.52% | det%=19.59%\n[Stage 4] split_11/test: parts=1 | rows=49,723 | neg%=39.36% | det%=20.17%\n[Stage 4] split_12/train: parts=1 | rows=25,546 | neg%=40.10% | det%=19.64%\n[Stage 4] split_12/test: parts=1 | rows=54,499 | neg%=38.64% | det%=19.29%\n[Stage 4] split_13/train: parts=1 | rows=23,203 | neg%=37.92% | det%=20.64%\n[Stage 4] split_13/test: parts=1 | rows=63,653 | neg%=39.26% | det%=19.56%\n[Stage 4] split_14/train: parts=1 | rows=25,706 | neg%=35.93% | det%=20.36%\n[Stage 4] split_14/test: parts=1 | rows=58,643 | neg%=36.82% | det%=17.91%\n[Stage 4] split_15/train: parts=1 | rows=23,972 | neg%=38.02% | det%=19.09%\n[Stage 4] split_15/test: parts=1 | rows=52,943 | neg%=38.31% | det%=20.03%\n[Stage 4] split_16/train: parts=1 | rows=25,173 | neg%=36.92% | det%=21.42%\n[Stage 4] split_16/test: parts=1 | rows=58,192 | neg%=37.85% | det%=20.12%\n[Stage 4] split_17/train: parts=1 | rows=22,705 | neg%=35.75% | det%=22.09%\n[Stage 4] split_17/test: parts=1 | rows=59,482 | neg%=38.08% | det%=19.59%\n[Stage 4] split_18/train: parts=1 | rows=21,536 | neg%=36.63% | det%=23.77%\n[Stage 4] split_18/test: parts=1 | rows=53,887 | neg%=35.71% | det%=23.88%\n[Stage 4] split_19/train: parts=1 | rows=22,087 | neg%=36.38% | det%=23.73%\n[Stage 4] split_19/test: parts=1 | rows=56,355 | neg%=35.52% | det%=24.17%\n[Stage 4] split_20/train: parts=1 | rows=23,519 | neg%=37.99% | det%=20.45%\n[Stage 4] split_20/test: parts=1 | rows=58,432 | neg%=38.29% | det%=19.65%\n\n[Stage 4] Done.\n- Saved manifest: /kaggle/working/mallorn_run/artifacts/lc_clean/lc_clean_manifest.csv\n- Saved summary : /kaggle/working/mallorn_run/artifacts/lc_clean/lc_clean_summary.csv\n- Saved config  : /kaggle/working/mallorn_run/artifacts/lc_clean/photometric_config.json\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"204"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Sequence Tokenization (Event-based Tokens)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL, Kaggle CPU-SAFE)\n#\n# Tujuan:\n# - Mengubah cleaned lightcurve (STAGE 4) -> token sequence per object_id\n# - Token berbasis event/observasi: 1 baris observasi = 1 token\n# - Output disimpan dalam shard .npz per split & (train/test)\n#\n# Input (dari stage sebelumnya):\n# - LC_CLEAN_DIR, get_clean_parts, lc_clean_manifest (STAGE 4)\n# - df_train_meta, df_test_meta, train_ids_by_split, test_ids_by_split, SPLIT_LIST (STAGE 2/3)\n# - ART_DIR\n#\n# Output:\n# - artifacts/seq_tokens/split_XX/{train|test}/shard_*.npz\n# - artifacts/seq_tokens/seq_manifest_{train|test}.csv   (mapping object_id -> shard + slice)\n# - artifacts/seq_tokens/seq_config.json                 (feature spec)\n#\n# Catatan:\n# - Default mencoba \"streaming contiguous object blocks\" (hemat disk, cepat).\n# - Jika dataset TIDAK contiguous per object_id, otomatis fallback bucket-hash (lebih berat tapi aman).\n# ============================================================\n\nimport gc, json, math, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"LC_CLEAN_DIR\", \"get_clean_parts\", \"lc_clean_manifest\",\n             \"df_train_meta\", \"df_test_meta\", \"train_ids_by_split\", \"test_ids_by_split\",\n             \"SPLIT_LIST\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 dulu.\")\n\n# ----------------------------\n# 1) Settings (CPU-safe)\n# ----------------------------\nONLY_SPLITS = None                 # None = proses semua; atau set list [\"split_01\",\"split_02\"] untuk debug\nCOMPRESS_NPZ = False               # True lebih kecil disk tapi jauh lebih lambat di CPU\nSHARD_MAX_OBJECTS = 1500           # jumlah object per shard file\nSNR_TANH_SCALE = 10.0              # snr_tanh = tanh(snr / scale)\nTIME_CLIP_MAX_DAYS = None          # None = no clip; atau mis. 2000.0\nDROP_BAD_TIME_ROWS = True          # drop rows with NaN/inf mjd\nFALLBACK_NUM_BUCKETS = 64          # dipakai jika fallback (hash bucket) diperlukan\n\nSEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\nSEQ_DIR.mkdir(parents=True, exist_ok=True)\n\nFEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\nFEATURE_DIM = len(FEATURE_NAMES)\n\n# ----------------------------\n# 2) Robust readers for cleaned parts (parquet or csv.gz)\n# ----------------------------\nREQ_CLEAN_COLS = {\"object_id\",\"mjd\",\"band_id\",\"flux_asinh\",\"err_log1p\",\"snr\",\"detected\"}\n\ndef _read_clean_part(path: str) -> pd.DataFrame:\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(f\"Clean part missing: {p}\")\n    if p.suffix == \".parquet\":\n        df = pd.read_parquet(p)\n    elif p.name.endswith(\".csv.gz\"):\n        df = pd.read_csv(p, compression=\"gzip\")\n    else:\n        # allow plain .csv\n        df = pd.read_csv(p)\n    df.columns = [c.strip() for c in df.columns]\n    missing = sorted(list(REQ_CLEAN_COLS - set(df.columns)))\n    if missing:\n        raise ValueError(f\"Clean part missing columns {missing}. Found: {list(df.columns)} | file={p}\")\n    # enforce dtypes lightly\n    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n    for c in [\"flux_asinh\",\"err_log1p\",\"snr\"]:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(np.float32)\n    df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n    if DROP_BAD_TIME_ROWS:\n        df = df[np.isfinite(df[\"mjd\"].to_numpy())]\n    return df\n\n# ----------------------------\n# 3) Build tokens for one object (sort by time inside object)\n# ----------------------------\ndef build_object_tokens(df_obj: pd.DataFrame):\n    \"\"\"\n    df_obj columns: mjd, band_id, flux_asinh, err_log1p, snr, detected\n    returns:\n      X: (L, FEATURE_DIM) float32\n      B: (L,) int8 band_id\n    \"\"\"\n    if df_obj.empty:\n        return None, None\n\n    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n    flux = df_obj[\"flux_asinh\"].to_numpy(dtype=np.float32, copy=False)\n    elog = df_obj[\"err_log1p\"].to_numpy(dtype=np.float32, copy=False)\n    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n\n    # sort by mjd, tie-break by band\n    order = np.lexsort((band, mjd))\n    mjd = mjd[order]\n    band = band[order]\n    flux = flux[order]\n    elog = elog[order]\n    snr  = snr[order]\n    det  = det[order]\n\n    # time features\n    t0 = mjd[0]\n    t_rel = mjd - t0\n    # dt: first token dt=0\n    dt = np.empty_like(t_rel)\n    dt[0] = 0.0\n    if len(t_rel) > 1:\n        dt[1:] = np.maximum(mjd[1:] - mjd[:-1], 0.0)\n\n    if TIME_CLIP_MAX_DAYS is not None:\n        t_rel = np.clip(t_rel, 0.0, float(TIME_CLIP_MAX_DAYS))\n        dt    = np.clip(dt,    0.0, float(TIME_CLIP_MAX_DAYS))\n\n    t_rel_log = np.log1p(t_rel).astype(np.float32)\n    dt_log    = np.log1p(dt).astype(np.float32)\n\n    # robust snr transform\n    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n\n    # detected -> float32\n    det_f = det.astype(np.float32)\n\n    # sanitize other features\n    flux = np.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n    elog = np.nan_to_num(elog, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n\n    X = np.stack([t_rel_log, dt_log, flux, elog, snr_tanh, det_f], axis=1).astype(np.float32)\n    B = band.astype(np.int8)\n\n    return X, B\n\n# ----------------------------\n# 4) Shard writer\n# ----------------------------\ndef save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    obj_arr = np.asarray(object_ids, dtype=\"S\")  # bytes to store in npz\n    if COMPRESS_NPZ:\n        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n    else:\n        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n\n# ----------------------------\n# 5) Streaming builder (expects contiguous blocks by object_id across the cleaned parts)\n# ----------------------------\ndef build_sequences_streaming(split_name: str, which: str, expected_ids: set, out_dir: Path):\n    \"\"\"\n    Returns:\n      manifest_rows (list of dict), n_objects_built (int), fallback_needed (bool)\n    \"\"\"\n    parts = get_clean_parts(split_name, which)\n    if not parts:\n        raise RuntimeError(f\"No cleaned parts for {split_name}/{which}. Pastikan STAGE 4 sukses.\")\n\n    # shard accumulators\n    manifest_rows = []\n    shard_idx = 0\n    batch_obj_ids = []\n    batch_X_list = []\n    batch_B_list = []\n    batch_lengths = []\n\n    # streaming buffers for current object\n    cur_oid = None\n    cur_buf = []  # list of DataFrames (small blocks)\n    seen_done = set()  # object_ids that are finalized\n    fallback_needed = False\n\n    def flush_object(oid, buf_blocks):\n        nonlocal batch_obj_ids, batch_X_list, batch_B_list, batch_lengths\n        if oid is None or not buf_blocks:\n            return\n        df_obj = pd.concat(buf_blocks, ignore_index=True)\n        # drop rows not expected (safety), but ideally none\n        if oid not in expected_ids:\n            return\n        X, B = build_object_tokens(df_obj)\n        if X is None:\n            return\n        batch_obj_ids.append(oid)\n        batch_X_list.append(X)\n        batch_B_list.append(B)\n        batch_lengths.append(X.shape[0])\n\n    def flush_shard():\n        nonlocal shard_idx, batch_obj_ids, batch_X_list, batch_B_list, batch_lengths, manifest_rows\n        if not batch_obj_ids:\n            return\n        # build concat + offsets\n        lengths = np.asarray(batch_lengths, dtype=np.int64)\n        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n        offsets[1:] = np.cumsum(lengths)\n        X_concat = np.concatenate(batch_X_list, axis=0).astype(np.float32)\n        B_concat = np.concatenate(batch_B_list, axis=0).astype(np.int8)\n\n        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n        save_shard(shard_path, batch_obj_ids, X_concat, B_concat, offsets)\n\n        # manifest entries\n        for i, oid in enumerate(batch_obj_ids):\n            start = int(offsets[i])\n            length = int(lengths[i])\n            manifest_rows.append({\n                \"object_id\": oid,\n                \"split\": split_name,\n                \"which\": which,\n                \"shard\": str(shard_path),\n                \"start\": start,\n                \"length\": length\n            })\n\n        shard_idx += 1\n        # reset batch\n        batch_obj_ids = []\n        batch_X_list = []\n        batch_B_list = []\n        batch_lengths = []\n        gc.collect()\n\n    # iterate parts sequentially (preserve original order)\n    for pi, p in enumerate(parts):\n        df = _read_clean_part(p)\n        if df.empty:\n            continue\n\n        # IMPORTANT: use original row order (do NOT sort by object_id)\n        oids = df[\"object_id\"].to_numpy(dtype=object, copy=False)\n\n        # detect contiguous segments where object_id constant\n        # boundaries: start indices of segments\n        change = np.empty(len(oids), dtype=bool)\n        change[0] = True\n        change[1:] = oids[1:] != oids[:-1]\n        seg_starts = np.flatnonzero(change)\n        seg_ends = np.append(seg_starts[1:], len(oids))\n\n        for s_idx, e_idx in zip(seg_starts, seg_ends):\n            oid = str(oids[s_idx])\n            block = df.iloc[s_idx:e_idx]\n\n            # If we see an oid that already finalized earlier and it's not the current ongoing oid,\n            # then file is not contiguous-by-object => need fallback\n            if (oid in seen_done) and (oid != cur_oid):\n                fallback_needed = True\n                break\n\n            if cur_oid is None:\n                cur_oid = oid\n                cur_buf = [block]\n            elif oid == cur_oid:\n                cur_buf.append(block)\n            else:\n                # finalize previous\n                flush_object(cur_oid, cur_buf)\n                seen_done.add(cur_oid)\n\n                # flush shard if too big\n                if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                    flush_shard()\n\n                # start new\n                cur_oid = oid\n                cur_buf = [block]\n\n        del df\n        if fallback_needed:\n            break\n\n        if (pi + 1) % 10 == 0:\n            gc.collect()\n\n    # finalize last object\n    if not fallback_needed:\n        flush_object(cur_oid, cur_buf)\n        if cur_oid is not None:\n            seen_done.add(cur_oid)\n        flush_shard()\n\n    built = len(seen_done.intersection(expected_ids))\n    return manifest_rows, built, fallback_needed\n\n# ----------------------------\n# 6) Fallback: Hash-bucket builder (robust if not contiguous), temp bucket files then delete\n# ----------------------------\ndef build_sequences_fallback_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 64):\n    \"\"\"\n    Robust method:\n    - Stream read cleaned parts -> write rows to temporary bucket parquet files based on hash(object_id)\n    - Process each bucket file: groupby object_id -> build tokens -> write shards\n    - Delete temp bucket files to save disk\n    \"\"\"\n    try:\n        import pyarrow as pa\n        import pyarrow.parquet as pq\n    except Exception as e:\n        raise RuntimeError(\"Fallback bucketization needs pyarrow. It seems unavailable.\") from e\n\n    parts = get_clean_parts(split_name, which)\n    if not parts:\n        raise RuntimeError(f\"No cleaned parts for {split_name}/{which}.\")\n\n    tmp_dir = Path(ART_DIR) / \"tmp_buckets\" / split_name / which\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n\n    # Parquet writers dict\n    writers = {}\n    schemas = {}\n\n    def _bucket_series(s: pd.Series) -> np.ndarray:\n        # fast vectorized pandas hash (stable within this env), mod num_buckets\n        h = pd.util.hash_pandas_object(s, index=False).to_numpy(dtype=np.uint64, copy=False)\n        return (h % np.uint64(num_buckets)).astype(np.int16)\n\n    # write buckets\n    for p in parts:\n        df = _read_clean_part(p)\n        if df.empty:\n            continue\n        # keep only expected ids to reduce disk\n        df = df[df[\"object_id\"].isin(expected_ids)]\n        if df.empty:\n            continue\n\n        bidx = _bucket_series(df[\"object_id\"])\n        df[\"_b\"] = bidx\n\n        for b in np.unique(bidx):\n            sub = df[df[\"_b\"] == b].drop(columns=[\"_b\"])\n            if sub.empty:\n                continue\n            file_path = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n\n            table = pa.Table.from_pandas(sub, preserve_index=False)\n            if int(b) not in writers:\n                schemas[int(b)] = table.schema\n                writers[int(b)] = pq.ParquetWriter(file_path, table.schema, compression=\"snappy\")\n            writers[int(b)].write_table(table)\n\n        del df\n        gc.collect()\n\n    # close writers\n    for w in writers.values():\n        w.close()\n\n    # now process each bucket file to create shards\n    manifest_rows = []\n    shard_idx = 0\n\n    batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n\n    def flush_shard_local():\n        nonlocal shard_idx, batch_obj_ids, batch_X_list, batch_B_list, batch_lengths, manifest_rows\n        if not batch_obj_ids:\n            return\n        lengths = np.asarray(batch_lengths, dtype=np.int64)\n        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n        offsets[1:] = np.cumsum(lengths)\n        X_concat = np.concatenate(batch_X_list, axis=0).astype(np.float32)\n        B_concat = np.concatenate(batch_B_list, axis=0).astype(np.int8)\n\n        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n        save_shard(shard_path, batch_obj_ids, X_concat, B_concat, offsets)\n\n        for i, oid in enumerate(batch_obj_ids):\n            manifest_rows.append({\n                \"object_id\": oid,\n                \"split\": split_name,\n                \"which\": which,\n                \"shard\": str(shard_path),\n                \"start\": int(offsets[i]),\n                \"length\": int(lengths[i])\n            })\n\n        shard_idx += 1\n        batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n        gc.collect()\n\n    bucket_files = sorted(tmp_dir.glob(\"bucket_*.parquet\"))\n    built_ids = set()\n\n    for bf in bucket_files:\n        dfb = pd.read_parquet(bf)\n        dfb.columns = [c.strip() for c in dfb.columns]\n        if dfb.empty:\n            bf.unlink(missing_ok=True)\n            continue\n\n        # groupby object_id (robust)\n        for oid, g in dfb.groupby(\"object_id\", sort=False):\n            oid = str(oid)\n            if oid in built_ids:\n                continue\n            X, B = build_object_tokens(g)\n            if X is None:\n                continue\n            batch_obj_ids.append(oid)\n            batch_X_list.append(X)\n            batch_B_list.append(B)\n            batch_lengths.append(X.shape[0])\n            built_ids.add(oid)\n\n            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n                flush_shard_local()\n\n        # delete bucket file to save disk\n        bf.unlink(missing_ok=True)\n        del dfb\n        gc.collect()\n\n    flush_shard_local()\n\n    # cleanup tmp dir\n    try:\n        tmp_dir.rmdir()\n    except Exception:\n        pass\n\n    return manifest_rows, len(built_ids)\n\n# ----------------------------\n# 7) Run tokenization for all splits (train & test)\n# ----------------------------\nsplits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\n\nall_manifest_train = []\nall_manifest_test  = []\n\ndef expected_set_for(split_name: str, which: str) -> set:\n    if which == \"train\":\n        return set(train_ids_by_split[split_name])\n    else:\n        return set(test_ids_by_split[split_name])\n\nfor split_name in splits_to_run:\n    for which in [\"train\", \"test\"]:\n        out_dir = SEQ_DIR / split_name / which\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        expected_ids = expected_set_for(split_name, which)\n        if len(expected_ids) == 0:\n            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}. Cek log/split mapping.\")\n\n        print(f\"\\n[Stage 5] Building sequences: {split_name}/{which} | expected_objects={len(expected_ids):,}\")\n\n        manifest_rows, built, fallback_needed = build_sequences_streaming(\n            split_name=split_name,\n            which=which,\n            expected_ids=expected_ids,\n            out_dir=out_dir\n        )\n\n        # If streaming failed or missing many objects, fallback\n        if fallback_needed or built != len(expected_ids):\n            print(f\"[Stage 5] Streaming not safe for {split_name}/{which} \"\n                  f\"(built={built:,} vs expected={len(expected_ids):,}, fallback_needed={fallback_needed}).\")\n            print(f\"[Stage 5] Switching to robust bucket fallback (temporary files, then cleaned).\")\n\n            # Clear any partial outputs in out_dir to avoid mixing\n            for f in out_dir.glob(\"shard_*.npz\"):\n                try: f.unlink()\n                except Exception: pass\n\n            manifest_rows, built2 = build_sequences_fallback_bucket(\n                split_name=split_name,\n                which=which,\n                expected_ids=expected_ids,\n                out_dir=out_dir,\n                num_buckets=FALLBACK_NUM_BUCKETS\n            )\n            if built2 != len(expected_ids):\n                raise RuntimeError(f\"Fallback still mismatch for {split_name}/{which}: built={built2:,} expected={len(expected_ids):,}\")\n            built = built2\n\n        print(f\"[Stage 5] OK: {split_name}/{which} built_objects={built:,} | shards={len(list(out_dir.glob('shard_*.npz'))):,}\")\n\n        if which == \"train\":\n            all_manifest_train.extend(manifest_rows)\n        else:\n            all_manifest_test.extend(manifest_rows)\n\n        gc.collect()\n\n# ----------------------------\n# 8) Save manifests + config\n# ----------------------------\ndf_m_train = pd.DataFrame(all_manifest_train).sort_values([\"split\",\"shard\",\"start\"]).reset_index(drop=True)\ndf_m_test  = pd.DataFrame(all_manifest_test).sort_values([\"split\",\"shard\",\"start\"]).reset_index(drop=True)\n\nmtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\nmtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\ndf_m_train.to_csv(mtrain_path, index=False)\ndf_m_test.to_csv(mtest_path, index=False)\n\ncfg = {\n    \"feature_names\": FEATURE_NAMES,\n    \"feature_dim\": FEATURE_DIM,\n    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n    \"compress_npz\": bool(COMPRESS_NPZ),\n    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n    \"fallback_num_buckets\": int(FALLBACK_NUM_BUCKETS),\n}\ncfg_path = SEQ_DIR / \"seq_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(cfg, f, indent=2)\n\nprint(\"\\n[Stage 5] DONE\")\nprint(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\nprint(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\nprint(f\"- Saved: {cfg_path}\")\n\n# ----------------------------\n# 9) Smoke test: load one object sequence\n# ----------------------------\ndef load_sequence(object_id: str, which: str):\n    \"\"\"Load one object's sequence from manifest + shard.\"\"\"\n    object_id = str(object_id).strip()\n    m = df_m_train if which == \"train\" else df_m_test\n    row = m[m[\"object_id\"] == object_id]\n    if row.empty:\n        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n    r = row.iloc[0]\n    data = np.load(r[\"shard\"], allow_pickle=False)\n    start = int(r[\"start\"])\n    length = int(r[\"length\"])\n    X = data[\"x\"][start:start+length]\n    B = data[\"band\"][start:start+length]\n    return X, B\n\n# pick one train object to test\n_smoke_oid = df_train_meta.index[0]\nX_sm, B_sm = load_sequence(_smoke_oid, \"train\")\nprint(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\nprint(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n\n# Export globals for next stages\nglobals().update({\n    \"SEQ_DIR\": SEQ_DIR,\n    \"seq_manifest_train\": df_m_train,\n    \"seq_manifest_test\": df_m_test,\n    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n    \"load_sequence\": load_sequence,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:34.669417Z","iopub.execute_input":"2026-01-01T15:21:34.669775Z","iopub.status.idle":"2026-01-01T15:21:53.533993Z","shell.execute_reply.started":"2026-01-01T15:21:34.669744Z","shell.execute_reply":"2026-01-01T15:21:53.532927Z"}},"outputs":[{"name":"stdout","text":"\n[Stage 5] Building sequences: split_01/train | expected_objects=155\n[Stage 5] OK: split_01/train built_objects=155 | shards=1\n\n[Stage 5] Building sequences: split_01/test | expected_objects=364\n[Stage 5] OK: split_01/test built_objects=364 | shards=1\n\n[Stage 5] Building sequences: split_02/train | expected_objects=170\n[Stage 5] OK: split_02/train built_objects=170 | shards=1\n\n[Stage 5] Building sequences: split_02/test | expected_objects=414\n[Stage 5] OK: split_02/test built_objects=414 | shards=1\n\n[Stage 5] Building sequences: split_03/train | expected_objects=138\n[Stage 5] OK: split_03/train built_objects=138 | shards=1\n\n[Stage 5] Building sequences: split_03/test | expected_objects=338\n[Stage 5] OK: split_03/test built_objects=338 | shards=1\n\n[Stage 5] Building sequences: split_04/train | expected_objects=145\n[Stage 5] OK: split_04/train built_objects=145 | shards=1\n\n[Stage 5] Building sequences: split_04/test | expected_objects=332\n[Stage 5] OK: split_04/test built_objects=332 | shards=1\n\n[Stage 5] Building sequences: split_05/train | expected_objects=165\n[Stage 5] OK: split_05/train built_objects=165 | shards=1\n\n[Stage 5] Building sequences: split_05/test | expected_objects=375\n[Stage 5] OK: split_05/test built_objects=375 | shards=1\n\n[Stage 5] Building sequences: split_06/train | expected_objects=155\n[Stage 5] OK: split_06/train built_objects=155 | shards=1\n\n[Stage 5] Building sequences: split_06/test | expected_objects=374\n[Stage 5] OK: split_06/test built_objects=374 | shards=1\n\n[Stage 5] Building sequences: split_07/train | expected_objects=165\n[Stage 5] OK: split_07/train built_objects=165 | shards=1\n\n[Stage 5] Building sequences: split_07/test | expected_objects=398\n[Stage 5] OK: split_07/test built_objects=398 | shards=1\n\n[Stage 5] Building sequences: split_08/train | expected_objects=162\n[Stage 5] OK: split_08/train built_objects=162 | shards=1\n\n[Stage 5] Building sequences: split_08/test | expected_objects=387\n[Stage 5] OK: split_08/test built_objects=387 | shards=1\n\n[Stage 5] Building sequences: split_09/train | expected_objects=128\n[Stage 5] OK: split_09/train built_objects=128 | shards=1\n\n[Stage 5] Building sequences: split_09/test | expected_objects=289\n[Stage 5] OK: split_09/test built_objects=289 | shards=1\n\n[Stage 5] Building sequences: split_10/train | expected_objects=144\n[Stage 5] OK: split_10/train built_objects=144 | shards=1\n\n[Stage 5] Building sequences: split_10/test | expected_objects=331\n[Stage 5] OK: split_10/test built_objects=331 | shards=1\n\n[Stage 5] Building sequences: split_11/train | expected_objects=146\n[Stage 5] OK: split_11/train built_objects=146 | shards=1\n\n[Stage 5] Building sequences: split_11/test | expected_objects=325\n[Stage 5] OK: split_11/test built_objects=325 | shards=1\n\n[Stage 5] Building sequences: split_12/train | expected_objects=155\n[Stage 5] OK: split_12/train built_objects=155 | shards=1\n\n[Stage 5] Building sequences: split_12/test | expected_objects=353\n[Stage 5] OK: split_12/test built_objects=353 | shards=1\n\n[Stage 5] Building sequences: split_13/train | expected_objects=143\n[Stage 5] OK: split_13/train built_objects=143 | shards=1\n\n[Stage 5] Building sequences: split_13/test | expected_objects=379\n[Stage 5] OK: split_13/test built_objects=379 | shards=1\n\n[Stage 5] Building sequences: split_14/train | expected_objects=154\n[Stage 5] OK: split_14/train built_objects=154 | shards=1\n\n[Stage 5] Building sequences: split_14/test | expected_objects=351\n[Stage 5] OK: split_14/test built_objects=351 | shards=1\n\n[Stage 5] Building sequences: split_15/train | expected_objects=158\n[Stage 5] OK: split_15/train built_objects=158 | shards=1\n\n[Stage 5] Building sequences: split_15/test | expected_objects=342\n[Stage 5] OK: split_15/test built_objects=342 | shards=1\n\n[Stage 5] Building sequences: split_16/train | expected_objects=155\n[Stage 5] OK: split_16/train built_objects=155 | shards=1\n\n[Stage 5] Building sequences: split_16/test | expected_objects=354\n[Stage 5] OK: split_16/test built_objects=354 | shards=1\n\n[Stage 5] Building sequences: split_17/train | expected_objects=153\n[Stage 5] OK: split_17/train built_objects=153 | shards=1\n\n[Stage 5] Building sequences: split_17/test | expected_objects=351\n[Stage 5] OK: split_17/test built_objects=351 | shards=1\n\n[Stage 5] Building sequences: split_18/train | expected_objects=152\n[Stage 5] OK: split_18/train built_objects=152 | shards=1\n\n[Stage 5] Building sequences: split_18/test | expected_objects=345\n[Stage 5] OK: split_18/test built_objects=345 | shards=1\n\n[Stage 5] Building sequences: split_19/train | expected_objects=147\n[Stage 5] OK: split_19/train built_objects=147 | shards=1\n\n[Stage 5] Building sequences: split_19/test | expected_objects=375\n[Stage 5] OK: split_19/test built_objects=375 | shards=1\n\n[Stage 5] Building sequences: split_20/train | expected_objects=153\n[Stage 5] OK: split_20/train built_objects=153 | shards=1\n\n[Stage 5] Building sequences: split_20/test | expected_objects=358\n[Stage 5] OK: split_20/test built_objects=358 | shards=1\n\n[Stage 5] DONE\n- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_manifest_train.csv (rows=3,043)\n- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_manifest_test.csv  (rows=7,135)\n- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_config.json\n\n[Stage 5] Smoke test object_id=Dornhoth_anwar_melethron\n- seq_len=184 | X_shape=(184, 6) | bands_unique=[0, 1, 2, 3, 4, 5]\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"55"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Sequence Length Policy (Padding, Truncation, Windowing)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n# ONE CELL, Kaggle CPU-SAFE, nyambung dengan STAGE 0..5\n#\n# Tujuan:\n# - Tentukan MAX_LEN otomatis (berdasarkan distribusi panjang sequence)\n# - Terapkan policy truncation/windowing yang informatif (center around peak)\n# - Buat fixed-length cache (memmap) untuk TRAIN & TEST:\n#     X: (N, MAX_LEN, F) float32\n#     B: (N, MAX_LEN) int8   (band_id)\n#     M: (N, MAX_LEN) int8   (attention mask: 1=real token, 0=pad)\n#     y_train: (N,) int8\n#     ids: (N,) bytes\n# - Simpan config agar tahap training tidak error karena mismatch shape\n#\n# Input:\n# - seq_manifest_train, seq_manifest_test, SEQ_FEATURE_NAMES, df_train_meta, df_test_meta\n# - (opsional) df_sub dari STAGE 0 untuk urutan test yang sesuai submission\n#\n# Output:\n# - /kaggle/working/mallorn_run/artifacts/fixed_seq/{train|test}_{X|B|M}.dat\n# - /kaggle/working/mallorn_run/artifacts/fixed_seq/{train|test}_{ids}.npy\n# - /kaggle/working/mallorn_run/artifacts/fixed_seq/train_y.npy\n# - /kaggle/working/mallorn_run/artifacts/fixed_seq/length_policy_config.json\n# ============================================================\n\nimport gc, json, math, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"seq_manifest_train\", \"seq_manifest_test\", \"SEQ_FEATURE_NAMES\",\n             \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 -> 5 dulu.\")\n\nm_train = seq_manifest_train.copy()\nm_test  = seq_manifest_test.copy()\n\n# feature indices (must match STAGE 5)\nfeat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\nREQ_FEATS = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\nfor k in REQ_FEATS:\n    if k not in feat:\n        raise ValueError(f\"SEQ_FEATURE_NAMES must include '{k}'. Found: {SEQ_FEATURE_NAMES}\")\n\n# ----------------------------\n# 1) Inspect length distribution -> choose MAX_LEN (CPU-friendly)\n# ----------------------------\ndef describe_lengths(m: pd.DataFrame, name: str):\n    L = m[\"length\"].to_numpy(dtype=np.int32, copy=False)\n    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n    print(f\"\\n{name} length stats\")\n    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[8])} | p95={int(q[9])} | p99={int(q[10])} | max={int(q[-1])}\")\n    return q\n\nq_tr = describe_lengths(m_train, \"TRAIN\")\nq_te = describe_lengths(m_test,  \"TEST\")\n\n# auto pick based on max(p95_train, p95_test) rounded up to multiple of 32, capped for CPU\np95 = int(max(q_tr[9], q_te[9]))\ndef round_up(x, base=32):\n    return int(base * math.ceil(x / base))\n\n# CPU-safe caps:\n# - if p95 <= 256 => 256\n# - elif <= 384 => 384\n# - else <= 512 => 512\nif p95 <= 256:\n    MAX_LEN = 256\nelif p95 <= 384:\n    MAX_LEN = 384\nelse:\n    MAX_LEN = 512\n\n# If you want to force smaller on CPU, set here:\nFORCE_MAX_LEN = None  # e.g. 256\nif FORCE_MAX_LEN is not None:\n    MAX_LEN = int(FORCE_MAX_LEN)\n\nprint(f\"\\n[Stage 6] Chosen MAX_LEN = {MAX_LEN} (based on p95={p95})\")\n\n# ----------------------------\n# 2) Windowing / truncation policy (informative on TDE-like peaks)\n# ----------------------------\n# Policy:\n# - compute token score = w1*|snr_tanh| + w2*|flux_asinh| + w3*detected\n# - center = argmax(score)\n# - take window [center - MAX_LEN//2, center + MAX_LEN//2]\n# - clamp to valid range\nW_SNR = 1.0\nW_FLX = 0.35\nW_DET = 0.25\n\ndef select_window(X: np.ndarray, max_len: int) -> tuple[int, int, int]:\n    \"\"\"\n    Returns (start, end, center).\n    X shape (L, F).\n    \"\"\"\n    L = int(X.shape[0])\n    if L <= max_len:\n        return 0, L, 0\n\n    snr = np.abs(X[:, feat[\"snr_tanh\"]])\n    flx = np.abs(X[:, feat[\"flux_asinh\"]])\n    det = X[:, feat[\"detected\"]]\n\n    score = (W_SNR * snr) + (W_FLX * flx) + (W_DET * det)\n    # if all zeros (rare), fallback to middle\n    if not np.isfinite(score).any() or float(score.max()) <= 0.0:\n        center = L // 2\n    else:\n        center = int(np.argmax(score))\n\n    half = max_len // 2\n    start = center - half\n    start = max(0, start)\n    start = min(start, L - max_len)\n    end = start + max_len\n    return start, end, center\n\ndef pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n    \"\"\"\n    Returns:\n      Xp: (max_len, F) float32\n      Bp: (max_len,) int8\n      Mp: (max_len,) int8  (1=real token)\n      orig_len, win_start, win_end\n    \"\"\"\n    L = int(X.shape[0])\n    if L <= 0:\n        # safety: should not happen\n        Xp = np.zeros((max_len, X.shape[1]), dtype=np.float32)\n        Bp = np.zeros((max_len,), dtype=np.int8)\n        Mp = np.zeros((max_len,), dtype=np.int8)\n        return Xp, Bp, Mp, 0, 0, 0\n\n    s, e, _ = select_window(X, max_len=max_len)\n    Xw = X[s:e]\n    Bw = B[s:e]\n    lw = int(Xw.shape[0])\n\n    Xp = np.zeros((max_len, X.shape[1]), dtype=np.float32)\n    Bp = np.zeros((max_len,), dtype=np.int8)\n    Mp = np.zeros((max_len,), dtype=np.int8)\n\n    Xp[:lw] = Xw.astype(np.float32, copy=False)\n    Bp[:lw] = Bw.astype(np.int8, copy=False)\n    Mp[:lw] = 1\n\n    return Xp, Bp, Mp, L, s, e\n\n# ----------------------------\n# 3) Fixed cache builder (efficient: process per shard)\n# ----------------------------\nFIX_DIR = Path(ART_DIR) / \"fixed_seq\"\nFIX_DIR.mkdir(parents=True, exist_ok=True)\n\n# Decide ordering for train/test arrays (important!)\n# - train: df_train_meta.index order (stable)\ntrain_ids = df_train_meta.index.to_list()\ny_train = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n\n# - test: if df_sub exists, follow it (ensures submission order); else df_test_meta.index\nif \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in df_sub.columns:\n    test_ids = df_sub[\"object_id\"].astype(str).str.strip().to_list()\nelse:\n    test_ids = df_test_meta.index.to_list()\n\n# mapping to row index\ntrain_row = {oid: i for i, oid in enumerate(train_ids)}\ntest_row  = {oid: i for i, oid in enumerate(test_ids)}\n\nNTR = len(train_ids)\nNTE = len(test_ids)\nF = len(SEQ_FEATURE_NAMES)\n\n# memmap paths\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\ntest_X_path  = FIX_DIR / \"test_X.dat\"\ntest_B_path  = FIX_DIR / \"test_B.dat\"\ntest_M_path  = FIX_DIR / \"test_M.dat\"\n\n# metadata arrays\ntrain_len_path = FIX_DIR / \"train_origlen.npy\"\ntrain_win_path = FIX_DIR / \"train_winstart.npy\"\ntest_len_path  = FIX_DIR / \"test_origlen.npy\"\ntest_win_path  = FIX_DIR / \"test_winstart.npy\"\n\n# create memmaps\nXtr = np.memmap(train_X_path, dtype=np.float32, mode=\"w+\", shape=(NTR, MAX_LEN, F))\nBtr = np.memmap(train_B_path, dtype=np.int8,   mode=\"w+\", shape=(NTR, MAX_LEN))\nMtr = np.memmap(train_M_path, dtype=np.int8,   mode=\"w+\", shape=(NTR, MAX_LEN))\n\nXte = np.memmap(test_X_path, dtype=np.float32, mode=\"w+\", shape=(NTE, MAX_LEN, F))\nBte = np.memmap(test_B_path, dtype=np.int8,   mode=\"w+\", shape=(NTE, MAX_LEN))\nMte = np.memmap(test_M_path, dtype=np.int8,   mode=\"w+\", shape=(NTE, MAX_LEN))\n\noriglen_tr = np.zeros((NTR,), dtype=np.int32)\nwinstart_tr = np.zeros((NTR,), dtype=np.int32)\noriglen_te = np.zeros((NTE,), dtype=np.int32)\nwinstart_te = np.zeros((NTE,), dtype=np.int32)\n\ndef process_manifest_into_memmap(m: pd.DataFrame, which: str):\n    \"\"\"\n    Process manifest (train/test) into the appropriate memmaps by reading each shard once.\n    \"\"\"\n    if which == \"train\":\n        row_map = train_row\n        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n        origlen, winstart = origlen_tr, winstart_tr\n        expected_n = NTR\n    else:\n        row_map = test_row\n        Xmm, Bmm, Mmm = Xte, Bte, Mte\n        origlen, winstart = origlen_te, winstart_te\n        expected_n = NTE\n\n    filled = 0\n    # group by shard to minimize IO\n    for shard_path, g in m.groupby(\"shard\", sort=False):\n        shard_path = str(shard_path)\n        data = np.load(shard_path, allow_pickle=False)\n        x_all = data[\"x\"]      # (total_tokens, F)\n        b_all = data[\"band\"]   # (total_tokens,)\n        # loop rows in this shard group\n        for _, r in g.iterrows():\n            oid = str(r[\"object_id\"])\n            idx = row_map.get(oid, None)\n            if idx is None:\n                # object_id not requested in ordering (should not happen, but skip safely)\n                continue\n            start = int(r[\"start\"])\n            length = int(r[\"length\"])\n            if length <= 0:\n                continue\n\n            X = x_all[start:start+length]\n            B = b_all[start:start+length]\n\n            Xp, Bp, Mp, L0, ws, _ = pad_to_fixed(X, B, max_len=MAX_LEN)\n\n            Xmm[idx, :, :] = Xp\n            Bmm[idx, :] = Bp\n            Mmm[idx, :] = Mp\n            origlen[idx] = int(L0)\n            winstart[idx] = int(ws)\n            filled += 1\n\n        del data\n        if filled % 2000 == 0:\n            gc.collect()\n\n    # sanity: some ids might not exist in manifest (should be 0 if pipeline correct)\n    return filled, expected_n\n\nprint(\"\\n[Stage 6] Building fixed-length cache (TRAIN)...\")\nfilled_tr, exp_tr = process_manifest_into_memmap(m_train, \"train\")\nprint(f\"[Stage 6] TRAIN filled: {filled_tr:,} rows (expected ordering size={exp_tr:,})\")\n\nprint(\"\\n[Stage 6] Building fixed-length cache (TEST)...\")\nfilled_te, exp_te = process_manifest_into_memmap(m_test, \"test\")\nprint(f\"[Stage 6] TEST filled:  {filled_te:,} rows (expected ordering size={exp_te:,})\")\n\n# Flush memmaps\nXtr.flush(); Btr.flush(); Mtr.flush()\nXte.flush(); Bte.flush(); Mte.flush()\n\n# Save ids + y + meta\nnp.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\nnp.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids, dtype=\"S\"))\nnp.save(FIX_DIR / \"train_y.npy\",   y_train)\n\nnp.save(train_len_path, origlen_tr)\nnp.save(train_win_path, winstart_tr)\nnp.save(test_len_path,  origlen_te)\nnp.save(test_win_path,  winstart_te)\n\n# ----------------------------\n# 4) Sanity checks (anti-error)\n# ----------------------------\ndef sanity_check(which: str, n_show: int = 3):\n    if which == \"train\":\n        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n        ids = train_ids\n        ol = origlen_tr\n    else:\n        Xmm, Bmm, Mmm = Xte, Bte, Mte\n        ids = test_ids\n        ol = origlen_te\n\n    # check masks and shapes\n    assert Xmm.shape[1] == MAX_LEN and Xmm.shape[2] == F\n    assert Bmm.shape[1] == MAX_LEN\n    assert Mmm.shape[1] == MAX_LEN\n\n    # show a few random examples\n    rng = np.random.default_rng(2025)\n    idxs = rng.choice(len(ids), size=min(n_show, len(ids)), replace=False)\n    print(f\"\\n[Stage 6] Sanity samples ({which}):\")\n    for i in idxs:\n        msum = int(Mmm[i].sum())\n        print(f\"- idx={i} oid={ids[i]} orig_len={int(ol[i])} kept={msum} | bands_unique={sorted(set(Bmm[i,:msum].tolist()))}\")\n\nsanity_check(\"train\", n_show=3)\nsanity_check(\"test\", n_show=3)\n\n# ----------------------------\n# 5) Save config\n# ----------------------------\npolicy_cfg = {\n    \"max_len\": int(MAX_LEN),\n    \"feature_names\": list(SEQ_FEATURE_NAMES),\n    \"weights\": {\"snr\": float(W_SNR), \"flux\": float(W_FLX), \"detected\": float(W_DET)},\n    \"snr_tanh_scale_used_in_stage5\": float(globals().get(\"SNR_TANH_SCALE\", 10.0)) if \"SNR_TANH_SCALE\" in globals() else None,\n    \"train_order\": \"df_train_meta.index\",\n    \"test_order\": \"df_sub.object_id\" if (\"df_sub\" in globals() and \"object_id\" in df_sub.columns) else \"df_test_meta.index\",\n    \"files\": {\n        \"train_X\": str(train_X_path),\n        \"train_B\": str(train_B_path),\n        \"train_M\": str(train_M_path),\n        \"train_y\": str(FIX_DIR / \"train_y.npy\"),\n        \"train_ids\": str(FIX_DIR / \"train_ids.npy\"),\n        \"test_X\": str(test_X_path),\n        \"test_B\": str(test_B_path),\n        \"test_M\": str(test_M_path),\n        \"test_ids\": str(FIX_DIR / \"test_ids.npy\"),\n    }\n}\ncfg_path = FIX_DIR / \"length_policy_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(policy_cfg, f, indent=2)\n\nprint(\"\\n[Stage 6] DONE\")\nprint(f\"- Saved fixed cache dir: {FIX_DIR}\")\nprint(f\"- Saved config: {cfg_path}\")\n\n# Export globals for training stage\nglobals().update({\n    \"FIX_DIR\": FIX_DIR,\n    \"MAX_LEN\": MAX_LEN,\n    \"FIX_TRAIN_X_PATH\": train_X_path,\n    \"FIX_TRAIN_B_PATH\": train_B_path,\n    \"FIX_TRAIN_M_PATH\": train_M_path,\n    \"FIX_TEST_X_PATH\": test_X_path,\n    \"FIX_TEST_B_PATH\": test_B_path,\n    \"FIX_TEST_M_PATH\": test_M_path,\n    \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n    \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n    \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n    \"FIX_POLICY_CFG_PATH\": cfg_path,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:53.537086Z","iopub.execute_input":"2026-01-01T15:21:53.537411Z","iopub.status.idle":"2026-01-01T15:21:55.660233Z","shell.execute_reply.started":"2026-01-01T15:21:53.537383Z","shell.execute_reply":"2026-01-01T15:21:55.658751Z"}},"outputs":[{"name":"stdout","text":"\nTRAIN length stats\n- n_objects=3,043 | min=17 | p50=150 | p90=194 | p95=386 | p99=908 | max=1164\n\nTEST length stats\n- n_objects=7,135 | min=18 | p50=152 | p90=193 | p95=542 | p99=990 | max=1186\n\n[Stage 6] Chosen MAX_LEN = 512 (based on p95=542)\n\n[Stage 6] Building fixed-length cache (TRAIN)...\n[Stage 6] TRAIN filled: 3,043 rows (expected ordering size=3,043)\n\n[Stage 6] Building fixed-length cache (TEST)...\n[Stage 6] TEST filled:  7,135 rows (expected ordering size=7,135)\n\n[Stage 6] Sanity samples (train):\n- idx=1360 oid=gwilwileth_adel_amloth orig_len=157 kept=157 | bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=3020 oid=vin_araf_gwador orig_len=151 kept=151 | bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=3025 oid=ylf_alph_mindon orig_len=167 kept=167 | bands_unique=[0, 1, 2, 3, 4, 5]\n\n[Stage 6] Sanity samples (test):\n- idx=3191 oid=rom_bellas_lebdas orig_len=142 kept=142 | bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=7082 oid=nim_nestad_thor orig_len=161 kept=161 | bands_unique=[0, 1, 2, 3, 4, 5]\n- idx=7094 oid=rach_bellas_dol orig_len=156 kept=156 | bands_unique=[0, 1, 2, 3, 4, 5]\n\n[Stage 6] DONE\n- Saved fixed cache dir: /kaggle/working/mallorn_run/artifacts/fixed_seq\n- Saved config: /kaggle/working/mallorn_run/artifacts/fixed_seq/length_policy_config.json\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"77"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# CV Split (Object-Level, Stratified)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL, Kaggle CPU-SAFE)\n#\n# Tujuan:\n# - Buat Stratified K-Fold di level object_id (bukan per baris lightcurve)\n# - Konsisten dengan urutan TRAIN yang dipakai di STAGE 6 (fixed_seq/train_ids.npy)\n#\n# Output:\n# - artifacts/cv_folds.csv                (object_id -> fold)\n# - artifacts/cv_folds.npz                (val_idx per fold, untuk training cepat)\n# - artifacts/cv_report.txt               (ringkasan distribusi kelas per fold)\n# - globals: fold_assign, folds (list of dict)\n# ============================================================\n\nimport gc, json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"df_train_meta\", \"ART_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 2 dulu (Load and Validate Train/Test Logs).\")\n\nSEED = int(globals().get(\"SEED\", 2025))\n\n# Prefer using the same ordering as STAGE 6 fixed cache (if available)\ntrain_ids = None\nif \"FIX_DIR\" in globals():\n    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n    if p.exists():\n        train_ids = np.load(p, allow_pickle=False).astype(\"S\").astype(str).tolist()\n\nif train_ids is None:\n    # fallback: df_train_meta index order\n    train_ids = df_train_meta.index.astype(str).tolist()\n\n# y aligned to train_ids\ny = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n\nN = len(train_ids)\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\nif pos == 0 or neg == 0:\n    raise RuntimeError(f\"Invalid class distribution: pos={pos}, neg={neg}. Tidak bisa Stratified CV.\")\n\n# ----------------------------\n# 1) Choose n_splits safely (anti-error when positives are very few)\n# ----------------------------\nDEFAULT_SPLITS = 5\n# Ensure each fold can contain at least 1 positive and 1 negative\nmax_splits_by_pos = pos\nmax_splits_by_neg = neg\nn_splits = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg)\n\nif n_splits < 2:\n    raise RuntimeError(\n        f\"Terlalu sedikit sampel untuk CV stratified. pos={pos}, neg={neg}. \"\n        \"Minimal butuh >=2 per kelas untuk KFold.\"\n    )\n\nprint(f\"[Stage 7] Building StratifiedKFold: n_splits={n_splits} | N={N:,} | pos={pos:,} | neg={neg:,} | seed={SEED}\")\n\n# ----------------------------\n# 2) Build folds\n# ----------------------------\ntry:\n    from sklearn.model_selection import StratifiedKFold\nexcept Exception as e:\n    raise RuntimeError(\"scikit-learn tidak tersedia. Pastikan Kaggle environment punya sklearn.\") from e\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n\nfold_assign = np.full(N, -1, dtype=np.int16)\nfolds = []\n\nfor fold, (_, val_idx) in enumerate(skf.split(np.zeros(N), y)):\n    fold_assign[val_idx] = fold\n    folds.append({\n        \"fold\": int(fold),\n        \"val_idx\": val_idx.astype(np.int32),\n    })\n\nif (fold_assign < 0).any():\n    raise RuntimeError(\"Fold assignment masih ada -1 (ada object belum ter-assign). Ini tidak seharusnya terjadi.\")\n\n# ----------------------------\n# 3) Validate distribution per fold (anti-silent-bug)\n# ----------------------------\nlines = []\nlines.append(f\"StratifiedKFold n_splits={n_splits} seed={SEED}\")\nlines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.4f}%\")\nlines.append(\"Per-fold distribution:\")\n\nok = True\nfor f in range(n_splits):\n    idx = np.where(fold_assign == f)[0]\n    yf = y[idx]\n    pf = int((yf == 1).sum())\n    nf = int((yf == 0).sum())\n    lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:7.4f}%\")\n    # basic checks\n    if pf == 0 or nf == 0:\n        ok = False\n\nif not ok:\n    raise RuntimeError(\n        \"Ada fold yang tidak mengandung salah satu kelas (pos=0 atau neg=0). \"\n        \"Coba turunkan n_splits atau pastikan stratify berjalan benar.\"\n    )\n\n# ----------------------------\n# 4) Save artifacts\n# ----------------------------\nART_DIR = Path(ART_DIR)\ncv_dir = ART_DIR / \"cv\"\ncv_dir.mkdir(parents=True, exist_ok=True)\n\ndf_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\nfolds_csv = cv_dir / \"cv_folds.csv\"\ndf_folds.to_csv(folds_csv, index=False)\n\n# Save val_idx per fold in one npz (train_idx can be derived as ~val_idx)\nnpz_path = cv_dir / \"cv_folds.npz\"\nnpz_kwargs = {f\"val_idx_{f}\": folds[f][\"val_idx\"] for f in range(n_splits)}\nnp.savez(npz_path, **npz_kwargs)\n\nreport_path = cv_dir / \"cv_report.txt\"\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\ncfg_path = cv_dir / \"cv_config.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(\n        {\n            \"seed\": SEED,\n            \"n_splits\": int(n_splits),\n            \"order_source\": \"fixed_seq/train_ids.npy\" if (\"FIX_DIR\" in globals() and (Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\").exists()) else \"df_train_meta.index\",\n            \"artifacts\": {\n                \"folds_csv\": str(folds_csv),\n                \"folds_npz\": str(npz_path),\n                \"report_txt\": str(report_path),\n            },\n        },\n        f,\n        indent=2,\n    )\n\nprint(\"\\n[Stage 7] CV split OK\")\nprint(f\"- Saved: {folds_csv}\")\nprint(f\"- Saved: {npz_path}\")\nprint(f\"- Saved: {report_path}\")\nprint(f\"- Saved: {cfg_path}\")\nprint(\"\\n\".join(lines[-(n_splits+2):]))  # show short tail\n\n# Export globals for next stage\nglobals().update({\n    \"CV_DIR\": cv_dir,\n    \"n_splits\": n_splits,\n    \"train_ids_ordered\": train_ids,  # matches fold_assign\n    \"y_ordered\": y,\n    \"fold_assign\": fold_assign,\n    \"folds\": folds,\n    \"CV_FOLDS_CSV\": folds_csv,\n    \"CV_FOLDS_NPZ\": npz_path,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:55.662033Z","iopub.execute_input":"2026-01-01T15:21:55.663839Z","iopub.status.idle":"2026-01-01T15:21:57.017771Z","shell.execute_reply.started":"2026-01-01T15:21:55.663787Z","shell.execute_reply":"2026-01-01T15:21:57.016603Z"}},"outputs":[{"name":"stdout","text":"[Stage 7] Building StratifiedKFold: n_splits=5 | N=3,043 | pos=148 | neg=2,895 | seed=2025\n\n[Stage 7] CV split OK\n- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_folds.csv\n- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_folds.npz\n- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_report.txt\n- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_config.json\nTotal: N=3043 | pos=148 | neg=2895 | pos%=4.8636%\nPer-fold distribution:\n- fold 0: n=   609 | pos=   30 | neg=   579 | pos%= 4.9261%\n- fold 1: n=   609 | pos=   30 | neg=   579 | pos%= 4.9261%\n- fold 2: n=   609 | pos=   30 | neg=   579 | pos%= 4.9261%\n- fold 3: n=   608 | pos=   29 | neg=   579 | pos%= 4.7697%\n- fold 4: n=   608 | pos=   29 | neg=   579 | pos%= 4.7697%\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"33"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Train Multiband Event Transformer (CPU-Safe Configuration)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 8 — Train Multiband Event Transformer (CPU-Safe Configuration) (ONE CELL)\n# - Kaggle Web (CPU only): small model, small batch, num_workers=0\n# - Trains Stratified CV folds from STAGE 7\n# - Saves best checkpoint per fold + OOF probabilities (for later threshold tuning)\n#\n# Requires globals from previous stages:\n#   FIX_DIR, MAX_LEN, SEQ_FEATURE_NAMES\n#   df_train_meta\n#   n_splits, folds, train_ids_ordered, y_ordered\n#   CKPT_DIR, OOF_DIR, LOG_DIR\n# ============================================================\n\nimport os, gc, json, math, time, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\n    \"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\n    \"df_train_meta\",\n    \"n_splits\",\"folds\",\"train_ids_ordered\",\"y_ordered\",\n    \"CKPT_DIR\",\"OOF_DIR\",\"LOG_DIR\",\n]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n\n# ----------------------------\n# 1) Imports (torch) + CPU safety\n# ----------------------------\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\ndevice = torch.device(\"cpu\")\n\n# CPU thread guard (avoid oversubscription)\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\n# ----------------------------\n# 2) Open fixed-length memmaps (do NOT load into RAM)\n# ----------------------------\nFIX_DIR = Path(FIX_DIR)\ntrain_ids = list(train_ids_ordered)\ny = np.asarray(y_ordered, dtype=np.int8)\nN = len(train_ids)\nL = int(MAX_LEN)\nFdim = len(SEQ_FEATURE_NAMES)\n\ntrain_X_path = FIX_DIR / \"train_X.dat\"\ntrain_B_path = FIX_DIR / \"train_B.dat\"\ntrain_M_path = FIX_DIR / \"train_M.dat\"\n\nfor p in [train_X_path, train_B_path, train_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\nX_mm = np.memmap(train_X_path, dtype=np.float32, mode=\"r\", shape=(N, L, Fdim))\nB_mm = np.memmap(train_B_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\nM_mm = np.memmap(train_M_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n\n# ----------------------------\n# 3) Build global features (object-level) aligned to train_ids\n# ----------------------------\n# Use only safe, available columns; fill missing with 0\nG_COLS = [\"Z\", \"Z_err\", \"EBV\", \"Z_missing\", \"Z_err_missing\", \"EBV_missing\", \"is_photoz\"]\nfor c in G_COLS:\n    if c not in df_train_meta.columns:\n        df_train_meta[c] = 0.0\n\nG = df_train_meta.loc[train_ids, G_COLS].copy()\nfor c in G_COLS:\n    G[c] = pd.to_numeric(G[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n\nG_np = G.to_numpy(dtype=np.float32, copy=False)\n\n# Simple standardization for numeric stability (avoid leakage: only within train set)\n# Standardize all cols (including flags) is okay; flags become small values.\ng_mean = G_np.mean(axis=0).astype(np.float32)\ng_std  = G_np.std(axis=0).astype(np.float32)\ng_std  = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\nG_np_z = ((G_np - g_mean) / g_std).astype(np.float32)\n\n# Save scaler for later test inference stage\nscaler_path = Path(LOG_DIR) / \"global_scaler.json\"\nwith open(scaler_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"cols\": G_COLS, \"mean\": g_mean.tolist(), \"std\": g_std.tolist()}, f, indent=2)\n\n# ----------------------------\n# 4) Dataset / DataLoader (num_workers=0)\n# ----------------------------\nclass MemmapSeqDataset(torch.utils.data.Dataset):\n    def __init__(self, idx, X_mm, B_mm, M_mm, G_np_z, y=None):\n        self.idx = np.asarray(idx, dtype=np.int32)\n        self.X_mm = X_mm\n        self.B_mm = B_mm\n        self.M_mm = M_mm\n        self.G = G_np_z\n        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n\n    def __len__(self):\n        return len(self.idx)\n\n    def __getitem__(self, i):\n        j = int(self.idx[i])\n        X = self.X_mm[j]  # (L,F) float32\n        B = self.B_mm[j]  # (L,) int8\n        M = self.M_mm[j]  # (L,) int8\n        G = self.G[j]     # (G,) float32\n        if self.y is None:\n            return (\n                torch.from_numpy(X),\n                torch.from_numpy(B.astype(np.int64, copy=False)),\n                torch.from_numpy(M.astype(np.int64, copy=False)),\n                torch.from_numpy(G),\n            )\n        else:\n            y = self.y[j]\n            return (\n                torch.from_numpy(X),\n                torch.from_numpy(B.astype(np.int64, copy=False)),\n                torch.from_numpy(M.astype(np.int64, copy=False)),\n                torch.from_numpy(G),\n                torch.tensor(float(y), dtype=torch.float32),\n            )\n\ndef make_loader(ds, batch_size, shuffle):\n    return torch.utils.data.DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n# ----------------------------\n# 5) Model: Multiband Event Transformer (CPU-safe small)\n# ----------------------------\nclass MultibandEventTransformer(nn.Module):\n    def __init__(self, feat_dim, n_bands=6, d_model=128, n_heads=4, n_layers=2, ff_mult=2, dropout=0.10, g_dim=7):\n        super().__init__()\n        self.feat_dim = feat_dim\n        self.n_bands = n_bands\n        self.d_model = d_model\n\n        self.x_proj = nn.Linear(feat_dim, d_model)\n        self.band_emb = nn.Embedding(n_bands, d_model)\n\n        # learnable positional embedding (fixed MAX_LEN)\n        self.pos_emb = nn.Parameter(torch.zeros(1, L, d_model))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=int(d_model * ff_mult),\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        # Attention pooling\n        self.attn = nn.Linear(d_model, 1)\n\n        # Global feature fusion\n        self.g_proj = nn.Sequential(\n            nn.Linear(g_dim, d_model // 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model + (d_model // 2), d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def forward(self, X, band_id, mask, G):\n        \"\"\"\n        X: (B,L,F) float\n        band_id: (B,L) long in [0..5]\n        mask: (B,L) long/int (1=real token, 0=pad)\n        G: (B,g_dim) float\n        \"\"\"\n        # ensure correct dtypes\n        X = X.to(torch.float32)\n        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n        mask = mask.to(torch.long)\n\n        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n\n        # Transformer expects src_key_padding_mask with True for pads\n        pad_mask = (mask == 0)  # (B,L) bool\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        # Attention pooling with mask\n        a = self.attn(h).squeeze(-1)  # (B,L)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)   # (B,L)\n        pooled = torch.sum(h * w.unsqueeze(-1), dim=1)  # (B,d_model)\n\n        g = self.g_proj(G.to(torch.float32))\n        z = torch.cat([pooled, g], dim=1)\n        logit = self.head(z).squeeze(-1)  # (B,)\n        return logit\n\n# ----------------------------\n# 6) Training hyperparams (CPU-safe)\n# ----------------------------\nCFG = {\n    \"d_model\": 128,\n    \"n_heads\": 4,\n    \"n_layers\": 2,\n    \"ff_mult\": 2,\n    \"dropout\": 0.10,\n    \"batch_size\": 16,          # keep modest for CPU\n    \"grad_accum\": 2,           # effective batch = 32\n    \"epochs\": 10,\n    \"lr\": 3e-4,\n    \"weight_decay\": 0.01,\n    \"patience\": 3,             # early stopping on val_loss\n    \"max_grad_norm\": 1.0,\n}\ncfg_path = Path(LOG_DIR) / \"train_cfg.json\"\nwith open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(CFG, f, indent=2)\n\n# ----------------------------\n# 7) Loss: imbalance handling\n# ----------------------------\npos = int((y == 1).sum())\nneg = int((y == 0).sum())\nif pos == 0:\n    raise RuntimeError(\"No positive samples in training. Tidak bisa training classifier.\")\npos_weight = float(neg / max(pos, 1))\npos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n\nprint(\"[Stage 8] TRAIN CONFIG (CPU)\")\nprint(f\"- N={N:,} | pos={pos:,} | neg={neg:,} | pos_weight={pos_weight:.4f}\")\nprint(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\nprint(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\nprint(f\"- Saved cfg: {cfg_path}\")\nprint(f\"- Saved global scaler: {scaler_path}\")\n\n# ----------------------------\n# 8) Metrics helpers (no sklearn dependency)\n# ----------------------------\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef f1_binary(y_true, y_pred01):\n    y_true = y_true.astype(np.int32)\n    y_pred01 = y_pred01.astype(np.int32)\n    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n    if tp == 0:\n        return 0.0\n    prec = tp / max(tp + fp, 1)\n    rec  = tp / max(tp + fn, 1)\n    if prec + rec == 0:\n        return 0.0\n    return float(2 * prec * rec / (prec + rec))\n\n@torch.no_grad()\ndef eval_model(model, loader):\n    model.eval()\n    losses = []\n    logits_all = []\n    y_all = []\n    for batch in loader:\n        Xb, Bb, Mb, Gb, yb = batch\n        Xb = Xb.to(device)\n        Bb = Bb.to(device)\n        Mb = Mb.to(device)\n        Gb = Gb.to(device)\n        yb = yb.to(device)\n\n        logit = model(Xb, Bb, Mb, Gb)\n        loss = criterion(logit, yb)\n        losses.append(float(loss.item()))\n        logits_all.append(logit.detach().cpu().numpy())\n        y_all.append(yb.detach().cpu().numpy())\n\n    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n\n    probs = sigmoid_np(logits_all)\n    pred01 = (probs >= 0.5).astype(np.int8)\n    f1 = f1_binary(y_all, pred01)\n    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1\n\n# ----------------------------\n# 9) Train CV\n# ----------------------------\nOOF_DIR = Path(OOF_DIR); OOF_DIR.mkdir(parents=True, exist_ok=True)\nCKPT_DIR = Path(CKPT_DIR); CKPT_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\n\noof_prob = np.zeros((N,), dtype=np.float32)\nfold_metrics = []\n\n# Precompute all indices\nall_idx = np.arange(N, dtype=np.int32)\n\nstart_time = time.time()\nfor fold_info in folds:\n    fold = int(fold_info[\"fold\"])\n    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n    val_mask = np.zeros(N, dtype=bool)\n    val_mask[val_idx] = True\n    tr_idx = all_idx[~val_mask]\n\n    print(f\"\\n[Stage 8] FOLD {fold}/{n_splits-1} | train={len(tr_idx):,} val={len(val_idx):,}\")\n\n    # Datasets/Loaders\n    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_np_z, y=y)\n    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_np_z, y=y)\n\n    dl_tr = make_loader(ds_tr, batch_size=CFG[\"batch_size\"], shuffle=True)\n    dl_va = make_loader(ds_va, batch_size=CFG[\"batch_size\"], shuffle=False)\n\n    # Model + optim\n    model = MultibandEventTransformer(\n        feat_dim=Fdim,\n        n_bands=6,\n        d_model=CFG[\"d_model\"],\n        n_heads=CFG[\"n_heads\"],\n        n_layers=CFG[\"n_layers\"],\n        ff_mult=CFG[\"ff_mult\"],\n        dropout=CFG[\"dropout\"],\n        g_dim=G_np_z.shape[1],\n    ).to(device)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n\n    best_val_loss = float(\"inf\")\n    best_epoch = -1\n    best_probs = None\n    patience_left = int(CFG[\"patience\"])\n\n    step = 0\n    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        running = 0.0\n        n_batches = 0\n\n        for batch in dl_tr:\n            Xb, Bb, Mb, Gb, yb = batch\n            Xb = Xb.to(device)\n            Bb = Bb.to(device)\n            Mb = Mb.to(device)\n            Gb = Gb.to(device)\n            yb = yb.to(device)\n\n            logit = model(Xb, Bb, Mb, Gb)\n            loss = criterion(logit, yb)\n            loss = loss / float(CFG[\"grad_accum\"])\n            loss.backward()\n\n            if (step + 1) % int(CFG[\"grad_accum\"]) == 0:\n                if CFG[\"max_grad_norm\"] is not None:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n                opt.step()\n                opt.zero_grad(set_to_none=True)\n\n            running += float(loss.item()) * float(CFG[\"grad_accum\"])\n            n_batches += 1\n            step += 1\n\n        train_loss = running / max(n_batches, 1)\n\n        # Validate\n        val_loss, probs, y_val, f1_05 = eval_model(model, dl_va)\n\n        improved = val_loss < (best_val_loss - 1e-6)\n        if improved:\n            best_val_loss = val_loss\n            best_epoch = epoch\n            best_probs = probs.copy()\n\n            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n            torch.save(\n                {\n                    \"fold\": fold,\n                    \"epoch\": epoch,\n                    \"model_state\": model.state_dict(),\n                    \"cfg\": CFG,\n                    \"seq_feature_names\": SEQ_FEATURE_NAMES,\n                    \"max_len\": L,\n                    \"global_cols\": G_COLS,\n                    \"global_scaler\": {\"mean\": g_mean, \"std\": g_std},\n                },\n                ckpt_path,\n            )\n            patience_left = int(CFG[\"patience\"])\n        else:\n            patience_left -= 1\n\n        print(f\"  epoch {epoch:02d} | train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | f1@0.5={f1_05:.4f} | best_epoch={best_epoch} | patience_left={patience_left}\")\n\n        if patience_left <= 0:\n            break\n\n    if best_probs is None:\n        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n\n    # Fill OOF\n    oof_prob[val_idx] = best_probs.astype(np.float32)\n\n    # Fold summary\n    pred01 = (best_probs >= 0.5).astype(np.int8)\n    best_f1_05 = f1_binary(y[val_idx], pred01)\n\n    fold_metrics.append({\n        \"fold\": fold,\n        \"val_size\": int(len(val_idx)),\n        \"best_epoch\": int(best_epoch),\n        \"best_val_loss\": float(best_val_loss),\n        \"f1_at_0p5\": float(best_f1_05),\n    })\n\n    # Cleanup\n    del model, opt, ds_tr, ds_va, dl_tr, dl_va\n    gc.collect()\n\nelapsed = time.time() - start_time\n\n# ----------------------------\n# 10) Save OOF artifacts\n# ----------------------------\noof_path_npy = OOF_DIR / \"oof_prob.npy\"\nnp.save(oof_path_npy, oof_prob)\n\n# Also save CSV for convenience\ndf_oof = pd.DataFrame({\n    \"object_id\": train_ids,\n    \"target\": y.astype(int),\n    \"oof_prob\": oof_prob.astype(np.float32),\n})\noof_path_csv = OOF_DIR / \"oof_prob.csv\"\ndf_oof.to_csv(oof_path_csv, index=False)\n\nmetrics_path = OOF_DIR / \"fold_metrics.json\"\nwith open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed)}, f, indent=2)\n\n# quick overall f1@0.5 on OOF (not final; threshold tuning comes later)\noof_pred01 = (oof_prob >= 0.5).astype(np.int8)\noof_f1_05 = f1_binary(y, oof_pred01)\n\nprint(\"\\n[Stage 8] CV TRAIN DONE\")\nprint(f\"- elapsed: {elapsed/60:.2f} min\")\nprint(f\"- OOF saved: {oof_path_npy}\")\nprint(f\"- OOF saved: {oof_path_csv}\")\nprint(f\"- fold metrics: {metrics_path}\")\nprint(f\"- OOF f1@0.5 (rough): {oof_f1_05:.4f}\")\n\n# Export globals for next stages (threshold tuning + test inference)\nglobals().update({\n    \"oof_prob\": oof_prob,\n    \"OOF_PROB_PATH\": oof_path_npy,\n    \"OOF_CSV_PATH\": oof_path_csv,\n    \"FOLD_METRICS_PATH\": metrics_path,\n    \"GLOBAL_SCALER_PATH\": scaler_path,\n    \"TRAIN_CFG_PATH\": cfg_path,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:21:57.019466Z","iopub.execute_input":"2026-01-01T15:21:57.019789Z"}},"outputs":[{"name":"stdout","text":"[Stage 8] TRAIN CONFIG (CPU)\n- N=3,043 | pos=148 | neg=2,895 | pos_weight=19.5608\n- Model: d_model=128 heads=4 layers=2 dropout=0.1\n- Batch=16 grad_accum=2 epochs=10 lr=0.0003\n- Saved cfg: /kaggle/working/mallorn_run/logs/train_cfg.json\n- Saved global scaler: /kaggle/working/mallorn_run/logs/global_scaler.json\n\n[Stage 8] FOLD 0/4 | train=2,434 val=609\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n/tmp/ipykernel_55/1596740308.py:138: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n  torch.from_numpy(X),\n","output_type":"stream"},{"name":"stdout","text":"  epoch 01 | train_loss=1.35337 | val_loss=1.30779 | f1@0.5=0.0986 | best_epoch=1 | patience_left=3\n  epoch 02 | train_loss=1.33828 | val_loss=1.31824 | f1@0.5=0.0000 | best_epoch=1 | patience_left=2\n  epoch 03 | train_loss=1.31927 | val_loss=1.31393 | f1@0.5=0.0990 | best_epoch=1 | patience_left=1\n  epoch 04 | train_loss=1.32508 | val_loss=1.32877 | f1@0.5=0.0997 | best_epoch=1 | patience_left=0\n\n[Stage 8] FOLD 1/4 | train=2,434 val=609\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# OOF Prediction + Threshold Tuning","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL, Kaggle CPU-SAFE)\n#\n# Tujuan:\n# - Pakai oof_prob (STAGE 8) untuk cari threshold terbaik (maksimalkan F1)\n# - Simpan:\n#   * best_threshold\n#   * curve ringkas (opsional)\n#   * report txt/json\n#\n# Output:\n# - /kaggle/working/mallorn_run/oof/threshold_tuning.json\n# - /kaggle/working/mallorn_run/oof/threshold_report.txt\n# - globals: BEST_THR, thr_table\n# ============================================================\n\nimport gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\"OOF_DIR\", \"df_train_meta\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n\nOOF_DIR = Path(OOF_DIR)\nOOF_DIR.mkdir(parents=True, exist_ok=True)\n\n# Load OOF probabilities\noof_prob = None\nif \"oof_prob\" in globals():\n    oof_prob = np.asarray(globals()[\"oof_prob\"], dtype=np.float32)\nelse:\n    p = OOF_DIR / \"oof_prob.npy\"\n    if not p.exists():\n        raise FileNotFoundError(\"OOF prob not found. Jalankan STAGE 8 (training) dulu.\")\n    oof_prob = np.load(p).astype(np.float32)\n\n# Align y ordering: prefer train_ids_ordered if available\nif \"train_ids_ordered\" in globals():\n    train_ids = list(globals()[\"train_ids_ordered\"])\n    y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\nelse:\n    # fallback: df_train_meta order must match oof_prob length; if not, fail\n    if len(oof_prob) != len(df_train_meta):\n        raise RuntimeError(\"Cannot align y: missing train_ids_ordered and lengths mismatch.\")\n    y = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n\nif len(oof_prob) != len(y):\n    raise RuntimeError(f\"Length mismatch: oof_prob={len(oof_prob)} vs y={len(y)}\")\n\n# ----------------------------\n# 1) F1 metric (binary)\n# ----------------------------\ndef f1_binary(y_true, y_pred01):\n    y_true = y_true.astype(np.int32)\n    y_pred01 = y_pred01.astype(np.int32)\n    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n    if tp == 0:\n        return 0.0\n    prec = tp / max(tp + fp, 1)\n    rec  = tp / max(tp + fn, 1)\n    if prec + rec == 0:\n        return 0.0\n    return float(2 * prec * rec / (prec + rec))\n\ndef precision_recall(y_true, y_pred01):\n    y_true = y_true.astype(np.int32)\n    y_pred01 = y_pred01.astype(np.int32)\n    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n    prec = tp / max(tp + fp, 1)\n    rec  = tp / max(tp + fn, 1)\n    return float(prec), float(rec), tp, fp, fn\n\n# ----------------------------\n# 2) Threshold sweep (fast + robust)\n# ----------------------------\n# Option A: sweep fixed grid\ngrid = np.concatenate([\n    np.linspace(0.01, 0.10, 19),\n    np.linspace(0.10, 0.90, 81),\n    np.linspace(0.90, 0.99, 19),\n]).astype(np.float32)\n\n# Option B: also evaluate thresholds at unique prob quantiles (more adaptive)\nqs = np.linspace(0.01, 0.99, 99, dtype=np.float32)\nquant_thr = np.quantile(oof_prob, qs).astype(np.float32)\nthr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr]), 0.0, 1.0))\n\nbest = {\"thr\": 0.5, \"f1\": -1.0, \"prec\": 0.0, \"rec\": 0.0, \"tp\": 0, \"fp\": 0, \"fn\": 0, \"pos_pred\": 0}\n\nrows = []\nfor thr in thr_candidates:\n    pred = (oof_prob >= thr).astype(np.int8)\n    f1 = f1_binary(y, pred)\n    prec, rec, tp, fp, fn = precision_recall(y, pred)\n    pos_pred = int(pred.sum())\n    rows.append((float(thr), float(f1), float(prec), float(rec), int(tp), int(fp), int(fn), pos_pred))\n\n    # tie-breaker: prefer higher recall if F1 equal, then fewer false positives\n    if (f1 > best[\"f1\"] + 1e-12) or (\n        abs(f1 - best[\"f1\"]) <= 1e-12 and (rec > best[\"rec\"] + 1e-12)\n    ) or (\n        abs(f1 - best[\"f1\"]) <= 1e-12 and abs(rec - best[\"rec\"]) <= 1e-12 and (fp < best[\"fp\"])\n    ):\n        best.update({\"thr\": float(thr), \"f1\": float(f1), \"prec\": float(prec), \"rec\": float(rec),\n                     \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn), \"pos_pred\": int(pos_pred)})\n\nthr_table = pd.DataFrame(rows, columns=[\"thr\",\"f1\",\"precision\",\"recall\",\"tp\",\"fp\",\"fn\",\"pos_pred\"])\nthr_table = thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).reset_index(drop=True)\n\nBEST_THR = float(best[\"thr\"])\n\n# ----------------------------\n# 3) Additional sanity: compare to default 0.5\n# ----------------------------\npred05 = (oof_prob >= 0.5).astype(np.int8)\nf1_05 = f1_binary(y, pred05)\nprec_05, rec_05, tp_05, fp_05, fn_05 = precision_recall(y, pred05)\n\n# ----------------------------\n# 4) Save report\n# ----------------------------\nout_json = OOF_DIR / \"threshold_tuning.json\"\nout_txt  = OOF_DIR / \"threshold_report.txt\"\nout_csv  = OOF_DIR / \"threshold_table_top200.csv\"\n\npayload = {\n    \"best_threshold\": BEST_THR,\n    \"best_f1\": best[\"f1\"],\n    \"best_precision\": best[\"prec\"],\n    \"best_recall\": best[\"rec\"],\n    \"best_counts\": {\"tp\": best[\"tp\"], \"fp\": best[\"fp\"], \"fn\": best[\"fn\"], \"pos_pred\": best[\"pos_pred\"]},\n    \"baseline_thr_0p5\": {\"f1\": f1_05, \"precision\": prec_05, \"recall\": rec_05, \"tp\": tp_05, \"fp\": fp_05, \"fn\": fn_05, \"pos_pred\": int(pred05.sum())},\n    \"n_objects\": int(len(y)),\n    \"pos\": int((y == 1).sum()),\n    \"neg\": int((y == 0).sum()),\n}\n\nwith open(out_json, \"w\", encoding=\"utf-8\") as f:\n    json.dump(payload, f, indent=2)\n\nlines = []\nlines.append(\"OOF Threshold Tuning Report\")\nlines.append(f\"- N={payload['n_objects']} | pos={payload['pos']} | neg={payload['neg']} | pos%={payload['pos']/max(payload['n_objects'],1)*100:.4f}%\")\nlines.append(\"\")\nlines.append(\"Baseline @ thr=0.5\")\nlines.append(f\"- F1={f1_05:.6f} | P={prec_05:.6f} | R={rec_05:.6f} | tp={tp_05} fp={fp_05} fn={fn_05} | pos_pred={int(pred05.sum())}\")\nlines.append(\"\")\nlines.append(f\"BEST @ thr={BEST_THR:.6f}\")\nlines.append(f\"- F1={best['f1']:.6f} | P={best['prec']:.6f} | R={best['rec']:.6f} | tp={best['tp']} fp={best['fp']} fn={best['fn']} | pos_pred={best['pos_pred']}\")\nlines.append(\"\")\nlines.append(\"Top 10 thresholds by (F1, recall, precision):\")\nfor i in range(min(10, len(thr_table))):\n    r = thr_table.iloc[i]\n    lines.append(\n        f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | P={r['precision']:.6f} | R={r['recall']:.6f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\"\n    )\n\nwith open(out_txt, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines) + \"\\n\")\n\nthr_table.head(200).to_csv(out_csv, index=False)\n\nprint(\"[Stage 9] Threshold tuning DONE\")\nprint(f\"- Best threshold: {BEST_THR:.6f}\")\nprint(f\"- Best F1:        {best['f1']:.6f}  (P={best['prec']:.6f} R={best['rec']:.6f})\")\nprint(f\"- Baseline F1@0.5:{f1_05:.6f}  (P={prec_05:.6f} R={rec_05:.6f})\")\nprint(f\"- Saved: {out_json}\")\nprint(f\"- Saved: {out_txt}\")\nprint(f\"- Saved: {out_csv}\")\n\n# Export globals for next stages (test inference + submission)\nglobals().update({\n    \"BEST_THR\": BEST_THR,\n    \"thr_table\": thr_table,\n    \"THR_JSON_PATH\": out_json,\n    \"THR_REPORT_PATH\": out_txt,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Inference (Fold Ensemble)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL, Kaggle CPU-SAFE)\n#\n# Tujuan:\n# - Load fixed-length TEST cache (memmap) dari STAGE 6\n# - Load checkpoint fold_*.pt dari STAGE 8\n# - Predict probability untuk test set per fold\n# - Ensemble (mean) antar fold -> test_prob_ens.npy + test_prob_ens.csv\n#\n# Output:\n# - /kaggle/working/mallorn_run/artifacts/test_prob_fold.npy    (N_test, n_folds)\n# - /kaggle/working/mallorn_run/artifacts/test_prob_ens.npy     (N_test,)\n# - /kaggle/working/mallorn_run/artifacts/test_prob_ens.csv     (object_id, prob)\n# - globals: test_prob_ens\n# ============================================================\n\nimport os, gc, json, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nneed = [\n    \"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\n    \"df_test_meta\",\n    \"CKPT_DIR\",\"LOG_DIR\",\n    \"n_splits\",\n]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n\n# Torch\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\nexcept Exception as e:\n    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n\ndevice = torch.device(\"cpu\")\nSEED = int(globals().get(\"SEED\", 2025))\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# Thread guard\ntry:\n    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n    torch.set_num_interop_threads(1)\nexcept Exception:\n    pass\n\nFIX_DIR = Path(FIX_DIR)\nART_DIR = Path(globals()[\"ART_DIR\"])\nART_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 1) Load TEST ordering (must match STAGE 6)\n# ----------------------------\ntest_ids_path = FIX_DIR / \"test_ids.npy\"\nif not test_ids_path.exists():\n    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\ntest_ids = np.load(test_ids_path, allow_pickle=False).astype(\"S\").astype(str).tolist()\nNTE = len(test_ids)\n\n# ----------------------------\n# 2) Open fixed-length TEST memmaps\n# ----------------------------\nFdim = len(SEQ_FEATURE_NAMES)\nL = int(MAX_LEN)\n\ntest_X_path = FIX_DIR / \"test_X.dat\"\ntest_B_path = FIX_DIR / \"test_B.dat\"\ntest_M_path = FIX_DIR / \"test_M.dat\"\n\nfor p in [test_X_path, test_B_path, test_M_path]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n\nXte = np.memmap(test_X_path, dtype=np.float32, mode=\"r\", shape=(NTE, L, Fdim))\nBte = np.memmap(test_B_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\nMte = np.memmap(test_M_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n\n# ----------------------------\n# 3) Load global scaler + build standardized global features for TEST\n# ----------------------------\n# Prefer global scaler from STAGE 8\nscaler_json = Path(globals().get(\"GLOBAL_SCALER_PATH\", Path(LOG_DIR) / \"global_scaler.json\"))\nif not scaler_json.exists():\n    raise FileNotFoundError(f\"Missing global scaler json: {scaler_json}. Jalankan STAGE 8 dulu.\")\n\nwith open(scaler_json, \"r\", encoding=\"utf-8\") as f:\n    scaler = json.load(f)\nG_COLS = scaler[\"cols\"]\ng_mean = np.asarray(scaler[\"mean\"], dtype=np.float32)\ng_std  = np.asarray(scaler[\"std\"], dtype=np.float32)\ng_std  = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n\n# Ensure columns exist\nfor c in G_COLS:\n    if c not in df_test_meta.columns:\n        df_test_meta[c] = 0.0\n\nG = df_test_meta.loc[test_ids, G_COLS].copy()\nfor c in G_COLS:\n    G[c] = pd.to_numeric(G[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n\nG_np = G.to_numpy(dtype=np.float32, copy=False)\nG_np_z = ((G_np - g_mean) / g_std).astype(np.float32)\n\n# ----------------------------\n# 4) Dataset/Loader for inference\n# ----------------------------\nclass TestMemmapDataset(torch.utils.data.Dataset):\n    def __init__(self, Xmm, Bmm, Mmm, G_np_z):\n        self.Xmm = Xmm\n        self.Bmm = Bmm\n        self.Mmm = Mmm\n        self.G = G_np_z\n\n    def __len__(self):\n        return self.Xmm.shape[0]\n\n    def __getitem__(self, i):\n        X = self.Xmm[i]\n        B = self.Bmm[i].astype(np.int64, copy=False)\n        M = self.Mmm[i].astype(np.int64, copy=False)\n        G = self.G[i]\n        return (\n            torch.from_numpy(X),\n            torch.from_numpy(B),\n            torch.from_numpy(M),\n            torch.from_numpy(G),\n        )\n\ndef make_loader(ds, batch_size=64):\n    return torch.utils.data.DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n# ----------------------------\n# 5) Model definition (must match STAGE 8)\n# ----------------------------\nclass MultibandEventTransformer(nn.Module):\n    def __init__(self, feat_dim, n_bands=6, d_model=128, n_heads=4, n_layers=2, ff_mult=2, dropout=0.10, g_dim=7, max_len=512):\n        super().__init__()\n        self.feat_dim = feat_dim\n        self.n_bands = n_bands\n        self.d_model = d_model\n        self.max_len = max_len\n\n        self.x_proj = nn.Linear(feat_dim, d_model)\n        self.band_emb = nn.Embedding(n_bands, d_model)\n\n        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=int(d_model * ff_mult),\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n        self.attn = nn.Linear(d_model, 1)\n\n        self.g_proj = nn.Sequential(\n            nn.Linear(g_dim, d_model // 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(d_model + (d_model // 2), d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1),\n        )\n\n    def forward(self, X, band_id, mask, G):\n        X = X.to(torch.float32)\n        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n        mask = mask.to(torch.long)\n\n        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n\n        pad_mask = (mask == 0)\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n\n        a = self.attn(h).squeeze(-1)\n        a = a.masked_fill(pad_mask, -1e9)\n        w = torch.softmax(a, dim=1)\n        pooled = torch.sum(h * w.unsqueeze(-1), dim=1)\n\n        g = self.g_proj(G.to(torch.float32))\n        z = torch.cat([pooled, g], dim=1)\n        logit = self.head(z).squeeze(-1)\n        return logit\n\ndef sigmoid_np(x):\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\n@torch.no_grad()\ndef predict_probs(model, loader):\n    model.eval()\n    outs = []\n    for batch in loader:\n        Xb, Bb, Mb, Gb = batch\n        Xb = Xb.to(device)\n        Bb = Bb.to(device)\n        Mb = Mb.to(device)\n        Gb = Gb.to(device)\n        logit = model(Xb, Bb, Mb, Gb)\n        outs.append(logit.detach().cpu().numpy())\n    logits = np.concatenate(outs, axis=0) if outs else np.zeros((0,), dtype=np.float32)\n    return sigmoid_np(logits).astype(np.float32)\n\n# ----------------------------\n# 6) Load fold checkpoints + infer\n# ----------------------------\nCKPT_DIR = Path(CKPT_DIR)\nckpts = []\nfor f in range(int(n_splits)):\n    p = CKPT_DIR / f\"fold_{f}.pt\"\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n    ckpts.append(p)\n\n# Small batch for CPU safety (increase if fast enough)\nBATCH_SIZE = 64\n\nds_test = TestMemmapDataset(Xte, Bte, Mte, G_np_z)\ndl_test = make_loader(ds_test, batch_size=BATCH_SIZE)\n\ntest_prob_folds = np.zeros((NTE, int(n_splits)), dtype=np.float32)\n\nprint(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} (CPU)\")\nfor fold, ckpt_path in enumerate(ckpts):\n    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n\n    cfg = ckpt.get(\"cfg\", {})\n    d_model  = int(cfg.get(\"d_model\", 128))\n    n_heads  = int(cfg.get(\"n_heads\", 4))\n    n_layers = int(cfg.get(\"n_layers\", 2))\n    ff_mult  = int(cfg.get(\"ff_mult\", 2))\n    dropout  = float(cfg.get(\"dropout\", 0.10))\n\n    model = MultibandEventTransformer(\n        feat_dim=Fdim,\n        n_bands=6,\n        d_model=d_model,\n        n_heads=n_heads,\n        n_layers=n_layers,\n        ff_mult=ff_mult,\n        dropout=dropout,\n        g_dim=G_np_z.shape[1],\n        max_len=L,\n    ).to(device)\n\n    model.load_state_dict(ckpt[\"model_state\"], strict=True)\n\n    probs = predict_probs(model, dl_test)\n    if len(probs) != NTE:\n        raise RuntimeError(f\"Fold {fold}: probs length mismatch {len(probs)} vs {NTE}\")\n\n    test_prob_folds[:, fold] = probs\n    print(f\"  fold {fold}: prob_mean={float(probs.mean()):.6f} | prob_std={float(probs.std()):.6f}\")\n\n    del model, ckpt, probs\n    gc.collect()\n\n# Ensemble mean\ntest_prob_ens = test_prob_folds.mean(axis=1).astype(np.float32)\n\n# ----------------------------\n# 7) Save artifacts\n# ----------------------------\nfold_path = ART_DIR / \"test_prob_fold.npy\"\nens_path  = ART_DIR / \"test_prob_ens.npy\"\ncsv_path  = ART_DIR / \"test_prob_ens.csv\"\n\nnp.save(fold_path, test_prob_folds)\nnp.save(ens_path, test_prob_ens)\n\ndf_pred = pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens})\ndf_pred.to_csv(csv_path, index=False)\n\nprint(\"\\n[Stage 10] DONE\")\nprint(f\"- Saved fold probs: {fold_path}\")\nprint(f\"- Saved ens probs : {ens_path}\")\nprint(f\"- Saved csv       : {csv_path}\")\nprint(f\"- ens mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n\n# Export globals for submission\nglobals().update({\n    \"test_ids\": test_ids,\n    \"test_prob_folds\": test_prob_folds,\n    \"test_prob_ens\": test_prob_ens,\n    \"TEST_PROB_FOLD_PATH\": fold_path,\n    \"TEST_PROB_ENS_PATH\": ens_path,\n    \"TEST_PROB_CSV_PATH\": csv_path,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission Build","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 11 — Submission Build (ONE CELL, Kaggle CPU-SAFE)\n#\n# Tujuan:\n# - Ambil test_prob_ens (STAGE 10) + threshold BEST_THR (STAGE 9, optional)\n# - Buat submission.csv sesuai sample_submission.csv\n# - Strict checks: order, missing ids, duplicates, NaN/inf\n#\n# Output:\n# - /kaggle/working/submission.csv\n# - /kaggle/working/mallorn_run/submissions/submission.csv (copy)\n# - /kaggle/working/mallorn_run/submissions/submission_proba.csv (optional)\n# ============================================================\n\nimport gc, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ----------------------------\n# 0) Require previous stages\n# ----------------------------\nfor need in [\"PATHS\", \"SUB_DIR\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n\nsample_path = Path(PATHS[\"SAMPLE_SUB\"])\nif not sample_path.exists():\n    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n\ndf_sub = pd.read_csv(sample_path)\nif not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n\n# Get test probabilities\ntest_prob = None\ntest_ids = None\n\nif \"test_prob_ens\" in globals() and \"test_ids\" in globals():\n    test_prob = np.asarray(globals()[\"test_prob_ens\"], dtype=np.float32)\n    test_ids = list(globals()[\"test_ids\"])\nelse:\n    # fallback to saved artifact\n    if \"TEST_PROB_ENS_PATH\" in globals():\n        p = Path(globals()[\"TEST_PROB_ENS_PATH\"])\n        if p.exists():\n            test_prob = np.load(p).astype(np.float32)\n    if test_prob is None:\n        # try default location\n        p = Path(globals().get(\"ART_DIR\", \"/kaggle/working\")) / \"test_prob_ens.npy\"\n        if p.exists():\n            test_prob = np.load(p).astype(np.float32)\n    # ids\n    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n    pids = fix_dir / \"test_ids.npy\"\n    if pids.exists():\n        test_ids = np.load(pids, allow_pickle=False).astype(\"S\").astype(str).tolist()\n\nif test_prob is None or test_ids is None:\n    raise RuntimeError(\"Missing test predictions. Jalankan STAGE 10 dulu (Test Inference).\")\n\nif len(test_prob) != len(test_ids):\n    raise RuntimeError(f\"Length mismatch: test_prob={len(test_prob)} vs test_ids={len(test_ids)}\")\n\n# Determine threshold (optional)\nthr = float(globals().get(\"BEST_THR\", 0.5))\n\n# ----------------------------\n# 1) Build mapping object_id -> prob\n# ----------------------------\ndf_pred = pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob})\n# strict checks\nif df_pred[\"object_id\"].duplicated().any():\n    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:5].tolist()\n    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n\n# sanitize probs\np = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\nif not np.isfinite(p).all():\n    bad = int((~np.isfinite(p)).sum())\n    raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\np = np.clip(p, 0.0, 1.0)\ndf_pred[\"prob\"] = p\n\n# ----------------------------\n# 2) Align to sample_submission order\n# ----------------------------\ndf_sub[\"object_id\"] = df_sub[\"object_id\"].astype(str).str.strip()\ndf_pred[\"object_id\"] = df_pred[\"object_id\"].astype(str).str.strip()\n\n# merge in sample order\ndf_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n\nif df_out[\"prob\"].isna().any():\n    missing_n = int(df_out[\"prob\"].isna().sum())\n    miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:5].tolist()\n    raise ValueError(\n        f\"Some sample_submission object_id have no prediction: {missing_n} missing. \"\n        f\"Examples: {miss_ids}\"\n    )\n\n# Build final prediction column:\n# If competition expects PROBABILITY: set prediction=prob\n# If expects BINARY: set prediction=(prob>=thr)\n# Default: probability (safer for most Kaggle binary classification submissions).\nSUBMISSION_MODE = \"prob\"  # choose: \"prob\" or \"binary\"\nif SUBMISSION_MODE == \"binary\":\n    df_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\nelse:\n    df_out[\"prediction\"] = df_out[\"prob\"].astype(np.float32)\n\ndf_out = df_out[[\"object_id\", \"prediction\"]]\n\n# final checks\nif df_out[\"object_id\"].duplicated().any():\n    raise ValueError(\"submission has duplicate object_id (unexpected).\")\nif len(df_out) != len(df_sub):\n    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n\n# ----------------------------\n# 3) Write submission files\n# ----------------------------\nSUB_DIR = Path(SUB_DIR)\nSUB_DIR.mkdir(parents=True, exist_ok=True)\n\nout_main = Path(\"/kaggle/working/submission.csv\")\nout_copy = SUB_DIR / \"submission.csv\"\nout_proba = SUB_DIR / \"submission_proba.csv\"\n\ndf_out.to_csv(out_main, index=False)\ndf_out.to_csv(out_copy, index=False)\n\n# also save proba version for debugging (always probability)\ndf_dbg = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\ndf_dbg = df_dbg.rename(columns={\"prob\": \"prediction\"}).astype({\"prediction\": np.float32})\ndf_dbg.to_csv(out_proba, index=False)\n\nprint(\"[Stage 11] SUBMISSION READY\")\nprint(f\"- mode: {SUBMISSION_MODE} | threshold_used={thr:.6f} (only relevant if binary)\")\nprint(f\"- wrote: {out_main}\")\nprint(f\"- copy : {out_copy}\")\nprint(f\"- debug proba: {out_proba}\")\nprint(f\"- rows: {len(df_out):,}\")\n\n# quick preview\nprint(\"\\nPreview:\")\nprint(df_out.head(5).to_string(index=False))\n\n# Export globals\nglobals().update({\n    \"SUBMISSION_PATH\": out_main,\n    \"SUBMISSION_COPY_PATH\": out_copy,\n    \"SUBMISSION_MODE\": SUBMISSION_MODE,\n    \"SUBMISSION_THRESHOLD\": thr,\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}