{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4555377",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-02T18:41:42.113366Z",
     "iopub.status.busy": "2026-01-02T18:41:42.113008Z",
     "iopub.status.idle": "2026-01-02T18:41:43.362202Z",
     "shell.execute_reply": "2026-01-02T18:41:43.361102Z"
    },
    "papermill": {
     "duration": 1.265148,
     "end_time": "2026-01-02T18:41:43.364425",
     "exception": false,
     "start_time": "2026-01-02T18:41:42.099277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mallorn-dataset/sample_submission.csv\n",
      "/kaggle/input/mallorn-dataset/test_log.csv\n",
      "/kaggle/input/mallorn-dataset/train_log.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/test_full_lightcurves.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176efe40",
   "metadata": {
    "papermill": {
     "duration": 0.009436,
     "end_time": "2026-01-02T18:41:43.383671",
     "exception": false,
     "start_time": "2026-01-02T18:41:43.374235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Kaggle CPU Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc32320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:41:43.404777Z",
     "iopub.status.busy": "2026-01-02T18:41:43.404289Z",
     "iopub.status.idle": "2026-01-02T18:41:51.856654Z",
     "shell.execute_reply": "2026-01-02T18:41:51.855722Z"
    },
    "papermill": {
     "duration": 8.465583,
     "end_time": "2026-01-02T18:41:51.858443",
     "exception": false,
     "start_time": "2026-01-02T18:41:43.392860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] train_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 4/2000 (0.20%) non-numeric/NaN in head. Examples: ['nan', 'nan', 'nan', 'nan']\n",
      "[WARN] test_full_lightcurves.csv col=Flux: 2/2000 (0.10%) non-numeric/NaN in head. Examples: ['nan', 'nan']\n",
      "[WARN] test_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 190/2000 (9.50%) non-numeric/NaN in head. Examples: ['nan', 'nan', 'nan', 'nan', 'nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] test_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 2/2000 (0.10%) non-numeric/NaN in head. Examples: ['nan', 'nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 2/2000 (0.10%) non-numeric/NaN in head. Examples: ['nan', 'nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 2/2000 (0.10%) non-numeric/NaN in head. Examples: ['nan', 'nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 4/2000 (0.20%) non-numeric/NaN in head. Examples: ['nan', 'nan', 'nan', 'nan']\n",
      "[WARN] test_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] test_full_lightcurves.csv col=Flux: 1/2000 (0.05%) non-numeric/NaN in head. Examples: ['nan']\n",
      "[WARN] train_full_lightcurves.csv col=Flux: 2/2000 (0.10%) non-numeric/NaN in head. Examples: ['nan', 'nan']\n",
      "[WARN] test_full_lightcurves.csv col=Flux: 4/2000 (0.20%) non-numeric/NaN in head. Examples: ['nan', 'nan', 'nan', 'nan']\n",
      "ENV OK (Kaggle CPU)\n",
      "- Python: 3.12.12\n",
      "- Numpy:  2.0.2\n",
      "- Pandas: 2.2.2\n",
      "- Torch:  2.8.0+cu126 | CUDA available: False\n",
      "\n",
      "RUN OK\n",
      "- RUN_DIR:  /kaggle/working/mallorn_run/run_20260102_184148_9f34156418\n",
      "- CFG_HASH: 9f34156418\n",
      "- manifest: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/logs/split_manifest.csv\n",
      "- lc head NA report: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/logs/lc_head_na_report.csv\n",
      "\n",
      "DATA OK\n",
      "- train_log objects: 3,043 | pos(TDE)=148 | neg=2,895 | pos%=4.86%\n",
      "- test_log objects:  7,135\n",
      "- sample_submission rows: 7,135\n",
      "- splits: 20 folders | lightcurve files: 40 CSV (train+test)\n",
      "- split stats saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/logs/train_split_stats.csv\n",
      "\n",
      "META SNAPSHOT (train vs test)\n",
      "- EBV quantiles train [1%,50%,99%]: [0.007, 0.037, 0.3051599999999998] | test: [0.007, 0.035, 0.33465999999999985]\n",
      "- Z   quantiles train [1%,50%,99%]: [0.0961178, 0.4818, 2.709199999999997]   | test: [0.09527759999999999, 0.4842, 2.7928399999999964]\n",
      "- has_zerr test %: 100.00% (train selalu 0)\n",
      "\n",
      "Saved config: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/config_stage0.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 0 — Kaggle CPU Environment Setup (ONE CELL, SAFE + COHESIVE) — REVISI FULL v5\n",
    "# Fix utama v5:\n",
    "# - Quick LC numeric check dibuat \"COLUMN-AWARE\":\n",
    "#     * Flux: TOLERANT (banyak NaN -> WARN + dicatat, bukan error)\n",
    "#     * Time(MJD) & Flux_err: tetap ketat (NaN besar -> error)\n",
    "# - Hanya error jika indikasi parsing rusak: semua/hampir semua Flux NaN\n",
    "# - Buat report ringkas NaN head per 40 file (untuk audit)\n",
    "# ============================================================\n",
    "\n",
    "import os, sys, gc, json, time, random, hashlib, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Quiet + deterministic\n",
    "# ----------------------------\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "SEED = 2025\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CPU thread limits (anti-freeze on Kaggle CPU)\n",
    "# ----------------------------\n",
    "THREADS = 2\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"TF_NUM_INTRAOP_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"TF_NUM_INTEROP_THREADS\", \"1\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.set_num_threads(THREADS)\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# ----------------------------\n",
    "# 2) PATHS\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\n",
    "PATHS = {\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n",
    "    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n",
    "    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n",
    "    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 3) WORKDIR (versioned run)\n",
    "# ----------------------------\n",
    "WORKDIR = Path(\"/kaggle/working\")\n",
    "BASE_RUN_DIR = WORKDIR / \"mallorn_run\"\n",
    "BASE_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) CONFIG / TOGGLES\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    # Core plan (stage berikutnya)\n",
    "    \"USE_TRANSFORMER\": True,\n",
    "    \"USE_GBDT\": True,\n",
    "    \"USE_ENSEMBLE\": True,\n",
    "    \"USE_THRESHOLD_TUNING\": True,\n",
    "\n",
    "    \"USE_DEEXTINCT\": True,\n",
    "    \"USE_RESTFRAME_TIME\": True,\n",
    "    \"USE_SNR_FEATURES\": True,\n",
    "    \"USE_ASINH_FLUX\": True,\n",
    "\n",
    "    \"L_MAX\": 256,\n",
    "    \"TRUNC_POLICY\": \"smart_band_peak\",\n",
    "    \"PAD_POLICY\": \"dynamic\",\n",
    "\n",
    "    \"N_FOLDS\": 5,\n",
    "    \"CV_STRATIFY\": True,\n",
    "    \"CV_USE_SPLIT_COL\": True,\n",
    "\n",
    "    \"SNR_DET_THR\": 3.0,\n",
    "    \"MIN_FLUXERR\": 1e-6,\n",
    "\n",
    "    # Stage 0 checks\n",
    "    \"QUICK_LC_SCHEMA_CHECK\": True,\n",
    "    \"SAMPLE_ID_CROSSCHECK\": True,\n",
    "    \"SAMPLE_ID_PER_SPLIT\": 5,\n",
    "    \"MAX_CHUNKS_PER_FILE\": 6,\n",
    "    \"CHUNK_ROWS\": 200_000,\n",
    "\n",
    "    \"LC_HEAD_ROWS\": 2000,\n",
    "\n",
    "    # Per-column NA policy untuk quick check (head-only)\n",
    "    # - Time(MJD), Flux_err harus hampir bersih\n",
    "    # - Flux boleh NaN lebih banyak (anggap missing obs) -> WARN saja\n",
    "    \"LC_NA_POLICY\": {\n",
    "        \"Time (MJD)\": {\"max_na_abs\": 3, \"max_na_frac\": 0.002, \"fail_if_all_na\": True, \"hard_fail\": True},\n",
    "        \"Flux_err\":   {\"max_na_abs\": 3, \"max_na_frac\": 0.002, \"fail_if_all_na\": True, \"hard_fail\": True},\n",
    "\n",
    "        # Flux: toleran. Hanya fail jika hampir semua NaN (indikasi parsing rusak)\n",
    "        \"Flux\":       {\"max_na_abs\": 10_000, \"max_na_frac\": 0.80, \"fail_if_all_na\": True, \"hard_fail\": False,\n",
    "                       \"fail_if_na_frac_ge\": 0.98},\n",
    "    }\n",
    "}\n",
    "\n",
    "def _hash_cfg(d: dict) -> str:\n",
    "    s = json.dumps(d, sort_keys=True)\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:10]\n",
    "\n",
    "CFG_HASH = _hash_cfg(CFG)\n",
    "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = BASE_RUN_DIR / f\"run_{RUN_TAG}_{CFG_HASH}\"\n",
    "\n",
    "ART_DIR   = RUN_DIR / \"artifacts\"\n",
    "CACHE_DIR = RUN_DIR / \"cache\"\n",
    "OOF_DIR   = RUN_DIR / \"oof\"\n",
    "SUB_DIR   = RUN_DIR / \"submissions\"\n",
    "LOG_DIR   = RUN_DIR / \"logs\"\n",
    "EMB_DIR   = CACHE_DIR / \"embeddings\"\n",
    "FEAT_DIR  = CACHE_DIR / \"features\"\n",
    "\n",
    "for d in [RUN_DIR, ART_DIR, CACHE_DIR, OOF_DIR, SUB_DIR, LOG_DIR, EMB_DIR, FEAT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Hard guards: files must exist\n",
    "# ----------------------------\n",
    "def _must_exist(p: Path, what: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n",
    "\n",
    "_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n",
    "_must_exist(PATHS[\"TRAIN_LOG\"],  \"train_log.csv\")\n",
    "_must_exist(PATHS[\"TEST_LOG\"],   \"test_log.csv\")\n",
    "\n",
    "missing_splits = [s for s in PATHS[\"SPLITS\"] if not s.exists()]\n",
    "if missing_splits:\n",
    "    sample = \"\\n\".join(str(x) for x in missing_splits[:5])\n",
    "    raise FileNotFoundError(f\"Some split folders are missing (showing up to 5):\\n{sample}\")\n",
    "\n",
    "bad = []\n",
    "for sd in PATHS[\"SPLITS\"]:\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        bad.append((sd.name, tr.exists(), te.exists()))\n",
    "if bad:\n",
    "    msg = \"\\n\".join([f\"- {name}: train={tr_ok}, test={te_ok}\" for name, tr_ok, te_ok in bad[:10]])\n",
    "    raise FileNotFoundError(\"Some split lightcurve files are missing (showing up to 10):\\n\" + msg)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) CSV read defaults (stabil untuk angka kosong)\n",
    "# ----------------------------\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Load metadata kecil (sample + logs)\n",
    "# ----------------------------\n",
    "df_sub = pd.read_csv(PATHS[\"SAMPLE_SUB\"], dtype={\"object_id\": \"string\"}, **SAFE_READ_KW)\n",
    "df_sub = _norm_cols(df_sub)\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission columns must include object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "df_train_log = pd.read_csv(PATHS[\"TRAIN_LOG\"], dtype={\"object_id\": \"string\", \"split\": \"string\"}, **SAFE_READ_KW)\n",
    "df_test_log  = pd.read_csv(PATHS[\"TEST_LOG\"],  dtype={\"object_id\": \"string\", \"split\": \"string\"}, **SAFE_READ_KW)\n",
    "df_train_log = _norm_cols(df_train_log)\n",
    "df_test_log  = _norm_cols(df_test_log)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Validate log schemas + normalize split + unify Z_err\n",
    "# ----------------------------\n",
    "need_train = {\"object_id\", \"EBV\", \"Z\", \"split\", \"target\"}\n",
    "need_test  = {\"object_id\", \"EBV\", \"Z\", \"split\"}  # Z_err opsional\n",
    "\n",
    "missing_train = sorted(list(need_train - set(df_train_log.columns)))\n",
    "missing_test  = sorted(list(need_test  - set(df_test_log.columns)))\n",
    "if missing_train:\n",
    "    raise ValueError(f\"train_log.csv missing required columns: {missing_train}\")\n",
    "if missing_test:\n",
    "    raise ValueError(f\"test_log.csv missing required columns: {missing_test}\")\n",
    "\n",
    "for col in [\"EBV\", \"Z\"]:\n",
    "    df_train_log[col] = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n",
    "    df_test_log[col]  = pd.to_numeric(df_test_log[col],  errors=\"coerce\")\n",
    "    if df_train_log[col].isna().any():\n",
    "        raise ValueError(f\"train_log {col} has NaN after numeric coercion: {int(df_train_log[col].isna().sum())}\")\n",
    "    if df_test_log[col].isna().any():\n",
    "        raise ValueError(f\"test_log {col} has NaN after numeric coercion: {int(df_test_log[col].isna().sum())}\")\n",
    "\n",
    "if \"Z_err\" in df_test_log.columns:\n",
    "    df_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\n",
    "else:\n",
    "    df_test_log[\"Z_err\"] = np.nan\n",
    "\n",
    "if \"Z_err\" in df_train_log.columns:\n",
    "    df_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\")\n",
    "else:\n",
    "    df_train_log[\"Z_err\"] = np.nan\n",
    "\n",
    "df_train_log[\"has_zerr\"] = 0\n",
    "df_test_log[\"has_zerr\"]  = (~df_test_log[\"Z_err\"].isna()).astype(\"int8\")\n",
    "df_train_log[\"Z_err\"] = df_train_log[\"Z_err\"].fillna(0.0)\n",
    "df_test_log[\"Z_err\"]  = df_test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "valid_split_names = {f\"split_{i:02d}\" for i in range(1, 21)}\n",
    "def _normalize_split(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    if s.isdigit():\n",
    "        return f\"split_{int(s):02d}\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.startswith(\"split_\"):\n",
    "        tail = s2.split(\"split_\", 1)[1].strip(\"_\")\n",
    "        if tail.isdigit():\n",
    "            return f\"split_{int(tail):02d}\"\n",
    "        return s2\n",
    "    return s\n",
    "\n",
    "df_train_log[\"split\"] = df_train_log[\"split\"].map(_normalize_split)\n",
    "df_test_log[\"split\"]  = df_test_log[\"split\"].map(_normalize_split)\n",
    "\n",
    "bad_train_split = sorted(set(df_train_log[\"split\"]) - valid_split_names)\n",
    "bad_test_split  = sorted(set(df_test_log[\"split\"])  - valid_split_names)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "if df_train_log[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(f\"train_log duplicated object_id: {int(df_train_log['object_id'].duplicated().sum())}\")\n",
    "if df_test_log[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(f\"test_log duplicated object_id: {int(df_test_log['object_id'].duplicated().sum())}\")\n",
    "\n",
    "df_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\")\n",
    "if df_train_log[\"target\"].isna().any():\n",
    "    raise ValueError(f\"train_log target has NaN after numeric coercion: {int(df_train_log['target'].isna().sum())}\")\n",
    "uniq_t = set(pd.unique(df_train_log[\"target\"]).tolist())\n",
    "if not uniq_t.issubset({0, 1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n",
    "\n",
    "train_ids = set(df_train_log[\"object_id\"].astype(\"string\"))\n",
    "test_ids  = set(df_test_log[\"object_id\"].astype(\"string\"))\n",
    "intersect = train_ids & test_ids\n",
    "if intersect:\n",
    "    raise ValueError(f\"train_log and test_log share object_id (should be disjoint). Examples: {list(intersect)[:5]}\")\n",
    "\n",
    "sub_ids = set(df_sub[\"object_id\"].astype(\"string\"))\n",
    "missing_in_test = sub_ids - test_ids\n",
    "missing_in_sub  = test_ids - sub_ids\n",
    "if missing_in_test:\n",
    "    raise ValueError(f\"sample_submission has object_id not in test_log (up to 5): {list(missing_in_test)[:5]}\")\n",
    "if missing_in_sub:\n",
    "    raise ValueError(f\"test_log has object_id not in sample_submission (up to 5): {list(missing_in_sub)[:5]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Build split manifest (40 CSV index) + save\n",
    "# ----------------------------\n",
    "split_rows = []\n",
    "for i in range(1, 21):\n",
    "    sp = f\"split_{i:02d}\"\n",
    "    sd = DATA_ROOT / sp\n",
    "    split_rows.append({\n",
    "        \"split\": sp,\n",
    "        \"dir\": str(sd),\n",
    "        \"train_csv\": str(sd / \"train_full_lightcurves.csv\"),\n",
    "        \"test_csv\":  str(sd / \"test_full_lightcurves.csv\"),\n",
    "        \"train_exists\": (sd / \"train_full_lightcurves.csv\").exists(),\n",
    "        \"test_exists\":  (sd / \"test_full_lightcurves.csv\").exists(),\n",
    "        \"n_train_objects_log\": int((df_train_log[\"split\"] == sp).sum()),\n",
    "        \"n_test_objects_log\":  int((df_test_log[\"split\"]  == sp).sum()),\n",
    "    })\n",
    "df_split_manifest = pd.DataFrame(split_rows)\n",
    "manifest_path = LOG_DIR / \"split_manifest.csv\"\n",
    "df_split_manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) OPTIONAL: quick lightcurve schema check (tolerant numeric)\n",
    "# ----------------------------\n",
    "LC_EXPECT = {\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"}\n",
    "LC_FILTER_OK = set(list(\"ugrizy\"))\n",
    "\n",
    "def _na_policy(col: str):\n",
    "    pol = CFG.get(\"LC_NA_POLICY\", {})\n",
    "    return pol.get(col, {\"max_na_abs\": 0, \"max_na_frac\": 0.0, \"fail_if_all_na\": True, \"hard_fail\": True})\n",
    "\n",
    "def _warn_or_raise_numeric(dfh: pd.DataFrame, col: str, csv_path: Path, report_row: dict):\n",
    "    pol = _na_policy(col)\n",
    "    vv = pd.to_numeric(dfh[col], errors=\"coerce\")\n",
    "    n = len(vv)\n",
    "    n_na = int(vv.isna().sum())\n",
    "    frac = (n_na / max(n, 1)) if n > 0 else 1.0\n",
    "\n",
    "    report_row[f\"na_{col}\"] = n_na\n",
    "    report_row[f\"nafrac_{col}\"] = float(frac)\n",
    "\n",
    "    if n == 0:\n",
    "        raise ValueError(f\"[LC NUM] Empty head in {csv_path.name}\")\n",
    "\n",
    "    if pol.get(\"fail_if_all_na\", True) and n_na == n:\n",
    "        ex = dfh[col].astype(str).head(5).tolist()\n",
    "        raise ValueError(f\"[LC NUM] All values non-numeric/NaN in {csv_path.name} col={col}. Examples: {ex}\")\n",
    "\n",
    "    # special: Flux only fail if NA fraction extremely high (indikasi parsing rusak)\n",
    "    fail_if_ge = pol.get(\"fail_if_na_frac_ge\", None)\n",
    "    if fail_if_ge is not None and frac >= float(fail_if_ge) and n >= 200:\n",
    "        ex = dfh.loc[vv.isna(), col].astype(str).head(5).tolist()\n",
    "        raise ValueError(\n",
    "            f\"[LC NUM] Suspicious: {csv_path.name} col={col} NA frac {frac*100:.2f}% (>= {float(fail_if_ge)*100:.1f}%). \"\n",
    "            f\"Examples: {ex}\"\n",
    "        )\n",
    "\n",
    "    # normal tolerance logic\n",
    "    max_abs = int(pol.get(\"max_na_abs\", 0))\n",
    "    max_frac = float(pol.get(\"max_na_frac\", 0.0))\n",
    "    hard_fail = bool(pol.get(\"hard_fail\", True))\n",
    "\n",
    "    if n_na > 0:\n",
    "        ex = dfh.loc[vv.isna(), col].astype(str).head(5).tolist()\n",
    "        if hard_fail and (n_na > max_abs) and (frac > max_frac):\n",
    "            raise ValueError(\n",
    "                f\"[LC NUM] Too many non-numeric/NaN in {csv_path.name} col={col}: \"\n",
    "                f\"{n_na}/{n} ({frac*100:.2f}%). Examples: {ex}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"[WARN] {csv_path.name} col={col}: {n_na}/{n} ({frac*100:.2f}%) non-numeric/NaN in head. Examples: {ex}\")\n",
    "\n",
    "    return vv\n",
    "\n",
    "def _quick_lc_check(csv_path: Path, n_head: int, report_row: dict):\n",
    "    dfh = pd.read_csv(csv_path, nrows=n_head, **SAFE_READ_KW)\n",
    "    dfh = _norm_cols(dfh)\n",
    "\n",
    "    miss = sorted(list(LC_EXPECT - set(dfh.columns)))\n",
    "    if miss:\n",
    "        raise ValueError(f\"[LC SCHEMA] Missing columns in {csv_path.name}: {miss}\")\n",
    "\n",
    "    filt = dfh[\"Filter\"].astype(\"string\").str.strip().str.lower()\n",
    "    filt = filt[~filt.isna()]\n",
    "    badf = sorted(list(set(filt.unique().tolist()) - LC_FILTER_OK))\n",
    "    if badf:\n",
    "        raise ValueError(f\"[LC FILTER] Invalid filter values in {csv_path.name}: {badf[:10]}\")\n",
    "\n",
    "    _ = _warn_or_raise_numeric(dfh, \"Time (MJD)\", csv_path, report_row)\n",
    "    flux = _warn_or_raise_numeric(dfh, \"Flux\", csv_path, report_row)\n",
    "    ferr = _warn_or_raise_numeric(dfh, \"Flux_err\", csv_path, report_row)\n",
    "\n",
    "    # flux_err should be >= 0 (ignore NaN)\n",
    "    ferr2 = ferr.dropna()\n",
    "    if len(ferr2) > 0 and (ferr2 < 0).any():\n",
    "        raise ValueError(f\"[LC NUM] Negative Flux_err in head of {csv_path.name}\")\n",
    "\n",
    "def _sample_id_crosscheck(csv_path: Path, want_ids: set, chunk_rows: int, max_chunks: int):\n",
    "    if not want_ids:\n",
    "        return True\n",
    "    remaining = set(want_ids)\n",
    "    read_ok = False\n",
    "    for i, chunk in enumerate(pd.read_csv(csv_path, usecols=[\"object_id\"], chunksize=chunk_rows, **SAFE_READ_KW)):\n",
    "        read_ok = True\n",
    "        found = set(chunk[\"object_id\"].astype(\"string\")) & remaining\n",
    "        remaining -= found\n",
    "        if not remaining:\n",
    "            return True\n",
    "        if i + 1 >= max_chunks:\n",
    "            break\n",
    "    if not read_ok:\n",
    "        raise ValueError(f\"[LC READ] Could not read {csv_path}\")\n",
    "    if remaining:\n",
    "        print(f\"[WARN] Sample ID crosscheck limited-scan miss for {csv_path.name}: missing {len(remaining)} ids (scan cap).\")\n",
    "    return True\n",
    "\n",
    "# Run checks for all 40 files; write report\n",
    "lc_report = []\n",
    "if CFG[\"QUICK_LC_SCHEMA_CHECK\"] or CFG[\"SAMPLE_ID_CROSSCHECK\"]:\n",
    "    for sp in sorted(valid_split_names):\n",
    "        sd  = DATA_ROOT / sp\n",
    "        trp = sd / \"train_full_lightcurves.csv\"\n",
    "        tep = sd / \"test_full_lightcurves.csv\"\n",
    "\n",
    "        row_tr = {\"split\": sp, \"kind\": \"train\", \"file\": str(trp)}\n",
    "        row_te = {\"split\": sp, \"kind\": \"test\",  \"file\": str(tep)}\n",
    "\n",
    "        if CFG[\"QUICK_LC_SCHEMA_CHECK\"]:\n",
    "            _quick_lc_check(trp, int(CFG[\"LC_HEAD_ROWS\"]), row_tr)\n",
    "            _quick_lc_check(tep, int(CFG[\"LC_HEAD_ROWS\"]), row_te)\n",
    "\n",
    "        if CFG[\"SAMPLE_ID_CROSSCHECK\"]:\n",
    "            k = int(CFG[\"SAMPLE_ID_PER_SPLIT\"])\n",
    "            tr_ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(\"string\")\n",
    "            te_ids = df_test_log.loc[df_test_log[\"split\"]  == sp, \"object_id\"].astype(\"string\")\n",
    "            samp_tr = set(tr_ids.sample(n=min(k, len(tr_ids)), random_state=SEED).tolist()) if len(tr_ids) else set()\n",
    "            samp_te = set(te_ids.sample(n=min(k, len(te_ids)), random_state=SEED).tolist()) if len(te_ids) else set()\n",
    "            _sample_id_crosscheck(trp, samp_tr, int(CFG[\"CHUNK_ROWS\"]), int(CFG[\"MAX_CHUNKS_PER_FILE\"]))\n",
    "            _sample_id_crosscheck(tep, samp_te, int(CFG[\"CHUNK_ROWS\"]), int(CFG[\"MAX_CHUNKS_PER_FILE\"]))\n",
    "\n",
    "        lc_report.append(row_tr)\n",
    "        lc_report.append(row_te)\n",
    "\n",
    "lc_report_path = LOG_DIR / \"lc_head_na_report.csv\"\n",
    "if lc_report:\n",
    "    pd.DataFrame(lc_report).to_csv(lc_report_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Summaries\n",
    "# ----------------------------\n",
    "pos = int((df_train_log[\"target\"] == 1).sum())\n",
    "neg = int((df_train_log[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_log))\n",
    "\n",
    "def _q(x, qs=(0.01, 0.5, 0.99)):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return [float(np.quantile(x, q)) for q in qs]\n",
    "\n",
    "df_split_stats = (\n",
    "    df_train_log.groupby(\"split\")[\"target\"]\n",
    "    .agg([\"count\",\"sum\"])\n",
    "    .rename(columns={\"count\":\"n_train\",\"sum\":\"n_pos\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_split_stats[\"pos_rate\"] = df_split_stats[\"n_pos\"] / df_split_stats[\"n_train\"].clip(lower=1)\n",
    "df_split_stats = df_split_stats.sort_values(\"split\")\n",
    "split_stats_path = LOG_DIR / \"train_split_stats.csv\"\n",
    "df_split_stats.to_csv(split_stats_path, index=False)\n",
    "\n",
    "print(\"ENV OK (Kaggle CPU)\")\n",
    "print(f\"- Python: {sys.version.split()[0]}\")\n",
    "print(f\"- Numpy:  {np.__version__}\")\n",
    "print(f\"- Pandas: {pd.__version__}\")\n",
    "if torch is not None:\n",
    "    print(f\"- Torch:  {torch.__version__} | CUDA available: {torch.cuda.is_available()}\")\n",
    "else:\n",
    "    print(\"- Torch:  not available\")\n",
    "\n",
    "print(\"\\nRUN OK\")\n",
    "print(f\"- RUN_DIR:  {RUN_DIR}\")\n",
    "print(f\"- CFG_HASH: {CFG_HASH}\")\n",
    "print(f\"- manifest: {manifest_path}\")\n",
    "if lc_report:\n",
    "    print(f\"- lc head NA report: {lc_report_path}\")\n",
    "\n",
    "print(\"\\nDATA OK\")\n",
    "print(f\"- train_log objects: {len(df_train_log):,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.2f}%\")\n",
    "print(f\"- test_log objects:  {len(df_test_log):,}\")\n",
    "print(f\"- sample_submission rows: {len(df_sub):,}\")\n",
    "print(f\"- splits: 20 folders | lightcurve files: 40 CSV (train+test)\")\n",
    "print(f\"- split stats saved: {split_stats_path}\")\n",
    "\n",
    "print(\"\\nMETA SNAPSHOT (train vs test)\")\n",
    "print(f\"- EBV quantiles train [1%,50%,99%]: {_q(df_train_log['EBV'].values)} | test: {_q(df_test_log['EBV'].values)}\")\n",
    "print(f\"- Z   quantiles train [1%,50%,99%]: {_q(df_train_log['Z'].values)}   | test: {_q(df_test_log['Z'].values)}\")\n",
    "print(f\"- has_zerr test %: {float(df_test_log['has_zerr'].mean()*100):.2f}% (train selalu 0)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Save config snapshot\n",
    "# ----------------------------\n",
    "snap = {\n",
    "    \"SEED\": SEED,\n",
    "    \"CFG_HASH\": CFG_HASH,\n",
    "    \"RUN_TAG\": RUN_TAG,\n",
    "    \"DATA_ROOT\": str(DATA_ROOT),\n",
    "    \"RUN_DIR\": str(RUN_DIR),\n",
    "    \"THREADS\": {k: os.environ.get(k, \"\") for k in [\n",
    "        \"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\n",
    "        \"TF_NUM_INTRAOP_THREADS\",\"TF_NUM_INTEROP_THREADS\"\n",
    "    ]},\n",
    "    \"CFG\": CFG,\n",
    "    \"FILES\": {\n",
    "        \"sample_submission\": str(PATHS[\"SAMPLE_SUB\"]),\n",
    "        \"train_log\": str(PATHS[\"TRAIN_LOG\"]),\n",
    "        \"test_log\": str(PATHS[\"TEST_LOG\"]),\n",
    "        \"split_manifest\": str(manifest_path),\n",
    "        \"lc_head_na_report\": str(lc_report_path) if lc_report else \"\",\n",
    "    }\n",
    "}\n",
    "cfg_path_json = RUN_DIR / \"config_stage0.json\"\n",
    "with open(cfg_path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(snap, f, indent=2)\n",
    "print(f\"\\nSaved config: {cfg_path_json}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Export to globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"SEED\": SEED,\n",
    "    \"THREADS\": THREADS,\n",
    "    \"CFG\": CFG,\n",
    "    \"CFG_HASH\": CFG_HASH,\n",
    "    \"PATHS\": PATHS,\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"WORKDIR\": WORKDIR,\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"CACHE_DIR\": CACHE_DIR,\n",
    "    \"EMB_DIR\": EMB_DIR,\n",
    "    \"FEAT_DIR\": FEAT_DIR,\n",
    "    \"OOF_DIR\": OOF_DIR,\n",
    "    \"SUB_DIR\": SUB_DIR,\n",
    "    \"LOG_DIR\": LOG_DIR,\n",
    "    \"df_sub\": df_sub,\n",
    "    \"df_train_log\": df_train_log,\n",
    "    \"df_test_log\": df_test_log,\n",
    "    \"df_split_manifest\": df_split_manifest,\n",
    "    \"df_split_stats\": df_split_stats,\n",
    "})\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fadaa8",
   "metadata": {
    "papermill": {
     "duration": 0.009707,
     "end_time": "2026-01-02T18:41:51.878131",
     "exception": false,
     "start_time": "2026-01-02T18:41:51.868424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Verify Dataset Paths & Split Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95937a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:41:51.899827Z",
     "iopub.status.busy": "2026-01-02T18:41:51.899459Z",
     "iopub.status.idle": "2026-01-02T18:41:54.635029Z",
     "shell.execute_reply": "2026-01-02T18:41:54.633546Z"
    },
    "papermill": {
     "duration": 2.749839,
     "end_time": "2026-01-02T18:41:54.637601",
     "exception": false,
     "start_time": "2026-01-02T18:41:51.887762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1 OK — SPLIT ROUTING READY\n",
      "- DATA_ROOT: /kaggle/input/mallorn-dataset\n",
      "- splits on disk: 20 (split_01..split_20)\n",
      "- routing saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/logs/split_routing.csv\n",
      "- lc sample stats saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/logs/lc_sample_stats.csv\n",
      "\n",
      "OBJECT COUNTS (from logs)\n",
      "- split_01: train=155 | test=364\n",
      "- split_02: train=170 | test=414\n",
      "- split_03: train=138 | test=338\n",
      "- split_04: train=145 | test=332\n",
      "- split_05: train=165 | test=375\n",
      "- split_06: train=155 | test=374\n",
      "- split_07: train=165 | test=398\n",
      "- split_08: train=162 | test=387\n",
      "- split_09: train=128 | test=289\n",
      "- split_10: train=144 | test=331\n",
      "- split_11: train=146 | test=325\n",
      "- split_12: train=155 | test=353\n",
      "- split_13: train=143 | test=379\n",
      "- split_14: train=154 | test=351\n",
      "- split_15: train=158 | test=342\n",
      "- split_16: train=155 | test=354\n",
      "- split_17: train=153 | test=351\n",
      "- split_18: train=152 | test=345\n",
      "- split_19: train=147 | test=375\n",
      "- split_20: train=153 | test=358\n",
      "\n",
      "WORST SAMPLE (highest flux_na_frac in head sample)\n",
      "   split  kind  flux_na_frac  time_na_frac  ferr_na_frac  file_mb\n",
      "split_07 train         0.095           0.0           0.0 1.257892\n",
      "split_04 train         0.002           0.0           0.0 1.173453\n",
      "split_19  test         0.002           0.0           0.0 2.904259\n",
      "split_17 train         0.002           0.0           0.0 1.173508\n",
      "split_05  test         0.001           0.0           0.0 3.122271\n",
      "split_11 train         0.001           0.0           0.0 1.180762\n",
      "\n",
      "Stage 1 complete: splits verified + routing/stats exported.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Split Routing + Lightcurve Micro-Profiling (ONE CELL, CPU-SAFE) — REVISI FULL v2\n",
    "# Fokus:\n",
    "# - Pakai globals STAGE 0: PATHS, df_train_log, df_test_log, (opsional) CFG, LOG_DIR, RUN_DIR\n",
    "# - TIDAK load full lightcurves: hanya header + sample kecil + limited chunk scan\n",
    "# - Validasi:\n",
    "#   * split folders + 40 CSV ada\n",
    "#   * schema kolom minimal sesuai\n",
    "#   * filter values wajar (ugrizy), toleran NaN/blank\n",
    "#   * numeric sample check:\n",
    "#       - Time (MJD), Flux_err: relatif ketat (kalau parah -> error)\n",
    "#       - Flux: toleran (boleh NaN lebih banyak -> warn)\n",
    "#   * crosscheck object_id (sample dari logs) benar-benar muncul di CSV (limited scan)\n",
    "# - Output penting utk F1 tinggi (dipakai stage berikutnya):\n",
    "#   * split_routing.csv: daftar 40 file + count objects log\n",
    "#   * lc_sample_stats.csv: missing rate/head stats per file (buat tuning cleaning/clipping)\n",
    "# ============================================================\n",
    "\n",
    "import re, gc, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "need0 = [\"PATHS\", \"df_train_log\", \"df_test_log\"]\n",
    "for need in need0:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n",
    "\n",
    "DATA_ROOT = Path(PATHS[\"DATA_ROOT\"])\n",
    "SPLIT_DIRS = {p.name: p for p in PATHS[\"SPLITS\"]}  # split_01..split_20 -> Path\n",
    "VALID_SPLITS = {f\"split_{i:02d}\" for i in range(1, 21)}\n",
    "\n",
    "# Optional dirs from stage 0\n",
    "RUN_DIR = Path(globals().get(\"RUN_DIR\", \"/kaggle/working/mallorn_run\"))\n",
    "LOG_DIR = Path(globals().get(\"LOG_DIR\", RUN_DIR / \"logs\"))\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional CFG\n",
    "CFG_LOCAL = globals().get(\"CFG\", {})\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Safe read config (konsisten dengan STAGE 0)\n",
    "# ----------------------------\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "# sampling knobs (CPU-safe)\n",
    "HEAD_ROWS = int(CFG_LOCAL.get(\"LC_HEAD_ROWS\", 2000))          # untuk numeric/head checks\n",
    "FILTER_ROWS = int(CFG_LOCAL.get(\"LC_HEAD_ROWS\", 2000))        # reuse\n",
    "SAMPLE_ID_PER_SPLIT = int(CFG_LOCAL.get(\"SAMPLE_ID_PER_SPLIT\", 5))\n",
    "CHUNK_ROWS = int(CFG_LOCAL.get(\"CHUNK_ROWS\", 200_000))\n",
    "MAX_CHUNKS_PER_FILE = int(CFG_LOCAL.get(\"MAX_CHUNKS_PER_FILE\", 6))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Helpers\n",
    "# ----------------------------\n",
    "REQ_LC_COLS = {\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"}\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def normalize_split_name(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.isdigit():\n",
    "        return f\"split_{int(s2):02d}\"\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    return s2\n",
    "\n",
    "def sizeof_mb(p: Path) -> float:\n",
    "    try:\n",
    "        return p.stat().st_size / (1024**2)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def read_header_cols(p: Path):\n",
    "    df0 = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n",
    "    return [c.strip() for c in df0.columns]\n",
    "\n",
    "def sample_filter_values(p: Path, nrows: int = 500):\n",
    "    df = pd.read_csv(p, usecols=[\"Filter\"], nrows=nrows, **SAFE_READ_KW)\n",
    "    v = df[\"Filter\"].astype(\"string\").str.strip().str.lower()\n",
    "    v = v[~v.isna()]\n",
    "    return sorted(set(v.tolist()))\n",
    "\n",
    "def _numeric_sample_report(p: Path, nrows: int):\n",
    "    \"\"\"\n",
    "    Baca sample kecil untuk numeric coercion.\n",
    "    - Time & Flux_err: kalau terlalu banyak NaN -> error (indikasi parsing rusak)\n",
    "    - Flux: toleran, hanya dicatat.\n",
    "    \"\"\"\n",
    "    usecols = [\"Time (MJD)\", \"Flux\", \"Flux_err\"]\n",
    "    dfh = pd.read_csv(p, usecols=usecols, nrows=nrows, **SAFE_READ_KW)\n",
    "    dfh = _norm_cols(dfh)\n",
    "\n",
    "    out = {\"n_sample\": int(len(dfh))}\n",
    "    if len(dfh) == 0:\n",
    "        out.update({\"time_na_frac\": 1.0, \"flux_na_frac\": 1.0, \"ferr_na_frac\": 1.0})\n",
    "        return out\n",
    "\n",
    "    t = pd.to_numeric(dfh[\"Time (MJD)\"], errors=\"coerce\")\n",
    "    f = pd.to_numeric(dfh[\"Flux\"], errors=\"coerce\")\n",
    "    e = pd.to_numeric(dfh[\"Flux_err\"], errors=\"coerce\")\n",
    "\n",
    "    out[\"time_na_frac\"] = float(t.isna().mean())\n",
    "    out[\"flux_na_frac\"] = float(f.isna().mean())\n",
    "    out[\"ferr_na_frac\"] = float(e.isna().mean())\n",
    "\n",
    "    # quick stats (ignore NaN)\n",
    "    if (~t.isna()).any():\n",
    "        out[\"time_min\"] = float(t.min())\n",
    "        out[\"time_max\"] = float(t.max())\n",
    "    if (~f.isna()).any():\n",
    "        out[\"flux_neg_frac\"] = float((f.dropna() < 0).mean())\n",
    "    if (~e.isna()).any():\n",
    "        out[\"ferr_min\"] = float(e.dropna().min())\n",
    "        out[\"ferr_p50\"] = float(e.dropna().median())\n",
    "\n",
    "    return out\n",
    "\n",
    "def _sample_id_presence(csv_path: Path, want_ids: set, chunk_rows: int, max_chunks: int):\n",
    "    \"\"\"\n",
    "    Limited scan untuk memastikan beberapa object_id dari log benar-benar ada di file.\n",
    "    Return: found_count, missing_ids(set)\n",
    "    \"\"\"\n",
    "    if not want_ids:\n",
    "        return 0, set()\n",
    "    remaining = set(want_ids)\n",
    "    found = set()\n",
    "    read_ok = False\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(csv_path, usecols=[\"object_id\"], chunksize=chunk_rows, **SAFE_READ_KW)):\n",
    "        read_ok = True\n",
    "        ids = set(chunk[\"object_id\"].astype(\"string\"))\n",
    "        hit = remaining & ids\n",
    "        if hit:\n",
    "            found |= hit\n",
    "            remaining -= hit\n",
    "        if not remaining:\n",
    "            break\n",
    "        if i + 1 >= max_chunks:\n",
    "            break\n",
    "\n",
    "    if not read_ok:\n",
    "        raise ValueError(f\"[LC READ] Could not read {csv_path}\")\n",
    "    return len(found), remaining\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Normalize split col in logs (idempotent, see STAGE 0)\n",
    "# ----------------------------\n",
    "for df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n",
    "    if \"split\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing 'split' column.\")\n",
    "    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n",
    "\n",
    "bad_train_split = sorted(set(df_train_log[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test_log[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Verify disk splits set\n",
    "# ----------------------------\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "if disk_splits != VALID_SPLITS:\n",
    "    missing = sorted(list(VALID_SPLITS - disk_splits))\n",
    "    extra   = sorted(list(disk_splits - VALID_SPLITS))\n",
    "    msg = []\n",
    "    if missing:\n",
    "        msg.append(f\"Missing split folders: {missing[:10]}\")\n",
    "    if extra:\n",
    "        msg.append(f\"Unexpected split folders: {extra[:10]}\")\n",
    "    raise RuntimeError(\"Split folder set mismatch.\\n\" + \"\\n\".join(msg))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Build routing manifest (40 file) + validate exists\n",
    "# ----------------------------\n",
    "train_counts = df_train_log[\"split\"].value_counts().to_dict()\n",
    "test_counts  = df_test_log[\"split\"].value_counts().to_dict()\n",
    "\n",
    "routing_rows = []\n",
    "missing_files = []\n",
    "\n",
    "for sp in sorted(VALID_SPLITS):\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "\n",
    "    if not tr.exists(): missing_files.append(str(tr))\n",
    "    if not te.exists(): missing_files.append(str(te))\n",
    "\n",
    "    routing_rows.append({\n",
    "        \"split\": sp,\n",
    "        \"train_csv\": str(tr),\n",
    "        \"test_csv\": str(te),\n",
    "        \"train_mb\": sizeof_mb(tr),\n",
    "        \"test_mb\": sizeof_mb(te),\n",
    "        \"n_train_objects_log\": int(train_counts.get(sp, 0)),\n",
    "        \"n_test_objects_log\": int(test_counts.get(sp, 0)),\n",
    "    })\n",
    "\n",
    "if missing_files:\n",
    "    raise FileNotFoundError(\"Some lightcurve files missing (showing up to 10):\\n\" + \"\\n\".join(missing_files[:10]))\n",
    "\n",
    "df_routing = pd.DataFrame(routing_rows)\n",
    "routing_path = LOG_DIR / \"split_routing.csv\"\n",
    "df_routing.to_csv(routing_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Header-only schema check (strict for required cols)\n",
    "# ----------------------------\n",
    "schema_issues = []\n",
    "for sp in sorted(VALID_SPLITS):\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "\n",
    "    cols_tr = read_header_cols(tr)\n",
    "    cols_te = read_header_cols(te)\n",
    "\n",
    "    miss_tr = sorted(list(REQ_LC_COLS - set(cols_tr)))\n",
    "    miss_te = sorted(list(REQ_LC_COLS - set(cols_te)))\n",
    "    if miss_tr or miss_te:\n",
    "        schema_issues.append((sp, miss_tr, miss_te, cols_tr, cols_te))\n",
    "\n",
    "if schema_issues:\n",
    "    sp, miss_tr, miss_te, cols_tr, cols_te = schema_issues[0]\n",
    "    raise ValueError(\n",
    "        \"Lightcurve column mismatch detected.\\n\"\n",
    "        f\"Example split: {sp}\\n\"\n",
    "        f\"Missing in train_full_lightcurves.csv: {miss_tr}\\n\"\n",
    "        f\"Missing in test_full_lightcurves.csv : {miss_te}\\n\"\n",
    "        f\"Train columns: {cols_tr}\\n\"\n",
    "        f\"Test columns : {cols_te}\\n\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Tiny filter sanity + numeric sample profiling + ID crosscheck\n",
    "# ----------------------------\n",
    "stats_rows = []\n",
    "warn_count = 0\n",
    "\n",
    "# numeric policy\n",
    "# - Time & Flux_err: fail kalau >2% NaN di sample (indikasi parsing kacau)\n",
    "MAX_TIME_NA_FRAC = 0.02\n",
    "MAX_FERR_NA_FRAC = 0.02\n",
    "# - Flux: TOLERANT (hanya dicatat)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "for sp in sorted(VALID_SPLITS):\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    for kind in [\"train\", \"test\"]:\n",
    "        p = sd / f\"{kind}_full_lightcurves.csv\"\n",
    "\n",
    "        # filter values from small sample\n",
    "        vals = sample_filter_values(p, nrows=min(500, FILTER_ROWS))\n",
    "        bad = sorted([v for v in vals if (v not in ALLOWED_FILTERS)])\n",
    "        # tolerate \"nan\" already removed; only bad true strings remain\n",
    "        if bad:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected Filter values detected: split={sp} kind={kind} bad={bad[:10]} all_sampled={vals[:20]}\"\n",
    "            )\n",
    "\n",
    "        # numeric sample report\n",
    "        nr = _numeric_sample_report(p, nrows=HEAD_ROWS)\n",
    "        # hard fail if Time/Flux_err parsing too broken\n",
    "        if nr.get(\"time_na_frac\", 0.0) > MAX_TIME_NA_FRAC:\n",
    "            raise ValueError(f\"[LC NUM] Time(MJD) NaN too high in sample: split={sp} kind={kind} frac={nr['time_na_frac']:.3f}\")\n",
    "        if nr.get(\"ferr_na_frac\", 0.0) > MAX_FERR_NA_FRAC:\n",
    "            raise ValueError(f\"[LC NUM] Flux_err NaN too high in sample: split={sp} kind={kind} frac={nr['ferr_na_frac']:.3f}\")\n",
    "        # Flux: warn only\n",
    "        if nr.get(\"flux_na_frac\", 0.0) > 0:\n",
    "            warn_count += 1\n",
    "\n",
    "        # sample ID crosscheck (limited scan)\n",
    "        if kind == \"train\":\n",
    "            ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(\"string\")\n",
    "        else:\n",
    "            ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(\"string\")\n",
    "\n",
    "        k = min(SAMPLE_ID_PER_SPLIT, len(ids))\n",
    "        want = set(ids.sample(n=k, random_state=SEED).tolist()) if k > 0 else set()\n",
    "        found_n, missing_ids = _sample_id_presence(p, want, CHUNK_ROWS, MAX_CHUNKS_PER_FILE)\n",
    "\n",
    "        # jika miss banyak sekali untuk split ini, itu red-flag\n",
    "        miss_frac = (len(missing_ids) / max(len(want), 1)) if want else 0.0\n",
    "        if want and miss_frac >= 0.8:\n",
    "            raise ValueError(\n",
    "                f\"[LC ID] Severe mismatch: split={sp} kind={kind} missing {len(missing_ids)}/{len(want)} \"\n",
    "                f\"within limited scan. Example missing: {list(missing_ids)[:3]}\"\n",
    "            )\n",
    "        if want and missing_ids:\n",
    "            # warn kalau ada yang miss tapi tidak parah (scan cap bisa jadi penyebab)\n",
    "            print(f\"[WARN] ID crosscheck limited-scan miss: split={sp} kind={kind} missing {len(missing_ids)}/{len(want)}\")\n",
    "\n",
    "        stats_rows.append({\n",
    "            \"split\": sp,\n",
    "            \"kind\": kind,\n",
    "            \"file\": str(p),\n",
    "            \"file_mb\": sizeof_mb(p),\n",
    "            \"n_sample\": nr.get(\"n_sample\", 0),\n",
    "            \"time_na_frac\": nr.get(\"time_na_frac\", np.nan),\n",
    "            \"flux_na_frac\": nr.get(\"flux_na_frac\", np.nan),\n",
    "            \"ferr_na_frac\": nr.get(\"ferr_na_frac\", np.nan),\n",
    "            \"time_min\": nr.get(\"time_min\", np.nan),\n",
    "            \"time_max\": nr.get(\"time_max\", np.nan),\n",
    "            \"flux_neg_frac\": nr.get(\"flux_neg_frac\", np.nan),\n",
    "            \"ferr_min\": nr.get(\"ferr_min\", np.nan),\n",
    "            \"ferr_p50\": nr.get(\"ferr_p50\", np.nan),\n",
    "            \"filter_sample\": \",\".join(vals[:10]),\n",
    "            \"id_check_k\": int(len(want)),\n",
    "            \"id_found\": int(found_n),\n",
    "            \"id_missing\": int(len(missing_ids)),\n",
    "        })\n",
    "\n",
    "df_lc_stats = pd.DataFrame(stats_rows)\n",
    "lc_stats_path = LOG_DIR / \"lc_sample_stats.csv\"\n",
    "df_lc_stats.to_csv(lc_stats_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Summary prints\n",
    "# ----------------------------\n",
    "print(\"STAGE 1 OK — SPLIT ROUTING READY\")\n",
    "print(f\"- DATA_ROOT: {DATA_ROOT}\")\n",
    "print(f\"- splits on disk: {len(VALID_SPLITS)} (split_01..split_20)\")\n",
    "print(f\"- routing saved: {routing_path}\")\n",
    "print(f\"- lc sample stats saved: {lc_stats_path}\")\n",
    "\n",
    "# object counts ringkas\n",
    "print(\"\\nOBJECT COUNTS (from logs)\")\n",
    "for sp in sorted(VALID_SPLITS):\n",
    "    print(f\"- {sp}: train={int(train_counts.get(sp,0)):,} | test={int(test_counts.get(sp,0)):,}\")\n",
    "\n",
    "# highlight worst flux_na_frac (optional info)\n",
    "worst = (\n",
    "    df_lc_stats.sort_values(\"flux_na_frac\", ascending=False)\n",
    "    .head(6)[[\"split\",\"kind\",\"flux_na_frac\",\"time_na_frac\",\"ferr_na_frac\",\"file_mb\"]]\n",
    ")\n",
    "print(\"\\nWORST SAMPLE (highest flux_na_frac in head sample)\")\n",
    "print(worst.to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Export to globals\n",
    "# ----------------------------\n",
    "SPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "globals().update({\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SPLIT_DIRS\": SPLIT_DIRS,\n",
    "    \"SPLIT_LIST\": SPLIT_LIST,\n",
    "    \"df_split_routing\": df_routing,\n",
    "    \"df_lc_sample_stats\": df_lc_stats,\n",
    "})\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\nStage 1 complete: splits verified + routing/stats exported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ddefc",
   "metadata": {
    "papermill": {
     "duration": 0.010235,
     "end_time": "2026-01-02T18:41:54.658383",
     "exception": false,
     "start_time": "2026-01-02T18:41:54.648148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and Validate Train/Test Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab336be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:41:54.681307Z",
     "iopub.status.busy": "2026-01-02T18:41:54.680987Z",
     "iopub.status.idle": "2026-01-02T18:41:55.175471Z",
     "shell.execute_reply": "2026-01-02T18:41:55.174641Z"
    },
    "papermill": {
     "duration": 0.508741,
     "end_time": "2026-01-02T18:41:55.177192",
     "exception": false,
     "start_time": "2026-01-02T18:41:54.668451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2 OK — META READY (clean + folds)\n",
      "- train objects: 3,043 | pos(TDE)=148 | neg=2,895 | pos%=4.864%\n",
      "- test objects : 7,135\n",
      "- saved train  : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/train_meta.parquet\n",
      "- saved test   : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/test_meta.parquet\n",
      "- saved stats  : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/split_stats.csv\n",
      "- saved folds  : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/train_folds.csv\n",
      "- scale_pos_weight (neg/pos): 19.561\n",
      "\n",
      "TRAIN-BASED CLIP RANGES\n",
      "- EBV clip: [0.005042, 0.581790]\n",
      "- Z   clip: [0.044923, 4.032352]\n",
      "\n",
      "FOLD BALANCE (count/pos/pos_rate)\n",
      "      count  pos  pos_rate\n",
      "fold                      \n",
      "0       607   38  0.062603\n",
      "1       619   33  0.053312\n",
      "2       612   29  0.047386\n",
      "3       606   26  0.042904\n",
      "4       599   22  0.036728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Clean Meta Logs + CV Fold Assignment (ONE CELL, CPU-SAFE) — REVISI FULL v2\n",
    "# Output:\n",
    "#   * df_train_meta, df_test_meta (index=object_id)\n",
    "#   * id2split_train, id2split_test\n",
    "#   * artifacts/train_meta.(parquet|csv), test_meta.(parquet|csv)\n",
    "#   * artifacts/split_stats.csv\n",
    "#   * artifacts/train_folds.csv\n",
    "#   * artifacts/id2split_train.json, artifacts/id2split_test.json\n",
    "# Notes:\n",
    "# - Tidak load full lightcurves.\n",
    "# - Clipping/transform pakai TRAIN statistics saja (anti leakage).\n",
    "# - Fold strategy: stratified within each split -> fold global stabil.\n",
    "# ============================================================\n",
    "\n",
    "import re, gc, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0/1 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"ART_DIR\", \"SPLIT_DIRS\", \"CFG\", \"SEED\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 & STAGE 1 dulu.\")\n",
    "\n",
    "TRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\n",
    "TEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n",
    "ART_DIR = Path(ART_DIR)\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = int(SEED)\n",
    "N_FOLDS = int(CFG.get(\"N_FOLDS\", 5))\n",
    "\n",
    "VALID_SPLITS = {f\"split_{i:02d}\" for i in range(1, 21)}\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "if disk_splits != VALID_SPLITS:\n",
    "    raise RuntimeError(\"SPLIT_DIRS tidak lengkap / mismatch. Jalankan ulang STAGE 1.\")\n",
    "\n",
    "# konsisten dengan STAGE 0/1\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers\n",
    "# ----------------------------\n",
    "def normalize_split_name(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.isdigit():\n",
    "        return f\"split_{int(s2):02d}\"\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    return s2\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _coerce_float32(df: pd.DataFrame, col: str):\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "def _safe_clip(series: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    # series float32 -> clip float32\n",
    "    return series.clip(lower=np.float32(lo), upper=np.float32(hi)).astype(\"float32\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Load logs (fresh read; stabil)\n",
    "# ----------------------------\n",
    "dtype_log = {\"object_id\": \"string\", \"split\": \"string\"}\n",
    "df_train = pd.read_csv(TRAIN_LOG_PATH, dtype=dtype_log, **SAFE_READ_KW)\n",
    "df_test  = pd.read_csv(TEST_LOG_PATH,  dtype=dtype_log, **SAFE_READ_KW)\n",
    "\n",
    "df_train = _norm_cols(df_train)\n",
    "df_test  = _norm_cols(df_test)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Required columns check\n",
    "# ----------------------------\n",
    "req_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\n",
    "req_train  = req_common | {\"target\"}\n",
    "req_test   = req_common\n",
    "\n",
    "miss_train = sorted(list(req_train - set(df_train.columns)))\n",
    "miss_test  = sorted(list(req_test  - set(df_test.columns)))\n",
    "if miss_train:\n",
    "    raise ValueError(f\"train_log.csv missing required columns: {miss_train} | found={list(df_train.columns)}\")\n",
    "if miss_test:\n",
    "    raise ValueError(f\"test_log.csv missing required columns: {miss_test} | found={list(df_test.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Basic cleaning (id + split canonical)\n",
    "# ----------------------------\n",
    "df_train[\"object_id\"] = df_train[\"object_id\"].astype(\"string\").str.strip()\n",
    "df_test[\"object_id\"]  = df_test[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "df_train[\"split\"] = df_train[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "df_test[\"split\"]  = df_test[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "\n",
    "bad_train_split = sorted(set(df_train[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "bad_train_disk = sorted([s for s in set(df_train[\"split\"].unique()) if s not in disk_splits])\n",
    "bad_test_disk  = sorted([s for s in set(df_test[\"split\"].unique())  if s not in disk_splits])\n",
    "if bad_train_disk:\n",
    "    raise FileNotFoundError(f\"train_log references unknown split(s) not on disk: {bad_train_disk[:10]}\")\n",
    "if bad_test_disk:\n",
    "    raise FileNotFoundError(f\"test_log references unknown split(s) not on disk: {bad_test_disk[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Ensure Z_err exists + numeric coercion (float32)\n",
    "# ----------------------------\n",
    "if \"Z_err\" not in df_train.columns:\n",
    "    df_train[\"Z_err\"] = np.nan\n",
    "if \"Z_err\" not in df_test.columns:\n",
    "    df_test[\"Z_err\"] = np.nan\n",
    "\n",
    "for c in [\"EBV\", \"Z\", \"Z_err\"]:\n",
    "    _coerce_float32(df_train, c)\n",
    "    _coerce_float32(df_test, c)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Duplicates / overlap checks (hard fail)\n",
    "# ----------------------------\n",
    "if df_train[\"object_id\"].duplicated().any():\n",
    "    ex = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in train_log (examples): {ex}\")\n",
    "if df_test[\"object_id\"].duplicated().any():\n",
    "    ex = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in test_log (examples): {ex}\")\n",
    "\n",
    "overlap = set(df_train[\"object_id\"].tolist()) & set(df_test[\"object_id\"].tolist())\n",
    "if overlap:\n",
    "    raise ValueError(f\"object_id overlap between train_log and test_log (examples): {list(overlap)[:5]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Target validation (train)\n",
    "# ----------------------------\n",
    "df_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    raise ValueError(f\"train_log target has NaN after coercion: {int(df_train['target'].isna().sum())} rows.\")\n",
    "uniq_t = set(pd.unique(df_train[\"target\"]).tolist())\n",
    "if not uniq_t.issubset({0, 1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n",
    "df_train[\"target\"] = df_train[\"target\"].astype(\"int8\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Missing flags + robust fills\n",
    "# ----------------------------\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"EBV_missing\"] = df[\"EBV\"].isna().astype(\"int8\")\n",
    "    df[\"Z_missing\"]   = df[\"Z\"].isna().astype(\"int8\")\n",
    "    df[\"Zerr_missing\"]= df[\"Z_err\"].isna().astype(\"int8\")\n",
    "\n",
    "# EBV: fill NaN -> 0.0 (fisik masuk akal sebagai default aman)\n",
    "df_train[\"EBV\"] = df_train[\"EBV\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "df_test[\"EBV\"]  = df_test[\"EBV\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "\n",
    "# Z: fill NaN -> median per split (fallback global median)\n",
    "def _fill_z_by_split(df: pd.DataFrame) -> pd.Series:\n",
    "    z = df[\"Z\"]\n",
    "    if not z.isna().any():\n",
    "        return z.astype(\"float32\")\n",
    "    z2 = z.copy()\n",
    "    z2 = z2.fillna(df.groupby(\"split\")[\"Z\"].transform(\"median\"))\n",
    "    gmed = float(np.nanmedian(z.values)) if np.isfinite(z.values).any() else 0.0\n",
    "    z2 = z2.fillna(np.float32(gmed))\n",
    "    return z2.astype(\"float32\")\n",
    "\n",
    "df_train[\"Z\"] = _fill_z_by_split(df_train)\n",
    "df_test[\"Z\"]  = _fill_z_by_split(df_test)\n",
    "\n",
    "# Z_err: train biasanya blank -> 0.0; test ada\n",
    "df_train[\"Z_err\"] = df_train[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "df_test[\"Z_err\"]  = df_test[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "\n",
    "# flags yang sering dipakai model\n",
    "df_train[\"has_zerr\"] = np.int8(0)   # train: spec-z\n",
    "df_test[\"has_zerr\"]  = np.int8(1)   # test : photo-z error tersedia\n",
    "df_train[\"is_photoz\"] = np.int8(0)\n",
    "df_test[\"is_photoz\"]  = np.int8(1)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Train-based clipping (anti outlier) + derived meta features\n",
    "# ----------------------------\n",
    "# clip pakai TRAIN saja -> apply ke train & test\n",
    "EBV_LO, EBV_HI = np.quantile(df_train[\"EBV\"].values.astype(float), [0.001, 0.999])\n",
    "Z_LO,   Z_HI   = np.quantile(df_train[\"Z\"].values.astype(float),   [0.001, 0.999])\n",
    "\n",
    "df_train[\"EBV_clip\"] = _safe_clip(df_train[\"EBV\"], EBV_LO, EBV_HI)\n",
    "df_test[\"EBV_clip\"]  = _safe_clip(df_test[\"EBV\"],  EBV_LO, EBV_HI)\n",
    "\n",
    "df_train[\"Z_clip\"] = _safe_clip(df_train[\"Z\"], Z_LO, Z_HI)\n",
    "df_test[\"Z_clip\"]  = _safe_clip(df_test[\"Z\"],  Z_LO, Z_HI)\n",
    "\n",
    "# log transform (lebih stabil untuk model tabular)\n",
    "df_train[\"log1pZ\"] = np.log1p(df_train[\"Z_clip\"].astype(\"float32\")).astype(\"float32\")\n",
    "df_test[\"log1pZ\"]  = np.log1p(df_test[\"Z_clip\"].astype(\"float32\")).astype(\"float32\")\n",
    "\n",
    "# zerr relative (test only meaningful, tapi aman diset untuk train=0)\n",
    "eps = np.float32(1e-6)\n",
    "df_train[\"zerr_rel\"] = (df_train[\"Z_err\"] / (df_train[\"Z_clip\"] + eps)).astype(\"float32\")\n",
    "df_test[\"zerr_rel\"]  = (df_test[\"Z_err\"]  / (df_test[\"Z_clip\"]  + eps)).astype(\"float32\")\n",
    "\n",
    "# split_id (numeric encoding stabil)\n",
    "split2id = {f\"split_{i:02d}\": i for i in range(1, 21)}\n",
    "df_train[\"split_id\"] = df_train[\"split\"].map(split2id).astype(\"int16\")\n",
    "df_test[\"split_id\"]  = df_test[\"split\"].map(split2id).astype(\"int16\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) CV fold assignment: stratified within each split (stabil untuk OOF + threshold)\n",
    "# ----------------------------\n",
    "# Strategi:\n",
    "# - Untuk tiap split: pisahkan pos/neg, shuffle, lalu bagikan merata ke fold 0..K-1\n",
    "# - Hasil: setiap fold mendapatkan “campuran” dari semua split dan proporsi kelas lebih stabil.\n",
    "df_train[\"fold\"] = -1\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "for sp in sorted(VALID_SPLITS):\n",
    "    idx = df_train.index[df_train[\"split\"] == sp].to_numpy()\n",
    "    if len(idx) == 0:\n",
    "        continue\n",
    "\n",
    "    y = df_train.loc[idx, \"target\"].to_numpy()\n",
    "    pos_idx = idx[y == 1]\n",
    "    neg_idx = idx[y == 0]\n",
    "\n",
    "    rng.shuffle(pos_idx)\n",
    "    rng.shuffle(neg_idx)\n",
    "\n",
    "    # round-robin assignment\n",
    "    for j, ii in enumerate(pos_idx):\n",
    "        df_train.at[ii, \"fold\"] = int(j % N_FOLDS)\n",
    "    for j, ii in enumerate(neg_idx):\n",
    "        # offset agar tidak semua kelas jatuh di fold yang sama di split kecil\n",
    "        df_train.at[ii, \"fold\"] = int((j + 1) % N_FOLDS)\n",
    "\n",
    "# hard check\n",
    "if (df_train[\"fold\"] < 0).any():\n",
    "    n_bad = int((df_train[\"fold\"] < 0).sum())\n",
    "    raise RuntimeError(f\"Fold assignment gagal: ada {n_bad} baris fold=-1\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Build meta tables (index=object_id)\n",
    "# ----------------------------\n",
    "# Simpan kolom yang benar-benar akan dipakai berikutnya.\n",
    "keep_train = [\n",
    "    \"object_id\",\"split\",\"split_id\",\n",
    "    \"EBV\",\"EBV_clip\",\"Z\",\"Z_clip\",\"log1pZ\",\"Z_err\",\"zerr_rel\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n",
    "    \"fold\",\"target\"\n",
    "]\n",
    "keep_test = [\n",
    "    \"object_id\",\"split\",\"split_id\",\n",
    "    \"EBV\",\"EBV_clip\",\"Z\",\"Z_clip\",\"log1pZ\",\"Z_err\",\"zerr_rel\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\"\n",
    "]\n",
    "\n",
    "# optional: SpecType untuk analisis (train only)\n",
    "if \"SpecType\" in df_train.columns:\n",
    "    keep_train.append(\"SpecType\")\n",
    "\n",
    "df_train_meta = df_train[keep_train].copy()\n",
    "df_test_meta  = df_test[keep_test].copy()\n",
    "\n",
    "df_train_meta = df_train_meta.set_index(\"object_id\", drop=True).sort_index()\n",
    "df_test_meta  = df_test_meta.set_index(\"object_id\", drop=True).sort_index()\n",
    "\n",
    "id2split_train = df_train_meta[\"split\"].to_dict()\n",
    "id2split_test  = df_test_meta[\"split\"].to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Save artifacts (parquet preferred, csv fallback)\n",
    "# ----------------------------\n",
    "train_pq = ART_DIR / \"train_meta.parquet\"\n",
    "test_pq  = ART_DIR / \"test_meta.parquet\"\n",
    "train_csv = ART_DIR / \"train_meta.csv\"\n",
    "test_csv  = ART_DIR / \"test_meta.csv\"\n",
    "\n",
    "saved_train = saved_test = None\n",
    "try:\n",
    "    df_train_meta.to_parquet(train_pq, index=True)\n",
    "    df_test_meta.to_parquet(test_pq, index=True)\n",
    "    saved_train, saved_test = str(train_pq), str(test_pq)\n",
    "except Exception:\n",
    "    df_train_meta.to_csv(train_csv, index=True)\n",
    "    df_test_meta.to_csv(test_csv, index=True)\n",
    "    saved_train, saved_test = str(train_csv), str(test_csv)\n",
    "\n",
    "# split stats (train/test counts + pos rate)\n",
    "split_stats = pd.DataFrame({\n",
    "    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(sorted(VALID_SPLITS)).fillna(0).astype(int),\n",
    "    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(sorted(VALID_SPLITS)).fillna(0).astype(int),\n",
    "})\n",
    "split_stats.index.name = \"split\"\n",
    "\n",
    "pos_by_split = df_train_meta.groupby(\"split\")[\"target\"].sum().reindex(sorted(VALID_SPLITS)).fillna(0).astype(int)\n",
    "split_stats[\"train_pos\"] = pos_by_split.values\n",
    "split_stats[\"train_pos_rate\"] = (split_stats[\"train_pos\"] / split_stats[\"train_objects\"].clip(lower=1)).astype(\"float32\")\n",
    "\n",
    "split_stats_path = ART_DIR / \"split_stats.csv\"\n",
    "split_stats.to_csv(split_stats_path)\n",
    "\n",
    "# folds report\n",
    "fold_path = ART_DIR / \"train_folds.csv\"\n",
    "df_train_meta.reset_index()[[\"object_id\",\"split\",\"fold\",\"target\"]].to_csv(fold_path, index=False)\n",
    "\n",
    "# id2split json\n",
    "with open(ART_DIR / \"id2split_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id2split_train, f)\n",
    "with open(ART_DIR / \"id2split_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id2split_test, f)\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Print summary\n",
    "# ----------------------------\n",
    "pos = int((df_train_meta[\"target\"] == 1).sum())\n",
    "neg = int((df_train_meta[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_meta))\n",
    "\n",
    "# class imbalance helper (buat model stage lanjut)\n",
    "pos_rate = pos / max(tot, 1)\n",
    "scale_pos_weight = float(neg / max(pos, 1))\n",
    "\n",
    "print(\"STAGE 2 OK — META READY (clean + folds)\")\n",
    "print(f\"- train objects: {tot:,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={pos_rate*100:.3f}%\")\n",
    "print(f\"- test objects : {len(df_test_meta):,}\")\n",
    "print(f\"- saved train  : {saved_train}\")\n",
    "print(f\"- saved test   : {saved_test}\")\n",
    "print(f\"- saved stats  : {split_stats_path}\")\n",
    "print(f\"- saved folds  : {fold_path}\")\n",
    "print(f\"- scale_pos_weight (neg/pos): {scale_pos_weight:.3f}\")\n",
    "\n",
    "print(\"\\nTRAIN-BASED CLIP RANGES\")\n",
    "print(f\"- EBV clip: [{float(EBV_LO):.6f}, {float(EBV_HI):.6f}]\")\n",
    "print(f\"- Z   clip: [{float(Z_LO):.6f}, {float(Z_HI):.6f}]\")\n",
    "\n",
    "# fold sanity\n",
    "fold_tab = df_train_meta.reset_index().groupby(\"fold\")[\"target\"].agg([\"count\",\"sum\"]).rename(columns={\"sum\":\"pos\"})\n",
    "fold_tab[\"pos_rate\"] = fold_tab[\"pos\"] / fold_tab[\"count\"].clip(lower=1)\n",
    "print(\"\\nFOLD BALANCE (count/pos/pos_rate)\")\n",
    "print(fold_tab.to_string())\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Export globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"df_train_meta\": df_train_meta,\n",
    "    \"df_test_meta\": df_test_meta,\n",
    "    \"id2split_train\": id2split_train,\n",
    "    \"id2split_test\": id2split_test,\n",
    "    \"split_stats\": split_stats,\n",
    "    \"split2id\": split2id,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555f4f0",
   "metadata": {
    "papermill": {
     "duration": 0.009873,
     "end_time": "2026-01-02T18:41:55.197225",
     "exception": false,
     "start_time": "2026-01-02T18:41:55.187352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lightcurve Loading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0825537d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:41:55.219305Z",
     "iopub.status.busy": "2026-01-02T18:41:55.218970Z",
     "iopub.status.idle": "2026-01-02T18:41:56.063021Z",
     "shell.execute_reply": "2026-01-02T18:41:56.062098Z"
    },
    "papermill": {
     "duration": 0.858067,
     "end_time": "2026-01-02T18:41:56.065046",
     "exception": false,
     "start_time": "2026-01-02T18:41:55.206979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 3 OK — LIGHTCURVE LOADING UTILITIES READY\n",
      "- Saved manifest: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/split_file_manifest.csv\n",
      "- Saved counts  : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/object_counts_by_split.csv\n",
      "- Ready for next stage: split-wise preprocessing + sequence/token building.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — Robust Lightcurve Loader Utilities (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL v2\n",
    "# - Split-wise file mapping + chunked reader utilities (no full concat)\n",
    "# - Tahan terhadap \"nan\" string / mixed dtype pada Flux\n",
    "# - Builds:\n",
    "#   * SPLIT_FILES: {split_XX: {\"train\": Path, \"test\": Path}}\n",
    "#   * train_ids_by_split / test_ids_by_split\n",
    "#   * iter_lightcurve_chunks(): generator chunk normalized\n",
    "#   * load_object_lightcurve(): per-object extraction (streaming) + optional early-stop heuristic\n",
    "# - Saves:\n",
    "#   * artifacts/split_file_manifest.csv\n",
    "#   * artifacts/object_counts_by_split.csv\n",
    "# ============================================================\n",
    "\n",
    "import gc, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"CFG\", \"SEED\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = int(SEED)\n",
    "MIN_FLUXERR = float(CFG.get(\"MIN_FLUXERR\", 1e-6))\n",
    "\n",
    "# konsisten dengan STAGE 0/2\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "REQ_LC_KEYS = [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "FILTER_ORDER = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build split file mapping (train/test lightcurves)\n",
    "# ----------------------------\n",
    "SPLIT_FILES = {}\n",
    "for s in SPLIT_LIST:\n",
    "    sd = Path(SPLIT_DIRS[s])\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n",
    "    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n",
    "\n",
    "# Save split file manifest\n",
    "manifest = []\n",
    "for s in SPLIT_LIST:\n",
    "    p_tr = SPLIT_FILES[s][\"train\"]\n",
    "    p_te = SPLIT_FILES[s][\"test\"]\n",
    "    manifest.append({\n",
    "        \"split\": s,\n",
    "        \"train_path\": str(p_tr),\n",
    "        \"test_path\": str(p_te),\n",
    "        \"train_mb\": float(p_tr.stat().st_size) / (1024**2),\n",
    "        \"test_mb\":  float(p_te.stat().st_size) / (1024**2),\n",
    "    })\n",
    "df_manifest = pd.DataFrame(manifest).sort_values(\"split\")\n",
    "manifest_path = ART_DIR / \"split_file_manifest.csv\"\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build object routing by split (fast, vectorized)\n",
    "# ----------------------------\n",
    "# df_train_meta/index = object_id, column split exists\n",
    "train_ids_by_split = {s: df_train_meta.index[df_train_meta[\"split\"] == s].astype(str).tolist() for s in SPLIT_LIST}\n",
    "test_ids_by_split  = {s: df_test_meta.index[df_test_meta[\"split\"] == s].astype(str).tolist()  for s in SPLIT_LIST}\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    \"split\": SPLIT_LIST,\n",
    "    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "})\n",
    "counts_path = ART_DIR / \"object_counts_by_split.csv\"\n",
    "df_counts.to_csv(counts_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Robust header mapping -> canonical columns\n",
    "# ----------------------------\n",
    "_LC_CFG_CACHE = {}  # (split_name, which) -> cfg dict\n",
    "\n",
    "def _canon_col(x: str) -> str:\n",
    "    # canonicalize header name for matching\n",
    "    s = str(x).strip().lower()\n",
    "    s = s.replace(\"\\ufeff\", \"\")  # BOM safety\n",
    "    s = re.sub(r\"\\s+\", \"\", s)    # remove whitespace\n",
    "    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    s = s.replace(\"-\", \"_\")\n",
    "    return s\n",
    "\n",
    "def _build_lc_read_cfg(p: Path):\n",
    "    \"\"\"\n",
    "    Robust mapping based on header only.\n",
    "    Accepts variants:\n",
    "      object_id, Object_ID\n",
    "      Time (MJD), Time(MJD), time_mjd, mjd, time\n",
    "      Flux, flux\n",
    "      Flux_err, FluxErr, flux_err\n",
    "      Filter, filter\n",
    "    \"\"\"\n",
    "    h = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n",
    "    orig_cols = list(h.columns)\n",
    "\n",
    "    # map canonical -> original col\n",
    "    c2o = {}\n",
    "    for c in orig_cols:\n",
    "        k = _canon_col(c)\n",
    "        if k not in c2o:\n",
    "            c2o[k] = c\n",
    "\n",
    "    # locate required columns\n",
    "    obj_col = c2o.get(\"object_id\", None)\n",
    "\n",
    "    # time candidates (canonical forms)\n",
    "    time_col = None\n",
    "    for k in [\"time_mjd\", \"timemjd\", \"mjd\", \"time\"]:\n",
    "        if k in c2o:\n",
    "            time_col = c2o[k]\n",
    "            break\n",
    "\n",
    "    flux_col = c2o.get(\"flux\", None)\n",
    "\n",
    "    # flux_err candidates\n",
    "    ferr_col = None\n",
    "    for k in [\"flux_err\", \"fluxerr\", \"fluxerror\"]:\n",
    "        if k in c2o:\n",
    "            ferr_col = c2o[k]\n",
    "            break\n",
    "\n",
    "    filt_col = c2o.get(\"filter\", None)\n",
    "\n",
    "    missing = []\n",
    "    if obj_col is None:  missing.append(\"object_id\")\n",
    "    if time_col is None: missing.append(\"Time (MJD)\")\n",
    "    if flux_col is None: missing.append(\"Flux\")\n",
    "    if ferr_col is None: missing.append(\"Flux_err\")\n",
    "    if filt_col is None: missing.append(\"Filter\")\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Missing required lightcurve columns in {p.name}: {missing}. \"\n",
    "            f\"Header sample: {orig_cols[:20]}\"\n",
    "        )\n",
    "\n",
    "    usecols = [obj_col, time_col, flux_col, ferr_col, filt_col]\n",
    "    rename = {obj_col:\"object_id\", time_col:\"mjd\", flux_col:\"flux\", ferr_col:\"flux_err\", filt_col:\"filter\"}\n",
    "\n",
    "    # dtype: string untuk id/filter; numeric biarkan parser -> nanti coercion robust\n",
    "    dtypes = {obj_col:\"string\", filt_col:\"string\"}\n",
    "\n",
    "    return {\"usecols\": usecols, \"dtype\": dtypes, \"rename\": rename}\n",
    "\n",
    "def _normalize_lc_chunk(df: pd.DataFrame, drop_bad_filter: bool = True, drop_bad_mjd: bool = True):\n",
    "    \"\"\"\n",
    "    Normalize to canonical columns:\n",
    "      object_id (string), mjd (float32), flux (float32), flux_err (float32), filter (string lower)\n",
    "    Rules:\n",
    "      - filter di-lower + strip; invalid filter -> NaN, bisa drop\n",
    "      - mjd/flux/flux_err -> to_numeric(errors='coerce')\n",
    "      - flux boleh NaN (missing obs) -> tetap dipertahankan\n",
    "      - flux_err dipaksa >= MIN_FLUXERR untuk nilai non-NaN\n",
    "      - drop row jika object_id kosong\n",
    "    \"\"\"\n",
    "    # ensure column set\n",
    "    df = df[[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"]].copy()\n",
    "\n",
    "    # clean strings\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "    df[\"filter\"] = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n",
    "\n",
    "    # normalize invalid filter -> <NA>\n",
    "    # note: string dtype uses <NA>\n",
    "    df.loc[~df[\"filter\"].isin(list(ALLOWED_FILTERS)), \"filter\"] = pd.NA\n",
    "\n",
    "    # numeric coercion\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(\"float32\")\n",
    "    df[\"flux\"] = pd.to_numeric(df[\"flux\"], errors=\"coerce\").astype(\"float32\")\n",
    "    df[\"flux_err\"] = pd.to_numeric(df[\"flux_err\"], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    # enforce min flux_err for stability (ignore NaN)\n",
    "    fe = df[\"flux_err\"]\n",
    "    if MIN_FLUXERR > 0:\n",
    "        df.loc[fe.notna() & (fe < MIN_FLUXERR), \"flux_err\"] = np.float32(MIN_FLUXERR)\n",
    "\n",
    "    # drop rows with empty id\n",
    "    df = df[df[\"object_id\"].notna() & (df[\"object_id\"] != \"\")]\n",
    "\n",
    "    # optional drops\n",
    "    if drop_bad_filter:\n",
    "        df = df[df[\"filter\"].notna()]\n",
    "    if drop_bad_mjd:\n",
    "        df = df[df[\"mjd\"].notna()]\n",
    "\n",
    "    return df[REQ_LC_KEYS]\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Chunked readers (core strategy)\n",
    "# ----------------------------\n",
    "def iter_lightcurve_chunks(\n",
    "    split_name: str,\n",
    "    which: str,\n",
    "    chunksize: int = 400_000,\n",
    "    drop_bad_filter: bool = True,\n",
    "    drop_bad_mjd: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream read a split lightcurve CSV in chunks.\n",
    "    Yields normalized chunks with columns:\n",
    "      object_id, mjd, flux, flux_err, filter\n",
    "\n",
    "    Notes:\n",
    "    - flux boleh NaN (missing), akan diproses di stage tokenization.\n",
    "    - filter invalid dibuang (default).\n",
    "    \"\"\"\n",
    "    if split_name not in SPLIT_FILES:\n",
    "        raise KeyError(f\"Unknown split_name={split_name}.\")\n",
    "    if which not in (\"train\", \"test\"):\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    p = SPLIT_FILES[split_name][which]\n",
    "    key = (split_name, which)\n",
    "    if key not in _LC_CFG_CACHE:\n",
    "        _LC_CFG_CACHE[key] = _build_lc_read_cfg(p)\n",
    "\n",
    "    cfg = _LC_CFG_CACHE[key]\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        p,\n",
    "        usecols=cfg[\"usecols\"],\n",
    "        dtype=cfg[\"dtype\"],          # only string enforced\n",
    "        chunksize=int(chunksize),\n",
    "        **SAFE_READ_KW\n",
    "    )\n",
    "    for chunk in reader:\n",
    "        chunk = chunk.rename(columns=cfg[\"rename\"])\n",
    "        yield _normalize_lc_chunk(chunk, drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd)\n",
    "\n",
    "def load_object_lightcurve(\n",
    "    object_id: str,\n",
    "    which: str,\n",
    "    chunksize: int = 400_000,\n",
    "    sort_time: bool = True,\n",
    "    max_chunks: int = None,\n",
    "    stop_after_found_block: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Debug-safe per-object extraction by streaming the relevant split file.\n",
    "    Heuristic stop:\n",
    "      - If file tends to be grouped by object_id, stop after we have found rows\n",
    "        and then we observe a full chunk with zero hits.\n",
    "      - This is optional and safe for debugging, not for training loop.\n",
    "    \"\"\"\n",
    "    object_id = str(object_id).strip()\n",
    "\n",
    "    if which == \"train\":\n",
    "        if object_id not in df_train_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n",
    "        split_name = str(df_train_meta.loc[object_id, \"split\"])\n",
    "    elif which == \"test\":\n",
    "        if object_id not in df_test_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n",
    "        split_name = str(df_test_meta.loc[object_id, \"split\"])\n",
    "    else:\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    pieces = []\n",
    "    seen = 0\n",
    "    found_any = False\n",
    "    last_hit = False\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=chunksize):\n",
    "        seen += 1\n",
    "        sub = ch[ch[\"object_id\"] == object_id]\n",
    "        hit = (len(sub) > 0)\n",
    "        if hit:\n",
    "            pieces.append(sub)\n",
    "            found_any = True\n",
    "\n",
    "        # heuristic stop: sudah pernah ketemu, lalu satu chunk penuh tidak ada hit\n",
    "        if stop_after_found_block and found_any and last_hit and (not hit):\n",
    "            break\n",
    "        last_hit = hit\n",
    "\n",
    "        if max_chunks is not None and seen >= int(max_chunks):\n",
    "            break\n",
    "\n",
    "    if not pieces:\n",
    "        out = pd.DataFrame(columns=REQ_LC_KEYS)\n",
    "    else:\n",
    "        out = pd.concat(pieces, ignore_index=True)\n",
    "        if sort_time and len(out) > 1:\n",
    "            # stable sort\n",
    "            out[\"filter_ord\"] = out[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n",
    "            out = out.sort_values([\"mjd\", \"filter_ord\"], kind=\"mergesort\").drop(columns=[\"filter_ord\"]).reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Quick smoke test (very light)\n",
    "# ----------------------------\n",
    "_smoke_splits = [\"split_01\", \"split_08\", \"split_17\"]\n",
    "for s in _smoke_splits:\n",
    "    if len(train_ids_by_split.get(s, [])) == 0 or len(test_ids_by_split.get(s, [])) == 0:\n",
    "        raise RuntimeError(f\"Split {s} has 0 objects in train/test log (unexpected).\")\n",
    "\n",
    "    ch_tr = next(iter_lightcurve_chunks(s, \"train\", chunksize=50_000))\n",
    "    ch_te = next(iter_lightcurve_chunks(s, \"test\",  chunksize=50_000))\n",
    "\n",
    "    if list(ch_tr.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Train chunk schema mismatch in {s}: {list(ch_tr.columns)}\")\n",
    "    if list(ch_te.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Test chunk schema mismatch in {s}: {list(ch_te.columns)}\")\n",
    "\n",
    "    badf_tr = sorted(set(ch_tr[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n",
    "    badf_te = sorted(set(ch_te[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n",
    "    if badf_tr or badf_te:\n",
    "        raise ValueError(f\"Unexpected filter values in smoke chunk split={s}: train_bad={badf_tr} test_bad={badf_te}\")\n",
    "\n",
    "print(\"STAGE 3 OK — LIGHTCURVE LOADING UTILITIES READY\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved counts  : {counts_path}\")\n",
    "print(\"- Ready for next stage: split-wise preprocessing + sequence/token building.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Export globals for next stages\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"SPLIT_FILES\": SPLIT_FILES,\n",
    "    \"train_ids_by_split\": train_ids_by_split,\n",
    "    \"test_ids_by_split\": test_ids_by_split,\n",
    "    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n",
    "    \"load_object_lightcurve\": load_object_lightcurve,\n",
    "    \"REQ_LC_KEYS\": REQ_LC_KEYS,\n",
    "    \"ALLOWED_FILTERS\": ALLOWED_FILTERS,\n",
    "    \"FILTER_ORDER\": FILTER_ORDER,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb914b1",
   "metadata": {
    "papermill": {
     "duration": 0.010635,
     "end_time": "2026-01-02T18:41:56.086531",
     "exception": false,
     "start_time": "2026-01-02T18:41:56.075896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c732df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:41:56.110976Z",
     "iopub.status.busy": "2026-01-02T18:41:56.110638Z",
     "iopub.status.idle": "2026-01-02T18:42:03.379962Z",
     "shell.execute_reply": "2026-01-02T18:42:03.378920Z"
    },
    "papermill": {
     "duration": 7.285042,
     "end_time": "2026-01-02T18:42:03.381835",
     "exception": false,
     "start_time": "2026-01-02T18:41:56.096793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4] REBUILD_MODE=wipe_all | Writing to: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/lc_clean_mag\n",
      "[Stage 4] split_01/train: parts=1 | rows=26,324 | det%=19.34% | nan_flux=11 | drop_time=0 | mag_range=[19.72, 25.96] | time=0.1s\n",
      "[Stage 4] split_01/test: parts=1 | rows=59,235 | det%=23.02% | nan_flux=23 | drop_time=0 | mag_range=[19.61, 26.20] | time=0.2s\n",
      "[Stage 4] split_02/train: parts=1 | rows=25,609 | det%=24.45% | nan_flux=6 | drop_time=0 | mag_range=[20.10, 26.04] | time=0.1s\n",
      "[Stage 4] split_02/test: parts=1 | rows=71,229 | det%=21.69% | nan_flux=8 | drop_time=0 | mag_range=[18.77, 26.32] | time=0.3s\n",
      "[Stage 4] split_03/train: parts=1 | rows=21,676 | det%=21.65% | nan_flux=5 | drop_time=0 | mag_range=[20.17, 26.23] | time=0.1s\n",
      "[Stage 4] split_03/test: parts=1 | rows=53,751 | det%=21.90% | nan_flux=8 | drop_time=0 | mag_range=[19.61, 26.37] | time=0.2s\n",
      "[Stage 4] split_04/train: parts=1 | rows=22,898 | det%=21.11% | nan_flux=12 | drop_time=0 | mag_range=[20.38, 26.16] | time=0.1s\n",
      "[Stage 4] split_04/test: parts=1 | rows=51,408 | det%=21.70% | nan_flux=50 | drop_time=0 | mag_range=[19.66, 26.25] | time=0.2s\n",
      "[Stage 4] split_05/train: parts=1 | rows=25,934 | det%=18.33% | nan_flux=15 | drop_time=0 | mag_range=[18.82, 26.33] | time=0.1s\n",
      "[Stage 4] split_05/test: parts=1 | rows=61,179 | det%=18.21% | nan_flux=42 | drop_time=0 | mag_range=[19.09, 26.23] | time=0.3s\n",
      "[Stage 4] split_06/train: parts=1 | rows=25,684 | det%=18.85% | nan_flux=11 | drop_time=0 | mag_range=[19.80, 26.15] | time=0.1s\n",
      "[Stage 4] split_06/test: parts=1 | rows=57,620 | det%=19.94% | nan_flux=21 | drop_time=0 | mag_range=[19.28, 26.04] | time=0.2s\n",
      "[Stage 4] split_07/train: parts=1 | rows=24,473 | det%=21.44% | nan_flux=306 | drop_time=0 | mag_range=[20.05, 26.32] | time=0.1s\n",
      "[Stage 4] split_07/test: parts=1 | rows=65,101 | det%=19.10% | nan_flux=904 | drop_time=0 | mag_range=[19.97, 26.32] | time=0.3s\n",
      "[Stage 4] split_08/train: parts=1 | rows=25,571 | det%=22.80% | nan_flux=6 | drop_time=0 | mag_range=[18.59, 26.30] | time=0.1s\n",
      "[Stage 4] split_08/test: parts=1 | rows=61,498 | det%=24.50% | nan_flux=47 | drop_time=0 | mag_range=[17.18, 25.96] | time=0.2s\n",
      "[Stage 4] split_09/train: parts=1 | rows=19,690 | det%=21.13% | nan_flux=315 | drop_time=0 | mag_range=[19.83, 26.31] | time=0.1s\n",
      "[Stage 4] split_09/test: parts=1 | rows=47,239 | det%=22.70% | nan_flux=514 | drop_time=0 | mag_range=[18.88, 26.11] | time=0.2s\n",
      "[Stage 4] split_10/train: parts=1 | rows=25,151 | det%=20.86% | nan_flux=7 | drop_time=0 | mag_range=[19.52, 26.21] | time=0.1s\n",
      "[Stage 4] split_10/test: parts=1 | rows=51,056 | det%=21.21% | nan_flux=51 | drop_time=0 | mag_range=[16.62, 26.43] | time=0.2s\n",
      "[Stage 4] split_11/train: parts=1 | rows=22,927 | det%=19.59% | nan_flux=4 | drop_time=0 | mag_range=[20.57, 26.42] | time=0.1s\n",
      "[Stage 4] split_11/test: parts=1 | rows=49,723 | det%=20.17% | nan_flux=12 | drop_time=0 | mag_range=[20.31, 26.42] | time=0.2s\n",
      "[Stage 4] split_12/train: parts=1 | rows=25,546 | det%=19.64% | nan_flux=12 | drop_time=0 | mag_range=[20.40, 26.32] | time=0.1s\n",
      "[Stage 4] split_12/test: parts=1 | rows=54,499 | det%=19.29% | nan_flux=13 | drop_time=0 | mag_range=[19.98, 26.26] | time=0.2s\n",
      "[Stage 4] split_13/train: parts=1 | rows=23,203 | det%=20.64% | nan_flux=2 | drop_time=0 | mag_range=[19.46, 26.05] | time=0.1s\n",
      "[Stage 4] split_13/test: parts=1 | rows=63,653 | det%=19.56% | nan_flux=15 | drop_time=0 | mag_range=[18.03, 26.15] | time=0.3s\n",
      "[Stage 4] split_14/train: parts=1 | rows=25,706 | det%=20.36% | nan_flux=23 | drop_time=0 | mag_range=[20.24, 26.40] | time=0.1s\n",
      "[Stage 4] split_14/test: parts=1 | rows=58,643 | det%=17.91% | nan_flux=43 | drop_time=0 | mag_range=[19.51, 26.04] | time=0.2s\n",
      "[Stage 4] split_15/train: parts=1 | rows=23,972 | det%=19.09% | nan_flux=116 | drop_time=0 | mag_range=[20.11, 26.04] | time=0.1s\n",
      "[Stage 4] split_15/test: parts=1 | rows=52,943 | det%=20.03% | nan_flux=197 | drop_time=0 | mag_range=[19.91, 26.24] | time=0.2s\n",
      "[Stage 4] split_16/train: parts=1 | rows=25,173 | det%=21.42% | nan_flux=3 | drop_time=0 | mag_range=[20.00, 26.26] | time=0.1s\n",
      "[Stage 4] split_16/test: parts=1 | rows=58,192 | det%=20.12% | nan_flux=13 | drop_time=0 | mag_range=[19.57, 26.17] | time=0.3s\n",
      "[Stage 4] split_17/train: parts=1 | rows=22,705 | det%=22.09% | nan_flux=12 | drop_time=0 | mag_range=[19.64, 26.17] | time=0.1s\n",
      "[Stage 4] split_17/test: parts=1 | rows=59,482 | det%=19.59% | nan_flux=21 | drop_time=0 | mag_range=[19.96, 26.42] | time=0.2s\n",
      "[Stage 4] split_18/train: parts=1 | rows=21,536 | det%=23.77% | nan_flux=7 | drop_time=0 | mag_range=[20.49, 26.05] | time=0.1s\n",
      "[Stage 4] split_18/test: parts=1 | rows=53,887 | det%=23.88% | nan_flux=14 | drop_time=0 | mag_range=[20.60, 26.44] | time=0.2s\n",
      "[Stage 4] split_19/train: parts=1 | rows=22,087 | det%=23.73% | nan_flux=8 | drop_time=0 | mag_range=[19.98, 26.31] | time=0.1s\n",
      "[Stage 4] split_19/test: parts=1 | rows=56,355 | det%=24.17% | nan_flux=16 | drop_time=0 | mag_range=[19.93, 26.34] | time=0.2s\n",
      "[Stage 4] split_20/train: parts=1 | rows=23,519 | det%=20.45% | nan_flux=10 | drop_time=0 | mag_range=[19.80, 25.93] | time=0.1s\n",
      "[Stage 4] split_20/test: parts=1 | rows=58,432 | det%=19.65% | nan_flux=10 | drop_time=0 | mag_range=[19.87, 26.36] | time=0.2s\n",
      "\n",
      "[Stage 4] Done.\n",
      "- LC_CLEAN_DIR : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/lc_clean_mag\n",
      "- Saved manifest: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
      "- Saved summary : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/lc_clean_mag/lc_clean_mag_summary.csv\n",
      "- Saved config  : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/lc_clean_mag/photometric_config_mag.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Photometric Cleaning (FORCE OVERWRITE) — REVISI FULL v6.1\n",
    "# FIX:\n",
    "# - Tidak pakai numpy.astype(\"string\") (error). Pakai pd.array(..., dtype=\"string\")\n",
    "# - FORCE overwrite: wipe lc_clean_mag lalu rebuild\n",
    "# - Atomic write (.tmp -> rename)\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings, time, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings\n",
    "# ----------------------------\n",
    "CHUNKSIZE   = 350_000\n",
    "ERR_EPS     = 1e-6\n",
    "SNR_DET     = 3.0\n",
    "DET_SIGMA   = 3.0\n",
    "\n",
    "MIN_FLUX_POS_UJY   = 1e-6\n",
    "MAG_MIN, MAG_MAX   = -10.0, 50.0\n",
    "MAGERR_FLOOR_DET   = 1e-3\n",
    "MAGERR_FLOOR_ND    = 0.75\n",
    "MAGERR_CAP         = 10.0\n",
    "\n",
    "WRITE_FORMAT = \"parquet\"\n",
    "ONLY_SPLITS  = None\n",
    "KEEP_FLUX_DEBUG = False\n",
    "DROP_BAD_TIME_ROWS = True\n",
    "\n",
    "# FORCE overwrite\n",
    "REBUILD_MODE = \"wipe_all\"  # \"wipe_all\" (recommended) | \"wipe_parts_only\"\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Extinction coefficients (placeholder; ganti kalau punya nilai resmi)\n",
    "# ----------------------------\n",
    "EXT_RLAMBDA = {\"u\": 4.8, \"g\": 3.6, \"r\": 2.7, \"i\": 2.1, \"z\": 1.6, \"y\": 1.3}\n",
    "BAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\n",
    "ID2BAND = {v: k for k, v in BAND2ID.items()}\n",
    "\n",
    "EBV_TRAIN_SER = df_train_meta[\"EBV\"]\n",
    "EBV_TEST_SER  = df_test_meta[\"EBV\"]\n",
    "\n",
    "MAG_ZP = float(2.5 * np.log10(3631e6))  # ~23.9\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Output root + WIPE (with safety guard)\n",
    "# ----------------------------\n",
    "LC_CLEAN_DIR = ART_DIR / \"lc_clean_mag\"\n",
    "\n",
    "# safety: ensure LC_CLEAN_DIR is under ART_DIR\n",
    "art_abs = ART_DIR.resolve()\n",
    "lc_abs  = LC_CLEAN_DIR.resolve()\n",
    "if str(art_abs) not in str(lc_abs):\n",
    "    raise RuntimeError(\"Safety guard: LC_CLEAN_DIR bukan turunan ART_DIR. Abort.\")\n",
    "\n",
    "if REBUILD_MODE == \"wipe_all\":\n",
    "    if LC_CLEAN_DIR.exists():\n",
    "        shutil.rmtree(LC_CLEAN_DIR, ignore_errors=True)\n",
    "    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "elif REBUILD_MODE == \"wipe_parts_only\":\n",
    "    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    raise ValueError(\"REBUILD_MODE must be 'wipe_all' or 'wipe_parts_only'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Atomic writer\n",
    "# ----------------------------\n",
    "def _atomic_write_parquet(df: pd.DataFrame, out_path: Path):\n",
    "    tmp = out_path.with_name(out_path.name + \".tmp\")\n",
    "    df.to_parquet(tmp, index=False)\n",
    "    tmp.replace(out_path)\n",
    "\n",
    "def _atomic_write_csv_gz(df: pd.DataFrame, out_path: Path):\n",
    "    out_path = out_path.with_suffix(\".csv.gz\")\n",
    "    tmp = out_path.with_name(out_path.name + \".tmp\")\n",
    "    df.to_csv(tmp, index=False, compression=\"gzip\")\n",
    "    tmp.replace(out_path)\n",
    "    return out_path\n",
    "\n",
    "def write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == \"parquet\":\n",
    "        try:\n",
    "            _atomic_write_parquet(df, out_path)\n",
    "            return \"parquet\", out_path\n",
    "        except Exception as e:\n",
    "            alt = _atomic_write_csv_gz(df, out_path)\n",
    "            return f\"csv.gz (fallback from parquet: {type(e).__name__})\", alt\n",
    "    elif fmt == \"csv.gz\":\n",
    "        alt = _atomic_write_csv_gz(df, out_path)\n",
    "        return \"csv.gz\", alt\n",
    "    else:\n",
    "        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Core cleaning (NaN/negative-safe)\n",
    "# ----------------------------\n",
    "def clean_chunk_to_mag(ch: pd.DataFrame, ebv_ser: pd.Series):\n",
    "    # object_id as pandas StringDtype (safe)\n",
    "    oid = ch[\"object_id\"].astype(\"string\").to_numpy(copy=False)\n",
    "\n",
    "    mjd = ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    err  = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    # sanitize err\n",
    "    err = np.nan_to_num(err, nan=np.float32(ERR_EPS), posinf=np.float32(ERR_EPS), neginf=np.float32(ERR_EPS))\n",
    "    err = np.maximum(err, np.float32(ERR_EPS))\n",
    "\n",
    "    # sanitize flux: keep NaN as NaN, but force inf -> NaN\n",
    "    flux = flux.astype(np.float32, copy=False)\n",
    "    flux[~np.isfinite(flux)] = np.float32(np.nan)\n",
    "\n",
    "    # filter normalize\n",
    "    filt = ch[\"filter\"].astype(\"string\").to_numpy(copy=False)\n",
    "    filt = np.char.lower(np.char.strip(filt.astype(str)))\n",
    "\n",
    "    # band_id\n",
    "    band_id = np.full(len(ch), -1, dtype=np.int8)\n",
    "    for b, bid in BAND2ID.items():\n",
    "        band_id[filt == b] = np.int8(bid)\n",
    "    if np.any(band_id < 0):\n",
    "        bad = sorted(set(filt[band_id < 0].tolist()))\n",
    "        raise ValueError(f\"Unknown filter values encountered (example up to 10): {bad[:10]}\")\n",
    "\n",
    "    # EBV lookup\n",
    "    ebv = ch[\"object_id\"].map(ebv_ser).fillna(0.0).to_numpy(dtype=np.float32)\n",
    "    ebv[~np.isfinite(ebv)] = np.float32(0.0)\n",
    "\n",
    "    # R_lambda lookup\n",
    "    rlam = np.zeros(len(ch), dtype=np.float32)\n",
    "    for b, rv in EXT_RLAMBDA.items():\n",
    "        rlam[filt == b] = np.float32(rv)\n",
    "\n",
    "    A = (rlam * ebv).astype(np.float32)\n",
    "    mul = np.power(np.float32(10.0), (np.float32(0.4) * A)).astype(np.float32)\n",
    "\n",
    "    flux_deext = (flux * mul).astype(np.float32)\n",
    "    err_deext  = (err  * mul).astype(np.float32)\n",
    "\n",
    "    # snr (NaN flux -> snr=0)\n",
    "    snr = np.zeros_like(err_deext, dtype=np.float32)\n",
    "    okf = np.isfinite(flux_deext)\n",
    "    snr[okf] = (flux_deext[okf] / np.maximum(err_deext[okf], np.float32(ERR_EPS))).astype(np.float32)\n",
    "\n",
    "    detected = (snr > np.float32(SNR_DET)).astype(np.int8)\n",
    "\n",
    "    nan_flux_rows = int((~okf).sum())\n",
    "    if nan_flux_rows:\n",
    "        detected[~okf] = np.int8(0)\n",
    "        snr[~okf] = np.float32(0.0)\n",
    "\n",
    "    flux_detlim = (np.float32(DET_SIGMA) * err_deext).astype(np.float32)\n",
    "\n",
    "    flux_for_mag = np.where(\n",
    "        detected == 1,\n",
    "        np.maximum(flux_deext, np.float32(MIN_FLUX_POS_UJY)),\n",
    "        np.maximum(flux_detlim, np.float32(MIN_FLUX_POS_UJY)),\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    mag = (np.float32(MAG_ZP) - np.float32(2.5) * np.log10(flux_for_mag)).astype(np.float32)\n",
    "    mag = np.clip(mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32)\n",
    "\n",
    "    mag_err = (np.float32(1.0857362) * (err_deext / flux_for_mag)).astype(np.float32)\n",
    "    mag_err = np.clip(mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32)\n",
    "\n",
    "    if MAGERR_FLOOR_ND is not None and float(MAGERR_FLOOR_ND) > 0:\n",
    "        mag_err = np.where(\n",
    "            detected == 1,\n",
    "            mag_err,\n",
    "            np.maximum(mag_err, np.float32(MAGERR_FLOOR_ND))\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"object_id\": pd.array(oid, dtype=\"string\"),\n",
    "        \"mjd\": mjd.astype(np.float32, copy=False),\n",
    "        \"band_id\": band_id.astype(np.int8, copy=False),\n",
    "        \"mag\": mag.astype(np.float32, copy=False),\n",
    "        \"mag_err\": mag_err.astype(np.float32, copy=False),\n",
    "        \"snr\": snr.astype(np.float32, copy=False),\n",
    "        \"detected\": detected.astype(np.int8, copy=False),\n",
    "    })\n",
    "\n",
    "    dropped_time = 0\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        t = out[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "        keep = np.isfinite(t)\n",
    "        dropped_time = int((~keep).sum())\n",
    "        if dropped_time:\n",
    "            out = out[keep]\n",
    "\n",
    "    if KEEP_FLUX_DEBUG:\n",
    "        out[\"flux_deext\"] = pd.Series(np.nan_to_num(flux_deext, nan=0.0), dtype=\"float32\")\n",
    "        out[\"err_deext\"]  = pd.Series(err_deext, dtype=\"float32\")\n",
    "\n",
    "    return out, dropped_time, nan_flux_rows\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Process split-wise\n",
    "# ----------------------------\n",
    "splits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\n",
    "\n",
    "summary_rows, manifest_rows = [], []\n",
    "\n",
    "def _wipe_parts_dir(out_dir: Path):\n",
    "    if out_dir.exists():\n",
    "        for pat in [\"part_*.parquet\", \"part_*.csv.gz\", \"*.tmp\"]:\n",
    "            for f in out_dir.glob(pat):\n",
    "                try:\n",
    "                    f.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def process_split(split_name: str, which: str):\n",
    "    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n",
    "    out_dir = LC_CLEAN_DIR / split_name / which\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if REBUILD_MODE == \"wipe_parts_only\":\n",
    "        _wipe_parts_dir(out_dir)\n",
    "\n",
    "    t0 = time.time()\n",
    "    part_idx = 0\n",
    "    n_rows_total = 0\n",
    "    n_neg_proxy = 0\n",
    "    n_det = 0\n",
    "    n_finite_mag = 0\n",
    "    mag_min = np.inf\n",
    "    mag_max = -np.inf\n",
    "    dropped_time_total = 0\n",
    "    nan_flux_total = 0\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n",
    "        cleaned, dropped_time, nan_flux = clean_chunk_to_mag(ch, ebv_ser)\n",
    "\n",
    "        dropped_time_total += dropped_time\n",
    "        nan_flux_total += nan_flux\n",
    "\n",
    "        n_rows = int(len(cleaned))\n",
    "        n_rows_total += n_rows\n",
    "\n",
    "        snr_arr = cleaned[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n",
    "        n_neg_proxy += int((snr_arr < 0).sum())\n",
    "\n",
    "        det_arr = cleaned[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n",
    "        n_det += int(det_arr.sum())\n",
    "\n",
    "        mag_arr = cleaned[\"mag\"].to_numpy(dtype=np.float32, copy=False)\n",
    "        fin = np.isfinite(mag_arr)\n",
    "        n_finite_mag += int(fin.sum())\n",
    "        if fin.any():\n",
    "            mag_min = float(min(mag_min, float(np.min(mag_arr[fin]))))\n",
    "            mag_max = float(max(mag_max, float(np.max(mag_arr[fin]))))\n",
    "\n",
    "        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n",
    "        used_fmt, final_path = write_part(cleaned, out_path, WRITE_FORMAT)\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"which\": which,\n",
    "            \"part\": part_idx,\n",
    "            \"path\": str(final_path),\n",
    "            \"rows\": n_rows,\n",
    "            \"format\": used_fmt,\n",
    "        })\n",
    "\n",
    "        part_idx += 1\n",
    "        del cleaned, ch\n",
    "        if part_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    summary_rows.append({\n",
    "        \"split\": split_name,\n",
    "        \"which\": which,\n",
    "        \"parts\": part_idx,\n",
    "        \"rows\": n_rows_total,\n",
    "        \"neg_flux_frac_proxy\": (n_neg_proxy / max(n_rows_total, 1)),\n",
    "        \"det_frac_snr_gt_thr\": (n_det / max(n_rows_total, 1)),\n",
    "        \"finite_mag_frac\": (n_finite_mag / max(n_rows_total, 1)),\n",
    "        \"mag_min\": (mag_min if np.isfinite(mag_min) else np.nan),\n",
    "        \"mag_max\": (mag_max if np.isfinite(mag_max) else np.nan),\n",
    "        \"dropped_time_rows\": int(dropped_time_total),\n",
    "        \"nan_flux_rows\": int(nan_flux_total),\n",
    "        \"sec\": float(dt),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | \"\n",
    "        f\"det%={100*(n_det/max(n_rows_total,1)):.2f}% | \"\n",
    "        f\"nan_flux={nan_flux_total:,} | drop_time={dropped_time_total:,} | \"\n",
    "        f\"mag_range=[{(mag_min if np.isfinite(mag_min) else np.nan):.2f}, {(mag_max if np.isfinite(mag_max) else np.nan):.2f}] | \"\n",
    "        f\"time={dt:.1f}s\"\n",
    "    )\n",
    "\n",
    "print(f\"[Stage 4] REBUILD_MODE={REBUILD_MODE} | Writing to: {LC_CLEAN_DIR}\")\n",
    "for s in splits_to_use:\n",
    "    process_split(s, \"train\")\n",
    "    process_split(s, \"test\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Save manifests + summary + config\n",
    "# ----------------------------\n",
    "df_manifest = pd.DataFrame(manifest_rows)\n",
    "df_summary  = pd.DataFrame(summary_rows)\n",
    "\n",
    "manifest_path = LC_CLEAN_DIR / \"lc_clean_mag_manifest.csv\"\n",
    "summary_path  = LC_CLEAN_DIR / \"lc_clean_mag_summary.csv\"\n",
    "\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "cfg_path = LC_CLEAN_DIR / \"photometric_config_mag.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "        \"SNR_DET\": float(SNR_DET),\n",
    "        \"DET_SIGMA\": float(DET_SIGMA),\n",
    "        \"ERR_EPS\": float(ERR_EPS),\n",
    "        \"MIN_FLUX_POS_UJY\": float(MIN_FLUX_POS_UJY),\n",
    "        \"MAG_ZP\": float(MAG_ZP),\n",
    "        \"MAG_MIN\": float(MAG_MIN),\n",
    "        \"MAG_MAX\": float(MAG_MAX),\n",
    "        \"MAGERR_FLOOR_DET\": float(MAGERR_FLOOR_DET),\n",
    "        \"MAGERR_FLOOR_ND\": float(MAGERR_FLOOR_ND),\n",
    "        \"MAGERR_CAP\": float(MAGERR_CAP),\n",
    "        \"CHUNKSIZE\": int(CHUNKSIZE),\n",
    "        \"WRITE_FORMAT\": str(WRITE_FORMAT),\n",
    "        \"ONLY_SPLITS\": list(splits_to_use),\n",
    "        \"KEEP_FLUX_DEBUG\": bool(KEEP_FLUX_DEBUG),\n",
    "        \"DROP_BAD_TIME_ROWS\": bool(DROP_BAD_TIME_ROWS),\n",
    "        \"REBUILD_MODE\": str(REBUILD_MODE),\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 4] Done.\")\n",
    "print(f\"- LC_CLEAN_DIR : {LC_CLEAN_DIR}\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved summary : {summary_path}\")\n",
    "print(f\"- Saved config  : {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Helper for next stages\n",
    "# ----------------------------\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    m = df_manifest[(df_manifest[\"split\"] == split_name) & (df_manifest[\"which\"] == which)].sort_values(\"part\")\n",
    "    return m[\"path\"].astype(str).tolist()\n",
    "\n",
    "globals().update({\n",
    "    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "    \"BAND2ID\": BAND2ID,\n",
    "    \"ID2BAND\": ID2BAND,\n",
    "    \"MAG_ZP\": MAG_ZP,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"lc_clean_mag_manifest\": df_manifest,\n",
    "    \"lc_clean_mag_summary\": df_summary,\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d3059",
   "metadata": {
    "papermill": {
     "duration": 0.011816,
     "end_time": "2026-01-02T18:42:03.405424",
     "exception": false,
     "start_time": "2026-01-02T18:42:03.393608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Tokenization (Event-based Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b46a225a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:42:03.431463Z",
     "iopub.status.busy": "2026-01-02T18:42:03.431039Z",
     "iopub.status.idle": "2026-01-02T18:47:47.684519Z",
     "shell.execute_reply": "2026-01-02T18:47:47.683432Z"
    },
    "papermill": {
     "duration": 344.281864,
     "end_time": "2026-01-02T18:47:47.698949",
     "exception": false,
     "start_time": "2026-01-02T18:42:03.417085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 5 ROUTING SYNC OK\n",
      "- RUN_DIR      : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418\n",
      "- ART_DIR      : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts\n",
      "- LC_CLEAN_DIR : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/lc_clean_mag\n",
      "- manifest_csv : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
      "\n",
      "[Stage 5] split_01/train | expected=155 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=155 | kept_rows=26,324 | len_mean 169.8->169.8 | p95 191.7->191.7 | trunc%=0.0% | time=8.76s | mode=mag\n",
      "\n",
      "[Stage 5] split_01/test | expected=364 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=364 | kept_rows=59,235 | len_mean 162.7->162.7 | p95 193.8->193.8 | trunc%=0.0% | time=8.90s | mode=mag\n",
      "\n",
      "[Stage 5] split_02/train | expected=170 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=170 | kept_rows=25,609 | len_mean 150.6->150.6 | p95 195.5->195.5 | trunc%=0.0% | time=8.37s | mode=mag\n",
      "\n",
      "[Stage 5] split_02/test | expected=414 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=414 | kept_rows=71,229 | len_mean 172.1->172.1 | p95 201.0->201.0 | trunc%=0.0% | time=9.00s | mode=mag\n",
      "\n",
      "[Stage 5] split_03/train | expected=138 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=138 | kept_rows=21,676 | len_mean 157.1->157.1 | p95 191.8->191.8 | trunc%=0.0% | time=7.49s | mode=mag\n",
      "\n",
      "[Stage 5] split_03/test | expected=338 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=338 | kept_rows=53,751 | len_mean 159.0->159.0 | p95 194.3->194.3 | trunc%=0.0% | time=8.85s | mode=mag\n",
      "\n",
      "[Stage 5] split_04/train | expected=145 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=145 | kept_rows=22,898 | len_mean 157.9->157.9 | p95 187.0->187.0 | trunc%=0.0% | time=8.14s | mode=mag\n",
      "\n",
      "[Stage 5] split_04/test | expected=332 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=332 | kept_rows=51,408 | len_mean 154.8->154.8 | p95 195.0->195.0 | trunc%=0.0% | time=8.96s | mode=mag\n",
      "\n",
      "[Stage 5] split_05/train | expected=165 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=165 | kept_rows=25,934 | len_mean 157.2->157.2 | p95 191.6->191.6 | trunc%=0.0% | time=8.50s | mode=mag\n",
      "\n",
      "[Stage 5] split_05/test | expected=375 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=375 | kept_rows=61,179 | len_mean 163.1->163.1 | p95 194.0->194.0 | trunc%=0.0% | time=8.86s | mode=mag\n",
      "\n",
      "[Stage 5] split_06/train | expected=155 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=155 | kept_rows=25,684 | len_mean 165.7->165.7 | p95 198.0->198.0 | trunc%=0.0% | time=7.87s | mode=mag\n",
      "\n",
      "[Stage 5] split_06/test | expected=374 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=374 | kept_rows=57,620 | len_mean 154.1->154.1 | p95 189.3->189.3 | trunc%=0.0% | time=8.87s | mode=mag\n",
      "\n",
      "[Stage 5] split_07/train | expected=165 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=165 | kept_rows=24,473 | len_mean 148.3->148.3 | p95 191.6->191.6 | trunc%=0.0% | time=8.34s | mode=mag\n",
      "\n",
      "[Stage 5] split_07/test | expected=398 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=398 | kept_rows=65,101 | len_mean 163.6->163.6 | p95 194.1->194.1 | trunc%=0.0% | time=8.95s | mode=mag\n",
      "\n",
      "[Stage 5] split_08/train | expected=162 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=162 | kept_rows=25,571 | len_mean 157.8->157.8 | p95 185.9->185.9 | trunc%=0.0% | time=8.12s | mode=mag\n",
      "\n",
      "[Stage 5] split_08/test | expected=387 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=387 | kept_rows=61,498 | len_mean 158.9->158.9 | p95 194.4->194.4 | trunc%=0.0% | time=8.90s | mode=mag\n",
      "\n",
      "[Stage 5] split_09/train | expected=128 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=128 | kept_rows=19,690 | len_mean 153.8->153.8 | p95 192.3->192.3 | trunc%=0.0% | time=7.28s | mode=mag\n",
      "\n",
      "[Stage 5] split_09/test | expected=289 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=289 | kept_rows=47,239 | len_mean 163.5->163.5 | p95 196.0->196.0 | trunc%=0.0% | time=8.79s | mode=mag\n",
      "\n",
      "[Stage 5] split_10/train | expected=144 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=144 | kept_rows=25,151 | len_mean 174.7->174.7 | p95 204.4->204.4 | trunc%=0.0% | time=8.11s | mode=mag\n",
      "\n",
      "[Stage 5] split_10/test | expected=331 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=331 | kept_rows=51,056 | len_mean 154.2->154.2 | p95 188.0->188.0 | trunc%=0.0% | time=8.74s | mode=mag\n",
      "\n",
      "[Stage 5] split_11/train | expected=146 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=146 | kept_rows=22,927 | len_mean 157.0->157.0 | p95 192.0->192.0 | trunc%=0.0% | time=8.02s | mode=mag\n",
      "\n",
      "[Stage 5] split_11/test | expected=325 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=325 | kept_rows=49,723 | len_mean 153.0->153.0 | p95 189.6->189.6 | trunc%=0.0% | time=8.67s | mode=mag\n",
      "\n",
      "[Stage 5] split_12/train | expected=155 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=155 | kept_rows=25,546 | len_mean 164.8->164.8 | p95 193.9->193.9 | trunc%=0.0% | time=8.21s | mode=mag\n",
      "\n",
      "[Stage 5] split_12/test | expected=353 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=353 | kept_rows=54,499 | len_mean 154.4->154.4 | p95 185.0->185.0 | trunc%=0.0% | time=8.60s | mode=mag\n",
      "\n",
      "[Stage 5] split_13/train | expected=143 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=143 | kept_rows=23,203 | len_mean 162.3->162.3 | p95 190.9->190.9 | trunc%=0.0% | time=7.93s | mode=mag\n",
      "\n",
      "[Stage 5] split_13/test | expected=379 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=379 | kept_rows=63,653 | len_mean 167.9->167.9 | p95 195.1->195.1 | trunc%=0.0% | time=8.83s | mode=mag\n",
      "\n",
      "[Stage 5] split_14/train | expected=154 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=154 | kept_rows=25,706 | len_mean 166.9->166.9 | p95 198.0->198.0 | trunc%=0.0% | time=7.93s | mode=mag\n",
      "\n",
      "[Stage 5] split_14/test | expected=351 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=351 | kept_rows=58,643 | len_mean 167.1->167.1 | p95 193.0->193.0 | trunc%=0.0% | time=8.87s | mode=mag\n",
      "\n",
      "[Stage 5] split_15/train | expected=158 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=158 | kept_rows=23,972 | len_mean 151.7->151.7 | p95 195.0->195.0 | trunc%=0.0% | time=7.95s | mode=mag\n",
      "\n",
      "[Stage 5] split_15/test | expected=342 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=342 | kept_rows=52,943 | len_mean 154.8->154.8 | p95 193.0->193.0 | trunc%=0.0% | time=8.86s | mode=mag\n",
      "\n",
      "[Stage 5] split_16/train | expected=155 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=155 | kept_rows=25,173 | len_mean 162.4->162.4 | p95 199.9->199.9 | trunc%=0.0% | time=7.83s | mode=mag\n",
      "\n",
      "[Stage 5] split_16/test | expected=354 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=354 | kept_rows=58,192 | len_mean 164.4->164.4 | p95 193.3->193.3 | trunc%=0.0% | time=8.74s | mode=mag\n",
      "\n",
      "[Stage 5] split_17/train | expected=153 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=153 | kept_rows=22,705 | len_mean 148.4->148.4 | p95 190.4->190.4 | trunc%=0.0% | time=7.73s | mode=mag\n",
      "\n",
      "[Stage 5] split_17/test | expected=351 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=351 | kept_rows=59,482 | len_mean 169.5->169.5 | p95 198.0->198.0 | trunc%=0.0% | time=8.73s | mode=mag\n",
      "\n",
      "[Stage 5] split_18/train | expected=152 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=152 | kept_rows=21,536 | len_mean 141.7->141.7 | p95 191.2->191.2 | trunc%=0.0% | time=8.02s | mode=mag\n",
      "\n",
      "[Stage 5] split_18/test | expected=345 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=345 | kept_rows=53,887 | len_mean 156.2->156.2 | p95 188.8->188.8 | trunc%=0.0% | time=9.17s | mode=mag\n",
      "\n",
      "[Stage 5] split_19/train | expected=147 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=147 | kept_rows=22,087 | len_mean 150.3->150.3 | p95 188.4->188.4 | trunc%=0.0% | time=7.96s | mode=mag\n",
      "\n",
      "[Stage 5] split_19/test | expected=375 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=375 | kept_rows=56,355 | len_mean 150.3->150.3 | p95 191.0->191.0 | trunc%=0.0% | time=9.17s | mode=mag\n",
      "\n",
      "[Stage 5] split_20/train | expected=153 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=153 | kept_rows=23,519 | len_mean 153.7->153.7 | p95 187.8->187.8 | trunc%=0.0% | time=8.78s | mode=mag\n",
      "\n",
      "[Stage 5] split_20/test | expected=358 | L_MAX=256 | TRUNC=smart_band_peak\n",
      "[Stage 5] OK: built=358 | kept_rows=58,432 | len_mean 163.2->163.2 | p95 190.0->190.0 | trunc%=0.0% | time=9.03s | mode=mag\n",
      "\n",
      "[Stage 5] DONE\n",
      "- token_mode : mag\n",
      "- features   : ['t_rel_log', 'dt_log', 'mag', 'mag_err_log', 'snr_tanh', 'detected']\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/seq_tokens/seq_manifest_train.csv (rows=3,043)\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/seq_tokens/seq_manifest_test.csv  (rows=7,135)\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/seq_tokens/seq_build_stats.csv\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/seq_tokens/seq_config.json\n",
      "\n",
      "[Stage 5] Smoke test object_id=Dornhoth_anwar_melethron\n",
      "- seq_len=184 | X_shape=(184, 6) | bands_unique=[0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v5 (FIX PATH SYNC + HARDENED + BUCKET ROBUST)\n",
    "#\n",
    "# FIX UTAMA:\n",
    "# - Auto-find STAGE 4 manifest (lc_clean_mag_manifest.csv) dari run manapun.\n",
    "# - Sync path BENAR:\n",
    "#     manifest: .../run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
    "#     LC_CLEAN_DIR = manifest.parent\n",
    "#     ART_DIR      = LC_CLEAN_DIR.parent\n",
    "#     RUN_DIR      = ART_DIR.parent\n",
    "# - Validasi semua part path ada (tidak hanya manifest-nya).\n",
    "#\n",
    "# OUTPUT:\n",
    "# - artifacts/seq_tokens/split_XX/{train|test}/shard_*.npz\n",
    "# - artifacts/seq_tokens/seq_manifest_{train|test}.csv\n",
    "# - artifacts/seq_tokens/seq_build_stats.csv\n",
    "# - artifacts/seq_tokens/seq_config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "for need in [\"ART_DIR\", \"df_train_meta\", \"df_test_meta\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 0 + STAGE 2 dulu (ART_DIR + meta).\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers\n",
    "# ----------------------------\n",
    "def _safe_string_series(s: pd.Series) -> pd.Series:\n",
    "    # Pandas 2.x aman pakai \"string\"; tapi fallback kalau environment aneh\n",
    "    try:\n",
    "        return s.astype(\"string\").str.strip()\n",
    "    except Exception:\n",
    "        return s.astype(str).str.strip()\n",
    "\n",
    "def _find_stage4_manifest(art_dir: Path) -> Path | None:\n",
    "    # 1) try current ART_DIR\n",
    "    cand = art_dir / \"lc_clean_mag\" / \"lc_clean_mag_manifest.csv\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "\n",
    "    # 2) scan all runs\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    if not root.exists():\n",
    "        return None\n",
    "\n",
    "    cands = list(root.glob(\"run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\"))\n",
    "    if not cands:\n",
    "        cands = list(root.glob(\"run_*/**/lc_clean_mag_manifest.csv\"))\n",
    "\n",
    "    if not cands:\n",
    "        return None\n",
    "\n",
    "    cands = sorted(cands, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def _sync_dirs_from_manifest(manifest_csv: Path):\n",
    "    # manifest: .../run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
    "    lc_clean_dir = manifest_csv.parent                  # .../lc_clean_mag\n",
    "    art_dir_new  = lc_clean_dir.parent                  # .../artifacts\n",
    "    run_dir_new  = art_dir_new.parent                   # .../run_*\n",
    "    return run_dir_new, art_dir_new, lc_clean_dir\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Locate STAGE 4 output (robust)\n",
    "# ----------------------------\n",
    "manifest_csv = _find_stage4_manifest(ART_DIR)\n",
    "if manifest_csv is None:\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    runs = sorted([p.name for p in root.glob(\"run_*\") if p.is_dir()])[-15:] if root.exists() else []\n",
    "    raise RuntimeError(\n",
    "        \"Output STAGE 4 (lc_clean_mag_manifest.csv) tidak ditemukan.\\n\"\n",
    "        f\"- ART_DIR saat ini: {ART_DIR}\\n\"\n",
    "        f\"- Expected: {ART_DIR/'lc_clean_mag'}\\n\"\n",
    "        f\"- Runs available (last 15): {runs}\\n\"\n",
    "        \"Solusi: pastikan STAGE 4 benar-benar selesai dan menulis artifacts/lc_clean_mag.\"\n",
    "    )\n",
    "\n",
    "RUN_DIR, ART_DIR, LC_CLEAN_DIR = _sync_dirs_from_manifest(manifest_csv)\n",
    "\n",
    "print(\"STAGE 5 ROUTING SYNC OK\")\n",
    "print(f\"- RUN_DIR      : {RUN_DIR}\")\n",
    "print(f\"- ART_DIR      : {ART_DIR}\")\n",
    "print(f\"- LC_CLEAN_DIR : {LC_CLEAN_DIR}\")\n",
    "print(f\"- manifest_csv : {manifest_csv}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load & validate Stage4 manifest\n",
    "# ----------------------------\n",
    "_df_clean_manifest = pd.read_csv(manifest_csv)\n",
    "_df_clean_manifest.columns = [c.strip() for c in _df_clean_manifest.columns]\n",
    "\n",
    "need_cols = {\"split\", \"which\", \"part\", \"path\"}\n",
    "miss = sorted(list(need_cols - set(_df_clean_manifest.columns)))\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Manifest STAGE 4 missing columns: {miss} | cols={list(_df_clean_manifest.columns)}\")\n",
    "\n",
    "# validate part files exist\n",
    "paths = _df_clean_manifest[\"path\"].astype(str).tolist()\n",
    "missing_paths = [p for p in paths if not Path(p).exists()]\n",
    "if missing_paths:\n",
    "    ex = missing_paths[:10]\n",
    "    raise RuntimeError(\n",
    "        \"Ada file part STAGE 4 yang hilang (manifest ada tapi file tidak ada).\\n\"\n",
    "        f\"Missing count={len(missing_paths)} | contoh={ex}\\n\"\n",
    "        \"Solusi: rerun STAGE 4 dengan mode rebuild/wipe untuk regenerasi cache.\"\n",
    "    )\n",
    "\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    m = _df_clean_manifest[(_df_clean_manifest[\"split\"] == split_name) & (_df_clean_manifest[\"which\"] == which)]\n",
    "    if m.empty:\n",
    "        return []\n",
    "    return m.sort_values(\"part\")[\"path\"].astype(str).tolist()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Recover SPLIT_LIST + routing ids if missing\n",
    "# ----------------------------\n",
    "if \"SPLIT_LIST\" not in globals() or not isinstance(SPLIT_LIST, (list, tuple)) or len(SPLIT_LIST) == 0:\n",
    "    splits = sorted(set(df_train_meta[\"split\"].astype(str).tolist()) | set(df_test_meta[\"split\"].astype(str).tolist()))\n",
    "    SPLIT_LIST = splits if splits else [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "\n",
    "if \"train_ids_by_split\" not in globals() or not isinstance(train_ids_by_split, dict):\n",
    "    train_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "    for oid, sp in df_train_meta[\"split\"].items():\n",
    "        train_ids_by_split[str(sp)].append(str(oid))\n",
    "\n",
    "if \"test_ids_by_split\" not in globals() or not isinstance(test_ids_by_split, dict):\n",
    "    test_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "    for oid, sp in df_test_meta[\"split\"].items():\n",
    "        test_ids_by_split[str(sp)].append(str(oid))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Settings\n",
    "# ----------------------------\n",
    "ONLY_SPLITS = None                 # None=all\n",
    "REBUILD_MODE = \"wipe_all\"          # \"wipe_all\" or \"reuse_if_exists\"\n",
    "\n",
    "COMPRESS_NPZ = False\n",
    "SHARD_MAX_OBJECTS = 1500\n",
    "\n",
    "SNR_TANH_SCALE = 10.0\n",
    "TIME_CLIP_MAX_DAYS = None\n",
    "DROP_BAD_TIME_ROWS = True\n",
    "\n",
    "L_MAX = int(CFG.get(\"L_MAX\", 256)) if \"CFG\" in globals() else 256\n",
    "TRUNC_POLICY = str(CFG.get(\"TRUNC_POLICY\", \"smart\")) if \"CFG\" in globals() else \"smart\"  # smart/head/none\n",
    "KEEP_DET_FRAC = float(CFG.get(\"KEEP_DET_FRAC\", 0.70)) if \"CFG\" in globals() else 0.70\n",
    "KEEP_EDGE = True\n",
    "USE_RESTFRAME_TIME = bool(CFG.get(\"USE_RESTFRAME_TIME\", True)) if \"CFG\" in globals() else True\n",
    "\n",
    "NUM_BUCKETS = 64\n",
    "\n",
    "SEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOKEN_MODE = None\n",
    "FEATURE_NAMES = None\n",
    "FEATURE_DIM = None\n",
    "\n",
    "BASE_COLS = {\"object_id\", \"mjd\", \"band_id\", \"snr\", \"detected\"}\n",
    "MODE_COLS = {\"mag\": {\"mag\", \"mag_err\"}, \"asinh\": {\"flux_asinh\", \"err_log1p\"}}\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Reader for cleaned parts\n",
    "# ----------------------------\n",
    "def _read_clean_part(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Clean part missing: {p}\")\n",
    "\n",
    "    if p.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    elif p.name.endswith(\".csv.gz\"):\n",
    "        df = pd.read_csv(p, compression=\"gzip\")\n",
    "    else:\n",
    "        df = pd.read_csv(p)\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    global TOKEN_MODE, FEATURE_NAMES, FEATURE_DIM\n",
    "    if TOKEN_MODE is None:\n",
    "        cols = set(df.columns)\n",
    "        if BASE_COLS.issubset(cols) and MODE_COLS[\"mag\"].issubset(cols):\n",
    "            TOKEN_MODE = \"mag\"\n",
    "            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"mag\", \"mag_err_log\", \"snr_tanh\", \"detected\"]\n",
    "        elif BASE_COLS.issubset(cols) and MODE_COLS[\"asinh\"].issubset(cols):\n",
    "            TOKEN_MODE = \"asinh\"\n",
    "            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot detect cleaned schema.\\n\"\n",
    "                f\"Found cols={list(df.columns)}\\n\"\n",
    "                \"Expected MAG or ASINH schema from STAGE 4.\"\n",
    "            )\n",
    "        FEATURE_DIM = len(FEATURE_NAMES)\n",
    "\n",
    "    req = set(BASE_COLS) | set(MODE_COLS[TOKEN_MODE])\n",
    "    miss = sorted(list(req - set(df.columns)))\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"Clean part missing columns: {miss} | file={p}\")\n",
    "\n",
    "    df[\"object_id\"] = _safe_string_series(df[\"object_id\"])\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n",
    "    df[\"snr\"] = pd.to_numeric(df[\"snr\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n",
    "\n",
    "    if TOKEN_MODE == \"mag\":\n",
    "        df[\"mag\"] = pd.to_numeric(df[\"mag\"], errors=\"coerce\").astype(np.float32)\n",
    "        df[\"mag_err\"] = pd.to_numeric(df[\"mag_err\"], errors=\"coerce\").astype(np.float32)\n",
    "    else:\n",
    "        df[\"flux_asinh\"] = pd.to_numeric(df[\"flux_asinh\"], errors=\"coerce\").astype(np.float32)\n",
    "        df[\"err_log1p\"] = pd.to_numeric(df[\"err_log1p\"], errors=\"coerce\").astype(np.float32)\n",
    "\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        df = df[np.isfinite(df[\"mjd\"].to_numpy())]\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Truncation\n",
    "# ----------------------------\n",
    "def _smart_truncate(mjd, det, snr, Lmax: int):\n",
    "    n = len(mjd)\n",
    "    if n <= Lmax:\n",
    "        return np.arange(n, dtype=np.int64)\n",
    "\n",
    "    idx_all = np.arange(n, dtype=np.int64)\n",
    "    keep = set()\n",
    "    if KEEP_EDGE:\n",
    "        keep.add(0); keep.add(n - 1)\n",
    "\n",
    "    det_idx = idx_all[det.astype(bool)]\n",
    "    k_det = int(max(0, min(len(det_idx), int(np.floor(Lmax * KEEP_DET_FRAC)))))\n",
    "    if k_det > 0 and len(det_idx) > 0:\n",
    "        score = np.abs(snr[det_idx])\n",
    "        top = det_idx[np.argsort(-score)[:k_det]]\n",
    "        for i in top.tolist():\n",
    "            keep.add(int(i))\n",
    "\n",
    "    if len(keep) < Lmax:\n",
    "        rem = [i for i in idx_all.tolist() if i not in keep]\n",
    "        need = Lmax - len(keep)\n",
    "        if rem and need > 0:\n",
    "            pick = np.linspace(0, len(rem) - 1, num=need, dtype=int)\n",
    "            for p in pick.tolist():\n",
    "                keep.add(int(rem[p]))\n",
    "\n",
    "    out = np.array(sorted(keep), dtype=np.int64)\n",
    "    if len(out) > Lmax:\n",
    "        pos = np.linspace(0, len(out) - 1, num=Lmax, dtype=int)\n",
    "        out = out[pos]\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Build tokens per object\n",
    "# ----------------------------\n",
    "def build_object_tokens(df_obj: pd.DataFrame, z_val: float = 0.0):\n",
    "    if df_obj is None or df_obj.empty:\n",
    "        return None, None, 0, 0\n",
    "\n",
    "    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n",
    "    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "    order = np.lexsort((band, mjd))\n",
    "    mjd = mjd[order]; band = band[order]; snr = snr[order]; det = det[order]\n",
    "\n",
    "    z = float(z_val) if (z_val is not None and np.isfinite(z_val)) else 0.0\n",
    "    denom = (1.0 + max(z, 0.0)) if USE_RESTFRAME_TIME else 1.0\n",
    "\n",
    "    t0 = mjd[0]\n",
    "    t_rel = (mjd - t0) / np.float32(denom)\n",
    "    dt = np.empty_like(t_rel); dt[0] = 0.0\n",
    "    if len(t_rel) > 1:\n",
    "        dt[1:] = np.maximum(t_rel[1:] - t_rel[:-1], 0.0)\n",
    "\n",
    "    if TIME_CLIP_MAX_DAYS is not None:\n",
    "        mx = np.float32(TIME_CLIP_MAX_DAYS)\n",
    "        t_rel = np.clip(t_rel, 0.0, mx)\n",
    "        dt    = np.clip(dt,    0.0, mx)\n",
    "\n",
    "    t_rel_log = np.log1p(t_rel).astype(np.float32)\n",
    "    dt_log    = np.log1p(dt).astype(np.float32)\n",
    "\n",
    "    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n",
    "    det_f = det.astype(np.float32)\n",
    "\n",
    "    if TOKEN_MODE == \"mag\":\n",
    "        mag = df_obj[\"mag\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        mag_err = df_obj[\"mag_err\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        mag = np.nan_to_num(mag, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        mag_err = np.nan_to_num(mag_err, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        mag_err = np.maximum(mag_err, np.float32(0.0))\n",
    "        mag_err_log = np.log1p(mag_err).astype(np.float32)\n",
    "        X = np.stack([t_rel_log, dt_log, mag, mag_err_log, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "    else:\n",
    "        flux = df_obj[\"flux_asinh\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        elog = df_obj[\"err_log1p\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        flux = np.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        elog = np.nan_to_num(elog, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        X = np.stack([t_rel_log, dt_log, flux, elog, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "\n",
    "    L0 = int(X.shape[0])\n",
    "    if L_MAX and int(L_MAX) > 0 and X.shape[0] > int(L_MAX):\n",
    "        if TRUNC_POLICY == \"smart\":\n",
    "            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n",
    "        elif TRUNC_POLICY == \"head\":\n",
    "            keep = np.arange(int(L_MAX), dtype=np.int64)\n",
    "        else:  # \"none\"\n",
    "            keep = np.arange(X.shape[0], dtype=np.int64)\n",
    "\n",
    "        if len(keep) != X.shape[0]:\n",
    "            X = X[keep]\n",
    "            band = band[keep]\n",
    "\n",
    "            # recompute dt_log (stabil)\n",
    "            sel_mjd = mjd[keep]\n",
    "            sel_t = (sel_mjd - sel_mjd[0]) / np.float32(denom)\n",
    "            sel_dt = np.empty_like(sel_t); sel_dt[0] = 0.0\n",
    "            if len(sel_t) > 1:\n",
    "                sel_dt[1:] = np.maximum(sel_t[1:] - sel_t[:-1], 0.0)\n",
    "            X[:, 1] = np.log1p(sel_dt).astype(np.float32)\n",
    "\n",
    "    return X, band.astype(np.int8), L0, int(X.shape[0])\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Shard writer\n",
    "# ----------------------------\n",
    "def save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    obj_arr = np.asarray(object_ids, dtype=\"S\")  # bytes\n",
    "    if COMPRESS_NPZ:\n",
    "        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "    else:\n",
    "        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Robust builder: bucketize -> groupby object -> shard\n",
    "# ----------------------------\n",
    "def build_sequences_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 64):\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"pyarrow tidak tersedia. Di Kaggle biasanya ada.\") from e\n",
    "\n",
    "    parts = get_clean_parts(split_name, which)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"Tidak ada cleaned parts untuk {split_name}/{which}. Cek STAGE 4 output.\")\n",
    "\n",
    "    tmp_dir = Path(ART_DIR) / \"tmp_seq_buckets\" / split_name / which\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    writers = {}\n",
    "\n",
    "    def bucket_idx(series_objid: pd.Series) -> np.ndarray:\n",
    "        h = pd.util.hash_pandas_object(series_objid, index=False).to_numpy(dtype=np.uint64, copy=False)\n",
    "        return (h % np.uint64(num_buckets)).astype(np.int16)\n",
    "\n",
    "    kept_rows = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for p in parts:\n",
    "        df = _read_clean_part(p)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df = df[df[\"object_id\"].isin(expected_ids)]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        kept_rows += int(len(df))\n",
    "        bidx = bucket_idx(df[\"object_id\"])\n",
    "        df[\"_b\"] = bidx\n",
    "\n",
    "        for b in np.unique(bidx):\n",
    "            sub = df[df[\"_b\"] == b].drop(columns=[\"_b\"])\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            fp = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n",
    "            table = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "            if int(b) not in writers:\n",
    "                writers[int(b)] = pq.ParquetWriter(fp, table.schema, compression=\"snappy\")\n",
    "            writers[int(b)].write_table(table)\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()\n",
    "\n",
    "    meta = df_train_meta if which == \"train\" else df_test_meta\n",
    "\n",
    "    manifest_rows = []\n",
    "    shard_idx = 0\n",
    "    batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n",
    "    built_ids = set()\n",
    "\n",
    "    len_before, len_after = [], []\n",
    "\n",
    "    def flush_shard_local():\n",
    "        nonlocal shard_idx, batch_obj_ids, batch_X, batch_B, batch_len, manifest_rows\n",
    "        if not batch_obj_ids:\n",
    "            return\n",
    "        lengths = np.asarray(batch_len, dtype=np.int64)\n",
    "        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(lengths)\n",
    "\n",
    "        Xc = np.concatenate(batch_X, axis=0).astype(np.float32)\n",
    "        Bc = np.concatenate(batch_B, axis=0).astype(np.int8)\n",
    "\n",
    "        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n",
    "        save_shard(shard_path, batch_obj_ids, Xc, Bc, offsets)\n",
    "\n",
    "        for i, oid in enumerate(batch_obj_ids):\n",
    "            manifest_rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(shard_path),\n",
    "                \"start\": int(offsets[i]),\n",
    "                \"length\": int(lengths[i]),\n",
    "            })\n",
    "\n",
    "        shard_idx += 1\n",
    "        batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n",
    "        gc.collect()\n",
    "\n",
    "    for bf in sorted(tmp_dir.glob(\"bucket_*.parquet\")):\n",
    "        dfb = pd.read_parquet(bf)\n",
    "        if dfb.empty:\n",
    "            try: bf.unlink()\n",
    "            except Exception: pass\n",
    "            continue\n",
    "\n",
    "        # group-by object_id\n",
    "        for oid, g in dfb.groupby(\"object_id\", sort=False):\n",
    "            oid = str(oid)\n",
    "            if oid in built_ids:\n",
    "                continue\n",
    "\n",
    "            z_val = float(meta.loc[oid, \"Z\"]) if (USE_RESTFRAME_TIME and oid in meta.index) else 0.0\n",
    "\n",
    "            L0 = int(len(g))\n",
    "            X, B, lb, la = build_object_tokens(g, z_val=z_val)\n",
    "            if X is None:\n",
    "                continue\n",
    "\n",
    "            len_before.append(lb)\n",
    "            len_after.append(la)\n",
    "\n",
    "            batch_obj_ids.append(oid)\n",
    "            batch_X.append(X)\n",
    "            batch_B.append(B)\n",
    "            batch_len.append(X.shape[0])\n",
    "            built_ids.add(oid)\n",
    "\n",
    "            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                flush_shard_local()\n",
    "\n",
    "        try: bf.unlink()\n",
    "        except Exception: pass\n",
    "        del dfb\n",
    "        gc.collect()\n",
    "\n",
    "    flush_shard_local()\n",
    "\n",
    "    try: tmp_dir.rmdir()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    st = {\n",
    "        \"kept_rows\": int(kept_rows),\n",
    "        \"built_objects\": int(len(built_ids)),\n",
    "        \"len_before_mean\": float(np.mean(len_before)) if len_before else 0.0,\n",
    "        \"len_before_p95\": float(np.quantile(len_before, 0.95)) if len_before else 0.0,\n",
    "        \"len_after_mean\": float(np.mean(len_after)) if len_after else 0.0,\n",
    "        \"len_after_p95\": float(np.quantile(len_after, 0.95)) if len_after else 0.0,\n",
    "        \"truncated_frac\": float(np.mean([a < b for a, b in zip(len_after, len_before)])) if len_before else 0.0,\n",
    "        \"time_s\": float(time.time() - t0),\n",
    "    }\n",
    "    return manifest_rows, st\n",
    "\n",
    "# ----------------------------\n",
    "# 11) RUN\n",
    "# ----------------------------\n",
    "splits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\n",
    "all_manifest_train, all_manifest_test, split_run_stats = [], [], []\n",
    "\n",
    "def expected_set_for(split_name: str, which: str) -> set:\n",
    "    return set(train_ids_by_split.get(split_name, [])) if which == \"train\" else set(test_ids_by_split.get(split_name, []))\n",
    "\n",
    "for split_name in splits_to_run:\n",
    "    for which in [\"train\", \"test\"]:\n",
    "        out_dir = SEQ_DIR / split_name / which\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        expected_ids = expected_set_for(split_name, which)\n",
    "        if len(expected_ids) == 0:\n",
    "            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}.\")\n",
    "\n",
    "        # rebuild handling\n",
    "        shard_exists = any(out_dir.glob(\"shard_*.npz\"))\n",
    "        if REBUILD_MODE == \"reuse_if_exists\" and shard_exists:\n",
    "            print(f\"\\n[Stage 5] SKIP (exists): {split_name}/{which}\")\n",
    "            continue\n",
    "        else:\n",
    "            for f in out_dir.glob(\"shard_*.npz\"):\n",
    "                try: f.unlink()\n",
    "                except Exception: pass\n",
    "\n",
    "        print(f\"\\n[Stage 5] {split_name}/{which} | expected={len(expected_ids):,} | L_MAX={L_MAX} | TRUNC={TRUNC_POLICY}\")\n",
    "\n",
    "        manifest_rows, st = build_sequences_bucket(\n",
    "            split_name=split_name,\n",
    "            which=which,\n",
    "            expected_ids=expected_ids,\n",
    "            out_dir=out_dir,\n",
    "            num_buckets=NUM_BUCKETS\n",
    "        )\n",
    "\n",
    "        built = st[\"built_objects\"]\n",
    "        if built != len(expected_ids):\n",
    "            missing = len(expected_ids) - built\n",
    "            raise RuntimeError(\n",
    "                f\"[Stage 5] Mismatch {split_name}/{which}: built={built:,} expected={len(expected_ids):,} missing={missing:,}\\n\"\n",
    "                \"Jika mismatch terjadi, biasanya ada object_id yang tidak muncul di cleaned parts (STAGE 4) atau file part korup.\"\n",
    "            )\n",
    "\n",
    "        print(f\"[Stage 5] OK: built={built:,} | kept_rows={st['kept_rows']:,} | \"\n",
    "              f\"len_mean {st['len_before_mean']:.1f}->{st['len_after_mean']:.1f} | \"\n",
    "              f\"p95 {st['len_before_p95']:.1f}->{st['len_after_p95']:.1f} | \"\n",
    "              f\"trunc%={st['truncated_frac']*100:.1f}% | \"\n",
    "              f\"time={st['time_s']:.2f}s | mode={TOKEN_MODE}\")\n",
    "\n",
    "        split_run_stats.append({\"split\": split_name, \"which\": which, **st})\n",
    "\n",
    "        if which == \"train\":\n",
    "            all_manifest_train.extend(manifest_rows)\n",
    "        else:\n",
    "            all_manifest_test.extend(manifest_rows)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Save manifests + stats + config\n",
    "# ----------------------------\n",
    "df_m_train = pd.DataFrame(all_manifest_train).sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n",
    "df_m_test  = pd.DataFrame(all_manifest_test).sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n",
    "\n",
    "mtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\n",
    "mtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\n",
    "df_m_train.to_csv(mtrain_path, index=False)\n",
    "df_m_test.to_csv(mtest_path, index=False)\n",
    "\n",
    "df_stats = pd.DataFrame(split_run_stats)\n",
    "stats_path = SEQ_DIR / \"seq_build_stats.csv\"\n",
    "df_stats.to_csv(stats_path, index=False)\n",
    "\n",
    "cfg = {\n",
    "    \"token_mode\": TOKEN_MODE,\n",
    "    \"feature_names\": FEATURE_NAMES,\n",
    "    \"feature_dim\": int(FEATURE_DIM),\n",
    "    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n",
    "    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n",
    "    \"compress_npz\": bool(COMPRESS_NPZ),\n",
    "    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n",
    "    \"num_buckets\": int(NUM_BUCKETS),\n",
    "    \"L_MAX\": int(L_MAX),\n",
    "    \"TRUNC_POLICY\": str(TRUNC_POLICY),\n",
    "    \"KEEP_DET_FRAC\": float(KEEP_DET_FRAC),\n",
    "    \"USE_RESTFRAME_TIME\": bool(USE_RESTFRAME_TIME),\n",
    "    \"REBUILD_MODE\": str(REBUILD_MODE),\n",
    "    \"RUN_DIR_USED\": str(RUN_DIR),\n",
    "    \"ART_DIR_USED\": str(ART_DIR),\n",
    "    \"LC_CLEAN_DIR_USED\": str(LC_CLEAN_DIR),\n",
    "    \"manifest_csv\": str(manifest_csv),\n",
    "}\n",
    "cfg_path = SEQ_DIR / \"seq_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 5] DONE\")\n",
    "print(f\"- token_mode : {TOKEN_MODE}\")\n",
    "print(f\"- features   : {FEATURE_NAMES}\")\n",
    "print(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\n",
    "print(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\n",
    "print(f\"- Saved: {stats_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Smoke test\n",
    "# ----------------------------\n",
    "def load_sequence(object_id: str, which: str):\n",
    "    object_id = str(object_id).strip()\n",
    "    m = df_m_train if which == \"train\" else df_m_test\n",
    "    row = m[m[\"object_id\"] == object_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n",
    "    r = row.iloc[0]\n",
    "    data = np.load(r[\"shard\"], allow_pickle=False)\n",
    "    start = int(r[\"start\"]); length = int(r[\"length\"])\n",
    "    X = data[\"x\"][start:start+length]\n",
    "    B = data[\"band\"][start:start+length]\n",
    "    return X, B\n",
    "\n",
    "_smoke_oid = str(df_train_meta.index[0])\n",
    "X_sm, B_sm = load_sequence(_smoke_oid, \"train\")\n",
    "print(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\n",
    "print(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Export globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"SEQ_DIR\": SEQ_DIR,\n",
    "    \"seq_manifest_train\": df_m_train,\n",
    "    \"seq_manifest_test\": df_m_test,\n",
    "    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n",
    "    \"SEQ_FEATURE_DIM\": int(FEATURE_DIM),\n",
    "    \"SEQ_TOKEN_MODE\": TOKEN_MODE,\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "    \"load_sequence\": load_sequence,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b06a4",
   "metadata": {
    "papermill": {
     "duration": 0.013551,
     "end_time": "2026-01-02T18:47:47.726106",
     "exception": false,
     "start_time": "2026-01-02T18:47:47.712555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Length Policy (Padding, Truncation, Windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c5ae1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:47:47.756089Z",
     "iopub.status.busy": "2026-01-02T18:47:47.755710Z",
     "iopub.status.idle": "2026-01-02T18:47:48.884609Z",
     "shell.execute_reply": "2026-01-02T18:47:48.883587Z"
    },
    "papermill": {
     "duration": 1.147288,
     "end_time": "2026-01-02T18:47:48.887044",
     "exception": false,
     "start_time": "2026-01-02T18:47:47.739756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 6] token_mode=mag | score_value_feat=mag | F=6\n",
      "\n",
      "TRAIN length stats\n",
      "- n_objects=3,043 | min=17 | p50=150 | p90=183 | p95=194 | p99=908 | max=1164\n",
      "\n",
      "TEST length stats\n",
      "- n_objects=7,135 | min=18 | p50=152 | p90=183 | p95=193 | p99=990 | max=1186\n",
      "\n",
      "[Stage 6] MAX_LEN=256 (based on p95=194)\n",
      "\n",
      "[Stage 6] Memmap X sizes approx: train=0.02 GB | test=0.04 GB | dtype=<class 'numpy.float32'>\n",
      "\n",
      "[Stage 6] Building fixed cache (TRAIN)...\n",
      "[Stage 6] TRAIN filled=3,043/3,043 | dup=0 | empty=0 | time=0.22s\n",
      "\n",
      "[Stage 6] Building fixed cache (TEST)...\n",
      "[Stage 6] TEST  filled=7,135/7,135 | dup=0 | empty=0 | time=0.44s\n",
      "\n",
      "[Stage 6] Sanity samples (train):\n",
      "- idx=1360 oid=gwilwileth_adel_amloth orig_len=157 kept=157 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3020 oid=vin_araf_gwador orig_len=151 kept=151 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3025 oid=ylf_alph_mindon orig_len=167 kept=167 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] Sanity samples (test):\n",
      "- idx=3191 oid=rom_bellas_lebdas orig_len=142 kept=142 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7082 oid=nim_nestad_thor orig_len=161 kept=161 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7094 oid=rach_bellas_dol orig_len=156 kept=156 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] DONE\n",
      "- FIX_DIR: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/fixed_seq\n",
      "- Saved config: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/fixed_seq/length_policy_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n",
    "# ONE CELL, Kaggle CPU-SAFE — REVISI FULL v2 (MAG/ASINH COMPAT, HARDENED)\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/fixed_seq/{train|test}_{X|B|M}.dat  (memmap)\n",
    "# - artifacts/fixed_seq/{train|test}_ids.npy\n",
    "# - artifacts/fixed_seq/train_y.npy\n",
    "# - artifacts/fixed_seq/{train|test}_origlen.npy, {train|test}_winstart.npy, {train|test}_winend.npy\n",
    "# - artifacts/fixed_seq/length_policy_config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"seq_manifest_train\", \"seq_manifest_test\", \"SEQ_FEATURE_NAMES\",\n",
    "             \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 -> 5 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "m_train = seq_manifest_train.copy()\n",
    "m_test  = seq_manifest_test.copy()\n",
    "\n",
    "SEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\n",
    "feat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Detect token_mode (MAG vs ASINH)\n",
    "# ----------------------------\n",
    "SEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    if (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"asinh\"\n",
    "    elif (\"mag\" in feat) and (\"mag_err_log\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"mag\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Cannot infer SEQ_TOKEN_MODE from SEQ_FEATURE_NAMES.\\n\"\n",
    "            f\"SEQ_FEATURE_NAMES={SEQ_FEATURE_NAMES}\\n\"\n",
    "            \"Expected either (flux_asinh, err_log1p) or (mag, mag_err_log).\"\n",
    "        )\n",
    "\n",
    "REQ_COMMON = [\"t_rel_log\", \"dt_log\", \"snr_tanh\", \"detected\"]\n",
    "for k in REQ_COMMON:\n",
    "    if k not in feat:\n",
    "        raise ValueError(f\"SEQ_FEATURE_NAMES must include '{k}'. Found: {SEQ_FEATURE_NAMES}\")\n",
    "\n",
    "if SEQ_TOKEN_MODE == \"asinh\":\n",
    "    if \"flux_asinh\" not in feat:\n",
    "        raise ValueError(\"token_mode=asinh requires 'flux_asinh'.\")\n",
    "    SCORE_VALUE_FEAT = \"flux_asinh\"\n",
    "elif SEQ_TOKEN_MODE == \"mag\":\n",
    "    if \"mag\" not in feat:\n",
    "        raise ValueError(\"token_mode=mag requires 'mag'.\")\n",
    "    SCORE_VALUE_FEAT = \"mag\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SEQ_TOKEN_MODE={SEQ_TOKEN_MODE}\")\n",
    "\n",
    "print(f\"[Stage 6] token_mode={SEQ_TOKEN_MODE} | score_value_feat={SCORE_VALUE_FEAT} | F={len(SEQ_FEATURE_NAMES)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings\n",
    "# ----------------------------\n",
    "FORCE_MAX_LEN = None         # e.g. 256 (kalau mau paksa)\n",
    "MAXLEN_CAPS = (256, 384, 512) # CPU-safe choices\n",
    "\n",
    "# Score weights (bisa kamu tweak)\n",
    "W_SNR = 1.00\n",
    "W_VAL = 0.35\n",
    "W_DET = 0.25\n",
    "\n",
    "# Padding policy\n",
    "PAD_BAND_ID = 0              # aman (tetap kompatibel kalau band 0..5)\n",
    "SHIFT_BAND_IDS = False       # True => simpan band+1 untuk token asli, pad=0 (butuh kompatibilitas downstream)\n",
    "\n",
    "# Build policy\n",
    "REBUILD_MODE = \"wipe_all\"    # \"wipe_all\" atau \"reuse_if_exists\"\n",
    "DTYPE_X = np.float32         # jangan ganti ke fp16 dulu kecuali kamu yakin\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Inspect length distribution -> choose MAX_LEN\n",
    "# ----------------------------\n",
    "def describe_lengths(m: pd.DataFrame, name: str):\n",
    "    L = m[\"length\"].to_numpy(dtype=np.int32, copy=False)\n",
    "    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n",
    "    print(f\"\\n{name} length stats\")\n",
    "    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[7])} | p95={int(q[8])} | p99={int(q[10])} | max={int(q[-1])}\")\n",
    "    return q\n",
    "\n",
    "q_tr = describe_lengths(m_train, \"TRAIN\")\n",
    "q_te = describe_lengths(m_test,  \"TEST\")\n",
    "\n",
    "p95 = int(max(q_tr[8], q_te[8]))\n",
    "if FORCE_MAX_LEN is not None:\n",
    "    MAX_LEN = int(FORCE_MAX_LEN)\n",
    "else:\n",
    "    if p95 <= 256:\n",
    "        MAX_LEN = 256\n",
    "    elif p95 <= 384:\n",
    "        MAX_LEN = 384\n",
    "    else:\n",
    "        MAX_LEN = 512\n",
    "\n",
    "if MAX_LEN not in MAXLEN_CAPS and FORCE_MAX_LEN is None:\n",
    "    # fallback to closest cap\n",
    "    MAX_LEN = int(min(MAXLEN_CAPS, key=lambda x: abs(x - MAX_LEN)))\n",
    "\n",
    "print(f\"\\n[Stage 6] MAX_LEN={MAX_LEN} (based on p95={p95})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Window scoring (adaptive)\n",
    "# ----------------------------\n",
    "def _brightness_proxy_from_mag(mag: np.ndarray) -> np.ndarray:\n",
    "    mag = np.nan_to_num(mag, nan=np.float32(0.0), posinf=np.float32(0.0), neginf=np.float32(0.0)).astype(np.float32, copy=False)\n",
    "    if mag.size == 0:\n",
    "        return np.zeros_like(mag, dtype=np.float32)\n",
    "    med = np.float32(np.median(mag))\n",
    "    br = np.maximum(med - mag, np.float32(0.0))\n",
    "    br = np.log1p(br).astype(np.float32, copy=False)\n",
    "    return br\n",
    "\n",
    "def _score_tokens(X: np.ndarray) -> np.ndarray:\n",
    "    snr = np.abs(X[:, feat[\"snr_tanh\"]]).astype(np.float32, copy=False)\n",
    "    det = X[:, feat[\"detected\"]].astype(np.float32, copy=False)\n",
    "\n",
    "    if SEQ_TOKEN_MODE == \"asinh\":\n",
    "        val = np.abs(X[:, feat[\"flux_asinh\"]]).astype(np.float32, copy=False)\n",
    "    else:\n",
    "        mag = X[:, feat[\"mag\"]].astype(np.float32, copy=False)\n",
    "        val = _brightness_proxy_from_mag(mag)\n",
    "\n",
    "    score = (np.float32(W_SNR) * snr) + (np.float32(W_VAL) * val) + (np.float32(W_DET) * det)\n",
    "    score = np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "    return score\n",
    "\n",
    "def select_best_window(score: np.ndarray, max_len: int) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Pilih window kontigu panjang max_len yang memaksimalkan sum(score).\n",
    "    O(L) pakai prefix sum.\n",
    "    \"\"\"\n",
    "    L = int(score.shape[0])\n",
    "    if L <= max_len:\n",
    "        return 0, L\n",
    "\n",
    "    # prefix sum\n",
    "    cs = np.empty(L + 1, dtype=np.float32)\n",
    "    cs[0] = 0.0\n",
    "    np.cumsum(score, out=cs[1:])\n",
    "\n",
    "    # window sums: cs[i+max_len]-cs[i], i=0..L-max_len\n",
    "    ws = cs[max_len:] - cs[:-max_len]\n",
    "    if not np.isfinite(ws).any():\n",
    "        start = (L - max_len) // 2\n",
    "    else:\n",
    "        start = int(np.argmax(ws))\n",
    "    end = start + max_len\n",
    "    return start, end\n",
    "\n",
    "def pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      Xp: (max_len, F) float32\n",
    "      Bp: (max_len,) int8\n",
    "      Mp: (max_len,) int8  (1=real token)\n",
    "      orig_len, win_start, win_end\n",
    "    \"\"\"\n",
    "    L = int(X.shape[0])\n",
    "    F = int(X.shape[1])\n",
    "\n",
    "    Xp = np.zeros((max_len, F), dtype=DTYPE_X)\n",
    "    Bp = np.full((max_len,), PAD_BAND_ID, dtype=np.int8)\n",
    "    Mp = np.zeros((max_len,), dtype=np.int8)\n",
    "\n",
    "    if L <= 0:\n",
    "        return Xp, Bp, Mp, 0, 0, 0\n",
    "\n",
    "    if L <= max_len:\n",
    "        Xw = X\n",
    "        Bw = B\n",
    "        ws, we = 0, L\n",
    "    else:\n",
    "        sc = _score_tokens(X)\n",
    "        ws, we = select_best_window(sc, max_len=max_len)\n",
    "        Xw = X[ws:we]\n",
    "        Bw = B[ws:we]\n",
    "\n",
    "    lw = int(Xw.shape[0])\n",
    "    Xp[:lw] = Xw.astype(DTYPE_X, copy=False)\n",
    "\n",
    "    if SHIFT_BAND_IDS:\n",
    "        # token band 1..K, pad=0\n",
    "        Bp[:lw] = (Bw.astype(np.int16, copy=False) + 1).astype(np.int8)\n",
    "    else:\n",
    "        Bp[:lw] = Bw.astype(np.int8, copy=False)\n",
    "\n",
    "    Mp[:lw] = 1\n",
    "    return Xp, Bp, Mp, L, int(ws), int(we)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Fixed cache builder (process per shard once)\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(ART_DIR) / \"fixed_seq\"\n",
    "FIX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# choose id ordering\n",
    "train_ids = [str(x) for x in df_train_meta.index.to_list()]\n",
    "\n",
    "# y column robust\n",
    "_y_col = None\n",
    "for cand in [\"target\", \"y\", \"label\", \"class\", \"target_id\"]:\n",
    "    if cand in df_train_meta.columns:\n",
    "        _y_col = cand\n",
    "        break\n",
    "if _y_col is None:\n",
    "    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols={list(df_train_meta.columns)[:30]}\")\n",
    "\n",
    "y_train = pd.to_numeric(df_train_meta[_y_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n",
    "\n",
    "# test ordering\n",
    "test_ids = None\n",
    "if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in df_sub.columns:\n",
    "    test_ids = df_sub[\"object_id\"].astype(str).str.strip().to_list()\n",
    "else:\n",
    "    # try sample_submission.csv from PATHS if exists\n",
    "    if \"PATHS\" in globals() and isinstance(PATHS, dict):\n",
    "        sp = PATHS.get(\"sample_submission\", None) or PATHS.get(\"sample_sub\", None)\n",
    "        if sp and Path(sp).exists():\n",
    "            _tmp = pd.read_csv(sp)\n",
    "            if \"object_id\" in _tmp.columns:\n",
    "                test_ids = _tmp[\"object_id\"].astype(str).str.strip().to_list()\n",
    "    if test_ids is None:\n",
    "        test_ids = [str(x) for x in df_test_meta.index.to_list()]\n",
    "\n",
    "# strict unique ids\n",
    "if len(set(train_ids)) != len(train_ids):\n",
    "    raise RuntimeError(\"train_ids contains duplicates. Check df_train_meta.index.\")\n",
    "if len(set(test_ids)) != len(test_ids):\n",
    "    raise RuntimeError(\"test_ids contains duplicates. Check ordering source (df_sub/sample_sub/df_test_meta).\")\n",
    "\n",
    "train_row = {oid: i for i, oid in enumerate(train_ids)}\n",
    "test_row  = {oid: i for i, oid in enumerate(test_ids)}\n",
    "\n",
    "NTR = len(train_ids)\n",
    "NTE = len(test_ids)\n",
    "F = len(SEQ_FEATURE_NAMES)\n",
    "\n",
    "# estimate disk usage\n",
    "def _gb(nbytes): return float(nbytes) / (1024**3)\n",
    "size_tr = NTR * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\n",
    "size_te = NTE * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\n",
    "print(f\"\\n[Stage 6] Memmap X sizes approx: train={_gb(size_tr):.2f} GB | test={_gb(size_te):.2f} GB | dtype={DTYPE_X}\")\n",
    "\n",
    "# memmap paths\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "test_X_path  = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path  = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path  = FIX_DIR / \"test_M.dat\"\n",
    "\n",
    "train_len_path = FIX_DIR / \"train_origlen.npy\"\n",
    "train_ws_path  = FIX_DIR / \"train_winstart.npy\"\n",
    "train_we_path  = FIX_DIR / \"train_winend.npy\"\n",
    "test_len_path  = FIX_DIR / \"test_origlen.npy\"\n",
    "test_ws_path   = FIX_DIR / \"test_winstart.npy\"\n",
    "test_we_path   = FIX_DIR / \"test_winend.npy\"\n",
    "\n",
    "# ----------------------------\n",
    "# 4b) Rebuild handling\n",
    "# ----------------------------\n",
    "def _all_exist(paths):\n",
    "    return all(Path(p).exists() for p in paths)\n",
    "\n",
    "reuse_paths = [\n",
    "    train_X_path, train_B_path, train_M_path,\n",
    "    test_X_path, test_B_path, test_M_path,\n",
    "    FIX_DIR / \"train_ids.npy\", FIX_DIR / \"test_ids.npy\", FIX_DIR / \"train_y.npy\",\n",
    "    train_len_path, train_ws_path, train_we_path,\n",
    "    test_len_path, test_ws_path, test_we_path,\n",
    "    FIX_DIR / \"length_policy_config.json\"\n",
    "]\n",
    "\n",
    "if REBUILD_MODE == \"reuse_if_exists\" and _all_exist(reuse_paths):\n",
    "    print(\"[Stage 6] REUSE (exists): fixed_seq cache already present.\")\n",
    "    globals().update({\n",
    "        \"FIX_DIR\": FIX_DIR, \"MAX_LEN\": MAX_LEN,\n",
    "        \"FIX_TRAIN_X_PATH\": train_X_path, \"FIX_TRAIN_B_PATH\": train_B_path, \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "        \"FIX_TEST_X_PATH\": test_X_path,  \"FIX_TEST_B_PATH\": test_B_path,  \"FIX_TEST_M_PATH\": test_M_path,\n",
    "        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "        \"FIX_POLICY_CFG_PATH\": FIX_DIR / \"length_policy_config.json\",\n",
    "        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n",
    "    })\n",
    "    raise SystemExit\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Create memmaps\n",
    "# ----------------------------\n",
    "Xtr = np.memmap(train_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTR, MAX_LEN, F))\n",
    "Btr = np.memmap(train_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "Mtr = np.memmap(train_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTE, MAX_LEN, F))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "\n",
    "origlen_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "winstart_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "winend_tr   = np.zeros((NTR,), dtype=np.int32)\n",
    "\n",
    "origlen_te = np.zeros((NTE,), dtype=np.int32)\n",
    "winstart_te = np.zeros((NTE,), dtype=np.int32)\n",
    "winend_te   = np.zeros((NTE,), dtype=np.int32)\n",
    "\n",
    "filled_tr = np.zeros((NTR,), dtype=np.uint8)\n",
    "filled_te = np.zeros((NTE,), dtype=np.uint8)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Fill memmaps per shard (fast path)\n",
    "# ----------------------------\n",
    "def process_manifest_into_memmap(m: pd.DataFrame, which: str):\n",
    "    if which == \"train\":\n",
    "        row_map = train_row\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        origlen, ws_arr, we_arr = origlen_tr, winstart_tr, winend_tr\n",
    "        filled_mask = filled_tr\n",
    "        expected_n = NTR\n",
    "    else:\n",
    "        row_map = test_row\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        origlen, ws_arr, we_arr = origlen_te, winstart_te, winend_te\n",
    "        filled_mask = filled_te\n",
    "        expected_n = NTE\n",
    "\n",
    "    # strict columns\n",
    "    for c in [\"object_id\", \"shard\", \"start\", \"length\"]:\n",
    "        if c not in m.columns:\n",
    "            raise RuntimeError(f\"Manifest missing column '{c}'. cols={list(m.columns)}\")\n",
    "\n",
    "    # check shard paths exist\n",
    "    shard_paths = m[\"shard\"].astype(str).unique().tolist()\n",
    "    miss_sh = [p for p in shard_paths if not Path(p).exists()]\n",
    "    if miss_sh:\n",
    "        raise RuntimeError(f\"Missing shard files ({which}): count={len(miss_sh)} | ex={miss_sh[:5]}\")\n",
    "\n",
    "    filled = 0\n",
    "    dup = 0\n",
    "    empty = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    # deterministic order: sort shard paths\n",
    "    for shard_path in sorted(shard_paths):\n",
    "        g = m[m[\"shard\"].astype(str) == shard_path]\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        data = np.load(shard_path, allow_pickle=False)\n",
    "        x_all = data[\"x\"]\n",
    "        b_all = data[\"band\"]\n",
    "\n",
    "        # map object_id -> row index in fixed order\n",
    "        oids = g[\"object_id\"].astype(str).to_numpy()\n",
    "        idxs = pd.Series(oids).map(row_map).to_numpy()  # object dtype with NaN\n",
    "\n",
    "        # filter valid idx\n",
    "        valid = np.isfinite(idxs)\n",
    "        if not valid.any():\n",
    "            del data\n",
    "            continue\n",
    "\n",
    "        idxs = idxs[valid].astype(np.int64)\n",
    "        starts = g[\"start\"].to_numpy(dtype=np.int64, copy=False)[valid]\n",
    "        lens   = g[\"length\"].to_numpy(dtype=np.int64, copy=False)[valid]\n",
    "        oids_v = oids[valid]\n",
    "\n",
    "        for oid, idx, st, ln in zip(oids_v, idxs, starts, lens):\n",
    "            if ln <= 0:\n",
    "                empty += 1\n",
    "                continue\n",
    "            if filled_mask[idx]:\n",
    "                dup += 1\n",
    "                continue\n",
    "\n",
    "            X = x_all[st:st+ln]\n",
    "            B = b_all[st:st+ln]\n",
    "            Xp, Bp, Mp, L0, ws, we = pad_to_fixed(X, B, max_len=MAX_LEN)\n",
    "\n",
    "            Xmm[idx, :, :] = Xp\n",
    "            Bmm[idx, :] = Bp\n",
    "            Mmm[idx, :] = Mp\n",
    "            origlen[idx] = int(L0)\n",
    "            ws_arr[idx] = int(ws)\n",
    "            we_arr[idx] = int(we)\n",
    "            filled_mask[idx] = 1\n",
    "            filled += 1\n",
    "\n",
    "        del data\n",
    "        if filled % 2000 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    st = {\"filled\": int(filled), \"dup_skipped\": int(dup), \"empty_len\": int(empty), \"time_s\": float(elapsed), \"expected\": int(expected_n)}\n",
    "    return st\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed cache (TRAIN)...\")\n",
    "st_tr = process_manifest_into_memmap(m_train, \"train\")\n",
    "print(f\"[Stage 6] TRAIN filled={st_tr['filled']:,}/{st_tr['expected']:,} | dup={st_tr['dup_skipped']:,} | empty={st_tr['empty_len']:,} | time={st_tr['time_s']:.2f}s\")\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed cache (TEST)...\")\n",
    "st_te = process_manifest_into_memmap(m_test, \"test\")\n",
    "print(f\"[Stage 6] TEST  filled={st_te['filled']:,}/{st_te['expected']:,} | dup={st_te['dup_skipped']:,} | empty={st_te['empty_len']:,} | time={st_te['time_s']:.2f}s\")\n",
    "\n",
    "# flush memmaps\n",
    "Xtr.flush(); Btr.flush(); Mtr.flush()\n",
    "Xte.flush(); Bte.flush(); Mte.flush()\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Hard sanity: must be 100% filled\n",
    "# ----------------------------\n",
    "miss_tr = np.where(filled_tr == 0)[0]\n",
    "miss_te = np.where(filled_te == 0)[0]\n",
    "if len(miss_tr) > 0:\n",
    "    ex = [train_ids[i] for i in miss_tr[:10]]\n",
    "    raise RuntimeError(f\"[Stage 6] TRAIN missing filled rows: {len(miss_tr):,}/{NTR:,} | ex={ex}\")\n",
    "if len(miss_te) > 0:\n",
    "    ex = [test_ids[i] for i in miss_te[:10]]\n",
    "    raise RuntimeError(f\"[Stage 6] TEST missing filled rows: {len(miss_te):,}/{NTE:,} | ex={ex}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save ids + y + meta arrays\n",
    "# ----------------------------\n",
    "np.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"train_y.npy\",   y_train)\n",
    "\n",
    "np.save(train_len_path, origlen_tr)\n",
    "np.save(train_ws_path,  winstart_tr)\n",
    "np.save(train_we_path,  winend_tr)\n",
    "\n",
    "np.save(test_len_path,  origlen_te)\n",
    "np.save(test_ws_path,   winstart_te)\n",
    "np.save(test_we_path,   winend_te)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Quick sanity samples\n",
    "# ----------------------------\n",
    "def sanity_samples(which: str, n_show: int = 3, seed: int = 2025):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if which == \"train\":\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        ids = train_ids\n",
    "        ol = origlen_tr\n",
    "    else:\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        ids = test_ids\n",
    "        ol = origlen_te\n",
    "\n",
    "    idxs = rng.choice(len(ids), size=min(n_show, len(ids)), replace=False)\n",
    "    print(f\"\\n[Stage 6] Sanity samples ({which}):\")\n",
    "    for i in idxs:\n",
    "        kept = int(Mmm[i].sum())\n",
    "        bands = sorted(set(Bmm[i, :kept].tolist())) if kept > 0 else []\n",
    "        print(f\"- idx={i} oid={ids[i]} orig_len={int(ol[i])} kept={kept} bands_unique={bands}\")\n",
    "\n",
    "sanity_samples(\"train\", 3)\n",
    "sanity_samples(\"test\", 3)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Save config\n",
    "# ----------------------------\n",
    "policy_cfg = {\n",
    "    \"token_mode\": SEQ_TOKEN_MODE,\n",
    "    \"max_len\": int(MAX_LEN),\n",
    "    \"feature_names\": list(SEQ_FEATURE_NAMES),\n",
    "    \"score_weights\": {\"W_SNR\": float(W_SNR), \"W_VAL\": float(W_VAL), \"W_DET\": float(W_DET)},\n",
    "    \"score_value_feat\": SCORE_VALUE_FEAT,\n",
    "    \"window_policy\": \"best_contiguous_window_by_max_sum(score)\",\n",
    "    \"padding\": {\"PAD_BAND_ID\": int(PAD_BAND_ID), \"SHIFT_BAND_IDS\": bool(SHIFT_BAND_IDS)},\n",
    "    \"dtype_X\": str(DTYPE_X),\n",
    "    \"order\": {\n",
    "        \"train\": \"df_train_meta.index\",\n",
    "        \"test\": (\"df_sub.object_id\" if (\"df_sub\" in globals() and isinstance(df_sub, pd.DataFrame) and \"object_id\" in df_sub.columns) else \"df_test_meta.index / sample_submission fallback\"),\n",
    "        \"y_col\": str(_y_col),\n",
    "    },\n",
    "    \"stats\": {\"train\": st_tr, \"test\": st_te},\n",
    "    \"files\": {\n",
    "        \"train_X\": str(train_X_path), \"train_B\": str(train_B_path), \"train_M\": str(train_M_path),\n",
    "        \"test_X\": str(test_X_path),   \"test_B\": str(test_B_path),   \"test_M\": str(test_M_path),\n",
    "        \"train_y\": str(FIX_DIR / \"train_y.npy\"),\n",
    "        \"train_ids\": str(FIX_DIR / \"train_ids.npy\"),\n",
    "        \"test_ids\": str(FIX_DIR / \"test_ids.npy\"),\n",
    "        \"train_origlen\": str(train_len_path), \"train_winstart\": str(train_ws_path), \"train_winend\": str(train_we_path),\n",
    "        \"test_origlen\": str(test_len_path),   \"test_winstart\": str(test_ws_path),   \"test_winend\": str(test_we_path),\n",
    "    }\n",
    "}\n",
    "cfg_path = FIX_DIR / \"length_policy_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(policy_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 6] DONE\")\n",
    "print(f\"- FIX_DIR: {FIX_DIR}\")\n",
    "print(f\"- Saved config: {cfg_path}\")\n",
    "\n",
    "# Export globals\n",
    "globals().update({\n",
    "    \"FIX_DIR\": FIX_DIR,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"FIX_TRAIN_X_PATH\": train_X_path,\n",
    "    \"FIX_TRAIN_B_PATH\": train_B_path,\n",
    "    \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "    \"FIX_TEST_X_PATH\": test_X_path,\n",
    "    \"FIX_TEST_B_PATH\": test_B_path,\n",
    "    \"FIX_TEST_M_PATH\": test_M_path,\n",
    "    \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "    \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "    \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "    \"FIX_POLICY_CFG_PATH\": cfg_path,\n",
    "    \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d19368",
   "metadata": {
    "papermill": {
     "duration": 0.015189,
     "end_time": "2026-01-02T18:47:48.916400",
     "exception": false,
     "start_time": "2026-01-02T18:47:48.901211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Split (Object-Level, Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb91409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:47:48.947037Z",
     "iopub.status.busy": "2026-01-02T18:47:48.946670Z",
     "iopub.status.idle": "2026-01-02T18:47:51.061522Z",
     "shell.execute_reply": "2026-01-02T18:47:51.060583Z"
    },
    "papermill": {
     "duration": 2.132798,
     "end_time": "2026-01-02T18:47:51.063476",
     "exception": false,
     "start_time": "2026-01-02T18:47:48.930678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 7] seed=2025 | default_splits=5 | MIN_POS_PER_FOLD=3 | enforce_minpos=True | group_by_split=False | fallback_group=True\n",
      "[Stage 7] Initial n_splits candidate=5 | N=3,043 pos=148 neg=2,895 pos%=4.863621% | order_source=/kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/fixed_seq/train_ids.npy\n",
      "[Stage 7] FINAL: n_splits=5 | cv_type=StratifiedKFold | min_pos_in_fold=29\n",
      "\n",
      "[Stage 7] CV split OK\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/cv/cv_folds.csv\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/cv/cv_folds.npz\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/cv/cv_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/cv/cv_config.json\n",
      "Order source: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/fixed_seq/train_ids.npy\n",
      "Target column: target\n",
      "Total: N=3043 | pos=148 | neg=2895 | pos%=4.863621%\n",
      "Per-fold distribution (val):\n",
      "- fold 0: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 1: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 2: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 3: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n",
      "- fold 4: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL v2\n",
    "#\n",
    "# Tujuan:\n",
    "# - Buat split CV di level object_id (bukan per baris lightcurve)\n",
    "# - Konsisten dengan urutan TRAIN yang dipakai di STAGE 6 (fixed_seq/train_ids.npy)\n",
    "#\n",
    "# Upgrade v2:\n",
    "# - Auto-find FIX_DIR/train_ids.npy (kalau FIX_DIR tidak ada tapi folder fixed_seq ada di ART_DIR)\n",
    "# - Robust decode bytes->str + strip\n",
    "# - Target column robust (auto-detect: target/y/label/class)\n",
    "# - Group column robust (auto-detect: split/split_id/split_name) jika USE_GROUP_BY_SPLIT=True\n",
    "# - n_splits adaptif + ENFORCE_MIN_POS_PER_FOLD (turun otomatis sampai valid)\n",
    "# - Fallback otomatis: kalau StratifiedGroupKFold gagal/ tidak tersedia -> StratifiedKFold (opsional)\n",
    "# - Validasi keras: missing ids, duplikasi, fold tanpa kelas, fold_assign -1\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/cv/cv_folds.csv\n",
    "# - artifacts/cv/cv_folds.npz   (train_idx_f + val_idx_f)\n",
    "# - artifacts/cv/cv_report.txt\n",
    "# - artifacts/cv/cv_config.json\n",
    "# - globals: fold_assign, folds, n_splits, CV_DIR\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, os, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "for need in [\"df_train_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 2 dulu (df_train_meta & ART_DIR).\")\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CV Settings\n",
    "# ----------------------------\n",
    "DEFAULT_SPLITS = 5\n",
    "FORCE_N_SPLITS = None              # set int kalau mau paksa (mis. 3), else None\n",
    "MIN_POS_PER_FOLD = 3               # stabilitas; 3–10 umum\n",
    "ENFORCE_MIN_POS_PER_FOLD = True    # kalau True: n_splits otomatis diturunkan sampai min_pos>=MIN_POS_PER_FOLD (atau minimal valid 1)\n",
    "USE_GROUP_BY_SPLIT = False         # True => prefer StratifiedGroupKFold (groups=df_train_meta[\"split\"])\n",
    "AUTO_FALLBACK_GROUP = True         # True => kalau group-cv tidak bisa, fallback ke StratifiedKFold\n",
    "\n",
    "print(f\"[Stage 7] seed={SEED} | default_splits={DEFAULT_SPLITS} | MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} \"\n",
    "      f\"| enforce_minpos={ENFORCE_MIN_POS_PER_FOLD} | group_by_split={USE_GROUP_BY_SPLIT} | fallback_group={AUTO_FALLBACK_GROUP}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Determine train_ids ordering (prefer fixed cache from STAGE 6)\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "order_source = \"df_train_meta.index\"\n",
    "\n",
    "def _decode_ids(arr) -> list:\n",
    "    out = []\n",
    "    for x in arr.tolist():\n",
    "        if isinstance(x, (bytes, bytearray)):\n",
    "            s = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            s = str(x)\n",
    "        out.append(s.strip())\n",
    "    return out\n",
    "\n",
    "# try: FIX_DIR/train_ids.npy\n",
    "p_ids = None\n",
    "if \"FIX_DIR\" in globals():\n",
    "    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        p_ids = p\n",
    "\n",
    "# try: ART_DIR/fixed_seq/train_ids.npy\n",
    "if p_ids is None:\n",
    "    p = ART_DIR / \"fixed_seq\" / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        p_ids = p\n",
    "\n",
    "if p_ids is not None:\n",
    "    raw = np.load(p_ids, allow_pickle=False)\n",
    "    train_ids = _decode_ids(raw)\n",
    "    order_source = str(p_ids)\n",
    "\n",
    "if train_ids is None:\n",
    "    # fallback\n",
    "    train_ids = [str(x).strip() for x in df_train_meta.index.astype(str).tolist()]\n",
    "    order_source = \"df_train_meta.index\"\n",
    "\n",
    "# uniqueness check\n",
    "if len(train_ids) != len(set(train_ids)):\n",
    "    s = pd.Series(train_ids)\n",
    "    dup = s[s.duplicated()].iloc[:10].tolist()\n",
    "    raise RuntimeError(f\"[Stage 7] train_ids has duplicates (examples): {dup}\")\n",
    "\n",
    "# ensure all ids exist in df_train_meta\n",
    "missing_in_meta = [oid for oid in train_ids if oid not in df_train_meta.index]\n",
    "if missing_in_meta:\n",
    "    raise RuntimeError(f\"[Stage 7] Some train_ids not found in df_train_meta (examples): {missing_in_meta[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Robust target column\n",
    "# ----------------------------\n",
    "target_col = None\n",
    "for cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\"]:\n",
    "    if cand in df_train_meta.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise RuntimeError(f\"[Stage 7] Cannot find target column in df_train_meta. cols(sample)={list(df_train_meta.columns)[:30]}\")\n",
    "\n",
    "y = pd.to_numeric(df_train_meta.loc[train_ids, target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n",
    "# force to 0/1 if it looks like boolean-ish\n",
    "y = (y > 0).astype(np.int8)\n",
    "\n",
    "N = len(train_ids)\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "if pos == 0 or neg == 0:\n",
    "    raise RuntimeError(f\"[Stage 7] Invalid class distribution: pos={pos}, neg={neg}. Cannot do stratified CV.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Optional groups (by split)\n",
    "# ----------------------------\n",
    "groups = None\n",
    "group_col = None\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    # robust group column detection\n",
    "    for cand in [\"split\", \"split_id\", \"split_name\", \"split_idx\"]:\n",
    "        if cand in df_train_meta.columns:\n",
    "            group_col = cand\n",
    "            break\n",
    "    if group_col is None:\n",
    "        if not AUTO_FALLBACK_GROUP:\n",
    "            raise RuntimeError(\"[Stage 7] USE_GROUP_BY_SPLIT=True but no split column found in df_train_meta.\")\n",
    "        print(\"[Stage 7] WARN: split column not found; fallback to StratifiedKFold.\")\n",
    "        USE_GROUP_BY_SPLIT = False\n",
    "    else:\n",
    "        groups = df_train_meta.loc[train_ids, group_col].astype(str).to_numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Choose n_splits safely + auto-adjust\n",
    "# ----------------------------\n",
    "max_splits_by_pos = pos\n",
    "max_splits_by_neg = neg\n",
    "max_splits_by_minpos = max(1, pos // max(int(MIN_POS_PER_FOLD), 1))\n",
    "\n",
    "n0 = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg, max_splits_by_minpos)\n",
    "if FORCE_N_SPLITS is not None:\n",
    "    n0 = int(FORCE_N_SPLITS)\n",
    "\n",
    "if n0 < 2:\n",
    "    raise RuntimeError(\n",
    "        f\"[Stage 7] Too few samples for CV. pos={pos}, neg={neg}, MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} => n_splits={n0}. \"\n",
    "        \"Turunkan MIN_POS_PER_FOLD atau pakai holdout.\"\n",
    "    )\n",
    "\n",
    "print(f\"[Stage 7] Initial n_splits candidate={n0} | N={N:,} pos={pos:,} neg={neg:,} pos%={pos/max(N,1)*100:.6f}% | order_source={order_source}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Build folds (sklearn) with robust fallback\n",
    "# ----------------------------\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedGroupKFold\n",
    "    except Exception:\n",
    "        StratifiedGroupKFold = None\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn is not available in this environment.\") from e\n",
    "\n",
    "def _try_split(k: int, use_group: bool):\n",
    "    \"\"\"Return (ok, cv_type, fold_assign, folds, per_fold_posneg)\"\"\"\n",
    "    fold_assign = np.full(N, -1, dtype=np.int16)\n",
    "    folds = []\n",
    "    per = []\n",
    "\n",
    "    if use_group:\n",
    "        if StratifiedGroupKFold is None:\n",
    "            return (False, \"StratifiedGroupKFold(unavailable)\", None, None, None)\n",
    "        splitter = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "        split_iter = splitter.split(np.zeros(N), y, groups=groups)\n",
    "        cv_type = f\"StratifiedGroupKFold({group_col})\"\n",
    "    else:\n",
    "        splitter = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "        split_iter = splitter.split(np.zeros(N), y)\n",
    "        cv_type = \"StratifiedKFold\"\n",
    "\n",
    "    try:\n",
    "        for fold, (tr_idx, val_idx) in enumerate(split_iter):\n",
    "            fold_assign[val_idx] = fold\n",
    "            yf = y[val_idx]\n",
    "            pf = int((yf == 1).sum())\n",
    "            nf = int((yf == 0).sum())\n",
    "            per.append((len(val_idx), pf, nf))\n",
    "            folds.append({\n",
    "                \"fold\": int(fold),\n",
    "                \"train_idx\": tr_idx.astype(np.int32),\n",
    "                \"val_idx\": val_idx.astype(np.int32),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        return (False, f\"{cv_type} (error: {type(e).__name__})\", None, None, None)\n",
    "\n",
    "    if (fold_assign < 0).any():\n",
    "        return (False, f\"{cv_type} (unassigned)\", None, None, None)\n",
    "\n",
    "    # hard check: each fold must have pos>=1 and neg>=1\n",
    "    for (_, pf, nf) in per:\n",
    "        if pf == 0 or nf == 0:\n",
    "            return (False, f\"{cv_type} (empty class in fold)\", None, None, None)\n",
    "\n",
    "    return (True, cv_type, fold_assign, folds, per)\n",
    "\n",
    "# attempt with decreasing k\n",
    "best = None\n",
    "used_group = bool(USE_GROUP_BY_SPLIT)\n",
    "\n",
    "for k in range(n0, 1, -1):\n",
    "    ok, cv_type, fa, folds, per = _try_split(k, use_group=used_group)\n",
    "    if not ok and used_group and AUTO_FALLBACK_GROUP:\n",
    "        # try fallback non-group for same k\n",
    "        ok2, cv_type2, fa2, folds2, per2 = _try_split(k, use_group=False)\n",
    "        if ok2:\n",
    "            ok, cv_type, fa, folds, per = ok2, cv_type2, fa2, folds2, per2\n",
    "            used_group = False\n",
    "\n",
    "    if not ok:\n",
    "        continue\n",
    "\n",
    "    min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n",
    "    if ENFORCE_MIN_POS_PER_FOLD and (min_pos_seen < MIN_POS_PER_FOLD) and (FORCE_N_SPLITS is None):\n",
    "        # k too big for minpos requirement, keep searching smaller k\n",
    "        continue\n",
    "\n",
    "    best = (k, cv_type, fa, folds, per, min_pos_seen)\n",
    "    break\n",
    "\n",
    "# if enforce failed completely, pick the first valid that has pos>=1 per fold (warn)\n",
    "if best is None:\n",
    "    for k in range(n0, 1, -1):\n",
    "        ok, cv_type, fa, folds, per = _try_split(k, use_group=bool(USE_GROUP_BY_SPLIT))\n",
    "        if not ok and USE_GROUP_BY_SPLIT and AUTO_FALLBACK_GROUP:\n",
    "            ok, cv_type, fa, folds, per = _try_split(k, use_group=False)\n",
    "        if ok:\n",
    "            min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n",
    "            best = (k, cv_type, fa, folds, per, min_pos_seen)\n",
    "            print(f\"[Stage 7] NOTE: Could not satisfy MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}. Using k={k} with min_pos={min_pos_seen}.\")\n",
    "            break\n",
    "\n",
    "if best is None:\n",
    "    raise RuntimeError(\"[Stage 7] Failed to build a valid stratified CV split. Try smaller DEFAULT_SPLITS / FORCE_N_SPLITS, or disable group_by_split.\")\n",
    "\n",
    "n_splits, cv_type, fold_assign, folds, per, min_pos_seen = best\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Report + validation lines\n",
    "# ----------------------------\n",
    "lines = []\n",
    "lines.append(f\"CV={cv_type} n_splits={n_splits} seed={SEED}\")\n",
    "lines.append(f\"Order source: {order_source}\")\n",
    "lines.append(f\"Target column: {target_col}\")\n",
    "lines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    lines.append(f\"Group col requested: {group_col} | used_group={('Group' in cv_type)}\")\n",
    "lines.append(\"Per-fold distribution (val):\")\n",
    "\n",
    "ok = True\n",
    "for f in range(n_splits):\n",
    "    idx = np.where(fold_assign == f)[0]\n",
    "    yf = y[idx]\n",
    "    pf = int((yf == 1).sum())\n",
    "    nf = int((yf == 0).sum())\n",
    "    lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:9.6f}%\")\n",
    "    if pf == 0 or nf == 0:\n",
    "        ok = False\n",
    "\n",
    "if not ok:\n",
    "    raise RuntimeError(\"[Stage 7] A fold has pos=0 or neg=0 after selection (should not happen).\")\n",
    "\n",
    "if min_pos_seen < MIN_POS_PER_FOLD:\n",
    "    lines.append(f\"NOTE: min positives in a fold = {min_pos_seen} (< MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}). \"\n",
    "                 \"Threshold/F1 tuning bisa noisy; pertimbangkan n_splits lebih kecil.\")\n",
    "\n",
    "print(f\"[Stage 7] FINAL: n_splits={n_splits} | cv_type={cv_type} | min_pos_in_fold={min_pos_seen}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save artifacts\n",
    "# ----------------------------\n",
    "CV_DIR = ART_DIR / \"cv\"\n",
    "CV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\n",
    "folds_csv = CV_DIR / \"cv_folds.csv\"\n",
    "df_folds.to_csv(folds_csv, index=False)\n",
    "\n",
    "npz_path = CV_DIR / \"cv_folds.npz\"\n",
    "npz_kwargs = {}\n",
    "for f in range(n_splits):\n",
    "    npz_kwargs[f\"train_idx_{f}\"] = folds[f][\"train_idx\"]\n",
    "    npz_kwargs[f\"val_idx_{f}\"]   = folds[f][\"val_idx\"]\n",
    "np.savez(npz_path, **npz_kwargs)\n",
    "\n",
    "report_path = CV_DIR / \"cv_report.txt\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "cfg_path = CV_DIR / \"cv_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"seed\": SEED,\n",
    "            \"n_splits\": int(n_splits),\n",
    "            \"cv_type\": cv_type,\n",
    "            \"min_pos_per_fold\": int(MIN_POS_PER_FOLD),\n",
    "            \"enforce_min_pos_per_fold\": bool(ENFORCE_MIN_POS_PER_FOLD),\n",
    "            \"use_group_by_split_requested\": bool(globals().get(\"USE_GROUP_BY_SPLIT\", USE_GROUP_BY_SPLIT)),\n",
    "            \"order_source\": order_source,\n",
    "            \"target_col\": target_col,\n",
    "            \"group_col\": group_col,\n",
    "            \"artifacts\": {\n",
    "                \"folds_csv\": str(folds_csv),\n",
    "                \"folds_npz\": str(npz_path),\n",
    "                \"report_txt\": str(report_path),\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n[Stage 7] CV split OK\")\n",
    "print(f\"- Saved: {folds_csv}\")\n",
    "print(f\"- Saved: {npz_path}\")\n",
    "print(f\"- Saved: {report_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "print(\"\\n\".join(lines[-(n_splits + 4):]))\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Export globals for next stage\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"CV_DIR\": CV_DIR,\n",
    "    \"n_splits\": int(n_splits),\n",
    "    \"train_ids_ordered\": train_ids,\n",
    "    \"y_ordered\": y,\n",
    "    \"fold_assign\": fold_assign,\n",
    "    \"folds\": folds,\n",
    "    \"CV_FOLDS_CSV\": folds_csv,\n",
    "    \"CV_FOLDS_NPZ\": npz_path,\n",
    "    \"CV_CFG_PATH\": cfg_path,\n",
    "    \"CV_TYPE\": cv_type,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af11bc6",
   "metadata": {
    "papermill": {
     "duration": 0.014042,
     "end_time": "2026-01-02T18:47:51.091660",
     "exception": false,
     "start_time": "2026-01-02T18:47:51.077618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model (CPU-Safe Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb6ff82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T18:47:51.123181Z",
     "iopub.status.busy": "2026-01-02T18:47:51.122805Z",
     "iopub.status.idle": "2026-01-02T20:38:00.645432Z",
     "shell.execute_reply": "2026-01-02T20:38:00.644267Z"
    },
    "papermill": {
     "duration": 6609.541199,
     "end_time": "2026-01-02T20:38:00.647551",
     "exception": false,
     "start_time": "2026-01-02T18:47:51.106352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8] Building AGG sequence features (one-time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/3905489207.py:255: RuntimeWarning: Mean of empty slice\n",
      "  mean_val_b = np.nanmean(val_b, axis=1).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8] AGG built: shape=(3043, 31) | time=0.3s\n",
      "[Stage 8] TRAIN CONFIG (CPU)\n",
      "- N=3,043 | pos=148 | neg=2,895 | pos%=4.863621%\n",
      "- token_mode=mag | val_feat=mag | g_dim=38 | use_agg_seq=True\n",
      "- Model: d_model=160 heads=4 layers=3 dropout=0.12\n",
      "- Batch=16 grad_accum=2 epochs=14 lr=0.0005\n",
      "- WeightedSampler=True | label_smoothing=0.03\n",
      "- CKPT_DIR=/kaggle/working/mallorn_run/run_20260102_184148_9f34156418/checkpoints\n",
      "- OOF_DIR =/kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof\n",
      "- LOG_DIR =/kaggle/working/mallorn_run/run_20260102_184148_9f34156418/logs\n",
      "\n",
      "[Stage 8] FOLD 0/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17/3905489207.py:352: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  Xt = torch.from_numpy(X.astype(np.float32, copy=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=1.80e-04 | train_loss=3.02638 | val_loss=2.76462 | val_auc=0.66177 | f1@0.5=0.0939 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.20e-04 | train_loss=1.86852 | val_loss=2.62025 | val_auc=0.68192 | f1@0.5=0.0980 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=5.00e-04 | train_loss=1.72584 | val_loss=2.06933 | val_auc=0.70351 | f1@0.5=0.1128 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.97e-04 | train_loss=1.53122 | val_loss=2.08821 | val_auc=0.74145 | f1@0.5=0.1198 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.90e-04 | train_loss=1.45095 | val_loss=2.03033 | val_auc=0.77012 | f1@0.5=0.1210 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=4.80e-04 | train_loss=1.34805 | val_loss=1.59334 | val_auc=0.80455 | f1@0.5=0.1405 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=4.66e-04 | train_loss=1.30704 | val_loss=1.37585 | val_auc=0.82372 | f1@0.5=0.1443 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=4.48e-04 | train_loss=1.24240 | val_loss=1.75375 | val_auc=0.82637 | f1@0.5=0.1408 | best_ep=8 | pat=4\n",
      "  epoch 09 | lr=4.28e-04 | train_loss=1.14137 | val_loss=1.44349 | val_auc=0.82873 | f1@0.5=0.1474 | best_ep=9 | pat=4\n",
      "  epoch 10 | lr=4.04e-04 | train_loss=1.13892 | val_loss=1.50557 | val_auc=0.83397 | f1@0.5=0.1580 | best_ep=10 | pat=4\n",
      "  epoch 11 | lr=3.78e-04 | train_loss=1.13220 | val_loss=1.65333 | val_auc=0.84024 | f1@0.5=0.1495 | best_ep=11 | pat=4\n",
      "  epoch 12 | lr=3.51e-04 | train_loss=1.12220 | val_loss=1.22572 | val_auc=0.84185 | f1@0.5=0.1783 | best_ep=12 | pat=4\n",
      "  epoch 13 | lr=3.21e-04 | train_loss=1.12562 | val_loss=1.36930 | val_auc=0.84761 | f1@0.5=0.1637 | best_ep=13 | pat=4\n",
      "  epoch 14 | lr=2.91e-04 | train_loss=1.09900 | val_loss=1.49128 | val_auc=0.85371 | f1@0.5=0.1573 | best_ep=14 | pat=4\n",
      "\n",
      "[Stage 8] FOLD 1/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=1.80e-04 | train_loss=3.03170 | val_loss=2.78285 | val_auc=0.67617 | f1@0.5=0.0939 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.20e-04 | train_loss=1.80916 | val_loss=2.12647 | val_auc=0.67340 | f1@0.5=0.1093 | best_ep=1 | pat=3\n",
      "  epoch 03 | lr=5.00e-04 | train_loss=1.71554 | val_loss=2.47786 | val_auc=0.70104 | f1@0.5=0.1085 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.97e-04 | train_loss=1.50153 | val_loss=2.11699 | val_auc=0.74116 | f1@0.5=0.1210 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.90e-04 | train_loss=1.41878 | val_loss=2.12937 | val_auc=0.77444 | f1@0.5=0.1307 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=4.80e-04 | train_loss=1.37165 | val_loss=1.98347 | val_auc=0.79591 | f1@0.5=0.1379 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=4.66e-04 | train_loss=1.24760 | val_loss=2.10138 | val_auc=0.80558 | f1@0.5=0.1357 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=4.48e-04 | train_loss=1.21571 | val_loss=2.05527 | val_auc=0.81071 | f1@0.5=0.1461 | best_ep=8 | pat=4\n",
      "  epoch 09 | lr=4.28e-04 | train_loss=1.17871 | val_loss=1.98948 | val_auc=0.81059 | f1@0.5=0.1585 | best_ep=8 | pat=3\n",
      "  epoch 10 | lr=4.04e-04 | train_loss=1.14092 | val_loss=1.90314 | val_auc=0.82303 | f1@0.5=0.1585 | best_ep=10 | pat=4\n",
      "  epoch 11 | lr=3.78e-04 | train_loss=1.11884 | val_loss=2.01756 | val_auc=0.83149 | f1@0.5=0.1593 | best_ep=11 | pat=4\n",
      "  epoch 12 | lr=3.51e-04 | train_loss=1.03697 | val_loss=1.95896 | val_auc=0.82913 | f1@0.5=0.1856 | best_ep=11 | pat=3\n",
      "  epoch 13 | lr=3.21e-04 | train_loss=1.04616 | val_loss=1.98460 | val_auc=0.82827 | f1@0.5=0.1877 | best_ep=11 | pat=2\n",
      "  epoch 14 | lr=2.91e-04 | train_loss=1.03770 | val_loss=2.01818 | val_auc=0.83074 | f1@0.5=0.1706 | best_ep=11 | pat=1\n",
      "\n",
      "[Stage 8] FOLD 2/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=1.80e-04 | train_loss=3.30461 | val_loss=2.77594 | val_auc=0.62642 | f1@0.5=0.0939 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.20e-04 | train_loss=1.88812 | val_loss=2.71278 | val_auc=0.65889 | f1@0.5=0.1012 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=5.00e-04 | train_loss=1.69895 | val_loss=2.33578 | val_auc=0.71255 | f1@0.5=0.1154 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.97e-04 | train_loss=1.53493 | val_loss=1.82583 | val_auc=0.81007 | f1@0.5=0.1247 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.90e-04 | train_loss=1.41205 | val_loss=1.82155 | val_auc=0.83765 | f1@0.5=0.1319 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=4.80e-04 | train_loss=1.38400 | val_loss=1.73573 | val_auc=0.86868 | f1@0.5=0.1333 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=4.66e-04 | train_loss=1.31153 | val_loss=1.93138 | val_auc=0.89194 | f1@0.5=0.1313 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=4.48e-04 | train_loss=1.23989 | val_loss=1.36608 | val_auc=0.89194 | f1@0.5=0.1567 | best_ep=8 | pat=4\n",
      "  epoch 09 | lr=4.28e-04 | train_loss=1.21021 | val_loss=1.86668 | val_auc=0.90363 | f1@0.5=0.1408 | best_ep=9 | pat=4\n",
      "  epoch 10 | lr=4.04e-04 | train_loss=1.21383 | val_loss=1.28803 | val_auc=0.90092 | f1@0.5=0.1681 | best_ep=9 | pat=3\n",
      "  epoch 11 | lr=3.78e-04 | train_loss=1.14822 | val_loss=1.53071 | val_auc=0.89896 | f1@0.5=0.1575 | best_ep=9 | pat=2\n",
      "  epoch 12 | lr=3.51e-04 | train_loss=1.09403 | val_loss=1.36820 | val_auc=0.91071 | f1@0.5=0.1671 | best_ep=12 | pat=4\n",
      "  epoch 13 | lr=3.21e-04 | train_loss=1.05723 | val_loss=1.29494 | val_auc=0.90760 | f1@0.5=0.1749 | best_ep=12 | pat=3\n",
      "  epoch 14 | lr=2.91e-04 | train_loss=1.09590 | val_loss=1.35495 | val_auc=0.89822 | f1@0.5=0.1724 | best_ep=12 | pat=2\n",
      "\n",
      "[Stage 8] FOLD 3/4 | train=2,435 val=608 | pos=119 neg=2,316 | pos_weight=19.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=1.80e-04 | train_loss=3.09341 | val_loss=2.82513 | val_auc=0.72205 | f1@0.5=0.0911 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.20e-04 | train_loss=1.83907 | val_loss=2.35031 | val_auc=0.71568 | f1@0.5=0.1010 | best_ep=1 | pat=3\n",
      "  epoch 03 | lr=5.00e-04 | train_loss=1.61819 | val_loss=2.55919 | val_auc=0.74129 | f1@0.5=0.1090 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.97e-04 | train_loss=1.52580 | val_loss=1.84636 | val_auc=0.84962 | f1@0.5=0.1191 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.90e-04 | train_loss=1.39777 | val_loss=1.85208 | val_auc=0.85451 | f1@0.5=0.1207 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=4.80e-04 | train_loss=1.33287 | val_loss=1.78760 | val_auc=0.87928 | f1@0.5=0.1264 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=4.66e-04 | train_loss=1.33138 | val_loss=1.32653 | val_auc=0.87827 | f1@0.5=0.1440 | best_ep=6 | pat=3\n",
      "  epoch 08 | lr=4.48e-04 | train_loss=1.27359 | val_loss=1.40111 | val_auc=0.88678 | f1@0.5=0.1472 | best_ep=8 | pat=4\n",
      "  epoch 09 | lr=4.28e-04 | train_loss=1.17156 | val_loss=1.52159 | val_auc=0.88536 | f1@0.5=0.1514 | best_ep=8 | pat=3\n",
      "  epoch 10 | lr=4.04e-04 | train_loss=1.16938 | val_loss=1.71557 | val_auc=0.88428 | f1@0.5=0.1407 | best_ep=8 | pat=2\n",
      "  epoch 11 | lr=3.78e-04 | train_loss=1.12856 | val_loss=1.35405 | val_auc=0.88559 | f1@0.5=0.1586 | best_ep=8 | pat=1\n",
      "  epoch 12 | lr=3.51e-04 | train_loss=1.14338 | val_loss=1.48256 | val_auc=0.88363 | f1@0.5=0.1518 | best_ep=8 | pat=0\n",
      "\n",
      "[Stage 8] FOLD 4/4 | train=2,435 val=608 | pos=119 neg=2,316 | pos_weight=19.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=1.80e-04 | train_loss=3.23834 | val_loss=2.99323 | val_auc=0.62027 | f1@0.5=0.0911 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.20e-04 | train_loss=1.83392 | val_loss=2.52247 | val_auc=0.66708 | f1@0.5=0.0961 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=5.00e-04 | train_loss=1.65972 | val_loss=2.25719 | val_auc=0.70484 | f1@0.5=0.1081 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.97e-04 | train_loss=1.59363 | val_loss=2.20872 | val_auc=0.77428 | f1@0.5=0.1131 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.90e-04 | train_loss=1.43721 | val_loss=1.53585 | val_auc=0.82211 | f1@0.5=0.1217 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=4.80e-04 | train_loss=1.32790 | val_loss=1.66908 | val_auc=0.83277 | f1@0.5=0.1339 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=4.66e-04 | train_loss=1.29979 | val_loss=1.38749 | val_auc=0.84754 | f1@0.5=0.1429 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=4.48e-04 | train_loss=1.23394 | val_loss=1.28281 | val_auc=0.85302 | f1@0.5=0.1518 | best_ep=8 | pat=4\n",
      "  epoch 09 | lr=4.28e-04 | train_loss=1.18229 | val_loss=1.67357 | val_auc=0.84623 | f1@0.5=0.1350 | best_ep=8 | pat=3\n",
      "  epoch 10 | lr=4.04e-04 | train_loss=1.14895 | val_loss=1.56088 | val_auc=0.84879 | f1@0.5=0.1395 | best_ep=8 | pat=2\n",
      "  epoch 11 | lr=3.78e-04 | train_loss=1.14727 | val_loss=1.69995 | val_auc=0.83789 | f1@0.5=0.1443 | best_ep=8 | pat=1\n",
      "  epoch 12 | lr=3.51e-04 | train_loss=1.12965 | val_loss=1.62097 | val_auc=0.84194 | f1@0.5=0.1452 | best_ep=8 | pat=0\n",
      "\n",
      "[Stage 8] CV TRAIN DONE\n",
      "- elapsed: 110.15 min\n",
      "- OOF saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/oof_prob.npy\n",
      "- OOF saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/oof_prob.csv\n",
      "- fold metrics: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/fold_metrics.json\n",
      "- OOF AUC (rough): 0.86377\n",
      "- OOF F1@0.5 (rough): 0.1564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 8 — Train Multiband Event Transformer (CPU-Safe) — REVISI FULL v3 (BOOST + NO-LEAK)\n",
    "#\n",
    "# Upgrade v3 (dibanding v2):\n",
    "# - Tambah AGG_SEQ_FEATURES: statistik token per object (global + per-band) => biasanya naik performa signifikan\n",
    "# - Early stopping by ROC-AUC (lebih stabil untuk imbalance), tetap log loss + F1@0.5\n",
    "# - OneCycleLR scheduler + grad_accum remainder aman\n",
    "# - Optional WeightedRandomSampler (imbalance fix) + light augmentation (token-drop + noise)\n",
    "# - Robust target column detection\n",
    "# - CKPT_DIR/OOF_DIR/LOG_DIR auto-create tetap\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, math, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal previous stages\n",
    "# ----------------------------\n",
    "need_min = [\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_train_meta\",\"n_splits\",\"folds\"]\n",
    "for k in need_min:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0a) Resolve train_ids ordering + labels (robust)\n",
    "# ----------------------------\n",
    "def _decode_ids(arr):\n",
    "    out = []\n",
    "    for x in arr.tolist():\n",
    "        if isinstance(x, (bytes, bytearray)):\n",
    "            s = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            s = str(x)\n",
    "        out.append(s.strip())\n",
    "    return out\n",
    "\n",
    "# ordering\n",
    "if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n",
    "    train_ids = list(globals()[\"train_ids_ordered\"])\n",
    "else:\n",
    "    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        raw = np.load(p, allow_pickle=False)\n",
    "        train_ids = _decode_ids(raw if raw.dtype.kind in (\"S\",\"O\") else raw.astype(str))\n",
    "    else:\n",
    "        train_ids = df_train_meta.index.astype(str).tolist()\n",
    "\n",
    "# target column robust\n",
    "target_col = None\n",
    "for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\"]:\n",
    "    if cand in df_train_meta.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols(sample)={list(df_train_meta.columns)[:40]}\")\n",
    "\n",
    "y = pd.to_numeric(df_train_meta.loc[train_ids, target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy()\n",
    "y = (y > 0).astype(np.int8)\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Ensure output dirs exist\n",
    "# ----------------------------\n",
    "if \"RUN_DIR\" in globals() and globals()[\"RUN_DIR\"] is not None:\n",
    "    RUN_DIR = Path(globals()[\"RUN_DIR\"])\n",
    "else:\n",
    "    if \"ART_DIR\" in globals() and globals()[\"ART_DIR\"] is not None:\n",
    "        RUN_DIR = Path(globals()[\"ART_DIR\"]).parent\n",
    "    else:\n",
    "        RUN_DIR = Path(\"/kaggle/working/mallorn_run\")\n",
    "\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", RUN_DIR / \"artifacts\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CKPT_DIR = Path(globals().get(\"CKPT_DIR\", RUN_DIR / \"checkpoints\"))\n",
    "OOF_DIR  = Path(globals().get(\"OOF_DIR\",  RUN_DIR / \"oof\"))\n",
    "LOG_DIR  = Path(globals().get(\"LOG_DIR\",  RUN_DIR / \"logs\"))\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "globals().update({\"RUN_DIR\": RUN_DIR, \"ART_DIR\": ART_DIR, \"CKPT_DIR\": CKPT_DIR, \"OOF_DIR\": OOF_DIR, \"LOG_DIR\": LOG_DIR})\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Torch imports + CPU safety\n",
    "# ----------------------------\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# thread guard\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# sklearn metrics for AUC\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn metrics tidak tersedia.\") from e\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open memmaps (fixed seq) — NO RAM load\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(globals()[\"FIX_DIR\"])\n",
    "N = len(train_ids)\n",
    "L = int(globals()[\"MAX_LEN\"])\n",
    "SEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "feat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "\n",
    "for p in [train_X_path, train_B_path, train_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "X_mm = np.memmap(train_X_path, dtype=np.float32, mode=\"r\", shape=(N, L, Fdim))\n",
    "B_mm = np.memmap(train_B_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "M_mm = np.memmap(train_M_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "\n",
    "# detect token mode\n",
    "SEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    if (\"mag\" in feat) and (\"mag_err_log\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"mag\"\n",
    "    elif (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"asinh\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"Cannot infer token_mode from features: {SEQ_FEATURE_NAMES}\")\n",
    "\n",
    "# required\n",
    "for k in [\"snr_tanh\",\"detected\"]:\n",
    "    if k not in feat:\n",
    "        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n",
    "\n",
    "VAL_FEAT = \"mag\" if SEQ_TOKEN_MODE == \"mag\" else \"flux_asinh\"\n",
    "if VAL_FEAT not in feat:\n",
    "    raise RuntimeError(f\"Feature '{VAL_FEAT}' missing for token_mode={SEQ_TOKEN_MODE}.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build RAW meta global features (NO scaling)\n",
    "# ----------------------------\n",
    "BASE_G_COLS = [\"Z\",\"Z_err\",\"EBV\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\n",
    "for c in BASE_G_COLS:\n",
    "    if c not in df_train_meta.columns:\n",
    "        df_train_meta[c] = 0.0\n",
    "\n",
    "G_meta = df_train_meta.loc[train_ids, BASE_G_COLS].copy()\n",
    "for c in BASE_G_COLS:\n",
    "    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "G_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "with open(Path(LOG_DIR)/\"global_meta_cols.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"cols\": BASE_G_COLS}, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 3b) NEW: Sequence aggregate features (global + per-band) — BIG BOOST in many cases\n",
    "# ----------------------------\n",
    "USE_AGG_SEQ_FEATURES = True\n",
    "N_BANDS = 6\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1.0)\n",
    "\n",
    "def build_agg_seq_features(X_mm, B_mm, M_mm, chunk=2048):\n",
    "    \"\"\"\n",
    "    Returns agg (N, agg_dim) float32.\n",
    "    Global:\n",
    "      - tok_count, det_frac, mean_abs_snr, max_abs_snr,\n",
    "      - value stats:\n",
    "         mag  : mean_mag, std_mag, min_mag\n",
    "         asinh: mean_abs_flux, std_flux, max_abs_flux\n",
    "    Per-band (for each band 0..5):\n",
    "      - count_b, det_frac_b, mean_abs_snr_b, mean_val_b\n",
    "    \"\"\"\n",
    "    snr_i = feat[\"snr_tanh\"]\n",
    "    det_i = feat[\"detected\"]\n",
    "    val_i = feat[VAL_FEAT]\n",
    "\n",
    "    agg_list = []\n",
    "    for start in range(0, N, chunk):\n",
    "        end = min(N, start+chunk)\n",
    "        Xc = np.asarray(X_mm[start:end])  # (B,L,F)\n",
    "        Bc = np.asarray(B_mm[start:end])  # (B,L)\n",
    "        Mc = np.asarray(M_mm[start:end])  # (B,L)\n",
    "\n",
    "        real = (Mc == 1)\n",
    "        tok_count = real.sum(axis=1).astype(np.float32)  # (B,)\n",
    "\n",
    "        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32)\n",
    "        det = (Xc[:, :, det_i] > 0.5).astype(np.float32)\n",
    "        val = Xc[:, :, val_i].astype(np.float32)\n",
    "\n",
    "        # mask apply\n",
    "        snr_r = snr * real\n",
    "        det_r = det * real\n",
    "\n",
    "        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n",
    "        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n",
    "        max_abs_snr = np.where(tok_count > 0, (snr * real).max(axis=1), 0.0).astype(np.float32)\n",
    "\n",
    "        if SEQ_TOKEN_MODE == \"mag\":\n",
    "            # for mag: keep only real tokens\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            mean_val = np.nanmean(val_r, axis=1).astype(np.float32)\n",
    "            std_val  = np.nanstd(val_r, axis=1).astype(np.float32)\n",
    "            min_val  = np.nanmin(val_r, axis=1).astype(np.float32)\n",
    "            mean_val = np.nan_to_num(mean_val, nan=0.0).astype(np.float32)\n",
    "            std_val  = np.nan_to_num(std_val,  nan=0.0).astype(np.float32)\n",
    "            min_val  = np.nan_to_num(min_val,  nan=0.0).astype(np.float32)\n",
    "            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1)\n",
    "        else:\n",
    "            # asinh: abs flux is more informative\n",
    "            aval = np.abs(val)\n",
    "            aval_r = aval * real\n",
    "            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count)\n",
    "            # std flux (non-abs) on real tokens\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            std_val = np.nanstd(val_r, axis=1).astype(np.float32)\n",
    "            max_aval = np.where(tok_count > 0, (aval * real).max(axis=1), 0.0).astype(np.float32)\n",
    "            std_val = np.nan_to_num(std_val, nan=0.0).astype(np.float32)\n",
    "            global_val_feats = np.stack([mean_aval.astype(np.float32), std_val, max_aval], axis=1)\n",
    "\n",
    "        # per-band features\n",
    "        per_band = []\n",
    "        for b in range(N_BANDS):\n",
    "            bm = (Bc == b) & real\n",
    "            cnt = bm.sum(axis=1).astype(np.float32)\n",
    "            detb = (det * bm).sum(axis=1).astype(np.float32)\n",
    "            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n",
    "\n",
    "            det_frac_b = _safe_div(detb, cnt)\n",
    "            mean_abs_snr_b = _safe_div(snrb, cnt)\n",
    "\n",
    "            if SEQ_TOKEN_MODE == \"mag\":\n",
    "                val_b = np.where(bm, val, np.nan)\n",
    "                mean_val_b = np.nanmean(val_b, axis=1).astype(np.float32)\n",
    "                mean_val_b = np.nan_to_num(mean_val_b, nan=0.0).astype(np.float32)\n",
    "            else:\n",
    "                aval_b = np.abs(val) * bm\n",
    "                mean_val_b = _safe_div(aval_b.sum(axis=1).astype(np.float32), cnt)\n",
    "\n",
    "            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n",
    "\n",
    "        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n",
    "\n",
    "        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n",
    "        agg = np.concatenate([glob, global_val_feats.astype(np.float32), per_band], axis=1).astype(np.float32)\n",
    "        agg_list.append(agg)\n",
    "\n",
    "        del Xc, Bc, Mc\n",
    "        if (start // chunk) % 3 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    agg_all = np.concatenate(agg_list, axis=0).astype(np.float32)\n",
    "    return agg_all\n",
    "\n",
    "if USE_AGG_SEQ_FEATURES:\n",
    "    print(\"[Stage 8] Building AGG sequence features (one-time)...\")\n",
    "    t0 = time.time()\n",
    "    G_seq_np = build_agg_seq_features(X_mm, B_mm, M_mm, chunk=2048)\n",
    "    print(f\"[Stage 8] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\n",
    "else:\n",
    "    G_seq_np = np.zeros((N,0), dtype=np.float32)\n",
    "\n",
    "# final global raw matrix\n",
    "G_raw_np = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\n",
    "g_dim = int(G_raw_np.shape[1])\n",
    "\n",
    "with open(Path(LOG_DIR)/\"global_feature_spec.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"meta_cols\": BASE_G_COLS,\n",
    "            \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n",
    "            \"token_mode\": SEQ_TOKEN_MODE,\n",
    "            \"val_feat\": VAL_FEAT,\n",
    "            \"agg_dim\": int(G_seq_np.shape[1]),\n",
    "            \"total_g_dim\": int(g_dim),\n",
    "            \"per_band_block\": [\"count\",\"det_frac\",\"mean_abs_snr\",\"mean_val\"],\n",
    "            \"global_block\": [\"tok_count\",\"det_frac\",\"mean_abs_snr\",\"max_abs_snr\",\"val_stat1\",\"val_stat2\",\"val_stat3\"],\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset / Loader (num_workers=0) + optional augmentation\n",
    "# ----------------------------\n",
    "AUG_TOKENDROP_P = 0.05     # 0.0 disable\n",
    "AUG_VALUE_NOISE = 0.01     # 0.0 disable (mag or flux noise, small)\n",
    "\n",
    "class MemmapSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, idx, X_mm, B_mm, M_mm, G_scaled_np, y=None, train_mode=False):\n",
    "        self.idx = np.asarray(idx, dtype=np.int32)\n",
    "        self.X_mm = X_mm\n",
    "        self.B_mm = B_mm\n",
    "        self.M_mm = M_mm\n",
    "        self.G = G_scaled_np\n",
    "        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n",
    "        self.train_mode = bool(train_mode)\n",
    "        self.rng = np.random.default_rng(SEED + (123 if train_mode else 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "        X = np.array(self.X_mm[j], copy=False)  # (L,F)\n",
    "        B = np.array(self.B_mm[j], copy=False)  # (L,)\n",
    "        M = np.array(self.M_mm[j], copy=False)  # (L,)\n",
    "        G = np.array(self.G[j], copy=False)     # (g_dim,)\n",
    "\n",
    "        if self.train_mode:\n",
    "            # token dropout: randomly mask out a small fraction of real tokens (avoid all-pad)\n",
    "            if AUG_TOKENDROP_P and AUG_TOKENDROP_P > 0:\n",
    "                real = (M == 1)\n",
    "                if real.any():\n",
    "                    drop = (self.rng.random(real.shape[0]) < AUG_TOKENDROP_P) & real\n",
    "                    # keep at least 1 token\n",
    "                    if drop.all():\n",
    "                        drop[self.rng.integers(0, real.sum())] = False\n",
    "                    M = M.copy()\n",
    "                    M[drop] = 0\n",
    "\n",
    "            # small value noise on real tokens\n",
    "            if AUG_VALUE_NOISE and AUG_VALUE_NOISE > 0:\n",
    "                vi = feat[VAL_FEAT]\n",
    "                real = (M == 1)\n",
    "                if real.any():\n",
    "                    X = X.copy()\n",
    "                    noise = self.rng.normal(0.0, AUG_VALUE_NOISE, size=real.sum()).astype(np.float32)\n",
    "                    X[real, vi] = (X[real, vi] + noise).astype(np.float32)\n",
    "\n",
    "        Xt = torch.from_numpy(X.astype(np.float32, copy=False))\n",
    "        Bt = torch.from_numpy(B.astype(np.int64, copy=False))\n",
    "        Mt = torch.from_numpy(M.astype(np.int64, copy=False))\n",
    "        Gt = torch.from_numpy(G.astype(np.float32, copy=False))\n",
    "\n",
    "        if self.y is None:\n",
    "            return Xt, Bt, Mt, Gt\n",
    "\n",
    "        yy = float(self.y[j])\n",
    "        return Xt, Bt, Mt, Gt, torch.tensor(yy, dtype=torch.float32)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, sampler=None):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(sampler is None and shuffle),\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Model — slightly stronger + stable pooling\n",
    "# ----------------------------\n",
    "class MultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, max_len, n_bands=6, d_model=160, n_heads=4, n_layers=3, ff_mult=2, dropout=0.12, g_dim=0):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.x_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.band_emb = nn.Embedding(n_bands, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * ff_mult),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # pooling: attn + mean mix (lebih stabil)\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "        self.pool_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        # global features projection\n",
    "        g_out = max(32, d_model // 2)\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(g_dim, g_out),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model + g_out, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        pad_mask = (mask == 0)  # True=pad\n",
    "        # ALL-PAD guard\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        # attn pooling\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        # mean pooling on valid tokens\n",
    "        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n",
    "        denom = valid.sum(dim=1).clamp_min(1.0)\n",
    "        pooled_mean = (h * valid).sum(dim=1) / denom\n",
    "\n",
    "        pooled = 0.6 * pooled_attn + 0.4 * pooled_mean\n",
    "        pooled = self.pool_ln(pooled)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        return self.head(z).squeeze(-1)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Training config (CPU safe, but stronger default)\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    \"d_model\": 160,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 3,\n",
    "    \"ff_mult\": 2,\n",
    "    \"dropout\": 0.12,\n",
    "\n",
    "    \"batch_size\": 16,\n",
    "    \"grad_accum\": 2,\n",
    "\n",
    "    \"epochs\": 14,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.02,\n",
    "\n",
    "    \"patience\": 4,            # early stop by AUC\n",
    "    \"max_grad_norm\": 1.0,\n",
    "\n",
    "    \"use_weighted_sampler\": True,  # imbalance\n",
    "    \"label_smoothing\": 0.03,        # small smoothing for stability\n",
    "    \"scheduler\": \"onecycle\",\n",
    "}\n",
    "\n",
    "# auto soften for long seq\n",
    "if L >= 512:\n",
    "    CFG[\"d_model\"] = 128\n",
    "    CFG[\"n_heads\"] = 4\n",
    "    CFG[\"n_layers\"] = 2\n",
    "    CFG[\"batch_size\"] = 12\n",
    "    CFG[\"grad_accum\"] = 2\n",
    "    CFG[\"lr\"] = 4e-4\n",
    "\n",
    "cfg_path = Path(LOG_DIR) / \"train_cfg_stage8.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "\n",
    "pos_all = int((y == 1).sum())\n",
    "neg_all = int((y == 0).sum())\n",
    "print(\"[Stage 8] TRAIN CONFIG (CPU)\")\n",
    "print(f\"- N={N:,} | pos={pos_all:,} | neg={neg_all:,} | pos%={pos_all/max(N,1)*100:.6f}%\")\n",
    "print(f\"- token_mode={SEQ_TOKEN_MODE} | val_feat={VAL_FEAT} | g_dim={g_dim} | use_agg_seq={USE_AGG_SEQ_FEATURES}\")\n",
    "print(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\n",
    "print(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\n",
    "print(f\"- WeightedSampler={CFG['use_weighted_sampler']} | label_smoothing={CFG['label_smoothing']}\")\n",
    "print(f\"- CKPT_DIR={CKPT_DIR}\")\n",
    "print(f\"- OOF_DIR ={OOF_DIR}\")\n",
    "print(f\"- LOG_DIR ={LOG_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Helpers\n",
    "# ----------------------------\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def f1_binary(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return float(2 * prec * rec / (prec + rec))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses, logits_all, y_all = [], [], []\n",
    "    for batch in loader:\n",
    "        Xb, Bb, Mb, Gb, yb = batch\n",
    "        Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n",
    "        logit = model(Xb, Bb, Mb, Gb)\n",
    "        loss = criterion(logit, yb)\n",
    "        losses.append(float(loss.item()))\n",
    "        logits_all.append(logit.detach().cpu().numpy())\n",
    "        y_all.append(yb.detach().cpu().numpy())\n",
    "    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n",
    "    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n",
    "    probs = sigmoid_np(logits_all)\n",
    "    pred01 = (probs >= 0.5).astype(np.int8)\n",
    "    f1 = f1_binary(y_all, pred01)\n",
    "    auc = float(roc_auc_score(y_all, probs)) if (len(np.unique(y_all)) == 2) else float(\"nan\")\n",
    "    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1, auc\n",
    "\n",
    "# fold-wise scaler (NO leakage)\n",
    "def fit_scaler_fold(G_raw_np, tr_idx):\n",
    "    X = G_raw_np[tr_idx]\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    std  = X.std(axis=0).astype(np.float32)\n",
    "    std  = np.where(std < 1e-6, 1.0, std).astype(np.float32)\n",
    "    return mean, std\n",
    "\n",
    "def apply_scaler(G_raw_np, mean, std):\n",
    "    return ((G_raw_np - mean) / std).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) CV Train\n",
    "# ----------------------------\n",
    "oof_prob = np.zeros((N,), dtype=np.float32)\n",
    "fold_metrics = []\n",
    "\n",
    "all_idx = np.arange(N, dtype=np.int32)\n",
    "n_splits = int(globals()[\"n_splits\"])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for fold_info in globals()[\"folds\"]:\n",
    "    fold = int(fold_info[\"fold\"])\n",
    "    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n",
    "\n",
    "    val_mask = np.zeros(N, dtype=bool)\n",
    "    val_mask[val_idx] = True\n",
    "    tr_idx = all_idx[~val_mask]\n",
    "\n",
    "    # fold-wise pos_weight (train only)\n",
    "    y_tr = y[tr_idx]\n",
    "    pos = int((y_tr == 1).sum())\n",
    "    neg = int((y_tr == 0).sum())\n",
    "    if pos == 0:\n",
    "        raise RuntimeError(f\"[Stage 8] Fold {fold}: no positives in training split.\")\n",
    "    pos_weight = float(neg / max(pos, 1))\n",
    "    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\n",
    "\n",
    "    # label smoothing for BCE\n",
    "    ls = float(CFG[\"label_smoothing\"])\n",
    "    def smooth(yb):\n",
    "        if ls <= 0:\n",
    "            return yb\n",
    "        return yb * (1.0 - ls) + 0.5 * ls\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "\n",
    "    print(f\"\\n[Stage 8] FOLD {fold}/{n_splits-1} | train={len(tr_idx):,} val={len(val_idx):,} | pos={pos:,} neg={neg:,} | pos_weight={pos_weight:.4f}\")\n",
    "\n",
    "    # fold-wise scaler (NO leakage)\n",
    "    g_mean, g_std = fit_scaler_fold(G_raw_np, tr_idx)\n",
    "    G_fold_z = apply_scaler(G_raw_np, g_mean, g_std)\n",
    "\n",
    "    # datasets\n",
    "    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_fold_z, y=y, train_mode=True)\n",
    "    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_fold_z, y=y, train_mode=False)\n",
    "\n",
    "    # optional weighted sampler (train only)\n",
    "    sampler = None\n",
    "    if bool(CFG[\"use_weighted_sampler\"]):\n",
    "        # weights: inverse freq in TRAIN split\n",
    "        w = np.ones((len(tr_idx),), dtype=np.float32)\n",
    "        ytr_local = y[tr_idx]\n",
    "        # heavier for positives\n",
    "        w[ytr_local == 1] = float(neg / max(pos, 1))\n",
    "        w = torch.from_numpy(w)\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(weights=w, num_samples=len(tr_idx), replacement=True)\n",
    "\n",
    "    dl_tr = make_loader(ds_tr, batch_size=int(CFG[\"batch_size\"]), shuffle=True, sampler=sampler)\n",
    "    dl_va = make_loader(ds_va, batch_size=int(CFG[\"batch_size\"]), shuffle=False)\n",
    "\n",
    "    model = MultibandEventTransformer(\n",
    "        feat_dim=Fdim,\n",
    "        max_len=L,\n",
    "        n_bands=6,\n",
    "        d_model=int(CFG[\"d_model\"]),\n",
    "        n_heads=int(CFG[\"n_heads\"]),\n",
    "        n_layers=int(CFG[\"n_layers\"]),\n",
    "        ff_mult=int(CFG[\"ff_mult\"]),\n",
    "        dropout=float(CFG[\"dropout\"]),\n",
    "        g_dim=g_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(CFG[\"lr\"]), weight_decay=float(CFG[\"weight_decay\"]))\n",
    "\n",
    "    # scheduler\n",
    "    scheduler = None\n",
    "    if str(CFG.get(\"scheduler\",\"\")).lower() == \"onecycle\":\n",
    "        steps_per_epoch = max(len(dl_tr), 1)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(CFG[\"lr\"]),\n",
    "            epochs=int(CFG[\"epochs\"]),\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"cos\",\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "\n",
    "    best_val_auc = -1e9\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_probs = None\n",
    "    patience_left = int(CFG[\"patience\"])\n",
    "\n",
    "    grad_accum = int(CFG[\"grad_accum\"])\n",
    "\n",
    "    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss_true = 0.0\n",
    "        n_batches = 0\n",
    "        accum = 0\n",
    "\n",
    "        for batch in dl_tr:\n",
    "            Xb, Bb, Mb, Gb, yb = batch\n",
    "            Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n",
    "\n",
    "            yb_s = smooth(yb)\n",
    "\n",
    "            logit = model(Xb, Bb, Mb, Gb)\n",
    "            loss = criterion(logit, yb_s)\n",
    "\n",
    "            total_loss_true += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "            (loss / float(grad_accum)).backward()\n",
    "            accum += 1\n",
    "\n",
    "            if accum == grad_accum:\n",
    "                if CFG[\"max_grad_norm\"] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                accum = 0\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        # remainder step (IMPORTANT)\n",
    "        if accum > 0:\n",
    "            if CFG[\"max_grad_norm\"] is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        train_loss = total_loss_true / max(n_batches, 1)\n",
    "\n",
    "        # validate\n",
    "        val_loss, probs, y_val, f1_05, val_auc = eval_model(model, dl_va, criterion)\n",
    "\n",
    "        improved = (val_auc > best_val_auc + 1e-6) or (math.isnan(best_val_auc) and not math.isnan(val_auc))\n",
    "        # fallback tie-breaker by loss\n",
    "        if (not improved) and (abs(val_auc - best_val_auc) <= 1e-6) and (val_loss < best_val_loss - 1e-6):\n",
    "            improved = True\n",
    "\n",
    "        if improved:\n",
    "            best_val_auc = float(val_auc)\n",
    "            best_val_loss = float(val_loss)\n",
    "            best_epoch = int(epoch)\n",
    "            best_probs = probs.copy()\n",
    "\n",
    "            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"fold\": fold,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"cfg\": CFG,\n",
    "                    \"seq_feature_names\": SEQ_FEATURE_NAMES,\n",
    "                    \"max_len\": L,\n",
    "                    \"token_mode\": SEQ_TOKEN_MODE,\n",
    "                    \"val_feat\": VAL_FEAT,\n",
    "                    \"global_meta_cols\": BASE_G_COLS,\n",
    "                    \"use_agg_seq_features\": bool(USE_AGG_SEQ_FEATURES),\n",
    "                    \"global_scaler\": {\"mean\": g_mean, \"std\": g_std},\n",
    "                    \"pos_weight\": pos_weight,\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "            patience_left = int(CFG[\"patience\"])\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"  epoch {epoch:02d} | lr={lr_now:.2e} | train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | val_auc={val_auc:.5f} | f1@0.5={f1_05:.4f} | best_ep={best_epoch} | pat={patience_left}\")\n",
    "\n",
    "        if patience_left <= 0:\n",
    "            break\n",
    "\n",
    "    if best_probs is None:\n",
    "        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n",
    "\n",
    "    # fill OOF\n",
    "    oof_prob[val_idx] = best_probs.astype(np.float32)\n",
    "\n",
    "    pred01 = (best_probs >= 0.5).astype(np.int8)\n",
    "    best_f1_05 = f1_binary(y[val_idx], pred01)\n",
    "\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold,\n",
    "        \"val_size\": int(len(val_idx)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_auc\": float(best_val_auc),\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"f1_at_0p5\": float(best_f1_05),\n",
    "        \"pos_weight\": float(pos_weight),\n",
    "        \"g_dim\": int(g_dim),\n",
    "        \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n",
    "    })\n",
    "\n",
    "    del model, opt, ds_tr, ds_va, dl_tr, dl_va, G_fold_z\n",
    "    gc.collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Save OOF artifacts + summary\n",
    "# ----------------------------\n",
    "oof_path_npy = OOF_DIR / \"oof_prob.npy\"\n",
    "np.save(oof_path_npy, oof_prob)\n",
    "\n",
    "df_oof = pd.DataFrame({\"object_id\": train_ids, \"target\": y.astype(int), \"oof_prob\": oof_prob.astype(np.float32)})\n",
    "oof_path_csv = OOF_DIR / \"oof_prob.csv\"\n",
    "df_oof.to_csv(oof_path_csv, index=False)\n",
    "\n",
    "metrics_path = OOF_DIR / \"fold_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed)}, f, indent=2)\n",
    "\n",
    "# overall quick metrics\n",
    "oof_pred01 = (oof_prob >= 0.5).astype(np.int8)\n",
    "oof_f1_05 = f1_binary(y, oof_pred01)\n",
    "oof_auc = float(roc_auc_score(y, oof_prob)) if (len(np.unique(y)) == 2) else float(\"nan\")\n",
    "\n",
    "print(\"\\n[Stage 8] CV TRAIN DONE\")\n",
    "print(f\"- elapsed: {elapsed/60:.2f} min\")\n",
    "print(f\"- OOF saved: {oof_path_npy}\")\n",
    "print(f\"- OOF saved: {oof_path_csv}\")\n",
    "print(f\"- fold metrics: {metrics_path}\")\n",
    "print(f\"- OOF AUC (rough): {oof_auc:.5f}\")\n",
    "print(f\"- OOF F1@0.5 (rough): {oof_f1_05:.4f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"oof_prob\": oof_prob,\n",
    "    \"OOF_PROB_PATH\": oof_path_npy,\n",
    "    \"OOF_CSV_PATH\": oof_path_csv,\n",
    "    \"FOLD_METRICS_PATH\": metrics_path,\n",
    "    \"TRAIN_CFG_PATH\": cfg_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae928f8",
   "metadata": {
    "papermill": {
     "duration": 0.016959,
     "end_time": "2026-01-02T20:38:00.681664",
     "exception": false,
     "start_time": "2026-01-02T20:38:00.664705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OOF Prediction + Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac7f1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T20:38:00.718885Z",
     "iopub.status.busy": "2026-01-02T20:38:00.718530Z",
     "iopub.status.idle": "2026-01-02T20:38:01.323398Z",
     "shell.execute_reply": "2026-01-02T20:38:01.322607Z"
    },
    "papermill": {
     "duration": 0.626735,
     "end_time": "2026-01-02T20:38:01.325130",
     "exception": false,
     "start_time": "2026-01-02T20:38:00.698395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9] Loaded OOF from: csv\n",
      "[Stage 9] N=3,043 | pos=148 | neg=2,895 | pos%=4.863621% | target_col=target\n",
      "[Stage 9] DONE\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/threshold_tuning.json\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/threshold_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/threshold_table_top500.csv\n",
      "- BEST_THR_F1  =0.970070 | F1=0.332117 (P=0.227500 R=0.614865)\n",
      "- BEST_THR_ACC =0.999111 | ACC=0.952350 BACC=0.516546 F1=0.064516\n",
      "- BEST_THR_BACC=0.840000 | BACC=0.786457 ACC=0.685179 F1=0.217320\n",
      "- BEST_THR_MCC =0.970070 | MCC=0.323482 F1=0.332117 BACC=0.754065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v3 (ALIGN SUPER ROBUST + MULTI-METRIC)\n",
    "#\n",
    "# Upgrade v3:\n",
    "# - Auto-detect target column in df_train_meta (target/y/label/class/...)\n",
    "# - Prefer oof_prob.csv (object_id + oof_prob) for safest alignment\n",
    "# - Clean oof_prob NaN/inf + clip [0,1]\n",
    "# - Threshold tuning for: F1, Accuracy, Balanced Accuracy, MCC (+ Precision/Recall)\n",
    "# - Exports multiple BEST thresholds, not only F1\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"OOF_DIR\", \"df_train_meta\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n",
    "\n",
    "OOF_DIR = Path(OOF_DIR)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: robust stringify id\n",
    "# ----------------------------\n",
    "def _to_str_list(ids):\n",
    "    out = []\n",
    "    for x in ids:\n",
    "        if isinstance(x, (bytes, np.bytes_)):\n",
    "            out.append(x.decode(\"utf-8\", errors=\"ignore\").strip())\n",
    "        else:\n",
    "            out.append(str(x).strip())\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: robust load float32 1D\n",
    "# ----------------------------\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Detect target column in df_train_meta\n",
    "# ----------------------------\n",
    "def _detect_target_col(df):\n",
    "    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "TARGET_COL = _detect_target_col(df_train_meta)\n",
    "if TARGET_COL is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot detect target column in df_train_meta. \"\n",
    "        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Load OOF (prefer CSV for safest alignment)\n",
    "# ----------------------------\n",
    "def _load_oof():\n",
    "    # 1) CSV (best alignment)\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n",
    "            df[\"object_id\"] = df[\"object_id\"].astype(str).str.strip()\n",
    "            prob = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n",
    "            return df[\"object_id\"].tolist(), prob, \"csv\"\n",
    "\n",
    "    # 2) globals\n",
    "    if \"oof_prob\" in globals():\n",
    "        prob = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        if isinstance(prob, np.ndarray) and prob.ndim != 0:\n",
    "            # need ids\n",
    "            if \"train_ids_ordered\" in globals():\n",
    "                ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n",
    "                return ids, prob, \"globals(train_ids_ordered)\"\n",
    "            # fallback to df_train_meta order if same length\n",
    "            if len(prob) == len(df_train_meta):\n",
    "                return df_train_meta.index.astype(str).tolist(), prob, \"globals(df_train_meta.index)\"\n",
    "\n",
    "    # 3) npy\n",
    "    pnpy = OOF_DIR / \"oof_prob.npy\"\n",
    "    if pnpy.exists():\n",
    "        prob = _as_1d_float32(np.load(pnpy, allow_pickle=False))\n",
    "        if \"train_ids_ordered\" in globals():\n",
    "            ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n",
    "            return ids, prob, \"npy(train_ids_ordered)\"\n",
    "        if len(prob) == len(df_train_meta):\n",
    "            return df_train_meta.index.astype(str).tolist(), prob, \"npy(df_train_meta.index)\"\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob not found (csv/globals/npy). Jalankan STAGE 8 dulu.\")\n",
    "\n",
    "train_ids, oof_prob, src = _load_oof()\n",
    "\n",
    "# guard scalar\n",
    "if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "    raise TypeError(\n",
    "        f\"Invalid oof_prob (scalar). Type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}.\"\n",
    "    )\n",
    "\n",
    "# sanitize prob\n",
    "oof_prob = np.nan_to_num(oof_prob, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32)\n",
    "oof_prob = np.clip(oof_prob, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "# validate ids existence in meta\n",
    "missing = [oid for oid in train_ids if oid not in df_train_meta.index]\n",
    "if missing:\n",
    "    raise KeyError(f\"OOF ids not in df_train_meta (examples): {missing[:10]} | missing_n={len(missing)}\")\n",
    "\n",
    "# load y aligned\n",
    "y = pd.to_numeric(df_train_meta.loc[train_ids, TARGET_COL], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy()\n",
    "y = (y > 0).astype(np.int8)\n",
    "\n",
    "if len(oof_prob) != len(y):\n",
    "    raise RuntimeError(f\"Length mismatch: oof_prob={len(oof_prob)} vs y={len(y)}\")\n",
    "\n",
    "uy = set(np.unique(y).tolist())\n",
    "if not uy.issubset({0, 1}):\n",
    "    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n",
    "\n",
    "N = int(len(y))\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "\n",
    "print(f\"[Stage 9] Loaded OOF from: {src}\")\n",
    "print(f\"[Stage 9] N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}% | target_col={TARGET_COL}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Metrics\n",
    "# ----------------------------\n",
    "def _counts(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred01 == 0)).sum())\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def f1_prec_rec(tp, fp, fn):\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if tp == 0 or (prec + rec) == 0:\n",
    "        return 0.0, float(prec), float(rec)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    return float(f1), float(prec), float(rec)\n",
    "\n",
    "def accuracy(tp, fp, fn, tn):\n",
    "    return float((tp + tn) / max(tp + fp + fn + tn, 1))\n",
    "\n",
    "def balanced_accuracy(tp, fp, fn, tn):\n",
    "    tpr = tp / max(tp + fn, 1)\n",
    "    tnr = tn / max(tn + fp, 1)\n",
    "    return float(0.5 * (tpr + tnr))\n",
    "\n",
    "def mcc(tp, fp, fn, tn):\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    if den <= 0:\n",
    "        return 0.0\n",
    "    return float(num / np.sqrt(den))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold candidates (grid + quantiles + unique-prob sampling)\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.00, 0.10, 41),\n",
    "    np.linspace(0.10, 0.90, 161),\n",
    "    np.linspace(0.90, 1.00, 41),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\n",
    "quant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n",
    "\n",
    "# add a small sample of unique probs (if too many, subsample)\n",
    "uniq = np.unique(oof_prob)\n",
    "if len(uniq) > 4000:\n",
    "    take = np.linspace(0, len(uniq)-1, 4000, dtype=int)\n",
    "    uniq = uniq[take].astype(np.float32)\n",
    "\n",
    "thr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr, uniq]), 0.0, 1.0)).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Sweep\n",
    "# ----------------------------\n",
    "rows = []\n",
    "best_f1  = {\"thr\": 0.5, \"score\": -1.0, \"rec\": -1.0, \"fp\": 10**18}\n",
    "best_acc = {\"thr\": 0.5, \"score\": -1.0, \"bacc\": -1.0}\n",
    "best_bac = {\"thr\": 0.5, \"score\": -1.0, \"acc\": -1.0}\n",
    "best_mcc = {\"thr\": 0.5, \"score\": -1.0}\n",
    "\n",
    "for thr in thr_candidates:\n",
    "    pred = (oof_prob >= thr).astype(np.int8)\n",
    "    tp, fp, fn, tn = _counts(y, pred)\n",
    "\n",
    "    f1, prec, rec = f1_prec_rec(tp, fp, fn)\n",
    "    acc  = accuracy(tp, fp, fn, tn)\n",
    "    bacc = balanced_accuracy(tp, fp, fn, tn)\n",
    "    mmc  = mcc(tp, fp, fn, tn)\n",
    "    pos_pred = int(pred.sum())\n",
    "\n",
    "    rows.append((float(thr), f1, prec, rec, acc, bacc, mmc, tp, fp, fn, tn, pos_pred))\n",
    "\n",
    "    # best F1 (tie: higher recall, then lower fp)\n",
    "    if (f1 > best_f1[\"score\"] + 1e-12) or (\n",
    "        abs(f1 - best_f1[\"score\"]) <= 1e-12 and rec > best_f1[\"rec\"] + 1e-12\n",
    "    ) or (\n",
    "        abs(f1 - best_f1[\"score\"]) <= 1e-12 and abs(rec - best_f1[\"rec\"]) <= 1e-12 and fp < best_f1[\"fp\"]\n",
    "    ):\n",
    "        best_f1.update({\"thr\": float(thr), \"score\": float(f1), \"rec\": float(rec), \"fp\": int(fp)})\n",
    "\n",
    "    # best ACC (tie: higher BACC)\n",
    "    if (acc > best_acc[\"score\"] + 1e-12) or (\n",
    "        abs(acc - best_acc[\"score\"]) <= 1e-12 and bacc > best_acc[\"bacc\"] + 1e-12\n",
    "    ):\n",
    "        best_acc.update({\"thr\": float(thr), \"score\": float(acc), \"bacc\": float(bacc)})\n",
    "\n",
    "    # best BACC (tie: higher ACC)\n",
    "    if (bacc > best_bac[\"score\"] + 1e-12) or (\n",
    "        abs(bacc - best_bac[\"score\"]) <= 1e-12 and acc > best_bac[\"acc\"] + 1e-12\n",
    "    ):\n",
    "        best_bac.update({\"thr\": float(thr), \"score\": float(bacc), \"acc\": float(acc)})\n",
    "\n",
    "    # best MCC\n",
    "    if (mmc > best_mcc[\"score\"] + 1e-12):\n",
    "        best_mcc.update({\"thr\": float(thr), \"score\": float(mmc)})\n",
    "\n",
    "thr_table = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"thr\",\"f1\",\"precision\",\"recall\",\"accuracy\",\"balanced_accuracy\",\"mcc\",\"tp\",\"fp\",\"fn\",\"tn\",\"pos_pred\"]\n",
    ")\n",
    "\n",
    "# baselines\n",
    "def _eval_at(thr):\n",
    "    pred = (oof_prob >= float(thr)).astype(np.int8)\n",
    "    tp, fp, fn, tn = _counts(y, pred)\n",
    "    f1, prec, rec = f1_prec_rec(tp, fp, fn)\n",
    "    acc  = accuracy(tp, fp, fn, tn)\n",
    "    bacc = balanced_accuracy(tp, fp, fn, tn)\n",
    "    mmc  = mcc(tp, fp, fn, tn)\n",
    "    return {\n",
    "        \"thr\": float(thr), \"f1\": float(f1), \"precision\": float(prec), \"recall\": float(rec),\n",
    "        \"accuracy\": float(acc), \"balanced_accuracy\": float(bacc), \"mcc\": float(mmc),\n",
    "        \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn), \"tn\": int(tn), \"pos_pred\": int(pred.sum())\n",
    "    }\n",
    "\n",
    "base05 = _eval_at(0.5)\n",
    "\n",
    "BEST_THR_F1  = float(best_f1[\"thr\"])\n",
    "BEST_THR_ACC = float(best_acc[\"thr\"])\n",
    "BEST_THR_BACC= float(best_bac[\"thr\"])\n",
    "BEST_THR_MCC = float(best_mcc[\"thr\"])\n",
    "\n",
    "best_f1_full   = _eval_at(BEST_THR_F1)\n",
    "best_acc_full  = _eval_at(BEST_THR_ACC)\n",
    "best_bac_full  = _eval_at(BEST_THR_BACC)\n",
    "best_mcc_full  = _eval_at(BEST_THR_MCC)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Save artifacts\n",
    "# ----------------------------\n",
    "out_json = OOF_DIR / \"threshold_tuning.json\"\n",
    "out_txt  = OOF_DIR / \"threshold_report.txt\"\n",
    "out_csv  = OOF_DIR / \"threshold_table_top500.csv\"\n",
    "\n",
    "payload = {\n",
    "    \"source\": src,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"n_objects\": int(N),\n",
    "    \"pos\": int(pos),\n",
    "    \"neg\": int(neg),\n",
    "\n",
    "    \"baseline_thr_0p5\": base05,\n",
    "\n",
    "    \"best_thr_f1\": best_f1_full,\n",
    "    \"best_thr_accuracy\": best_acc_full,\n",
    "    \"best_thr_balanced_accuracy\": best_bac_full,\n",
    "    \"best_thr_mcc\": best_mcc_full,\n",
    "}\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "# sort tables for convenience\n",
    "top_f1   = thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).head(50).reset_index(drop=True)\n",
    "top_acc  = thr_table.sort_values([\"accuracy\",\"balanced_accuracy\",\"f1\"], ascending=[False, False, False]).head(50).reset_index(drop=True)\n",
    "top_bacc = thr_table.sort_values([\"balanced_accuracy\",\"accuracy\",\"f1\"], ascending=[False, False, False]).head(50).reset_index(drop=True)\n",
    "top_mcc  = thr_table.sort_values([\"mcc\",\"f1\",\"balanced_accuracy\"], ascending=[False, False, False]).head(50).reset_index(drop=True)\n",
    "\n",
    "# save a big top slice sorted by f1\n",
    "thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).head(500).to_csv(out_csv, index=False)\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Threshold Tuning Report (v3)\")\n",
    "lines.append(f\"- source={src}\")\n",
    "lines.append(f\"- target_col={TARGET_COL}\")\n",
    "lines.append(f\"- N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"- F1={base05['f1']:.6f} | P={base05['precision']:.6f} | R={base05['recall']:.6f} | ACC={base05['accuracy']:.6f} | BACC={base05['balanced_accuracy']:.6f} | MCC={base05['mcc']:.6f}\")\n",
    "lines.append(f\"- tp={base05['tp']} fp={base05['fp']} fn={base05['fn']} tn={base05['tn']} | pos_pred={base05['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F1   @ thr={best_f1_full['thr']:.6f} | F1={best_f1_full['f1']:.6f} | P={best_f1_full['precision']:.6f} | R={best_f1_full['recall']:.6f} | pos_pred={best_f1_full['pos_pred']}\")\n",
    "lines.append(f\"BEST-ACC  @ thr={best_acc_full['thr']:.6f} | ACC={best_acc_full['accuracy']:.6f} | BACC={best_acc_full['balanced_accuracy']:.6f} | F1={best_acc_full['f1']:.6f}\")\n",
    "lines.append(f\"BEST-BACC @ thr={best_bac_full['thr']:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f} | ACC={best_bac_full['accuracy']:.6f} | F1={best_bac_full['f1']:.6f}\")\n",
    "lines.append(f\"BEST-MCC  @ thr={best_mcc_full['thr']:.6f} | MCC={best_mcc_full['mcc']:.6f} | F1={best_mcc_full['f1']:.6f} | BACC={best_mcc_full['balanced_accuracy']:.6f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 by F1:\")\n",
    "for i in range(min(10, len(top_f1))):\n",
    "    r = top_f1.iloc[i]\n",
    "    lines.append(f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | P={r['precision']:.6f} | R={r['recall']:.6f} | ACC={r['accuracy']:.6f} | BACC={r['balanced_accuracy']:.6f} | MCC={r['mcc']:.6f} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print(\"[Stage 9] DONE\")\n",
    "print(f\"- Saved: {out_json}\")\n",
    "print(f\"- Saved: {out_txt}\")\n",
    "print(f\"- Saved: {out_csv}\")\n",
    "print(f\"- BEST_THR_F1  ={BEST_THR_F1:.6f} | F1={best_f1_full['f1']:.6f} (P={best_f1_full['precision']:.6f} R={best_f1_full['recall']:.6f})\")\n",
    "print(f\"- BEST_THR_ACC ={BEST_THR_ACC:.6f} | ACC={best_acc_full['accuracy']:.6f} BACC={best_acc_full['balanced_accuracy']:.6f} F1={best_acc_full['f1']:.6f}\")\n",
    "print(f\"- BEST_THR_BACC={BEST_THR_BACC:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f} ACC={best_bac_full['accuracy']:.6f} F1={best_bac_full['f1']:.6f}\")\n",
    "print(f\"- BEST_THR_MCC ={BEST_THR_MCC:.6f} | MCC={best_mcc_full['mcc']:.6f} F1={best_mcc_full['f1']:.6f} BACC={best_mcc_full['balanced_accuracy']:.6f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"train_ids_oof\": train_ids,\n",
    "    \"oof_prob\": oof_prob,\n",
    "    \"BEST_THR\": BEST_THR_F1,          # default tetap F1\n",
    "    \"BEST_THR_F1\": BEST_THR_F1,\n",
    "    \"BEST_THR_ACC\": BEST_THR_ACC,\n",
    "    \"BEST_THR_BACC\": BEST_THR_BACC,\n",
    "    \"BEST_THR_MCC\": BEST_THR_MCC,\n",
    "    \"thr_table\": thr_table,\n",
    "    \"THR_JSON_PATH\": out_json,\n",
    "    \"THR_REPORT_PATH\": out_txt,\n",
    "    \"THR_TABLE_CSV_PATH\": out_csv,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7186d6c",
   "metadata": {
    "papermill": {
     "duration": 0.01851,
     "end_time": "2026-01-02T20:38:01.361469",
     "exception": false,
     "start_time": "2026-01-02T20:38:01.342959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Inference (Fold Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c4cfa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T20:38:01.399639Z",
     "iopub.status.busy": "2026-01-02T20:38:01.399253Z",
     "iopub.status.idle": "2026-01-02T20:42:40.481208Z",
     "shell.execute_reply": "2026-01-02T20:42:40.480414Z"
    },
    "papermill": {
     "duration": 279.103823,
     "end_time": "2026-01-02T20:42:40.482999",
     "exception": false,
     "start_time": "2026-01-02T20:38:01.379176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 10] Test inference (AUTO-ARCH + LOGIT ensemble): N_test=7,135 | folds=5 | batch=64 | CPU\n",
      "  fold 0: d_model=160 g_dim=38 | logit_mean=6.670060 | prob_mean=0.973709 | prob_std=0.050629\n",
      "  fold 1: d_model=160 g_dim=38 | logit_mean=13.037735 | prob_mean=0.999956 | prob_std=0.000081\n",
      "  fold 2: d_model=160 g_dim=38 | logit_mean=2.777563 | prob_mean=0.729435 | prob_std=0.301560\n",
      "  fold 3: d_model=160 g_dim=38 | logit_mean=8.799269 | prob_mean=0.999768 | prob_std=0.000222\n",
      "  fold 4: d_model=160 g_dim=38 | logit_mean=-3.924105 | prob_mean=0.020096 | prob_std=0.005309\n",
      "\n",
      "[Stage 10] DONE\n",
      "- Saved logits folds: /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/preds/test_logit_folds.npy\n",
      "- Saved logits ens  : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/preds/test_logit_ens.npy\n",
      "- Saved probs folds : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/preds/test_prob_folds.npy\n",
      "- Saved probs ens   : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/preds/test_prob_ens.npy\n",
      "- Saved csv         : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/preds/test_prob_ens.csv\n",
      "- Saved config      : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/artifacts/preds/test_infer_config.json\n",
      "- ens prob mean=0.981647 | std=0.025451 | min=0.789080 | max=0.999878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v3.1 (AUTO-ARCH FROM CKPT + LOGIT ENSEMBLE + ID ALIGN HARD)\n",
    "#\n",
    "# Fix untuk error mismatch:\n",
    "# - Auto-detect x_proj as Linear vs Sequential(Linear)\n",
    "# - Auto-detect optional pool_ln\n",
    "# - Infer d_model / max_len / dim_feedforward / n_layers / g_dim langsung dari state_dict\n",
    "# - If ckpt is pure state_dict (no metadata), still runs (best-effort G features)\n",
    "#\n",
    "# Output:\n",
    "# - ART_DIR/preds/test_logit_folds.npy\n",
    "# - ART_DIR/preds/test_logit_ens.npy\n",
    "# - ART_DIR/preds/test_prob_folds.npy\n",
    "# - ART_DIR/preds/test_prob_ens.npy\n",
    "# - ART_DIR/preds/test_prob_ens.csv\n",
    "# - ART_DIR/preds/test_infer_config.json\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, re, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# mute nested tensor warning (harmless)\n",
    "warnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True.*\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"ART_DIR\",\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_test_meta\",\"CKPT_DIR\",\"n_splits\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n",
    "\n",
    "# Torch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Thread guard (CPU)\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FIX_DIR = Path(FIX_DIR)\n",
    "ART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR = Path(CKPT_DIR)\n",
    "\n",
    "OUT_DIR = ART_DIR / \"preds\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# helper: normalize id robustly\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _load_ids_npy(path: Path):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    xs = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n",
    "    return [_norm_id(z) for z in xs]\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load TEST ordering (must match STAGE 6)\n",
    "# ----------------------------\n",
    "test_ids_path = FIX_DIR / \"test_ids.npy\"\n",
    "if not test_ids_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "test_ids = _load_ids_npy(test_ids_path)\n",
    "NTE = len(test_ids)\n",
    "if NTE <= 0:\n",
    "    raise RuntimeError(\"test_ids kosong (NTE=0). Pastikan STAGE 6 sukses membuat test_ids.npy.\")\n",
    "\n",
    "# Normalize df_test_meta.index (always)\n",
    "df_test_meta = df_test_meta.copy(deep=False)\n",
    "df_test_meta.index = pd.Index([_norm_id(z) for z in df_test_meta.index], name=df_test_meta.index.name)\n",
    "\n",
    "missing_ids = [oid for oid in test_ids if oid not in df_test_meta.index]\n",
    "if missing_ids:\n",
    "    raise KeyError(f\"Some test_ids not found in df_test_meta.index (examples): {missing_ids[:10]} | missing_n={len(missing_ids)}\")\n",
    "\n",
    "if len(set(test_ids)) != len(test_ids):\n",
    "    s = pd.Series(test_ids)\n",
    "    dup = s[s.duplicated()].head(10).tolist()\n",
    "    raise ValueError(f\"Duplicate object_id in test_ids ordering (examples): {dup}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open fixed-length TEST memmaps\n",
    "# ----------------------------\n",
    "SEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "L = int(MAX_LEN)\n",
    "\n",
    "test_X_path = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path = FIX_DIR / \"test_M.dat\"\n",
    "for p in [test_X_path, test_B_path, test_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=np.float32, mode=\"r\", shape=(NTE, L, Fdim))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset/Loader for inference\n",
    "# ----------------------------\n",
    "class TestMemmapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xmm, Bmm, Mmm, G_np_z):\n",
    "        self.Xmm = Xmm\n",
    "        self.Bmm = Bmm\n",
    "        self.Mmm = Mmm\n",
    "        self.G = G_np_z\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.Xmm.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        X = self.Xmm[i]\n",
    "        B = self.Bmm[i].astype(np.int64, copy=False)\n",
    "        M = self.Mmm[i].astype(np.int64, copy=False)\n",
    "        G = self.G[i]\n",
    "        return (\n",
    "            torch.from_numpy(X),\n",
    "            torch.from_numpy(B),\n",
    "            torch.from_numpy(M),\n",
    "            torch.from_numpy(G),\n",
    "        )\n",
    "\n",
    "def make_loader(ds, batch_size=64):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds, batch_size=int(batch_size), shuffle=False,\n",
    "        num_workers=0, pin_memory=False, drop_last=False\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Safe/compat checkpoint loader\n",
    "# ----------------------------\n",
    "def torch_load_compat(path: Path):\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "        # if dict with metadata, keep\n",
    "        if isinstance(obj, dict) and (\"model_state\" in obj or \"cfg\" in obj or \"global_scaler\" in obj or \"global_cols\" in obj):\n",
    "            return obj\n",
    "        # likely pure state_dict -> reload full\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        return torch.load(path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "def extract_state_and_meta(ckpt_obj):\n",
    "    \"\"\"\n",
    "    Returns: state_dict, meta_dict\n",
    "    - if ckpt is full dict -> state_dict=ckpt[\"model_state\"], meta=ckpt\n",
    "    - if ckpt is pure state_dict -> state_dict=ckpt, meta={}\n",
    "    \"\"\"\n",
    "    if isinstance(ckpt_obj, dict) and \"model_state\" in ckpt_obj and isinstance(ckpt_obj[\"model_state\"], dict):\n",
    "        return ckpt_obj[\"model_state\"], ckpt_obj\n",
    "    if isinstance(ckpt_obj, dict):\n",
    "        # could be pure state_dict OR a metadata dict without model_state\n",
    "        # assume pure state_dict if it contains tensor-ish values\n",
    "        any_tensor = any(hasattr(v, \"shape\") for v in ckpt_obj.values())\n",
    "        if any_tensor:\n",
    "            return ckpt_obj, {}\n",
    "        return ckpt_obj, ckpt_obj\n",
    "    raise RuntimeError(f\"Unsupported ckpt object type: {type(ckpt_obj)}\")\n",
    "\n",
    "# checkpoints\n",
    "ckpts = []\n",
    "for f in range(int(n_splits)):\n",
    "    p = CKPT_DIR / f\"fold_{f}.pt\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n",
    "    ckpts.append(p)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Infer architecture from state_dict (IMPORTANT)\n",
    "# ----------------------------\n",
    "def infer_from_state(sd: dict):\n",
    "    keys = set(sd.keys())\n",
    "\n",
    "    # d_model & n_bands from band_emb\n",
    "    if \"band_emb.weight\" not in sd:\n",
    "        raise RuntimeError(\"state_dict missing band_emb.weight (ckpt tidak cocok dengan model ini).\")\n",
    "    n_bands = int(sd[\"band_emb.weight\"].shape[0])\n",
    "    d_model = int(sd[\"band_emb.weight\"].shape[1])\n",
    "\n",
    "    # max_len from pos_emb\n",
    "    if \"pos_emb\" not in sd:\n",
    "        raise RuntimeError(\"state_dict missing pos_emb.\")\n",
    "    max_len_ckpt = int(sd[\"pos_emb\"].shape[1])\n",
    "\n",
    "    # feat_dim from x_proj\n",
    "    xproj_is_seq = (\"x_proj.0.weight\" in keys)\n",
    "    if xproj_is_seq:\n",
    "        feat_dim = int(sd[\"x_proj.0.weight\"].shape[1])\n",
    "    else:\n",
    "        if \"x_proj.weight\" not in sd:\n",
    "            raise RuntimeError(\"state_dict missing x_proj.weight or x_proj.0.weight.\")\n",
    "        feat_dim = int(sd[\"x_proj.weight\"].shape[1])\n",
    "\n",
    "    # g_dim from g_proj\n",
    "    g_dim = None\n",
    "    if \"g_proj.0.weight\" in sd:\n",
    "        g_dim = int(sd[\"g_proj.0.weight\"].shape[1])\n",
    "        g_hidden = int(sd[\"g_proj.0.weight\"].shape[0])\n",
    "    else:\n",
    "        # fallback: infer from head input\n",
    "        g_dim = 0\n",
    "        g_hidden = d_model // 2\n",
    "\n",
    "    # encoder layers count + dim_ff\n",
    "    layer_ids = set()\n",
    "    dim_ff = None\n",
    "    for k in keys:\n",
    "        m = re.match(r\"encoder\\.layers\\.(\\d+)\\.\", k)\n",
    "        if m:\n",
    "            layer_ids.add(int(m.group(1)))\n",
    "    n_layers = (max(layer_ids) + 1) if layer_ids else 0\n",
    "    if n_layers <= 0:\n",
    "        raise RuntimeError(\"Cannot infer n_layers from state_dict (encoder.layers.* not found).\")\n",
    "\n",
    "    # dim_feedforward from linear1.weight\n",
    "    k_lin1 = f\"encoder.layers.0.linear1.weight\"\n",
    "    if k_lin1 in sd:\n",
    "        dim_ff = int(sd[k_lin1].shape[0])\n",
    "    else:\n",
    "        # older/newer naming unlikely, but try search\n",
    "        lin1_keys = [k for k in keys if k.endswith(\"linear1.weight\")]\n",
    "        if not lin1_keys:\n",
    "            raise RuntimeError(\"Cannot infer dim_feedforward (linear1.weight not found).\")\n",
    "        dim_ff = int(sd[sorted(lin1_keys)[0]].shape[0])\n",
    "\n",
    "    # optional pool_ln\n",
    "    has_pool_ln = (\"pool_ln.weight\" in keys and \"pool_ln.bias\" in keys)\n",
    "\n",
    "    # head layout (detect weight indices)\n",
    "    head_w_idx = []\n",
    "    for k in keys:\n",
    "        m = re.match(r\"head\\.(\\d+)\\.weight\", k)\n",
    "        if m:\n",
    "            head_w_idx.append(int(m.group(1)))\n",
    "    head_w_idx = sorted(set(head_w_idx))\n",
    "    if not head_w_idx:\n",
    "        raise RuntimeError(\"Cannot infer head structure (head.*.weight not found).\")\n",
    "\n",
    "    # determine final linear index\n",
    "    # common patterns: weights at {0,3} or {0,2} or {0,1}\n",
    "    final_idx = max(head_w_idx)\n",
    "\n",
    "    return {\n",
    "        \"n_bands\": n_bands,\n",
    "        \"d_model\": d_model,\n",
    "        \"max_len_ckpt\": max_len_ckpt,\n",
    "        \"feat_dim\": feat_dim,\n",
    "        \"g_dim\": g_dim,\n",
    "        \"g_hidden\": g_hidden,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"dim_ff\": dim_ff,\n",
    "        \"has_pool_ln\": has_pool_ln,\n",
    "        \"xproj_is_seq\": xproj_is_seq,\n",
    "        \"head_final_idx\": final_idx,\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Build model matching ckpt\n",
    "# ----------------------------\n",
    "class FlexMultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, max_len, n_bands, d_model, n_heads, n_layers, dim_ff, dropout, g_dim, g_hidden,\n",
    "                 xproj_is_seq=False, has_pool_ln=False, head_final_idx=3):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        if xproj_is_seq:\n",
    "            self.x_proj = nn.Sequential(nn.Linear(feat_dim, d_model))\n",
    "        else:\n",
    "            self.x_proj = nn.Linear(feat_dim, d_model)\n",
    "\n",
    "        self.band_emb = nn.Embedding(n_bands, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(dim_ff),\n",
    "            dropout=float(dropout),\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=int(n_layers))\n",
    "\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "\n",
    "        # keep same param naming for g_proj: g_proj.0.weight/bias\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(int(g_dim), int(g_hidden)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "        )\n",
    "\n",
    "        self.has_pool_ln = bool(has_pool_ln)\n",
    "        if self.has_pool_ln:\n",
    "            self.pool_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        # head structure based on detected final idx\n",
    "        # final_idx=3 => Linear, GELU, Dropout, Linear\n",
    "        # final_idx=2 => Linear, GELU, Linear\n",
    "        # final_idx=1 => Linear, Linear\n",
    "        in_head = int(d_model + g_hidden)\n",
    "        if head_final_idx == 3:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, d_model),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(float(dropout)),\n",
    "                nn.Linear(d_model, 1),\n",
    "            )\n",
    "        elif head_final_idx == 2:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, d_model),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_model, 1),\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, d_model),\n",
    "                nn.Linear(d_model, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        pad_mask = (mask == 0)\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        if self.has_pool_ln:\n",
    "            pooled = self.pool_ln(pooled)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        return self.head(z).squeeze(-1)  # logit\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_logits(model, loader):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.inference_mode():\n",
    "        for Xb, Bb, Mb, Gb in loader:\n",
    "            logit = model(Xb.to(device), Bb.to(device), Mb.to(device), Gb.to(device))\n",
    "            outs.append(logit.detach().cpu().numpy())\n",
    "    return (np.concatenate(outs, axis=0) if outs else np.zeros((0,), dtype=np.float32)).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Build G features for test (best-effort)\n",
    "# ----------------------------\n",
    "def load_global_cols_fallback():\n",
    "    # try explicit GLOBAL_COLS_PATH (if you saved it)\n",
    "    p = globals().get(\"GLOBAL_COLS_PATH\", None)\n",
    "    if p is not None:\n",
    "        p = Path(p)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                j = json.load(open(p, \"r\", encoding=\"utf-8\"))\n",
    "                if isinstance(j, dict) and \"cols\" in j and isinstance(j[\"cols\"], list):\n",
    "                    return [str(x) for x in j[\"cols\"]]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # try LOG_DIR/global_feature_cols.json\n",
    "    log_dir = Path(globals().get(\"LOG_DIR\", ART_DIR.parent / \"logs\"))\n",
    "    p2 = log_dir / \"global_feature_cols.json\"\n",
    "    if p2.exists():\n",
    "        try:\n",
    "            j = json.load(open(p2, \"r\", encoding=\"utf-8\"))\n",
    "            if isinstance(j, dict) and \"cols\" in j and isinstance(j[\"cols\"], list):\n",
    "                return [str(x) for x in j[\"cols\"]]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Inference per fold -> LOGIT ensemble\n",
    "# ----------------------------\n",
    "BATCH_SIZE = 64\n",
    "test_logit_folds = np.zeros((NTE, int(n_splits)), dtype=np.float32)\n",
    "\n",
    "print(f\"[Stage 10] Test inference (AUTO-ARCH + LOGIT ensemble): N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} | CPU\")\n",
    "\n",
    "arch_used = None\n",
    "gcols_used = None\n",
    "\n",
    "for fold, ckpt_path in enumerate(ckpts):\n",
    "    ckpt_obj = torch_load_compat(ckpt_path)\n",
    "    sd, meta = extract_state_and_meta(ckpt_obj)\n",
    "\n",
    "    arch = infer_from_state(sd)\n",
    "    if arch_used is None:\n",
    "        arch_used = dict(arch)\n",
    "\n",
    "    # choose n_heads (must divide d_model; if cfg exists use it)\n",
    "    cfg = meta.get(\"cfg\", {}) if isinstance(meta, dict) else {}\n",
    "    n_heads = int(cfg.get(\"n_heads\", 0)) if isinstance(cfg, dict) else 0\n",
    "    if n_heads <= 0 or (arch[\"d_model\"] % n_heads != 0):\n",
    "        # fallback: pick a divisor\n",
    "        for h in [8, 4, 2, 1, 16, 32]:\n",
    "            if arch[\"d_model\"] % h == 0:\n",
    "                n_heads = h\n",
    "                break\n",
    "        if n_heads <= 0:\n",
    "            n_heads = 4\n",
    "\n",
    "    # HARD SCHEMA CHECKS to avoid silent mismatch\n",
    "    if arch[\"feat_dim\"] != Fdim:\n",
    "        raise RuntimeError(\n",
    "            f\"Fold {fold}: feature_dim mismatch.\\n\"\n",
    "            f\"- ckpt expects feat_dim={arch['feat_dim']}\\n\"\n",
    "            f\"- memmap has Fdim={Fdim}\\n\"\n",
    "            \"Solusi: pastikan STAGE 6 feature list sama saat training ckpt dibuat.\"\n",
    "        )\n",
    "    if arch[\"max_len_ckpt\"] != L:\n",
    "        raise RuntimeError(\n",
    "            f\"Fold {fold}: max_len mismatch.\\n\"\n",
    "            f\"- ckpt max_len={arch['max_len_ckpt']}\\n\"\n",
    "            f\"- memmap MAX_LEN={L}\\n\"\n",
    "        )\n",
    "\n",
    "    # Determine global cols\n",
    "    G_COLS = meta.get(\"global_cols\", None) if isinstance(meta, dict) else None\n",
    "    if G_COLS is None:\n",
    "        G_COLS = load_global_cols_fallback()\n",
    "\n",
    "    # Build G_np with correct g_dim\n",
    "    g_dim = int(arch[\"g_dim\"])\n",
    "    if G_COLS is not None and isinstance(G_COLS, (list, tuple)) and len(G_COLS) > 0:\n",
    "        G_COLS = [str(x) for x in G_COLS]\n",
    "        # adjust length to g_dim\n",
    "        if len(G_COLS) > g_dim:\n",
    "            G_COLS = G_COLS[:g_dim]\n",
    "        for c in G_COLS:\n",
    "            if c not in df_test_meta.columns:\n",
    "                df_test_meta[c] = 0.0\n",
    "        G_raw = df_test_meta.loc[test_ids, G_COLS].copy()\n",
    "        for c in G_COLS:\n",
    "            G_raw[c] = pd.to_numeric(G_raw[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "        G_np = G_raw.to_numpy(dtype=np.float32, copy=False)\n",
    "        if G_np.shape[1] < g_dim:\n",
    "            pad = np.zeros((NTE, g_dim - G_np.shape[1]), dtype=np.float32)\n",
    "            G_np = np.concatenate([G_np, pad], axis=1)\n",
    "    else:\n",
    "        # last resort: take first g_dim numeric columns from df_test_meta\n",
    "        cand = []\n",
    "        for c in df_test_meta.columns:\n",
    "            if c in (\"target\", \"split\", \"object_id\"):\n",
    "                continue\n",
    "            if pd.api.types.is_numeric_dtype(df_test_meta[c]):\n",
    "                cand.append(c)\n",
    "        use = cand[:g_dim]\n",
    "        if len(use) > 0:\n",
    "            G_raw = df_test_meta.loc[test_ids, use].copy()\n",
    "            for c in use:\n",
    "                G_raw[c] = pd.to_numeric(G_raw[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "            G_np = G_raw.to_numpy(dtype=np.float32, copy=False)\n",
    "        else:\n",
    "            G_np = np.zeros((NTE, 0), dtype=np.float32)\n",
    "\n",
    "        if G_np.shape[1] < g_dim:\n",
    "            pad = np.zeros((NTE, g_dim - G_np.shape[1]), dtype=np.float32)\n",
    "            G_np = np.concatenate([G_np, pad], axis=1)\n",
    "\n",
    "        G_COLS = use if len(use) else [f\"_auto_{i}\" for i in range(g_dim)]\n",
    "\n",
    "    # Apply scaler if available (meta dict)\n",
    "    scaler = meta.get(\"global_scaler\", None) if isinstance(meta, dict) else None\n",
    "    if scaler is not None and isinstance(scaler, dict) and (\"mean\" in scaler) and (\"std\" in scaler):\n",
    "        g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32).reshape(-1)\n",
    "        g_std  = np.asarray(scaler[\"std\"],  dtype=np.float32).reshape(-1)\n",
    "        g_std  = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n",
    "        if g_mean.shape[0] == g_dim and g_std.shape[0] == g_dim:\n",
    "            G_np_z = ((G_np - g_mean) / g_std).astype(np.float32)\n",
    "        else:\n",
    "            G_np_z = G_np.astype(np.float32, copy=False)\n",
    "    else:\n",
    "        G_np_z = G_np.astype(np.float32, copy=False)\n",
    "\n",
    "    if gcols_used is None:\n",
    "        gcols_used = list(G_COLS)\n",
    "\n",
    "    # Build model to match ckpt\n",
    "    model = FlexMultibandEventTransformer(\n",
    "        feat_dim=arch[\"feat_dim\"],\n",
    "        max_len=arch[\"max_len_ckpt\"],\n",
    "        n_bands=arch[\"n_bands\"],\n",
    "        d_model=arch[\"d_model\"],\n",
    "        n_heads=n_heads,\n",
    "        n_layers=arch[\"n_layers\"],\n",
    "        dim_ff=arch[\"dim_ff\"],\n",
    "        dropout=float(cfg.get(\"dropout\", 0.0)) if isinstance(cfg, dict) else 0.0,\n",
    "        g_dim=g_dim,\n",
    "        g_hidden=arch[\"g_hidden\"],\n",
    "        xproj_is_seq=arch[\"xproj_is_seq\"],\n",
    "        has_pool_ln=arch[\"has_pool_ln\"],\n",
    "        head_final_idx=arch[\"head_final_idx\"],\n",
    "    ).to(device)\n",
    "\n",
    "    # load weights\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "\n",
    "    # predict logits\n",
    "    ds_test = TestMemmapDataset(Xte, Bte, Mte, G_np_z)\n",
    "    dl_test = make_loader(ds_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    logits = predict_logits(model, dl_test)\n",
    "    if len(logits) != NTE:\n",
    "        raise RuntimeError(f\"Fold {fold}: logits length mismatch {len(logits)} vs {NTE}\")\n",
    "\n",
    "    test_logit_folds[:, fold] = logits\n",
    "    probs_tmp = sigmoid_np(logits)\n",
    "    print(f\"  fold {fold}: d_model={arch['d_model']} g_dim={g_dim} | logit_mean={float(logits.mean()):.6f} | prob_mean={float(probs_tmp.mean()):.6f} | prob_std={float(probs_tmp.std()):.6f}\")\n",
    "\n",
    "    del model, ds_test, dl_test, logits, probs_tmp, G_np, G_np_z\n",
    "    gc.collect()\n",
    "\n",
    "# ensemble on logits\n",
    "test_logit_ens = test_logit_folds.mean(axis=1).astype(np.float32)\n",
    "test_prob_folds = sigmoid_np(test_logit_folds).astype(np.float32)\n",
    "test_prob_ens   = sigmoid_np(test_logit_ens).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Save artifacts\n",
    "# ----------------------------\n",
    "logit_fold_path = OUT_DIR / \"test_logit_folds.npy\"\n",
    "logit_ens_path  = OUT_DIR / \"test_logit_ens.npy\"\n",
    "prob_fold_path  = OUT_DIR / \"test_prob_folds.npy\"\n",
    "prob_ens_path   = OUT_DIR / \"test_prob_ens.npy\"\n",
    "csv_path        = OUT_DIR / \"test_prob_ens.csv\"\n",
    "cfg_path        = OUT_DIR / \"test_infer_config.json\"\n",
    "\n",
    "np.save(logit_fold_path, test_logit_folds)\n",
    "np.save(logit_ens_path,  test_logit_ens)\n",
    "np.save(prob_fold_path,  test_prob_folds)\n",
    "np.save(prob_ens_path,   test_prob_ens)\n",
    "\n",
    "pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens}).to_csv(csv_path, index=False)\n",
    "\n",
    "infer_cfg = {\n",
    "    \"seed\": int(SEED),\n",
    "    \"n_splits\": int(n_splits),\n",
    "    \"ensemble\": \"mean_logits_then_sigmoid\",\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "    \"max_len\": int(L),\n",
    "    \"feature_dim\": int(Fdim),\n",
    "    \"feature_names\": list(SEQ_FEATURE_NAMES),\n",
    "    \"ckpt_dir\": str(CKPT_DIR),\n",
    "    \"ckpts\": [str(p) for p in ckpts],\n",
    "    \"arch_inferred_from_first_fold\": arch_used,\n",
    "    \"global_cols_used_first_fold\": gcols_used,\n",
    "    \"outputs\": {\n",
    "        \"test_logit_folds\": str(logit_fold_path),\n",
    "        \"test_logit_ens\": str(logit_ens_path),\n",
    "        \"test_prob_folds\": str(prob_fold_path),\n",
    "        \"test_prob_ens\": str(prob_ens_path),\n",
    "        \"test_prob_ens_csv\": str(csv_path),\n",
    "    }\n",
    "}\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(infer_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 10] DONE\")\n",
    "print(f\"- Saved logits folds: {logit_fold_path}\")\n",
    "print(f\"- Saved logits ens  : {logit_ens_path}\")\n",
    "print(f\"- Saved probs folds : {prob_fold_path}\")\n",
    "print(f\"- Saved probs ens   : {prob_ens_path}\")\n",
    "print(f\"- Saved csv         : {csv_path}\")\n",
    "print(f\"- Saved config      : {cfg_path}\")\n",
    "print(f\"- ens prob mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n",
    "\n",
    "# Export globals for submission stage\n",
    "globals().update({\n",
    "    \"test_ids\": test_ids,\n",
    "    \"test_logit_folds\": test_logit_folds,\n",
    "    \"test_logit_ens\": test_logit_ens,\n",
    "    \"test_prob_folds\": test_prob_folds,\n",
    "    \"test_prob_ens\": test_prob_ens,\n",
    "    \"TEST_LOGIT_FOLDS_PATH\": logit_fold_path,\n",
    "    \"TEST_LOGIT_ENS_PATH\": logit_ens_path,\n",
    "    \"TEST_PROB_FOLDS_PATH\": prob_fold_path,\n",
    "    \"TEST_PROB_ENS_PATH\": prob_ens_path,\n",
    "    \"TEST_PROB_CSV_PATH\": csv_path,\n",
    "    \"TEST_INFER_CFG_PATH\": cfg_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f3f72",
   "metadata": {
    "papermill": {
     "duration": 0.017499,
     "end_time": "2026-01-02T20:42:40.518299",
     "exception": false,
     "start_time": "2026-01-02T20:42:40.500800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8437cb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T20:42:40.559023Z",
     "iopub.status.busy": "2026-01-02T20:42:40.558690Z",
     "iopub.status.idle": "2026-01-02T20:42:40.851114Z",
     "shell.execute_reply": "2026-01-02T20:42:40.850290Z"
    },
    "papermill": {
     "duration": 0.317576,
     "end_time": "2026-01-02T20:42:40.853083",
     "exception": false,
     "start_time": "2026-01-02T20:42:40.535507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION (OOF) — Precision/Recall/F1\n",
      "- N=3,043 | pos=148 | neg=2,895 | pos%=4.8636%\n",
      "- ROC-AUC=0.863769 | PR-AUC=0.243643\n",
      "\n",
      "Baseline @ thr=0.5\n",
      "- F1=0.156352 | P=0.085006 | R=0.972973 | ACC=0.489320\n",
      "  tp=144 fp=1550 fn=4 tn=1345 | pos_pred=1694\n",
      "\n",
      "BEST-F1  @ thr=0.970000\n",
      "- F1=0.331512 | P=0.226933 | R=0.614865 | ACC=0.879395\n",
      "  tp=91 fp=310 fn=57 tn=2585 | pos_pred=401\n",
      "\n",
      "BEST-F0.5 @ thr=0.991352 (lebih condong precision)\n",
      "- F0.5=0.283019 | P=0.295082 | R=0.243243 | F1=0.266667\n",
      "\n",
      "BEST-F2   @ thr=0.961601 (lebih condong recall)\n",
      "- F2=0.458758 | P=0.203285 | R=0.668919 | F1=0.311811\n",
      "\n",
      "Top 10 thresholds by F1:\n",
      "01. thr=0.970000 | f1=0.331512 | f0.5=0.259703 | f2=0.458207 | P=0.226933 R=0.614865 | tp=91 fp=310 fn=57 | pos_pred=401\n",
      "02. thr=0.970762 | f1=0.330882 | f0.5=0.259815 | f2=0.455466 | P=0.227273 R=0.608108 | tp=90 fp=306 fn=58 | pos_pred=396\n",
      "03. thr=0.975000 | f1=0.329317 | f0.5=0.264858 | f2=0.435244 | P=0.234286 R=0.554054 | tp=82 fp=268 fn=66 | pos_pred=350\n",
      "04. thr=0.973192 | f1=0.326848 | f0.5=0.260546 | f2=0.438413 | P=0.229508 R=0.567568 | tp=84 fp=282 fn=64 | pos_pred=366\n",
      "05. thr=0.976376 | f1=0.322981 | f0.5=0.262097 | f2=0.420712 | P=0.232836 R=0.527027 | tp=78 fp=257 fn=70 | pos_pred=335\n",
      "06. thr=0.978806 | f1=0.317881 | f0.5=0.263158 | f2=0.401338 | P=0.236066 R=0.486486 | tp=72 fp=233 fn=76 | pos_pred=305\n",
      "07. thr=0.967391 | f1=0.317073 | f0.5=0.245680 | f2=0.446955 | P=0.213615 R=0.614865 | tp=91 fp=335 fn=57 | pos_pred=426\n",
      "08. thr=0.980000 | f1=0.316742 | f0.5=0.264350 | f2=0.395034 | P=0.238095 R=0.472973 | tp=70 fp=224 fn=78 | pos_pred=294\n",
      "09. thr=0.961601 | f1=0.311811 | f0.5=0.236164 | f2=0.458758 | P=0.203285 R=0.668919 | tp=99 fp=388 fn=49 | pos_pred=487\n",
      "10. thr=0.981961 | f1=0.308057 | f0.5=0.261254 | f2=0.375289 | P=0.237226 R=0.439189 | tp=65 fp=209 fn=83 | pos_pred=274\n",
      "\n",
      "Saved:\n",
      "- /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/eval_report.txt\n",
      "- /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/eval_threshold_table.csv\n",
      "- /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/oof/eval_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ONE CELL — EVALUATION (Precision / Recall / F1) + Threshold Sweep (OOF)\n",
    "# (REVISI FULL v2 — Robust Align + Sanitize + Fbeta option + AUC optional)\n",
    "#\n",
    "# Input minimal:\n",
    "# - df_train_meta (kolom: target; index: object_id)\n",
    "# - oof_prob (globals) ATAU file OOF_DIR/oof_prob.npy ATAU OOF_DIR/oof_prob.csv\n",
    "#\n",
    "# Output:\n",
    "# - Print ringkasan metrik\n",
    "# - Save: eval_report.txt + eval_threshold_table.csv + eval_summary.json\n",
    "# - Export globals: BEST_THR_F1, BEST_THR_F05, BEST_THR_F2, thr_table_eval\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal\n",
    "# ----------------------------\n",
    "if \"df_train_meta\" not in globals():\n",
    "    raise RuntimeError(\"Missing df_train_meta. Jalankan STAGE 2 dulu (meta).\")\n",
    "\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "OOF_DIR = Path(globals().get(\"OOF_DIR\", ART_DIR / \"oof\"))\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Utils: id normalize + robust 1D float32\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _sanitize_prob(p):\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    return p.astype(np.float32)\n",
    "\n",
    "# ensure meta index normalized (no data copy)\n",
    "df_train_meta = df_train_meta.copy(deep=False)\n",
    "df_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load oof_prob (prefer csv if exists for alignment)\n",
    "# ----------------------------\n",
    "def load_oof():\n",
    "    # If oof_prob.csv exists -> most robust (has object_id)\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n",
    "            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "            p = _sanitize_prob(_as_1d_float32(df[\"oof_prob\"].to_numpy()))\n",
    "            # ensure same length\n",
    "            if len(p) != len(df):\n",
    "                raise RuntimeError(\"oof_prob.csv: length mismatch after parsing.\")\n",
    "            return p, df\n",
    "\n",
    "    # globals\n",
    "    if \"oof_prob\" in globals():\n",
    "        p = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        if isinstance(p, np.ndarray) and p.ndim != 0:\n",
    "            return _sanitize_prob(p), None\n",
    "\n",
    "    # npy\n",
    "    pnpy = OOF_DIR / \"oof_prob.npy\"\n",
    "    if pnpy.exists():\n",
    "        p = _sanitize_prob(_as_1d_float32(np.load(pnpy, allow_pickle=False)))\n",
    "        return p, None\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob tidak ditemukan (oof_prob.csv / globals oof_prob / oof_prob.npy). Jalankan STAGE 8 dulu.\")\n",
    "\n",
    "oof_prob, df_oof_csv = load_oof()\n",
    "if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "    raise TypeError(f\"Invalid oof_prob (scalar/unsized). type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Align y (target) ke urutan oof_prob\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "y = None\n",
    "\n",
    "if df_oof_csv is not None:\n",
    "    train_ids = df_oof_csv[\"object_id\"].tolist()\n",
    "\n",
    "    # drop rows whose id not in meta (avoid KeyError)\n",
    "    mask_ok = [oid in df_train_meta.index for oid in train_ids]\n",
    "    if not all(mask_ok):\n",
    "        bad = [train_ids[i] for i, ok in enumerate(mask_ok) if not ok][:10]\n",
    "        n_bad = int((~np.asarray(mask_ok, dtype=bool)).sum())\n",
    "        print(f\"[WARN] oof_prob.csv contains ids not in df_train_meta: missing_n={n_bad} examples={bad}\")\n",
    "        df_oof_csv = df_oof_csv.loc[mask_ok].reset_index(drop=True)\n",
    "        train_ids = df_oof_csv[\"object_id\"].tolist()\n",
    "        oof_prob = _sanitize_prob(_as_1d_float32(df_oof_csv[\"oof_prob\"].to_numpy()))\n",
    "\n",
    "    y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# fallback: train_ids_ordered\n",
    "if y is None and (\"train_ids_ordered\" in globals()):\n",
    "    ids = [_norm_id(z) for z in list(globals()[\"train_ids_ordered\"])]\n",
    "    if len(ids) == len(oof_prob):\n",
    "        missing = [oid for oid in ids if oid not in df_train_meta.index]\n",
    "        if missing:\n",
    "            raise KeyError(f\"train_ids_ordered contains ids not in df_train_meta. examples={missing[:10]} missing_n={len(missing)}\")\n",
    "        train_ids = ids\n",
    "        y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# last fallback: meta order must match length\n",
    "if y is None:\n",
    "    if len(oof_prob) != len(df_train_meta):\n",
    "        raise RuntimeError(\n",
    "            f\"Tidak bisa align y. len(oof_prob)={len(oof_prob)} != len(df_train_meta)={len(df_train_meta)} \"\n",
    "            \"dan tidak ada oof_prob.csv (object_id).\"\n",
    "        )\n",
    "    train_ids = df_train_meta.index.astype(str).tolist()\n",
    "    y = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "if len(y) != len(oof_prob):\n",
    "    raise RuntimeError(f\"Length mismatch: y={len(y)} vs oof_prob={len(oof_prob)}\")\n",
    "\n",
    "# y sanity\n",
    "uy = set(np.unique(y).tolist())\n",
    "if not uy.issubset({0, 1}):\n",
    "    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Metrics sesuai materi (TP/FP/FN -> P/R/F1)\n",
    "# ----------------------------\n",
    "def prf_from_pred(y_true, y_pred01):\n",
    "    y_true = np.asarray(y_true, dtype=np.int32)\n",
    "    y_pred01 = np.asarray(y_pred01, dtype=np.int32)\n",
    "\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred01 == 0)).sum())\n",
    "\n",
    "    precision = tp / max(tp + fp, 1)   # TP/(TP+FP)\n",
    "    recall    = tp / max(tp + fn, 1)   # TP/(TP+FN)\n",
    "    f1 = 0.0 if (precision + recall) == 0 else (2.0 * precision * recall / (precision + recall))\n",
    "\n",
    "    return {\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"pos_pred\": int(y_pred01.sum()),\n",
    "        \"acc\": float((tp + tn) / max(len(y_true), 1)),\n",
    "    }\n",
    "\n",
    "def fbeta_from_pr(precision, recall, beta=1.0):\n",
    "    b2 = beta * beta\n",
    "    denom = (b2 * precision + recall)\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    return float((1 + b2) * precision * recall / denom)\n",
    "\n",
    "def eval_at_threshold(prob, y_true, thr):\n",
    "    pred = (prob >= float(thr)).astype(np.int8)\n",
    "    met = prf_from_pred(y_true, pred)\n",
    "    met[\"thr\"] = float(thr)\n",
    "    met[\"f0.5\"] = fbeta_from_pr(met[\"precision\"], met[\"recall\"], beta=0.5)\n",
    "    met[\"f2\"]   = fbeta_from_pr(met[\"precision\"], met[\"recall\"], beta=2.0)\n",
    "    return met\n",
    "\n",
    "# optional AUC (if sklearn exists)\n",
    "roc_auc = None\n",
    "pr_auc  = None\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    # only valid if both classes exist\n",
    "    if (y.max() == 1) and (y.min() == 0):\n",
    "        roc_auc = float(roc_auc_score(y, oof_prob))\n",
    "        pr_auc  = float(average_precision_score(y, oof_prob))\n",
    "except Exception:\n",
    "    roc_auc = None\n",
    "    pr_auc  = None\n",
    "\n",
    "# Baseline thr=0.5\n",
    "base = eval_at_threshold(oof_prob, y, 0.5)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Threshold sweep (lebih stabil: grid + quantile)\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.01, 0.10, 19),\n",
    "    np.linspace(0.10, 0.90, 81),\n",
    "    np.linspace(0.90, 0.99, 19),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.01, 0.99, 99, dtype=np.float32)\n",
    "quant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n",
    "\n",
    "thr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr]), 0.0, 1.0)).astype(np.float32)\n",
    "\n",
    "rows = []\n",
    "best_f1  = base.copy()\n",
    "best_f05 = base.copy()\n",
    "best_f2  = base.copy()\n",
    "\n",
    "for thr in thr_candidates:\n",
    "    met = eval_at_threshold(oof_prob, y, float(thr))\n",
    "    rows.append([\n",
    "        met[\"thr\"], met[\"f1\"], met[\"f0.5\"], met[\"f2\"],\n",
    "        met[\"precision\"], met[\"recall\"], met[\"acc\"],\n",
    "        met[\"tp\"], met[\"fp\"], met[\"fn\"], met[\"tn\"], met[\"pos_pred\"]\n",
    "    ])\n",
    "\n",
    "    # best F1 (tie-break: recall higher, then fp lower)\n",
    "    if (met[\"f1\"] > best_f1[\"f1\"] + 1e-12) or (\n",
    "        abs(met[\"f1\"] - best_f1[\"f1\"]) <= 1e-12 and (met[\"recall\"] > best_f1[\"recall\"] + 1e-12)\n",
    "    ) or (\n",
    "        abs(met[\"f1\"] - best_f1[\"f1\"]) <= 1e-12 and abs(met[\"recall\"] - best_f1[\"recall\"]) <= 1e-12 and (met[\"fp\"] < best_f1[\"fp\"])\n",
    "    ):\n",
    "        best_f1 = met.copy()\n",
    "\n",
    "    # best F0.5 (precision-leaning)\n",
    "    if met[\"f0.5\"] > best_f05.get(\"f0.5\", -1) + 1e-12:\n",
    "        best_f05 = met.copy()\n",
    "\n",
    "    # best F2 (recall-leaning)\n",
    "    if met[\"f2\"] > best_f2.get(\"f2\", -1) + 1e-12:\n",
    "        best_f2 = met.copy()\n",
    "\n",
    "thr_table = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"thr\",\"f1\",\"f0.5\",\"f2\",\"precision\",\"recall\",\"acc\",\"tp\",\"fp\",\"fn\",\"tn\",\"pos_pred\"]\n",
    ").sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "BEST_THR_F1  = float(best_f1[\"thr\"])\n",
    "BEST_THR_F05 = float(best_f05[\"thr\"])\n",
    "BEST_THR_F2  = float(best_f2[\"thr\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Print report\n",
    "# ----------------------------\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "N = int(len(y))\n",
    "\n",
    "print(\"EVALUATION (OOF) — Precision/Recall/F1\")\n",
    "print(f\"- N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.4f}%\")\n",
    "if roc_auc is not None:\n",
    "    print(f\"- ROC-AUC={roc_auc:.6f} | PR-AUC={pr_auc:.6f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Baseline @ thr=0.5\")\n",
    "print(f\"- F1={base['f1']:.6f} | P={base['precision']:.6f} | R={base['recall']:.6f} | ACC={base['acc']:.6f}\")\n",
    "print(f\"  tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\\n\")\n",
    "\n",
    "print(f\"BEST-F1  @ thr={BEST_THR_F1:.6f}\")\n",
    "print(f\"- F1={best_f1['f1']:.6f} | P={best_f1['precision']:.6f} | R={best_f1['recall']:.6f} | ACC={best_f1['acc']:.6f}\")\n",
    "print(f\"  tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\\n\")\n",
    "\n",
    "print(f\"BEST-F0.5 @ thr={BEST_THR_F05:.6f} (lebih condong precision)\")\n",
    "print(f\"- F0.5={best_f05['f0.5']:.6f} | P={best_f05['precision']:.6f} | R={best_f05['recall']:.6f} | F1={best_f05['f1']:.6f}\\n\")\n",
    "\n",
    "print(f\"BEST-F2   @ thr={BEST_THR_F2:.6f} (lebih condong recall)\")\n",
    "print(f\"- F2={best_f2['f2']:.6f} | P={best_f2['precision']:.6f} | R={best_f2['recall']:.6f} | F1={best_f2['f1']:.6f}\\n\")\n",
    "\n",
    "print(\"Top 10 thresholds by F1:\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    print(f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | f0.5={r['f0.5']:.6f} | f2={r['f2']:.6f} | \"\n",
    "          f\"P={r['precision']:.6f} R={r['recall']:.6f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save artifacts\n",
    "# ----------------------------\n",
    "out_txt  = OOF_DIR / \"eval_report.txt\"\n",
    "out_csv  = OOF_DIR / \"eval_threshold_table.csv\"\n",
    "out_json = OOF_DIR / \"eval_summary.json\"\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Evaluation Report (Precision/Recall/F1)\")\n",
    "lines.append(f\"N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "if roc_auc is not None:\n",
    "    lines.append(f\"ROC-AUC={roc_auc:.8f} | PR-AUC={pr_auc:.8f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"F1={base['f1']:.8f} | P={base['precision']:.8f} | R={base['recall']:.8f} | ACC={base['acc']:.8f}\")\n",
    "lines.append(f\"tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F1 @ thr={BEST_THR_F1:.8f}\")\n",
    "lines.append(f\"F1={best_f1['f1']:.8f} | P={best_f1['precision']:.8f} | R={best_f1['recall']:.8f} | ACC={best_f1['acc']:.8f}\")\n",
    "lines.append(f\"tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F0.5 @ thr={BEST_THR_F05:.8f}\")\n",
    "lines.append(f\"F0.5={best_f05['f0.5']:.8f} | P={best_f05['precision']:.8f} | R={best_f05['recall']:.8f} | F1={best_f05['f1']:.8f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F2 @ thr={BEST_THR_F2:.8f}\")\n",
    "lines.append(f\"F2={best_f2['f2']:.8f} | P={best_f2['precision']:.8f} | R={best_f2['recall']:.8f} | F1={best_f2['f1']:.8f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 thresholds by F1:\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    lines.append(f\"{i+1:02d}. thr={r['thr']:.8f} | f1={r['f1']:.8f} | f0.5={r['f0.5']:.8f} | f2={r['f2']:.8f} | \"\n",
    "                 f\"P={r['precision']:.8f} R={r['recall']:.8f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "thr_table.to_csv(out_csv, index=False)\n",
    "\n",
    "payload = {\n",
    "    \"N\": N, \"pos\": pos, \"neg\": neg,\n",
    "    \"roc_auc\": roc_auc, \"pr_auc\": pr_auc,\n",
    "    \"baseline_thr_0p5\": base,\n",
    "    \"best_f1\": best_f1,\n",
    "    \"best_f0.5\": best_f05,\n",
    "    \"best_f2\": best_f2,\n",
    "    \"paths\": {\"report\": str(out_txt), \"table\": str(out_csv)}\n",
    "}\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(f\"- {out_txt}\")\n",
    "print(f\"- {out_csv}\")\n",
    "print(f\"- {out_json}\")\n",
    "\n",
    "# Export for next stages\n",
    "globals().update({\n",
    "    \"BEST_THR_F1\": BEST_THR_F1,\n",
    "    \"BEST_THR_F05\": BEST_THR_F05,\n",
    "    \"BEST_THR_F2\": BEST_THR_F2,\n",
    "    \"thr_table_eval\": thr_table,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794fb76",
   "metadata": {
    "papermill": {
     "duration": 0.017523,
     "end_time": "2026-01-02T20:42:40.888586",
     "exception": false,
     "start_time": "2026-01-02T20:42:40.871063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9301564a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T20:42:40.927455Z",
     "iopub.status.busy": "2026-01-02T20:42:40.927119Z",
     "iopub.status.idle": "2026-01-02T20:42:41.215972Z",
     "shell.execute_reply": "2026-01-02T20:42:41.215222Z"
    },
    "papermill": {
     "duration": 0.311635,
     "end_time": "2026-01-02T20:42:41.217757",
     "exception": false,
     "start_time": "2026-01-02T20:42:40.906122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11] SUBMISSION READY (BINARY 0/1)\n",
      "- threshold_used=0.970000\n",
      "- rows=7,135 | pos_pred=5,381 (75.4170%)\n",
      "- wrote: /kaggle/working/submission.csv\n",
      "- copy : /kaggle/working/mallorn_run/run_20260102_184148_9f34156418/submissions/submission.csv\n",
      "\n",
      "Preview:\n",
      "                   object_id  prediction\n",
      "    Eluwaith_Mithrim_nothrim           1\n",
      "          Eru_heledir_archam           1\n",
      "           Gonhir_anann_fuin           1\n",
      "Gwathuirim_haradrim_tegilbor           1\n",
      "            achas_minai_maen           1\n",
      "               adab_fae_gath           1\n",
      "             adel_draug_gaur           1\n",
      "     aderthad_cuil_galadhrim           1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 11 — Submission Build (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL v2\n",
    "#\n",
    "# Output wajib kompetisi:\n",
    "# - header: object_id,prediction\n",
    "# - prediction HARUS 0/1\n",
    "# - file utama: /kaggle/working/submission.csv\n",
    "# ============================================================\n",
    "\n",
    "import gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"SUB_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n",
    "\n",
    "sample_path = Path(PATHS[\"SAMPLE_SUB\"])\n",
    "if not sample_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n",
    "\n",
    "df_sub = pd.read_csv(sample_path)\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _sanitize_prob(p):\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    return p.astype(np.float32)\n",
    "\n",
    "def _load_ids_npy(path: Path):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    ids = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n",
    "    return [_norm_id(x) for x in ids]\n",
    "\n",
    "def _load_pred_df():\n",
    "    \"\"\"\n",
    "    Return df_pred with columns: object_id, prob\n",
    "    Priority:\n",
    "      A) globals: test_ids + test_prob_ens\n",
    "      B) csv: TEST_PROB_CSV_PATH / ART_DIR/test_prob_ens.csv\n",
    "      C) npy: FIX_DIR/test_ids.npy + ART_DIR/test_prob_ens.npy\n",
    "    \"\"\"\n",
    "    # ---- A) globals ----\n",
    "    if (\"test_prob_ens\" in globals()) and (globals()[\"test_prob_ens\"] is not None) and \\\n",
    "       (\"test_ids\" in globals()) and (globals()[\"test_ids\"] is not None):\n",
    "        try:\n",
    "            ids = [_norm_id(x) for x in list(globals()[\"test_ids\"])]\n",
    "            prob = _sanitize_prob(_as_1d_float32(globals()[\"test_prob_ens\"]))\n",
    "            if isinstance(prob, np.ndarray) and prob.ndim != 0 and len(ids) == len(prob) and len(ids) > 0:\n",
    "                return pd.DataFrame({\"object_id\": ids, \"prob\": prob})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- B) csv fallback (best if already aligned with object_id) ----\n",
    "    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "    cand_csv = []\n",
    "\n",
    "    if \"TEST_PROB_CSV_PATH\" in globals() and globals()[\"TEST_PROB_CSV_PATH\"] is not None:\n",
    "        cand_csv.append(Path(globals()[\"TEST_PROB_CSV_PATH\"]))\n",
    "    cand_csv.append(art_dir / \"test_prob_ens.csv\")\n",
    "\n",
    "    for p in cand_csv:\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p)\n",
    "            # support either (object_id, prob) or (object_id, prediction) naming\n",
    "            if \"object_id\" in df.columns and (\"prob\" in df.columns or \"prediction\" in df.columns):\n",
    "                df = df.copy()\n",
    "                df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "                colp = \"prob\" if \"prob\" in df.columns else \"prediction\"\n",
    "                prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n",
    "                if len(prob) != len(df):\n",
    "                    raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n",
    "                return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob})\n",
    "\n",
    "    # ---- C) npy fallback ----\n",
    "    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n",
    "    p_ids = fix_dir / \"test_ids.npy\"\n",
    "    if not p_ids.exists():\n",
    "        raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat fixed_seq/test_ids.npy atau STAGE 10 export test_ids.\")\n",
    "\n",
    "    ids = _load_ids_npy(p_ids)\n",
    "    if len(ids) == 0:\n",
    "        raise RuntimeError(\"test_ids.npy kosong.\")\n",
    "\n",
    "    # test prob npy from globals path or default\n",
    "    cand_npy = []\n",
    "    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n",
    "        cand_npy.append(Path(globals()[\"TEST_PROB_ENS_PATH\"]))\n",
    "    cand_npy.append(art_dir / \"test_prob_ens.npy\")\n",
    "\n",
    "    prob = None\n",
    "    for p in cand_npy:\n",
    "        if p.exists():\n",
    "            prob = _sanitize_prob(_as_1d_float32(np.load(p, allow_pickle=False)))\n",
    "            break\n",
    "    if prob is None:\n",
    "        raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n",
    "\n",
    "    if not isinstance(prob, np.ndarray) or prob.ndim == 0:\n",
    "        raise TypeError(f\"Invalid test_prob (scalar/unsized). type={type(prob)} ndim={getattr(prob,'ndim',None)}\")\n",
    "\n",
    "    if len(prob) != len(ids):\n",
    "        raise RuntimeError(f\"Length mismatch (NPY): test_prob={len(prob)} vs test_ids={len(ids)}\")\n",
    "\n",
    "    return pd.DataFrame({\"object_id\": ids, \"prob\": prob})\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load prediction df\n",
    "# ----------------------------\n",
    "df_pred = _load_pred_df()\n",
    "if df_pred.empty:\n",
    "    raise RuntimeError(\"df_pred empty (unexpected).\")\n",
    "\n",
    "df_pred[\"object_id\"] = df_pred[\"object_id\"].apply(_norm_id)\n",
    "if df_pred[\"object_id\"].duplicated().any():\n",
    "    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n",
    "\n",
    "p = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "if not np.isfinite(p).all():\n",
    "    bad = int((~np.isfinite(p)).sum())\n",
    "    raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\n",
    "df_pred[\"prob\"] = _sanitize_prob(p)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold selection (priority)\n",
    "# ----------------------------\n",
    "FORCE_THR = None  # set manual if you want, e.g. 0.37\n",
    "thr = None\n",
    "if FORCE_THR is not None:\n",
    "    thr = float(FORCE_THR)\n",
    "elif \"BEST_THR_F1\" in globals() and globals()[\"BEST_THR_F1\"] is not None:\n",
    "    thr = float(globals()[\"BEST_THR_F1\"])\n",
    "elif \"BEST_THR\" in globals() and globals()[\"BEST_THR\"] is not None:\n",
    "    thr = float(globals()[\"BEST_THR\"])\n",
    "else:\n",
    "    thr = 0.5\n",
    "\n",
    "thr = float(np.clip(thr, 0.0, 1.0))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Align to sample_submission order + build BINARY prediction (0/1)\n",
    "# ----------------------------\n",
    "df_sub = df_sub.copy()\n",
    "df_sub[\"object_id\"] = df_sub[\"object_id\"].apply(_norm_id)\n",
    "\n",
    "if df_sub[\"object_id\"].duplicated().any():\n",
    "    dup = df_sub.loc[df_sub[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(f\"sample_submission has duplicate object_id (unexpected). examples={dup}\")\n",
    "\n",
    "df_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if df_out[\"prob\"].isna().any():\n",
    "    missing_n = int(df_out[\"prob\"].isna().sum())\n",
    "    miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(\n",
    "        f\"Some sample_submission object_id have no prediction: missing_n={missing_n}. Examples: {miss_ids}\\n\"\n",
    "        \"Biasanya karena mismatch id normalization atau pred df tidak lengkap.\"\n",
    "    )\n",
    "\n",
    "# REQUIRED: binary 0/1\n",
    "df_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\n",
    "df_out = df_out[[\"object_id\", \"prediction\"]]\n",
    "\n",
    "# strict format checks\n",
    "u = set(np.unique(df_out[\"prediction\"].to_numpy()).tolist())\n",
    "if not u.issubset({0, 1}):\n",
    "    raise RuntimeError(f\"submission prediction contains values outside {{0,1}}: {sorted(list(u))}\")\n",
    "\n",
    "if len(df_out) != len(df_sub):\n",
    "    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n",
    "\n",
    "# quick stats\n",
    "pos_pred = int(df_out[\"prediction\"].sum())\n",
    "print(\"[Stage 11] SUBMISSION READY (BINARY 0/1)\")\n",
    "print(f\"- threshold_used={thr:.6f}\")\n",
    "print(f\"- rows={len(df_out):,} | pos_pred={pos_pred:,} ({pos_pred/max(len(df_out),1)*100:.4f}%)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Write files\n",
    "# ----------------------------\n",
    "SUB_DIR = Path(SUB_DIR)\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_main  = Path(\"/kaggle/working/submission.csv\")\n",
    "out_copy  = SUB_DIR / \"submission.csv\"\n",
    "\n",
    "df_out.to_csv(out_main, index=False)\n",
    "df_out.to_csv(out_copy, index=False)\n",
    "\n",
    "print(f\"- wrote: {out_main}\")\n",
    "print(f\"- copy : {out_copy}\")\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(df_out.head(8).to_string(index=False))\n",
    "\n",
    "globals().update({\n",
    "    \"SUBMISSION_PATH\": out_main,\n",
    "    \"SUBMISSION_COPY_PATH\": out_copy,\n",
    "    \"SUBMISSION_MODE\": \"binary\",\n",
    "    \"SUBMISSION_THRESHOLD\": thr,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8925232,
     "sourceId": 14010596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7265.008556,
   "end_time": "2026-01-02T20:42:43.735405",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-02T18:41:38.726849",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
