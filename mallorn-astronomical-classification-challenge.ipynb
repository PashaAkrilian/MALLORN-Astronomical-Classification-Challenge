{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29474210",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-03T15:24:37.215023Z",
     "iopub.status.busy": "2026-01-03T15:24:37.214565Z",
     "iopub.status.idle": "2026-01-03T15:24:38.573334Z",
     "shell.execute_reply": "2026-01-03T15:24:38.572222Z"
    },
    "papermill": {
     "duration": 1.379039,
     "end_time": "2026-01-03T15:24:38.576004",
     "exception": false,
     "start_time": "2026-01-03T15:24:37.196965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mallorn-dataset/sample_submission.csv\n",
      "/kaggle/input/mallorn-dataset/test_log.csv\n",
      "/kaggle/input/mallorn-dataset/train_log.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/test_full_lightcurves.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa471363",
   "metadata": {
    "papermill": {
     "duration": 0.009288,
     "end_time": "2026-01-03T15:24:38.595305",
     "exception": false,
     "start_time": "2026-01-03T15:24:38.586017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Kaggle CPU Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0efe1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:24:38.617007Z",
     "iopub.status.busy": "2026-01-03T15:24:38.616429Z",
     "iopub.status.idle": "2026-01-03T15:24:43.833973Z",
     "shell.execute_reply": "2026-01-03T15:24:43.832804Z"
    },
    "papermill": {
     "duration": 5.231486,
     "end_time": "2026-01-03T15:24:43.836406",
     "exception": false,
     "start_time": "2026-01-03T15:24:38.604920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV OK (Kaggle CPU)\n",
      "- Python: 3.12.12\n",
      "- Numpy:  2.0.2\n",
      "- Pandas: 2.2.2\n",
      "- Torch:  2.8.0+cu126 | CUDA: False\n",
      "\n",
      "DATA OK\n",
      "- train_log objects: 3,043 | pos=148 | neg=2,895 | pos%=4.86%\n",
      "- test_log objects:  7,135\n",
      "- sample_submission: 7,135\n",
      "- splits: 20 folders (01..20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 0 — Kaggle CPU Environment Setup (ONE CELL) — REVISI FULL v6\n",
    "# Fokus v6:\n",
    "# - Iterasi split pakai PATHS[\"SPLITS\"] (bukan set) -> deterministik\n",
    "# - Validasi: test_log == sample_submission (set & count)\n",
    "# - Siapkan mapping: object_id -> split & meta\n",
    "# - Guard untuk stage-stage berikutnya (resume friendly)\n",
    "# ============================================================\n",
    "\n",
    "import os, sys, gc, json, time, random, hashlib, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "SEED = 2025\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# CPU thread limits (anti-freeze)\n",
    "# ----------------------------\n",
    "THREADS = 2\n",
    "for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
    "    os.environ.setdefault(k, str(THREADS))\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.set_num_threads(THREADS)\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\n",
    "PATHS = {\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n",
    "    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n",
    "    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n",
    "    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# WORKDIR (versioned run)\n",
    "# ----------------------------\n",
    "WORKDIR = Path(\"/kaggle/working\")\n",
    "BASE_RUN_DIR = WORKDIR / \"mallorn_run\"\n",
    "BASE_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    # Pipeline toggles\n",
    "    \"USE_GBDT\": True,\n",
    "    \"USE_DEEP_LITE\": False,      # CNN binned (CPU-friendly). Nyalakan kalau mau hybrid.\n",
    "    \"USE_HYBRID_BLEND\": False,   # blend GBDT + DEEP_LITE\n",
    "    \"USE_THRESHOLD_TUNING\": True,\n",
    "\n",
    "    # Feature settings\n",
    "    \"USE_ASINH_FLUX\": True,\n",
    "    \"SNR_CLIP\": 30.0,\n",
    "    \"SNR_DET_THR\": 3.0,\n",
    "    \"SNR_STRONG_THR\": 5.0,\n",
    "    \"MIN_FLUXERR\": 1e-6,\n",
    "\n",
    "    # Streaming\n",
    "    \"CHUNK_ROWS\": 200_000,\n",
    "\n",
    "    # CV\n",
    "    \"N_FOLDS\": 5,\n",
    "    \"CV_STRATIFY\": True,\n",
    "    \"CV_USE_SPLIT_COL\": True,    # pakai group=split agar anti leakage split\n",
    "    # Note: jika sklearn tidak punya StratifiedGroupKFold, fallback GroupKFold\n",
    "\n",
    "    # Deep-lite (binned)\n",
    "    \"BINS\": 48,                  # time bins\n",
    "    \"DEEP_EPOCHS\": 8,            # CPU-friendly\n",
    "    \"DEEP_BS\": 128,\n",
    "}\n",
    "\n",
    "def _hash_cfg(d: dict) -> str:\n",
    "    s = json.dumps(d, sort_keys=True)\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:10]\n",
    "\n",
    "CFG_HASH = _hash_cfg(CFG)\n",
    "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = BASE_RUN_DIR / f\"run_{RUN_TAG}_{CFG_HASH}\"\n",
    "\n",
    "ART_DIR   = RUN_DIR / \"artifacts\"\n",
    "CACHE_DIR = RUN_DIR / \"cache\"\n",
    "OOF_DIR   = RUN_DIR / \"oof\"\n",
    "SUB_DIR   = RUN_DIR / \"submissions\"\n",
    "LOG_DIR   = RUN_DIR / \"logs\"\n",
    "FEAT_DIR  = CACHE_DIR / \"features\"\n",
    "SEQ_DIR   = CACHE_DIR / \"seq\"      # untuk deep-lite bins\n",
    "\n",
    "for d in [RUN_DIR, ART_DIR, CACHE_DIR, OOF_DIR, SUB_DIR, LOG_DIR, FEAT_DIR, SEQ_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _must_exist(p: Path, what: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n",
    "\n",
    "_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n",
    "_must_exist(PATHS[\"TRAIN_LOG\"],  \"train_log.csv\")\n",
    "_must_exist(PATHS[\"TEST_LOG\"],   \"test_log.csv\")\n",
    "\n",
    "for sd in PATHS[\"SPLITS\"]:\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    _must_exist(tr, f\"{sd.name}/train_full_lightcurves.csv\")\n",
    "    _must_exist(te, f\"{sd.name}/test_full_lightcurves.csv\")\n",
    "\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Load logs + sample\n",
    "# ----------------------------\n",
    "df_sub = _norm_cols(pd.read_csv(PATHS[\"SAMPLE_SUB\"], dtype={\"object_id\": \"string\"}, **SAFE_READ_KW))\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission must have object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "df_train_log = _norm_cols(pd.read_csv(PATHS[\"TRAIN_LOG\"], dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n",
    "df_test_log  = _norm_cols(pd.read_csv(PATHS[\"TEST_LOG\"],  dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n",
    "\n",
    "need_train = {\"object_id\",\"EBV\",\"Z\",\"split\",\"target\"}\n",
    "need_test  = {\"object_id\",\"EBV\",\"Z\",\"split\"}\n",
    "if not need_train.issubset(df_train_log.columns):\n",
    "    raise ValueError(f\"train_log missing: {sorted(list(need_train-set(df_train_log.columns)))}\")\n",
    "if not need_test.issubset(df_test_log.columns):\n",
    "    raise ValueError(f\"test_log missing: {sorted(list(need_test-set(df_test_log.columns)))}\")\n",
    "\n",
    "def _normalize_split(x):\n",
    "    if pd.isna(x): return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s: return \"\"\n",
    "    if s.isdigit(): return f\"split_{int(s):02d}\"\n",
    "    s = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s.startswith(\"split_\"):\n",
    "        tail = s.split(\"split_\",1)[1].strip(\"_\")\n",
    "        if tail.isdigit():\n",
    "            return f\"split_{int(tail):02d}\"\n",
    "    return s\n",
    "\n",
    "df_train_log[\"split\"] = df_train_log[\"split\"].map(_normalize_split)\n",
    "df_test_log[\"split\"]  = df_test_log[\"split\"].map(_normalize_split)\n",
    "\n",
    "valid_splits = {f\"split_{i:02d}\" for i in range(1,21)}\n",
    "bad_tr = sorted(set(df_train_log[\"split\"]) - valid_splits)\n",
    "bad_te = sorted(set(df_test_log[\"split\"]) - valid_splits)\n",
    "if bad_tr: raise ValueError(f\"Invalid split in train_log: {bad_tr[:10]}\")\n",
    "if bad_te: raise ValueError(f\"Invalid split in test_log:  {bad_te[:10]}\")\n",
    "\n",
    "for col in [\"EBV\",\"Z\"]:\n",
    "    df_train_log[col] = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n",
    "    df_test_log[col]  = pd.to_numeric(df_test_log[col],  errors=\"coerce\")\n",
    "    if df_train_log[col].isna().any():\n",
    "        raise ValueError(f\"train_log {col} has NaN after numeric coercion: {int(df_train_log[col].isna().sum())}\")\n",
    "    if df_test_log[col].isna().any():\n",
    "        raise ValueError(f\"test_log {col} has NaN after numeric coercion: {int(df_test_log[col].isna().sum())}\")\n",
    "\n",
    "df_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\")\n",
    "if df_train_log[\"target\"].isna().any():\n",
    "    raise ValueError(f\"train_log target NaN after coercion: {int(df_train_log['target'].isna().sum())}\")\n",
    "u = set(pd.unique(df_train_log[\"target\"]).tolist())\n",
    "if not u.issubset({0,1}):\n",
    "    raise ValueError(f\"train_log target must be 0/1. Found: {sorted(list(u))}\")\n",
    "\n",
    "# Z_err handling\n",
    "if \"Z_err\" not in df_test_log.columns:\n",
    "    df_test_log[\"Z_err\"] = np.nan\n",
    "df_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\n",
    "df_test_log[\"has_zerr\"] = (~df_test_log[\"Z_err\"].isna()).astype(\"int8\")\n",
    "df_test_log[\"Z_err\"] = df_test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "if \"Z_err\" not in df_train_log.columns:\n",
    "    df_train_log[\"Z_err\"] = 0.0\n",
    "df_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\").fillna(0.0)\n",
    "df_train_log[\"has_zerr\"] = 0\n",
    "\n",
    "# Uniqueness\n",
    "if df_train_log[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(f\"Duplicated object_id in train_log: {int(df_train_log['object_id'].duplicated().sum())}\")\n",
    "if df_test_log[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(f\"Duplicated object_id in test_log:  {int(df_test_log['object_id'].duplicated().sum())}\")\n",
    "\n",
    "# sample_submission alignment check\n",
    "sub_ids = df_sub[\"object_id\"].astype(\"string\")\n",
    "test_ids = df_test_log[\"object_id\"].astype(\"string\")\n",
    "if len(sub_ids) != len(test_ids):\n",
    "    raise ValueError(f\"Row mismatch: sample_submission={len(sub_ids)} vs test_log={len(test_ids)}\")\n",
    "\n",
    "s_sub = set(sub_ids.tolist())\n",
    "s_tst = set(test_ids.tolist())\n",
    "if s_sub != s_tst:\n",
    "    missing_in_test = list(s_sub - s_tst)[:5]\n",
    "    missing_in_sub  = list(s_tst - s_sub)[:5]\n",
    "    raise ValueError(\n",
    "        \"sample_submission and test_log object_id set mismatch.\\n\"\n",
    "        f\"- sample not in test_log (up to5): {missing_in_test}\\n\"\n",
    "        f\"- test_log not in sample (up to5): {missing_in_sub}\"\n",
    "    )\n",
    "\n",
    "# Basic counts\n",
    "pos = int((df_train_log[\"target\"]==1).sum())\n",
    "neg = int((df_train_log[\"target\"]==0).sum())\n",
    "tot = int(len(df_train_log))\n",
    "\n",
    "print(\"ENV OK (Kaggle CPU)\")\n",
    "print(f\"- Python: {sys.version.split()[0]}\")\n",
    "print(f\"- Numpy:  {np.__version__}\")\n",
    "print(f\"- Pandas: {pd.__version__}\")\n",
    "if torch is not None:\n",
    "    print(f\"- Torch:  {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\n",
    "else:\n",
    "    print(\"- Torch:  not available\")\n",
    "\n",
    "print(\"\\nDATA OK\")\n",
    "print(f\"- train_log objects: {tot:,} | pos={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.2f}%\")\n",
    "print(f\"- test_log objects:  {len(df_test_log):,}\")\n",
    "print(f\"- sample_submission: {len(df_sub):,}\")\n",
    "print(f\"- splits: {len(PATHS['SPLITS'])} folders (01..20)\")\n",
    "\n",
    "# Save snapshot\n",
    "snap = {\n",
    "    \"SEED\": SEED,\n",
    "    \"CFG\": CFG,\n",
    "    \"CFG_HASH\": CFG_HASH,\n",
    "    \"RUN_DIR\": str(RUN_DIR),\n",
    "    \"DATA_ROOT\": str(DATA_ROOT),\n",
    "}\n",
    "with open(RUN_DIR / \"config_stage0.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(snap, f, indent=2)\n",
    "\n",
    "globals().update({\n",
    "    \"SEED\": SEED, \"THREADS\": THREADS, \"CFG\": CFG, \"CFG_HASH\": CFG_HASH,\n",
    "    \"PATHS\": PATHS, \"DATA_ROOT\": DATA_ROOT, \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR, \"CACHE_DIR\": CACHE_DIR, \"OOF_DIR\": OOF_DIR,\n",
    "    \"SUB_DIR\": SUB_DIR, \"LOG_DIR\": LOG_DIR, \"FEAT_DIR\": FEAT_DIR, \"SEQ_DIR\": SEQ_DIR,\n",
    "    \"df_sub\": df_sub, \"df_train_log\": df_train_log, \"df_test_log\": df_test_log\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb2fb5",
   "metadata": {
    "papermill": {
     "duration": 0.010511,
     "end_time": "2026-01-03T15:24:43.857134",
     "exception": false,
     "start_time": "2026-01-03T15:24:43.846623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Verify Dataset Paths & Split Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c81c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:24:43.880376Z",
     "iopub.status.busy": "2026-01-03T15:24:43.879595Z",
     "iopub.status.idle": "2026-01-03T15:24:47.984549Z",
     "shell.execute_reply": "2026-01-03T15:24:47.983324Z"
    },
    "papermill": {
     "duration": 4.119784,
     "end_time": "2026-01-03T15:24:47.986987",
     "exception": false,
     "start_time": "2026-01-03T15:24:43.867203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1 OK — SPLIT ROUTING READY\n",
      "- routing saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/logs/split_routing.csv\n",
      "- lc sample stats saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/logs/lc_sample_stats.csv\n",
      "- summary json saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/logs/stage1_summary.json\n",
      "- elapsed: 0.06 min | warn_flux_na_files=18\n",
      "\n",
      "OBJECT COUNTS (from logs)\n",
      "- split_01: train=155 | test=364\n",
      "- split_02: train=170 | test=414\n",
      "- split_03: train=138 | test=338\n",
      "- split_04: train=145 | test=332\n",
      "- split_05: train=165 | test=375\n",
      "- split_06: train=155 | test=374\n",
      "- split_07: train=165 | test=398\n",
      "- split_08: train=162 | test=387\n",
      "- split_09: train=128 | test=289\n",
      "- split_10: train=144 | test=331\n",
      "- split_11: train=146 | test=325\n",
      "- split_12: train=155 | test=353\n",
      "- split_13: train=143 | test=379\n",
      "- split_14: train=154 | test=351\n",
      "- split_15: train=158 | test=342\n",
      "- split_16: train=155 | test=354\n",
      "- split_17: train=153 | test=351\n",
      "- split_18: train=152 | test=345\n",
      "- split_19: train=147 | test=375\n",
      "- split_20: train=153 | test=358\n",
      "\n",
      "WORST SAMPLE (highest flux_na_frac in sample head)\n",
      "   split  kind  flux_na_frac  time_na_frac  ferr_na_frac  file_mb\n",
      "split_07 train         0.095           0.0           0.0 1.257892\n",
      "split_04 train         0.002           0.0           0.0 1.173453\n",
      "split_19  test         0.002           0.0           0.0 2.904259\n",
      "split_17 train         0.002           0.0           0.0 1.173508\n",
      "split_05  test         0.001           0.0           0.0 3.122271\n",
      "split_11 train         0.001           0.0           0.0 1.180762\n",
      "split_14 train         0.001           0.0           0.0 1.316985\n",
      "split_19 train         0.001           0.0           0.0 1.140910\n",
      "\n",
      "WORST SAMPLE (highest time_na_frac in sample head)\n",
      "   split  kind  time_na_frac  flux_na_frac  ferr_na_frac  file_mb\n",
      "split_01 train           0.0        0.0000           0.0 1.351567\n",
      "split_01  test           0.0        0.0000           0.0 3.052048\n",
      "split_02 train           0.0        0.0000           0.0 1.326915\n",
      "split_02  test           0.0        0.0000           0.0 3.658606\n",
      "split_03 train           0.0        0.0005           0.0 1.124688\n",
      "split_03  test           0.0        0.0000           0.0 2.770512\n",
      "split_04 train           0.0        0.0020           0.0 1.173453\n",
      "split_04  test           0.0        0.0000           0.0 2.658786\n",
      "\n",
      "Stage 1 complete: splits verified + routing/stats exported.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Split Routing + Lightcurve Micro-Profiling (ONE CELL, CPU-SAFE)\n",
    "# REVISI FULL v3 (IO-LEBIH IRIT + DETERM + LEBIH BANYAK STATS)\n",
    "#\n",
    "# Output:\n",
    "# - logs/split_routing.csv\n",
    "# - logs/lc_sample_stats.csv\n",
    "# - logs/stage1_summary.json\n",
    "#\n",
    "# Catatan:\n",
    "# - Stage ini tidak \"meningkatkan akurasi\" langsung, tapi memastikan ALL SPLITS kebaca\n",
    "#   dan memberi statistik penting untuk tuning stage modelling berikutnya.\n",
    "# ============================================================\n",
    "\n",
    "import re, gc, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "need0 = [\"PATHS\", \"df_train_log\", \"df_test_log\"]\n",
    "for need in need0:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n",
    "\n",
    "DATA_ROOT = Path(PATHS[\"DATA_ROOT\"])\n",
    "\n",
    "# deterministic split list (JANGAN set)\n",
    "SPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "VALID_SPLITS = set(SPLIT_LIST)\n",
    "\n",
    "# split dirs mapping (deterministic)\n",
    "SPLIT_DIRS = {p.name: p for p in PATHS[\"SPLITS\"]}\n",
    "# Optional dirs from stage 0\n",
    "RUN_DIR = Path(globals().get(\"RUN_DIR\", \"/kaggle/working/mallorn_run\"))\n",
    "LOG_DIR = Path(globals().get(\"LOG_DIR\", RUN_DIR / \"logs\"))\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG_LOCAL = globals().get(\"CFG\", {})\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Safe read config (konsisten dengan STAGE 0)\n",
    "# ----------------------------\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "# sampling knobs (CPU-safe)\n",
    "HEAD_ROWS = int(CFG_LOCAL.get(\"LC_HEAD_ROWS\", 2000))           # sample rows per file\n",
    "SAMPLE_ID_PER_SPLIT = int(CFG_LOCAL.get(\"SAMPLE_ID_PER_SPLIT\", 5))\n",
    "CHUNK_ROWS = int(CFG_LOCAL.get(\"CHUNK_ROWS\", 200_000))\n",
    "MAX_CHUNKS_PER_FILE = int(CFG_LOCAL.get(\"MAX_CHUNKS_PER_FILE\", 6))\n",
    "\n",
    "# numeric policy thresholds (ketat untuk Time/Flux_err)\n",
    "MAX_TIME_NA_FRAC = float(CFG_LOCAL.get(\"MAX_TIME_NA_FRAC\", 0.02))\n",
    "MAX_FERR_NA_FRAC = float(CFG_LOCAL.get(\"MAX_FERR_NA_FRAC\", 0.02))\n",
    "ID_MISS_FAIL_FRAC = float(CFG_LOCAL.get(\"ID_MISS_FAIL_FRAC\", 0.80))  # fail jika >= 80% ID sample tidak ketemu (scan cap)\n",
    "MIN_SAMPLE_ROWS = int(CFG_LOCAL.get(\"MIN_SAMPLE_ROWS\", 100))         # fail kalau sample terlalu kosong\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Helpers\n",
    "# ----------------------------\n",
    "REQ_LC_COLS = [\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"]\n",
    "REQ_LC_COLS_SET = set(REQ_LC_COLS)\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def normalize_split_name(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.isdigit():\n",
    "        return f\"split_{int(s2):02d}\"\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    return s2\n",
    "\n",
    "def sizeof_mb(p: Path) -> float:\n",
    "    try:\n",
    "        return p.stat().st_size / (1024**2)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def _read_sample_df(p: Path, nrows: int):\n",
    "    \"\"\"\n",
    "    Single read for:\n",
    "    - schema presence (via usecols)\n",
    "    - filter sanity\n",
    "    - numeric coercion sample stats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dfh = pd.read_csv(p, usecols=REQ_LC_COLS, nrows=nrows, **SAFE_READ_KW)\n",
    "    except ValueError as e:\n",
    "        # Usually \"Usecols do not match columns\"\n",
    "        # Provide clearer diagnostics\n",
    "        df0 = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n",
    "        cols = [c.strip() for c in df0.columns]\n",
    "        miss = sorted(list(REQ_LC_COLS_SET - set(cols)))\n",
    "        raise ValueError(\n",
    "            f\"[LC SCHEMA] {p} missing required columns: {miss}. Found columns: {cols}\"\n",
    "        ) from e\n",
    "    return _norm_cols(dfh)\n",
    "\n",
    "def _numeric_and_filter_stats(dfh: pd.DataFrame):\n",
    "    out = {\"n_sample\": int(len(dfh))}\n",
    "    if len(dfh) == 0:\n",
    "        out.update({\n",
    "            \"time_na_frac\": 1.0, \"flux_na_frac\": 1.0, \"ferr_na_frac\": 1.0,\n",
    "            \"filter_bad\": \"\", \"filter_sample\": \"\"\n",
    "        })\n",
    "        return out\n",
    "\n",
    "    # Filter values\n",
    "    filt = dfh[\"Filter\"].astype(\"string\").str.strip().str.lower()\n",
    "    filt = filt[~filt.isna()]\n",
    "    uniq = sorted(set(filt.tolist()))\n",
    "    bad = sorted([v for v in uniq if v not in ALLOWED_FILTERS])\n",
    "    out[\"filter_bad\"] = \",\".join(bad[:10]) if bad else \"\"\n",
    "    out[\"filter_sample\"] = \",\".join(uniq[:10]) if uniq else \"\"\n",
    "\n",
    "    # Band coverage from sample\n",
    "    if len(filt) > 0:\n",
    "        vc = filt.value_counts()\n",
    "        denom = float(vc.sum())\n",
    "        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n",
    "            out[f\"frac_{b}\"] = float(vc.get(b, 0) / denom)\n",
    "    else:\n",
    "        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n",
    "            out[f\"frac_{b}\"] = 0.0\n",
    "\n",
    "    # Numeric coercion\n",
    "    t = pd.to_numeric(dfh[\"Time (MJD)\"], errors=\"coerce\")\n",
    "    f = pd.to_numeric(dfh[\"Flux\"], errors=\"coerce\")\n",
    "    e = pd.to_numeric(dfh[\"Flux_err\"], errors=\"coerce\")\n",
    "\n",
    "    out[\"time_na_frac\"] = float(t.isna().mean())\n",
    "    out[\"flux_na_frac\"] = float(f.isna().mean())\n",
    "    out[\"ferr_na_frac\"] = float(e.isna().mean())\n",
    "\n",
    "    # Quick stats (ignore NaN)\n",
    "    if (~t.isna()).any():\n",
    "        out[\"time_min\"] = float(t.min())\n",
    "        out[\"time_max\"] = float(t.max())\n",
    "        out[\"time_span\"] = float(t.max() - t.min())\n",
    "    else:\n",
    "        out[\"time_min\"] = np.nan\n",
    "        out[\"time_max\"] = np.nan\n",
    "        out[\"time_span\"] = np.nan\n",
    "\n",
    "    if (~f.isna()).any():\n",
    "        fd = f.dropna()\n",
    "        out[\"flux_neg_frac\"] = float((fd < 0).mean())\n",
    "        out[\"flux_p01\"] = float(np.quantile(fd, 0.01))\n",
    "        out[\"flux_p50\"] = float(np.quantile(fd, 0.50))\n",
    "        out[\"flux_p99\"] = float(np.quantile(fd, 0.99))\n",
    "    else:\n",
    "        out[\"flux_neg_frac\"] = np.nan\n",
    "        out[\"flux_p01\"] = np.nan\n",
    "        out[\"flux_p50\"] = np.nan\n",
    "        out[\"flux_p99\"] = np.nan\n",
    "\n",
    "    if (~e.isna()).any():\n",
    "        ed = e.dropna()\n",
    "        out[\"ferr_min\"] = float(ed.min())\n",
    "        out[\"ferr_p50\"] = float(np.quantile(ed, 0.50))\n",
    "        out[\"ferr_p99\"] = float(np.quantile(ed, 0.99))\n",
    "        out[\"ferr_neg_any\"] = int((ed < 0).any())\n",
    "    else:\n",
    "        out[\"ferr_min\"] = np.nan\n",
    "        out[\"ferr_p50\"] = np.nan\n",
    "        out[\"ferr_p99\"] = np.nan\n",
    "        out[\"ferr_neg_any\"] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def _sample_id_presence(csv_path: Path, want_ids: set, chunk_rows: int, max_chunks: int):\n",
    "    \"\"\"\n",
    "    Limited scan memastikan beberapa object_id dari log benar-benar muncul di file.\n",
    "    Scan hanya kolom object_id (lebih murah).\n",
    "    \"\"\"\n",
    "    if not want_ids:\n",
    "        return 0, set(), 0\n",
    "    remaining = set(want_ids)\n",
    "    found = set()\n",
    "    nread_chunks = 0\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(csv_path, usecols=[\"object_id\"], chunksize=chunk_rows, **SAFE_READ_KW)):\n",
    "        nread_chunks += 1\n",
    "        ids = set(chunk[\"object_id\"].astype(\"string\"))\n",
    "        hit = remaining & ids\n",
    "        if hit:\n",
    "            found |= hit\n",
    "            remaining -= hit\n",
    "        if not remaining:\n",
    "            break\n",
    "        if i + 1 >= max_chunks:\n",
    "            break\n",
    "\n",
    "    return len(found), remaining, nread_chunks\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Normalize split col in logs (idempotent)\n",
    "# ----------------------------\n",
    "for df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n",
    "    if \"split\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing 'split' column.\")\n",
    "    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n",
    "\n",
    "bad_train_split = sorted(set(df_train_log[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test_log[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Verify disk splits set + required files exist\n",
    "# ----------------------------\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "missing_dirs = sorted(list(VALID_SPLITS - disk_splits))\n",
    "extra_dirs   = sorted(list(disk_splits - VALID_SPLITS))\n",
    "if missing_dirs or extra_dirs:\n",
    "    msg = []\n",
    "    if missing_dirs: msg.append(f\"Missing split folders: {missing_dirs[:10]}\")\n",
    "    if extra_dirs:   msg.append(f\"Unexpected split folders: {extra_dirs[:10]}\")\n",
    "    raise RuntimeError(\"Split folder set mismatch.\\n\" + \"\\n\".join(msg))\n",
    "\n",
    "missing_files = []\n",
    "for sp in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    for kind in [\"train\", \"test\"]:\n",
    "        p = sd / f\"{kind}_full_lightcurves.csv\"\n",
    "        if not p.exists():\n",
    "            missing_files.append(str(p))\n",
    "if missing_files:\n",
    "    raise FileNotFoundError(\"Some lightcurve files missing (showing up to 10):\\n\" + \"\\n\".join(missing_files[:10]))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Build routing manifest (40 file)\n",
    "# ----------------------------\n",
    "train_counts = df_train_log[\"split\"].value_counts().to_dict()\n",
    "test_counts  = df_test_log[\"split\"].value_counts().to_dict()\n",
    "\n",
    "routing_rows = []\n",
    "for sp in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    routing_rows.append({\n",
    "        \"split\": sp,\n",
    "        \"train_csv\": str(tr),\n",
    "        \"test_csv\": str(te),\n",
    "        \"train_mb\": sizeof_mb(tr),\n",
    "        \"test_mb\": sizeof_mb(te),\n",
    "        \"n_train_objects_log\": int(train_counts.get(sp, 0)),\n",
    "        \"n_test_objects_log\":  int(test_counts.get(sp, 0)),\n",
    "    })\n",
    "\n",
    "df_routing = pd.DataFrame(routing_rows)\n",
    "routing_path = LOG_DIR / \"split_routing.csv\"\n",
    "df_routing.to_csv(routing_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Sample profiling + ID crosscheck (single read sample)\n",
    "# ----------------------------\n",
    "stats_rows = []\n",
    "warn_flux_na_files = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for sp in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    for kind in [\"train\", \"test\"]:\n",
    "        p = sd / f\"{kind}_full_lightcurves.csv\"\n",
    "\n",
    "        # read sample once\n",
    "        dfh = _read_sample_df(p, nrows=HEAD_ROWS)\n",
    "\n",
    "        # minimum sanity: sample should not be empty\n",
    "        if len(dfh) < MIN_SAMPLE_ROWS:\n",
    "            raise ValueError(f\"[LC SAMPLE] Too few rows sampled from {p} (n={len(dfh)}). Possible read issue.\")\n",
    "\n",
    "        # compute stats\n",
    "        st = _numeric_and_filter_stats(dfh)\n",
    "\n",
    "        # Filter sanity\n",
    "        if st.get(\"filter_bad\", \"\"):\n",
    "            raise ValueError(f\"[LC FILTER] Unexpected Filter values in {p}: {st['filter_bad']} (sample={st.get('filter_sample','')})\")\n",
    "\n",
    "        # numeric policy\n",
    "        if st.get(\"time_na_frac\", 0.0) > MAX_TIME_NA_FRAC:\n",
    "            raise ValueError(f\"[LC NUM] Time(MJD) NaN too high in sample: {p} frac={st['time_na_frac']:.4f}\")\n",
    "        if st.get(\"ferr_na_frac\", 0.0) > MAX_FERR_NA_FRAC:\n",
    "            raise ValueError(f\"[LC NUM] Flux_err NaN too high in sample: {p} frac={st['ferr_na_frac']:.4f}\")\n",
    "        if int(st.get(\"ferr_neg_any\", 0)) == 1:\n",
    "            raise ValueError(f\"[LC NUM] Negative Flux_err detected in sample of {p} (should be >=0).\")\n",
    "\n",
    "        if st.get(\"flux_na_frac\", 0.0) > 0:\n",
    "            warn_flux_na_files += 1\n",
    "\n",
    "        # sample ID crosscheck (limited scan)\n",
    "        if kind == \"train\":\n",
    "            ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(\"string\")\n",
    "        else:\n",
    "            ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(\"string\")\n",
    "\n",
    "        k = min(SAMPLE_ID_PER_SPLIT, len(ids))\n",
    "        want = set(ids.sample(n=k, random_state=SEED).tolist()) if k > 0 else set()\n",
    "\n",
    "        found_n, missing_ids, nread_chunks = _sample_id_presence(p, want, CHUNK_ROWS, MAX_CHUNKS_PER_FILE)\n",
    "        miss_frac = (len(missing_ids) / max(len(want), 1)) if want else 0.0\n",
    "\n",
    "        if want and miss_frac >= ID_MISS_FAIL_FRAC:\n",
    "            raise ValueError(\n",
    "                f\"[LC ID] Severe mismatch within limited scan: {p} missing {len(missing_ids)}/{len(want)} \"\n",
    "                f\"(chunks_read={nread_chunks}). Example missing: {list(missing_ids)[:3]}\"\n",
    "            )\n",
    "        if want and missing_ids:\n",
    "            print(f\"[WARN] ID limited-scan miss: split={sp} kind={kind} miss {len(missing_ids)}/{len(want)} (chunks_read={nread_chunks})\")\n",
    "\n",
    "        row = {\n",
    "            \"split\": sp,\n",
    "            \"kind\": kind,\n",
    "            \"file\": str(p),\n",
    "            \"file_mb\": sizeof_mb(p),\n",
    "            \"n_sample\": st.get(\"n_sample\", 0),\n",
    "            \"time_na_frac\": st.get(\"time_na_frac\", np.nan),\n",
    "            \"flux_na_frac\": st.get(\"flux_na_frac\", np.nan),\n",
    "            \"ferr_na_frac\": st.get(\"ferr_na_frac\", np.nan),\n",
    "            \"time_min\": st.get(\"time_min\", np.nan),\n",
    "            \"time_max\": st.get(\"time_max\", np.nan),\n",
    "            \"time_span\": st.get(\"time_span\", np.nan),\n",
    "            \"flux_neg_frac\": st.get(\"flux_neg_frac\", np.nan),\n",
    "            \"flux_p01\": st.get(\"flux_p01\", np.nan),\n",
    "            \"flux_p50\": st.get(\"flux_p50\", np.nan),\n",
    "            \"flux_p99\": st.get(\"flux_p99\", np.nan),\n",
    "            \"ferr_min\": st.get(\"ferr_min\", np.nan),\n",
    "            \"ferr_p50\": st.get(\"ferr_p50\", np.nan),\n",
    "            \"ferr_p99\": st.get(\"ferr_p99\", np.nan),\n",
    "            \"filter_sample\": st.get(\"filter_sample\", \"\"),\n",
    "            \"id_check_k\": int(len(want)),\n",
    "            \"id_found\": int(found_n),\n",
    "            \"id_missing\": int(len(missing_ids)),\n",
    "            \"id_scan_chunks\": int(nread_chunks),\n",
    "        }\n",
    "        # band fractions\n",
    "        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n",
    "            row[f\"frac_{b}\"] = st.get(f\"frac_{b}\", 0.0)\n",
    "\n",
    "        stats_rows.append(row)\n",
    "\n",
    "df_lc_stats = pd.DataFrame(stats_rows)\n",
    "lc_stats_path = LOG_DIR / \"lc_sample_stats.csv\"\n",
    "df_lc_stats.to_csv(lc_stats_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Summary prints + JSON summary\n",
    "# ----------------------------\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "# worst by flux_na_frac (informational only)\n",
    "worst_flux_na = (\n",
    "    df_lc_stats.sort_values(\"flux_na_frac\", ascending=False)\n",
    "    .head(8)[[\"split\",\"kind\",\"flux_na_frac\",\"time_na_frac\",\"ferr_na_frac\",\"file_mb\"]]\n",
    ")\n",
    "\n",
    "# worst by time_na_frac (hard policy already, but show)\n",
    "worst_time_na = (\n",
    "    df_lc_stats.sort_values(\"time_na_frac\", ascending=False)\n",
    "    .head(8)[[\"split\",\"kind\",\"time_na_frac\",\"flux_na_frac\",\"ferr_na_frac\",\"file_mb\"]]\n",
    ")\n",
    "\n",
    "summary = {\n",
    "    \"stage\": \"stage1\",\n",
    "    \"data_root\": str(DATA_ROOT),\n",
    "    \"log_dir\": str(LOG_DIR),\n",
    "    \"head_rows\": HEAD_ROWS,\n",
    "    \"sample_id_per_split\": SAMPLE_ID_PER_SPLIT,\n",
    "    \"chunk_rows\": CHUNK_ROWS,\n",
    "    \"max_chunks_per_file\": MAX_CHUNKS_PER_FILE,\n",
    "    \"thresholds\": {\n",
    "        \"MAX_TIME_NA_FRAC\": MAX_TIME_NA_FRAC,\n",
    "        \"MAX_FERR_NA_FRAC\": MAX_FERR_NA_FRAC,\n",
    "        \"ID_MISS_FAIL_FRAC\": ID_MISS_FAIL_FRAC,\n",
    "        \"MIN_SAMPLE_ROWS\": MIN_SAMPLE_ROWS\n",
    "    },\n",
    "    \"warn_flux_na_files\": int(warn_flux_na_files),\n",
    "    \"routing_csv\": str(routing_path),\n",
    "    \"lc_sample_stats_csv\": str(lc_stats_path),\n",
    "    \"elapsed_sec\": float(elapsed),\n",
    "}\n",
    "\n",
    "summary_path = LOG_DIR / \"stage1_summary.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"STAGE 1 OK — SPLIT ROUTING READY\")\n",
    "print(f\"- routing saved: {routing_path}\")\n",
    "print(f\"- lc sample stats saved: {lc_stats_path}\")\n",
    "print(f\"- summary json saved: {summary_path}\")\n",
    "print(f\"- elapsed: {elapsed/60:.2f} min | warn_flux_na_files={warn_flux_na_files}\")\n",
    "\n",
    "print(\"\\nOBJECT COUNTS (from logs)\")\n",
    "for sp in SPLIT_LIST:\n",
    "    print(f\"- {sp}: train={int(train_counts.get(sp,0)):,} | test={int(test_counts.get(sp,0)):,}\")\n",
    "\n",
    "print(\"\\nWORST SAMPLE (highest flux_na_frac in sample head)\")\n",
    "print(worst_flux_na.to_string(index=False))\n",
    "\n",
    "print(\"\\nWORST SAMPLE (highest time_na_frac in sample head)\")\n",
    "print(worst_time_na.to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Export to globals (dipakai stage berikutnya)\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SPLIT_DIRS\": SPLIT_DIRS,\n",
    "    \"SPLIT_LIST\": SPLIT_LIST,\n",
    "    \"df_split_routing\": df_routing,\n",
    "    \"df_lc_sample_stats\": df_lc_stats,\n",
    "    \"STAGE1_SUMMARY_PATH\": summary_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\nStage 1 complete: splits verified + routing/stats exported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541ea79",
   "metadata": {
    "papermill": {
     "duration": 0.010095,
     "end_time": "2026-01-03T15:24:48.007053",
     "exception": false,
     "start_time": "2026-01-03T15:24:47.996958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and Validate Train/Test Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8da5ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:24:48.030140Z",
     "iopub.status.busy": "2026-01-03T15:24:48.029412Z",
     "iopub.status.idle": "2026-01-03T15:24:48.492837Z",
     "shell.execute_reply": "2026-01-03T15:24:48.491886Z"
    },
    "papermill": {
     "duration": 0.477403,
     "end_time": "2026-01-03T15:24:48.494914",
     "exception": false,
     "start_time": "2026-01-03T15:24:48.017511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2 OK — META READY (clean + folds)\n",
      "- CV_USE_SPLIT_COL: True | N_FOLDS=5\n",
      "- train objects: 3,043 | pos(TDE)=148 | neg=2,895 | pos%=4.864%\n",
      "- test objects : 7,135\n",
      "- saved train  : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/train_meta.parquet\n",
      "- saved test   : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/test_meta.parquet\n",
      "- saved stats  : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/split_stats.csv\n",
      "- saved folds  : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/train_folds.csv\n",
      "- scale_pos_weight (neg/pos): 19.561\n",
      "\n",
      "CLIP RANGES\n",
      "- EBV clip (train): [0.005042, 0.581790]\n",
      "- Z   clip (train): [0.044923, 4.032352]\n",
      "- Zerr clip (test ): [0.000000, 0.106846]\n",
      "\n",
      "FOLD BALANCE (count/pos/pos_rate) — MUST SHOW 0..K-1\n",
      "      count  pos  pos_rate\n",
      "fold                      \n",
      "0       607   20  0.032949\n",
      "1       565    6  0.010619\n",
      "2       632   53  0.083861\n",
      "3       641   27  0.042122\n",
      "4       598   42  0.070234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Clean Meta Logs + CV Fold Assignment (ONE CELL, CPU-SAFE)\n",
    "# REVISI FULL v4 (FIX: 5-FOLD TERPAKAI + Z_ERR TIDAK MATI)\n",
    "#\n",
    "# Output:\n",
    "#   * df_train_meta, df_test_meta (index=object_id)\n",
    "#   * id2split_train, id2split_test\n",
    "#   * artifacts: train_meta.(parquet|csv), test_meta.(parquet|csv)\n",
    "#   * artifacts: split_stats.csv, train_folds.csv\n",
    "#   * artifacts: id2split_train.json, id2split_test.json\n",
    "#   * artifacts: split2fold.json (kalau CV_USE_SPLIT_COL=True)\n",
    "#\n",
    "# Notes:\n",
    "# - Tidak load full lightcurves.\n",
    "# - EBV/Z clip pakai TRAIN saja (anti leakage).\n",
    "# - Z_err clip pakai TEST quantiles (tanpa label) -> supaya tidak 0..0.\n",
    "# - Fold strategy:\n",
    "#     * Jika CFG[\"CV_USE_SPLIT_COL\"]=True -> assign split->fold dengan quota 4 split/fold\n",
    "#     * else -> StratifiedKFold object-level\n",
    "# ============================================================\n",
    "\n",
    "import re, gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0/1 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"ART_DIR\", \"SPLIT_DIRS\", \"CFG\", \"SEED\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 & STAGE 1 dulu.\")\n",
    "\n",
    "TRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\n",
    "TEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = int(SEED)\n",
    "N_FOLDS = int(CFG.get(\"N_FOLDS\", 5))\n",
    "CV_USE_SPLIT_COL = bool(CFG.get(\"CV_USE_SPLIT_COL\", True))\n",
    "\n",
    "SPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "VALID_SPLITS = set(SPLIT_LIST)\n",
    "\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "if disk_splits != VALID_SPLITS:\n",
    "    miss = sorted(list(VALID_SPLITS - disk_splits))\n",
    "    extra = sorted(list(disk_splits - VALID_SPLITS))\n",
    "    raise RuntimeError(f\"SPLIT_DIRS mismatch. missing={miss[:5]} extra={extra[:5]} (jalankan ulang STAGE 1)\")\n",
    "\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers\n",
    "# ----------------------------\n",
    "def normalize_split_name(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.isdigit():\n",
    "        return f\"split_{int(s2):02d}\"\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    return s2\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _coerce_float32(df: pd.DataFrame, col: str):\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "def _safe_clip(series: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    return series.clip(lower=np.float32(lo), upper=np.float32(hi)).astype(\"float32\")\n",
    "\n",
    "def _qclip_bounds(arr: np.ndarray, qlo=0.001, qhi=0.999, default=(0.0, 0.0)):\n",
    "    x = np.asarray(arr, dtype=float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if len(x) == 0:\n",
    "        return float(default[0]), float(default[1])\n",
    "    lo, hi = np.quantile(x, [qlo, qhi])\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def _load_or_use_global(global_name: str, path: Path) -> pd.DataFrame:\n",
    "    if global_name in globals() and isinstance(globals()[global_name], pd.DataFrame):\n",
    "        return _norm_cols(globals()[global_name].copy())\n",
    "    return _norm_cols(pd.read_csv(path, dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Load logs\n",
    "# ----------------------------\n",
    "df_train = _load_or_use_global(\"df_train_log\", TRAIN_LOG_PATH)\n",
    "df_test  = _load_or_use_global(\"df_test_log\",  TEST_LOG_PATH)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Required columns check\n",
    "# ----------------------------\n",
    "req_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\n",
    "req_train  = req_common | {\"target\"}\n",
    "req_test   = req_common\n",
    "\n",
    "miss_train = sorted(list(req_train - set(df_train.columns)))\n",
    "miss_test  = sorted(list(req_test  - set(df_test.columns)))\n",
    "if miss_train:\n",
    "    raise ValueError(f\"train_log missing columns: {miss_train} | found={list(df_train.columns)}\")\n",
    "if miss_test:\n",
    "    raise ValueError(f\"test_log missing columns: {miss_test} | found={list(df_test.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Basic cleaning\n",
    "# ----------------------------\n",
    "df_train[\"object_id\"] = df_train[\"object_id\"].astype(\"string\").str.strip()\n",
    "df_test[\"object_id\"]  = df_test[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "df_train[\"split\"] = df_train[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "df_test[\"split\"]  = df_test[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "\n",
    "bad_train_split = sorted(set(df_train[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log invalid split values: {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log invalid split values: {bad_test_split[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Ensure Z_err exists + numeric coercion\n",
    "# ----------------------------\n",
    "if \"Z_err\" not in df_train.columns:\n",
    "    df_train[\"Z_err\"] = np.nan\n",
    "if \"Z_err\" not in df_test.columns:\n",
    "    df_test[\"Z_err\"] = np.nan\n",
    "\n",
    "# has_zerr BEFORE fill\n",
    "df_train[\"has_zerr\"] = (~pd.to_numeric(df_train[\"Z_err\"], errors=\"coerce\").isna()).astype(\"int8\")\n",
    "df_test[\"has_zerr\"]  = (~pd.to_numeric(df_test[\"Z_err\"],  errors=\"coerce\").isna()).astype(\"int8\")\n",
    "\n",
    "for c in [\"EBV\",\"Z\",\"Z_err\"]:\n",
    "    _coerce_float32(df_train, c)\n",
    "    _coerce_float32(df_test, c)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Duplicate / overlap checks\n",
    "# ----------------------------\n",
    "if df_train[\"object_id\"].duplicated().any():\n",
    "    ex = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in train_log (examples): {ex}\")\n",
    "if df_test[\"object_id\"].duplicated().any():\n",
    "    ex = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in test_log (examples): {ex}\")\n",
    "\n",
    "overlap = set(df_train[\"object_id\"].tolist()) & set(df_test[\"object_id\"].tolist())\n",
    "if overlap:\n",
    "    raise ValueError(f\"object_id overlap train vs test (examples): {list(overlap)[:5]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Target validation\n",
    "# ----------------------------\n",
    "df_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    raise ValueError(f\"train_log target has NaN after coercion: {int(df_train['target'].isna().sum())} rows.\")\n",
    "uniq_t = set(pd.unique(df_train[\"target\"]).tolist())\n",
    "if not uniq_t.issubset({0,1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n",
    "df_train[\"target\"] = df_train[\"target\"].astype(\"int8\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Missing flags + fills\n",
    "# ----------------------------\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"EBV_missing\"] = df[\"EBV\"].isna().astype(\"int8\")\n",
    "    df[\"Z_missing\"]   = df[\"Z\"].isna().astype(\"int8\")\n",
    "    df[\"Zerr_missing\"]= df[\"Z_err\"].isna().astype(\"int8\")\n",
    "\n",
    "df_train[\"EBV\"] = df_train[\"EBV\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "df_test[\"EBV\"]  = df_test[\"EBV\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "\n",
    "# Z fill pakai TRAIN stats\n",
    "train_split_med = df_train.groupby(\"split\")[\"Z\"].median().to_dict()\n",
    "train_gmed = float(np.nanmedian(df_train[\"Z\"].values.astype(float))) if np.isfinite(df_train[\"Z\"].values.astype(float)).any() else 0.0\n",
    "train_gmed = np.float32(train_gmed)\n",
    "\n",
    "def _fill_z(df: pd.DataFrame, split_med: dict, gmed: np.float32):\n",
    "    z = df[\"Z\"].copy()\n",
    "    if z.isna().any():\n",
    "        z = z.fillna(df[\"split\"].map(split_med))\n",
    "        z = z.fillna(gmed)\n",
    "    return z.astype(\"float32\")\n",
    "\n",
    "df_train[\"Z\"] = _fill_z(df_train, train_split_med, train_gmed)\n",
    "df_test[\"Z\"]  = _fill_z(df_test,  train_split_med, train_gmed)\n",
    "\n",
    "# Z_err fill NaN -> 0 (train mostly 0, test sebagian ada)\n",
    "df_train[\"Z_err\"] = df_train[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "df_test[\"Z_err\"]  = df_test[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "\n",
    "# photo-z indicator (dataset-level)\n",
    "df_train[\"is_photoz\"] = np.int8(0)\n",
    "df_test[\"is_photoz\"]  = np.int8(1)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Clipping + derived meta features\n",
    "# ----------------------------\n",
    "# EBV/Z clip pakai TRAIN (anti leakage)\n",
    "EBV_LO, EBV_HI = _qclip_bounds(df_train[\"EBV\"].values, 0.001, 0.999)\n",
    "Z_LO,   Z_HI   = _qclip_bounds(df_train[\"Z\"].values,   0.001, 0.999)\n",
    "\n",
    "df_train[\"EBV_clip\"] = _safe_clip(df_train[\"EBV\"], EBV_LO, EBV_HI)\n",
    "df_test[\"EBV_clip\"]  = _safe_clip(df_test[\"EBV\"],  EBV_LO, EBV_HI)\n",
    "\n",
    "df_train[\"Z_clip\"] = _safe_clip(df_train[\"Z\"], Z_LO, Z_HI)\n",
    "df_test[\"Z_clip\"]  = _safe_clip(df_test[\"Z\"],  Z_LO, Z_HI)\n",
    "\n",
    "# Z_err clip: PAKAI TEST quantiles supaya tidak 0..0\n",
    "# (tanpa label, aman)\n",
    "# guard: kalau test juga semua 0, tetap aman -> hi=0\n",
    "ZE_LO = 0.0\n",
    "_, ZE_HI = _qclip_bounds(df_test[\"Z_err\"].values, 0.001, 0.999, default=(0.0, 0.0))\n",
    "ZE_HI = max(float(ZE_HI), 0.0)\n",
    "\n",
    "df_train[\"Zerr_clip\"] = _safe_clip(df_train[\"Z_err\"], ZE_LO, ZE_HI)\n",
    "df_test[\"Zerr_clip\"]  = _safe_clip(df_test[\"Z_err\"],  ZE_LO, ZE_HI)\n",
    "\n",
    "df_train[\"log1pZ\"] = np.log1p(df_train[\"Z_clip\"].astype(\"float32\")).astype(\"float32\")\n",
    "df_test[\"log1pZ\"]  = np.log1p(df_test[\"Z_clip\"].astype(\"float32\")).astype(\"float32\")\n",
    "\n",
    "df_train[\"log1pZerr\"] = np.log1p(df_train[\"Zerr_clip\"].astype(\"float32\")).astype(\"float32\")\n",
    "df_test[\"log1pZerr\"]  = np.log1p(df_test[\"Zerr_clip\"].astype(\"float32\")).astype(\"float32\")\n",
    "\n",
    "eps = np.float32(1e-6)\n",
    "df_train[\"zerr_rel\"] = (df_train[\"Zerr_clip\"] / (df_train[\"Z_clip\"] + eps)).astype(\"float32\")\n",
    "df_test[\"zerr_rel\"]  = (df_test[\"Zerr_clip\"]  / (df_test[\"Z_clip\"]  + eps)).astype(\"float32\")\n",
    "\n",
    "split2id = {f\"split_{i:02d}\": i for i in range(1, 21)}\n",
    "df_train[\"split_id\"] = df_train[\"split\"].map(split2id).astype(\"int16\")\n",
    "df_test[\"split_id\"]  = df_test[\"split\"].map(split2id).astype(\"int16\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Fold assignment\n",
    "# ----------------------------\n",
    "df_train[\"fold\"] = -1\n",
    "\n",
    "if CV_USE_SPLIT_COL:\n",
    "    # Quota per fold: 20 split / 5 fold = 4 split/fold\n",
    "    total_splits = len(SPLIT_LIST)\n",
    "    if total_splits % N_FOLDS != 0:\n",
    "        # tetap bisa jalan, tapi quota pakai ceil\n",
    "        quota = int(np.ceil(total_splits / N_FOLDS))\n",
    "    else:\n",
    "        quota = int(total_splits / N_FOLDS)\n",
    "\n",
    "    sp_stat = (\n",
    "        df_train.groupby(\"split\")[\"target\"]\n",
    "        .agg([\"count\",\"sum\"])\n",
    "        .rename(columns={\"count\":\"n\",\"sum\":\"pos\"})\n",
    "        .reindex(SPLIT_LIST)\n",
    "        .fillna(0)\n",
    "        .astype({\"n\":int,\"pos\":int})\n",
    "        .reset_index()\n",
    "    )\n",
    "    sp_stat[\"neg\"] = sp_stat[\"n\"] - sp_stat[\"pos\"]\n",
    "    sp_stat[\"pos_rate\"] = sp_stat[\"pos\"] / sp_stat[\"n\"].clip(lower=1)\n",
    "\n",
    "    # sort: split paling \"berat\" dulu\n",
    "    sp_stat = sp_stat.sort_values([\"pos\",\"n\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    global_pos_rate = float(df_train[\"target\"].mean())\n",
    "    target_fold_n = float(len(df_train) / max(N_FOLDS, 1))\n",
    "\n",
    "    fold_n = np.zeros(N_FOLDS, dtype=float)\n",
    "    fold_pos = np.zeros(N_FOLDS, dtype=float)\n",
    "    fold_k = np.zeros(N_FOLDS, dtype=int)\n",
    "    split2fold = {}\n",
    "\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    for _, r in sp_stat.iterrows():\n",
    "        sp = r[\"split\"]\n",
    "        n  = float(r[\"n\"])\n",
    "        p  = float(r[\"pos\"])\n",
    "\n",
    "        # candidate folds yang masih belum penuh quota split\n",
    "        cand = np.where(fold_k < quota)[0]\n",
    "        if len(cand) == 0:\n",
    "            # fallback: semua sudah quota (harusnya tidak terjadi jika quota benar)\n",
    "            cand = np.arange(N_FOLDS)\n",
    "\n",
    "        scores = []\n",
    "        for f in cand:\n",
    "            n2 = fold_n[f] + n\n",
    "            p2 = fold_pos[f] + p\n",
    "            pr2 = (p2 / n2) if n2 > 0 else global_pos_rate\n",
    "\n",
    "            # score = keseimbangan class + keseimbangan size + penalti kalau fold mendekati penuh\n",
    "            score = abs(pr2 - global_pos_rate) \\\n",
    "                    + 0.20 * abs(n2 - target_fold_n) / max(target_fold_n, 1.0) \\\n",
    "                    + 0.05 * (fold_k[f] / max(quota, 1))\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        scores = np.asarray(scores, dtype=float)\n",
    "        best_idx = np.where(scores == scores.min())[0]\n",
    "        choose = int(cand[int(rng.choice(best_idx))]) if len(best_idx) > 1 else int(cand[int(best_idx[0])])\n",
    "\n",
    "        split2fold[sp] = choose\n",
    "        fold_n[choose] += n\n",
    "        fold_pos[choose] += p\n",
    "        fold_k[choose] += 1\n",
    "\n",
    "    # apply\n",
    "    df_train[\"fold\"] = df_train[\"split\"].map(split2fold).astype(\"int16\")\n",
    "\n",
    "    # HARD GUARD: semua fold 0..K-1 harus muncul dan tidak kosong\n",
    "    uniq_folds = sorted(df_train[\"fold\"].unique().tolist())\n",
    "    if uniq_folds != list(range(N_FOLDS)):\n",
    "        print(f\"[WARN] split->fold tidak memakai semua fold. uniq_folds={uniq_folds}. Fallback ke StratifiedKFold object-level.\")\n",
    "        CV_USE_SPLIT_COL = False\n",
    "    else:\n",
    "        # save mapping untuk audit\n",
    "        with open(ART_DIR / \"split2fold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({k:int(v) for k,v in split2fold.items()}, f)\n",
    "\n",
    "if not CV_USE_SPLIT_COL:\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "        y = df_train[\"target\"].to_numpy()\n",
    "        idx = np.arange(len(df_train))\n",
    "        for fold_id, (_, va_idx) in enumerate(skf.split(idx, y)):\n",
    "            df_train.iloc[va_idx, df_train.columns.get_loc(\"fold\")] = fold_id\n",
    "        df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] StratifiedKFold unavailable ({type(e).__name__}). Using round-robin fallback.\")\n",
    "        df_train[\"fold\"] = -1\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        pos_idx = df_train.index[df_train[\"target\"] == 1].to_numpy()\n",
    "        neg_idx = df_train.index[df_train[\"target\"] == 0].to_numpy()\n",
    "        rng.shuffle(pos_idx); rng.shuffle(neg_idx)\n",
    "        for j, ii in enumerate(pos_idx):\n",
    "            df_train.at[ii, \"fold\"] = int(j % N_FOLDS)\n",
    "        for j, ii in enumerate(neg_idx):\n",
    "            df_train.at[ii, \"fold\"] = int(j % N_FOLDS)\n",
    "        df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n",
    "\n",
    "# hard check\n",
    "if (df_train[\"fold\"] < 0).any():\n",
    "    n_bad = int((df_train[\"fold\"] < 0).sum())\n",
    "    raise RuntimeError(f\"Fold assignment gagal: ada {n_bad} baris fold=-1\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Build meta tables (index=object_id)\n",
    "# ----------------------------\n",
    "keep_train = [\n",
    "    \"object_id\",\"split\",\"split_id\",\n",
    "    \"EBV\",\"EBV_clip\",\"Z\",\"Z_clip\",\"log1pZ\",\n",
    "    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_rel\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n",
    "    \"fold\",\"target\"\n",
    "]\n",
    "keep_test = [\n",
    "    \"object_id\",\"split\",\"split_id\",\n",
    "    \"EBV\",\"EBV_clip\",\"Z\",\"Z_clip\",\"log1pZ\",\n",
    "    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_rel\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\"\n",
    "]\n",
    "\n",
    "if \"SpecType\" in df_train.columns:\n",
    "    keep_train.append(\"SpecType\")\n",
    "\n",
    "df_train_meta = df_train[keep_train].copy().set_index(\"object_id\", drop=True).sort_index()\n",
    "df_test_meta  = df_test[keep_test].copy().set_index(\"object_id\", drop=True).sort_index()\n",
    "\n",
    "id2split_train = df_train_meta[\"split\"].to_dict()\n",
    "id2split_test  = df_test_meta[\"split\"].to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Save artifacts\n",
    "# ----------------------------\n",
    "train_pq = ART_DIR / \"train_meta.parquet\"\n",
    "test_pq  = ART_DIR / \"test_meta.parquet\"\n",
    "train_csv = ART_DIR / \"train_meta.csv\"\n",
    "test_csv  = ART_DIR / \"test_meta.csv\"\n",
    "\n",
    "try:\n",
    "    df_train_meta.to_parquet(train_pq, index=True)\n",
    "    df_test_meta.to_parquet(test_pq, index=True)\n",
    "    saved_train, saved_test = str(train_pq), str(test_pq)\n",
    "except Exception:\n",
    "    df_train_meta.to_csv(train_csv, index=True)\n",
    "    df_test_meta.to_csv(test_csv, index=True)\n",
    "    saved_train, saved_test = str(train_csv), str(test_csv)\n",
    "\n",
    "split_stats = pd.DataFrame({\n",
    "    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n",
    "    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n",
    "})\n",
    "split_stats.index.name = \"split\"\n",
    "pos_by_split = df_train_meta.groupby(\"split\")[\"target\"].sum().reindex(SPLIT_LIST).fillna(0).astype(int)\n",
    "split_stats[\"train_pos\"] = pos_by_split.values\n",
    "split_stats[\"train_pos_rate\"] = (split_stats[\"train_pos\"] / split_stats[\"train_objects\"].clip(lower=1)).astype(\"float32\")\n",
    "\n",
    "split_stats_path = ART_DIR / \"split_stats.csv\"\n",
    "split_stats.to_csv(split_stats_path)\n",
    "\n",
    "fold_path = ART_DIR / \"train_folds.csv\"\n",
    "df_train_meta.reset_index()[[\"object_id\",\"split\",\"fold\",\"target\"]].to_csv(fold_path, index=False)\n",
    "\n",
    "with open(ART_DIR / \"id2split_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id2split_train, f)\n",
    "with open(ART_DIR / \"id2split_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id2split_test, f)\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Print summary\n",
    "# ----------------------------\n",
    "pos = int((df_train_meta[\"target\"] == 1).sum())\n",
    "neg = int((df_train_meta[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_meta))\n",
    "pos_rate = pos / max(tot, 1)\n",
    "scale_pos_weight = float(neg / max(pos, 1))\n",
    "\n",
    "print(\"STAGE 2 OK — META READY (clean + folds)\")\n",
    "print(f\"- CV_USE_SPLIT_COL: {CV_USE_SPLIT_COL} | N_FOLDS={N_FOLDS}\")\n",
    "print(f\"- train objects: {tot:,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={pos_rate*100:.3f}%\")\n",
    "print(f\"- test objects : {len(df_test_meta):,}\")\n",
    "print(f\"- saved train  : {saved_train}\")\n",
    "print(f\"- saved test   : {saved_test}\")\n",
    "print(f\"- saved stats  : {split_stats_path}\")\n",
    "print(f\"- saved folds  : {fold_path}\")\n",
    "print(f\"- scale_pos_weight (neg/pos): {scale_pos_weight:.3f}\")\n",
    "\n",
    "print(\"\\nCLIP RANGES\")\n",
    "print(f\"- EBV clip (train): [{EBV_LO:.6f}, {EBV_HI:.6f}]\")\n",
    "print(f\"- Z   clip (train): [{Z_LO:.6f}, {Z_HI:.6f}]\")\n",
    "print(f\"- Zerr clip (test ): [{ZE_LO:.6f}, {ZE_HI:.6f}]\")\n",
    "\n",
    "fold_tab = (\n",
    "    df_train_meta.reset_index().groupby(\"fold\")[\"target\"]\n",
    "    .agg([\"count\",\"sum\"]).rename(columns={\"sum\":\"pos\"})\n",
    "    .reindex(range(N_FOLDS)).fillna(0)\n",
    ")\n",
    "fold_tab[\"pos_rate\"] = fold_tab[\"pos\"] / fold_tab[\"count\"].clip(lower=1)\n",
    "print(\"\\nFOLD BALANCE (count/pos/pos_rate) — MUST SHOW 0..K-1\")\n",
    "print(fold_tab.to_string())\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Export globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"df_train_meta\": df_train_meta,\n",
    "    \"df_test_meta\": df_test_meta,\n",
    "    \"id2split_train\": id2split_train,\n",
    "    \"id2split_test\": id2split_test,\n",
    "    \"split_stats\": split_stats,\n",
    "    \"split2id\": split2id,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"CV_USE_SPLIT_COL_USED\": CV_USE_SPLIT_COL,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308d0e6",
   "metadata": {
    "papermill": {
     "duration": 0.009791,
     "end_time": "2026-01-03T15:24:48.514466",
     "exception": false,
     "start_time": "2026-01-03T15:24:48.504675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lightcurve Loading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d860bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:24:48.536758Z",
     "iopub.status.busy": "2026-01-03T15:24:48.535921Z",
     "iopub.status.idle": "2026-01-03T15:24:49.382770Z",
     "shell.execute_reply": "2026-01-03T15:24:49.381640Z"
    },
    "papermill": {
     "duration": 0.860658,
     "end_time": "2026-01-03T15:24:49.384867",
     "exception": false,
     "start_time": "2026-01-03T15:24:48.524209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 3 OK — LIGHTCURVE LOADING UTILITIES READY\n",
      "- Saved manifest: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/split_file_manifest.csv\n",
      "- Saved counts  : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/object_counts_by_split.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — Robust Lightcurve Loader Utilities (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v3.1 (FIX IndexError + routing by split lebih aman)\n",
    "#\n",
    "# FIX utama:\n",
    "# - groupby().groups menghasilkan label object_id (string), bukan integer positions\n",
    "#   -> jangan pakai df.index[idx]; pakai idx langsung / idx.astype(str).tolist()\n",
    "# ============================================================\n",
    "\n",
    "import gc, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"CFG\", \"SEED\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = int(SEED)\n",
    "MIN_FLUXERR = float(CFG.get(\"MIN_FLUXERR\", 1e-6))\n",
    "\n",
    "# konsisten dengan STAGE 0/2\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "REQ_LC_KEYS = [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "FILTER_ORDER = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build split file mapping (train/test lightcurves)\n",
    "# ----------------------------\n",
    "SPLIT_FILES = {}\n",
    "for s in SPLIT_LIST:\n",
    "    sd = Path(SPLIT_DIRS[s])\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n",
    "    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n",
    "\n",
    "# Save split file manifest\n",
    "manifest = []\n",
    "for s in SPLIT_LIST:\n",
    "    p_tr = SPLIT_FILES[s][\"train\"]\n",
    "    p_te = SPLIT_FILES[s][\"test\"]\n",
    "    manifest.append({\n",
    "        \"split\": s,\n",
    "        \"train_path\": str(p_tr),\n",
    "        \"test_path\": str(p_te),\n",
    "        \"train_mb\": float(p_tr.stat().st_size) / (1024**2),\n",
    "        \"test_mb\":  float(p_te.stat().st_size) / (1024**2),\n",
    "    })\n",
    "df_manifest = pd.DataFrame(manifest).sort_values(\"split\")\n",
    "manifest_path = ART_DIR / \"split_file_manifest.csv\"\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build object routing by split (FIX IndexError)\n",
    "# ----------------------------\n",
    "train_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "test_ids_by_split  = {s: [] for s in SPLIT_LIST}\n",
    "\n",
    "# groups: split -> Index(labels=object_id)\n",
    "tr_groups = df_train_meta.groupby(\"split\").groups\n",
    "te_groups = df_test_meta.groupby(\"split\").groups\n",
    "\n",
    "for sp, idx in tr_groups.items():\n",
    "    if sp in train_ids_by_split:\n",
    "        # idx sudah berisi object_id labels\n",
    "        train_ids_by_split[sp] = pd.Index(idx).astype(str).tolist()\n",
    "\n",
    "for sp, idx in te_groups.items():\n",
    "    if sp in test_ids_by_split:\n",
    "        test_ids_by_split[sp] = pd.Index(idx).astype(str).tolist()\n",
    "\n",
    "# sanity\n",
    "if sum(len(v) for v in train_ids_by_split.values()) != len(df_train_meta):\n",
    "    raise RuntimeError(\"Routing train_ids_by_split mismatch total vs df_train_meta length.\")\n",
    "if sum(len(v) for v in test_ids_by_split.values()) != len(df_test_meta):\n",
    "    raise RuntimeError(\"Routing test_ids_by_split mismatch total vs df_test_meta length.\")\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    \"split\": SPLIT_LIST,\n",
    "    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "})\n",
    "counts_path = ART_DIR / \"object_counts_by_split.csv\"\n",
    "df_counts.to_csv(counts_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Robust header mapping -> canonical columns\n",
    "# ----------------------------\n",
    "_LC_CFG_CACHE = {}  # (split_name, which) -> cfg dict\n",
    "\n",
    "def _canon_col(x: str) -> str:\n",
    "    s = str(x).strip().lower()\n",
    "    s = s.replace(\"\\ufeff\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    s = s.replace(\"-\", \"_\")\n",
    "    return s\n",
    "\n",
    "def _build_lc_read_cfg(p: Path):\n",
    "    h = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n",
    "    orig_cols = list(h.columns)\n",
    "\n",
    "    c2o = {}\n",
    "    for c in orig_cols:\n",
    "        k = _canon_col(c)\n",
    "        if k not in c2o:\n",
    "            c2o[k] = c\n",
    "\n",
    "    obj_col = c2o.get(\"object_id\", None)\n",
    "\n",
    "    time_col = None\n",
    "    for k in [\"time_mjd\", \"timemjd\", \"mjd\", \"time\"]:\n",
    "        if k in c2o:\n",
    "            time_col = c2o[k]\n",
    "            break\n",
    "\n",
    "    flux_col = c2o.get(\"flux\", None)\n",
    "\n",
    "    ferr_col = None\n",
    "    for k in [\"flux_err\", \"fluxerr\", \"fluxerror\"]:\n",
    "        if k in c2o:\n",
    "            ferr_col = c2o[k]\n",
    "            break\n",
    "\n",
    "    filt_col = c2o.get(\"filter\", None)\n",
    "\n",
    "    missing = []\n",
    "    if obj_col is None:  missing.append(\"object_id\")\n",
    "    if time_col is None: missing.append(\"Time (MJD)\")\n",
    "    if flux_col is None: missing.append(\"Flux\")\n",
    "    if ferr_col is None: missing.append(\"Flux_err\")\n",
    "    if filt_col is None: missing.append(\"Filter\")\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Missing required lightcurve columns in {p.name}: {missing}. \"\n",
    "            f\"Header sample: {orig_cols[:20]}\"\n",
    "        )\n",
    "\n",
    "    usecols = [obj_col, time_col, flux_col, ferr_col, filt_col]\n",
    "    rename = {obj_col:\"object_id\", time_col:\"mjd\", flux_col:\"flux\", ferr_col:\"flux_err\", filt_col:\"filter\"}\n",
    "\n",
    "    # enforce string only for id/filter (numeric we'll coerce later)\n",
    "    dtypes = {obj_col:\"string\", filt_col:\"string\"}\n",
    "\n",
    "    return {\"usecols\": usecols, \"dtype\": dtypes, \"rename\": rename}\n",
    "\n",
    "def _normalize_lc_chunk(df: pd.DataFrame, drop_bad_filter: bool = True, drop_bad_mjd: bool = True):\n",
    "    df = df[[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"]].copy()\n",
    "\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "    df[\"filter\"] = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n",
    "\n",
    "    df.loc[~df[\"filter\"].isin(list(ALLOWED_FILTERS)), \"filter\"] = pd.NA\n",
    "\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(\"float32\")\n",
    "    df[\"flux\"] = pd.to_numeric(df[\"flux\"], errors=\"coerce\").astype(\"float32\")\n",
    "    df[\"flux_err\"] = pd.to_numeric(df[\"flux_err\"], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    fe = df[\"flux_err\"]\n",
    "    if MIN_FLUXERR > 0:\n",
    "        df.loc[fe.notna() & (fe < MIN_FLUXERR), \"flux_err\"] = np.float32(MIN_FLUXERR)\n",
    "\n",
    "    df = df[df[\"object_id\"].notna() & (df[\"object_id\"] != \"\")]\n",
    "    if drop_bad_filter:\n",
    "        df = df[df[\"filter\"].notna()]\n",
    "    if drop_bad_mjd:\n",
    "        df = df[df[\"mjd\"].notna()]\n",
    "\n",
    "    return df[REQ_LC_KEYS]\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Chunked readers\n",
    "# ----------------------------\n",
    "def iter_lightcurve_chunks(\n",
    "    split_name: str,\n",
    "    which: str,\n",
    "    chunksize: int = 400_000,\n",
    "    drop_bad_filter: bool = True,\n",
    "    drop_bad_mjd: bool = True\n",
    "):\n",
    "    if split_name not in SPLIT_FILES:\n",
    "        raise KeyError(f\"Unknown split_name={split_name}.\")\n",
    "    if which not in (\"train\", \"test\"):\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    p = SPLIT_FILES[split_name][which]\n",
    "    key = (split_name, which)\n",
    "    if key not in _LC_CFG_CACHE:\n",
    "        _LC_CFG_CACHE[key] = _build_lc_read_cfg(p)\n",
    "    cfg = _LC_CFG_CACHE[key]\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        p,\n",
    "        usecols=cfg[\"usecols\"],\n",
    "        dtype=cfg[\"dtype\"],\n",
    "        chunksize=int(chunksize),\n",
    "        **SAFE_READ_KW\n",
    "    )\n",
    "    for chunk in reader:\n",
    "        chunk = chunk.rename(columns=cfg[\"rename\"])\n",
    "        yield _normalize_lc_chunk(chunk, drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd)\n",
    "\n",
    "def load_object_lightcurve(\n",
    "    object_id: str,\n",
    "    which: str,\n",
    "    chunksize: int = 400_000,\n",
    "    sort_time: bool = True,\n",
    "    max_chunks: int = None,\n",
    "    stop_after_found_block: bool = True\n",
    "):\n",
    "    object_id = str(object_id).strip()\n",
    "\n",
    "    if which == \"train\":\n",
    "        if object_id not in df_train_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n",
    "        split_name = str(df_train_meta.loc[object_id, \"split\"])\n",
    "    elif which == \"test\":\n",
    "        if object_id not in df_test_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n",
    "        split_name = str(df_test_meta.loc[object_id, \"split\"])\n",
    "    else:\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    pieces = []\n",
    "    seen = 0\n",
    "    found_any = False\n",
    "    last_hit = False\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=chunksize):\n",
    "        seen += 1\n",
    "        sub = ch[ch[\"object_id\"] == object_id]\n",
    "        hit = (len(sub) > 0)\n",
    "        if hit:\n",
    "            pieces.append(sub)\n",
    "            found_any = True\n",
    "\n",
    "        if stop_after_found_block and found_any and last_hit and (not hit):\n",
    "            break\n",
    "        last_hit = hit\n",
    "\n",
    "        if max_chunks is not None and seen >= int(max_chunks):\n",
    "            break\n",
    "\n",
    "    if not pieces:\n",
    "        out = pd.DataFrame(columns=REQ_LC_KEYS)\n",
    "    else:\n",
    "        out = pd.concat(pieces, ignore_index=True)\n",
    "        if sort_time and len(out) > 1:\n",
    "            out[\"filter_ord\"] = out[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n",
    "            out = out.sort_values([\"mjd\", \"filter_ord\"], kind=\"mergesort\").drop(columns=[\"filter_ord\"]).reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Quick smoke test\n",
    "# ----------------------------\n",
    "_smoke_splits = [\"split_01\", \"split_08\", \"split_17\"]\n",
    "for s in _smoke_splits:\n",
    "    if len(train_ids_by_split.get(s, [])) == 0 or len(test_ids_by_split.get(s, [])) == 0:\n",
    "        raise RuntimeError(f\"Split {s} has 0 objects in train/test meta (unexpected).\")\n",
    "\n",
    "    ch_tr = next(iter_lightcurve_chunks(s, \"train\", chunksize=50_000))\n",
    "    ch_te = next(iter_lightcurve_chunks(s, \"test\",  chunksize=50_000))\n",
    "\n",
    "    if list(ch_tr.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Train chunk schema mismatch in {s}: {list(ch_tr.columns)}\")\n",
    "    if list(ch_te.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Test chunk schema mismatch in {s}: {list(ch_te.columns)}\")\n",
    "\n",
    "    badf_tr = sorted(set(ch_tr[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n",
    "    badf_te = sorted(set(ch_te[\"filter\"].dropna().unique()) - ALLOWED_FILTERS)\n",
    "    if badf_tr or badf_te:\n",
    "        raise ValueError(f\"Unexpected filter values in smoke chunk split={s}: train_bad={badf_tr} test_bad={badf_te}\")\n",
    "\n",
    "print(\"STAGE 3 OK — LIGHTCURVE LOADING UTILITIES READY\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved counts  : {counts_path}\")\n",
    "\n",
    "globals().update({\n",
    "    \"SPLIT_FILES\": SPLIT_FILES,\n",
    "    \"train_ids_by_split\": train_ids_by_split,\n",
    "    \"test_ids_by_split\": test_ids_by_split,\n",
    "    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n",
    "    \"load_object_lightcurve\": load_object_lightcurve,\n",
    "    \"REQ_LC_KEYS\": REQ_LC_KEYS,\n",
    "    \"ALLOWED_FILTERS\": ALLOWED_FILTERS,\n",
    "    \"FILTER_ORDER\": FILTER_ORDER,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68b50d",
   "metadata": {
    "papermill": {
     "duration": 0.010408,
     "end_time": "2026-01-03T15:24:49.406406",
     "exception": false,
     "start_time": "2026-01-03T15:24:49.395998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c14386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:24:49.431800Z",
     "iopub.status.busy": "2026-01-03T15:24:49.431200Z",
     "iopub.status.idle": "2026-01-03T15:24:57.386204Z",
     "shell.execute_reply": "2026-01-03T15:24:57.385104Z"
    },
    "papermill": {
     "duration": 7.97135,
     "end_time": "2026-01-03T15:24:57.388266",
     "exception": false,
     "start_time": "2026-01-03T15:24:49.416916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4] REBUILD_MODE=wipe_all | Writing to: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/lc_clean_mag\n",
      "[Stage 4] split_01/train: parts=1 | rows=26,324 | det%=19.34% | nan_flux=11 | drop_time=0 | mag_range=[19.72, 25.96] | time=0.1s\n",
      "[Stage 4] split_01/test: parts=1 | rows=59,235 | det%=23.02% | nan_flux=23 | drop_time=0 | mag_range=[19.61, 26.20] | time=0.3s\n",
      "[Stage 4] split_02/train: parts=1 | rows=25,609 | det%=24.45% | nan_flux=6 | drop_time=0 | mag_range=[20.10, 26.04] | time=0.1s\n",
      "[Stage 4] split_02/test: parts=1 | rows=71,229 | det%=21.69% | nan_flux=8 | drop_time=0 | mag_range=[18.77, 26.32] | time=0.3s\n",
      "[Stage 4] split_03/train: parts=1 | rows=21,676 | det%=21.65% | nan_flux=5 | drop_time=0 | mag_range=[20.17, 26.23] | time=0.1s\n",
      "[Stage 4] split_03/test: parts=1 | rows=53,751 | det%=21.90% | nan_flux=8 | drop_time=0 | mag_range=[19.61, 26.37] | time=0.2s\n",
      "[Stage 4] split_04/train: parts=1 | rows=22,898 | det%=21.11% | nan_flux=12 | drop_time=0 | mag_range=[20.38, 26.16] | time=0.1s\n",
      "[Stage 4] split_04/test: parts=1 | rows=51,408 | det%=21.70% | nan_flux=50 | drop_time=0 | mag_range=[19.66, 26.25] | time=0.2s\n",
      "[Stage 4] split_05/train: parts=1 | rows=25,934 | det%=18.33% | nan_flux=15 | drop_time=0 | mag_range=[18.82, 26.33] | time=0.1s\n",
      "[Stage 4] split_05/test: parts=1 | rows=61,179 | det%=18.21% | nan_flux=42 | drop_time=0 | mag_range=[19.09, 26.23] | time=0.3s\n",
      "[Stage 4] split_06/train: parts=1 | rows=25,684 | det%=18.85% | nan_flux=11 | drop_time=0 | mag_range=[19.80, 26.15] | time=0.1s\n",
      "[Stage 4] split_06/test: parts=1 | rows=57,620 | det%=19.94% | nan_flux=21 | drop_time=0 | mag_range=[19.28, 26.04] | time=0.3s\n",
      "[Stage 4] split_07/train: parts=1 | rows=24,473 | det%=21.44% | nan_flux=306 | drop_time=0 | mag_range=[20.05, 26.32] | time=0.1s\n",
      "[Stage 4] split_07/test: parts=1 | rows=65,101 | det%=19.10% | nan_flux=904 | drop_time=0 | mag_range=[19.97, 26.32] | time=0.3s\n",
      "[Stage 4] split_08/train: parts=1 | rows=25,571 | det%=22.80% | nan_flux=6 | drop_time=0 | mag_range=[18.59, 26.30] | time=0.1s\n",
      "[Stage 4] split_08/test: parts=1 | rows=61,498 | det%=24.50% | nan_flux=47 | drop_time=0 | mag_range=[19.13, 25.96] | time=0.3s\n",
      "[Stage 4] split_09/train: parts=1 | rows=19,690 | det%=21.13% | nan_flux=315 | drop_time=0 | mag_range=[19.83, 26.31] | time=0.1s\n",
      "[Stage 4] split_09/test: parts=1 | rows=47,239 | det%=22.70% | nan_flux=514 | drop_time=0 | mag_range=[18.88, 26.11] | time=0.2s\n",
      "[Stage 4] split_10/train: parts=1 | rows=25,151 | det%=20.86% | nan_flux=7 | drop_time=0 | mag_range=[19.52, 26.21] | time=0.1s\n",
      "[Stage 4] split_10/test: parts=1 | rows=51,056 | det%=21.21% | nan_flux=51 | drop_time=0 | mag_range=[16.62, 26.43] | time=0.2s\n",
      "[Stage 4] split_11/train: parts=1 | rows=22,927 | det%=19.59% | nan_flux=4 | drop_time=0 | mag_range=[20.57, 26.42] | time=0.1s\n",
      "[Stage 4] split_11/test: parts=1 | rows=49,723 | det%=20.17% | nan_flux=12 | drop_time=0 | mag_range=[20.31, 26.42] | time=0.2s\n",
      "[Stage 4] split_12/train: parts=1 | rows=25,546 | det%=19.64% | nan_flux=12 | drop_time=0 | mag_range=[20.40, 26.32] | time=0.1s\n",
      "[Stage 4] split_12/test: parts=1 | rows=54,499 | det%=19.29% | nan_flux=13 | drop_time=0 | mag_range=[19.98, 26.26] | time=0.3s\n",
      "[Stage 4] split_13/train: parts=1 | rows=23,203 | det%=20.64% | nan_flux=2 | drop_time=0 | mag_range=[19.46, 26.05] | time=0.1s\n",
      "[Stage 4] split_13/test: parts=1 | rows=63,653 | det%=19.56% | nan_flux=15 | drop_time=0 | mag_range=[18.30, 26.15] | time=0.3s\n",
      "[Stage 4] split_14/train: parts=1 | rows=25,706 | det%=20.36% | nan_flux=23 | drop_time=0 | mag_range=[20.52, 26.40] | time=0.1s\n",
      "[Stage 4] split_14/test: parts=1 | rows=58,643 | det%=17.91% | nan_flux=43 | drop_time=0 | mag_range=[20.41, 26.04] | time=0.3s\n",
      "[Stage 4] split_15/train: parts=1 | rows=23,972 | det%=19.09% | nan_flux=116 | drop_time=0 | mag_range=[20.11, 26.04] | time=0.1s\n",
      "[Stage 4] split_15/test: parts=1 | rows=52,943 | det%=20.03% | nan_flux=197 | drop_time=0 | mag_range=[19.91, 26.24] | time=0.2s\n",
      "[Stage 4] split_16/train: parts=1 | rows=25,173 | det%=21.42% | nan_flux=3 | drop_time=0 | mag_range=[20.00, 26.26] | time=0.1s\n",
      "[Stage 4] split_16/test: parts=1 | rows=58,192 | det%=20.12% | nan_flux=13 | drop_time=0 | mag_range=[19.57, 26.17] | time=0.3s\n",
      "[Stage 4] split_17/train: parts=1 | rows=22,705 | det%=22.09% | nan_flux=12 | drop_time=0 | mag_range=[19.64, 26.17] | time=0.1s\n",
      "[Stage 4] split_17/test: parts=1 | rows=59,482 | det%=19.59% | nan_flux=21 | drop_time=0 | mag_range=[19.96, 26.42] | time=0.3s\n",
      "[Stage 4] split_18/train: parts=1 | rows=21,536 | det%=23.77% | nan_flux=7 | drop_time=0 | mag_range=[20.49, 26.05] | time=0.1s\n",
      "[Stage 4] split_18/test: parts=1 | rows=53,887 | det%=23.88% | nan_flux=14 | drop_time=0 | mag_range=[20.60, 26.44] | time=0.3s\n",
      "[Stage 4] split_19/train: parts=1 | rows=22,087 | det%=23.73% | nan_flux=8 | drop_time=0 | mag_range=[19.98, 26.31] | time=0.1s\n",
      "[Stage 4] split_19/test: parts=1 | rows=56,355 | det%=24.17% | nan_flux=16 | drop_time=0 | mag_range=[20.30, 26.34] | time=0.3s\n",
      "[Stage 4] split_20/train: parts=1 | rows=23,519 | det%=20.45% | nan_flux=10 | drop_time=0 | mag_range=[19.80, 25.93] | time=0.1s\n",
      "[Stage 4] split_20/test: parts=1 | rows=58,432 | det%=19.65% | nan_flux=10 | drop_time=0 | mag_range=[19.87, 26.36] | time=0.3s\n",
      "\n",
      "[Stage 4] Done.\n",
      "- LC_CLEAN_DIR  : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/lc_clean_mag\n",
      "- Saved manifest: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
      "- Saved summary : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/lc_clean_mag/lc_clean_mag_summary.csv\n",
      "- Saved config  : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/lc_clean_mag/photometric_config_mag.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Photometric Cleaning (FORCE OVERWRITE) — REVISI FULL v6.2\n",
    "# FIX v6.2:\n",
    "# - Safety guard: Path.is_relative_to (anti false-positive substring)\n",
    "# - Atomic parquet: tmp file tetap .parquet (part_0000.tmp.parquet)\n",
    "# - Filter normalization tanpa np.char (aman untuk <NA>/mixed dtype)\n",
    "# - EBV pakai EBV_clip jika tersedia (lebih stabil), fallback EBV\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings, time, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings\n",
    "# ----------------------------\n",
    "CHUNKSIZE   = 350_000\n",
    "ERR_EPS     = 1e-6\n",
    "SNR_DET     = 3.0\n",
    "DET_SIGMA   = 3.0\n",
    "\n",
    "MIN_FLUX_POS_UJY   = 1e-6\n",
    "MAG_MIN, MAG_MAX   = -10.0, 50.0\n",
    "MAGERR_FLOOR_DET   = 1e-3\n",
    "MAGERR_FLOOR_ND    = 0.75\n",
    "MAGERR_CAP         = 10.0\n",
    "\n",
    "WRITE_FORMAT = \"parquet\"   # \"parquet\" or \"csv.gz\"\n",
    "ONLY_SPLITS  = None        # e.g. [\"split_01\"] untuk test cepat\n",
    "KEEP_FLUX_DEBUG = False\n",
    "DROP_BAD_TIME_ROWS = True\n",
    "\n",
    "# FORCE overwrite\n",
    "REBUILD_MODE = \"wipe_all\"  # \"wipe_all\" | \"wipe_parts_only\"\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Extinction coefficients (placeholder; ganti kalau punya nilai resmi)\n",
    "# ----------------------------\n",
    "EXT_RLAMBDA = {\"u\": 4.8, \"g\": 3.6, \"r\": 2.7, \"i\": 2.1, \"z\": 1.6, \"y\": 1.3}\n",
    "BAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\n",
    "ID2BAND = {v: k for k, v in BAND2ID.items()}\n",
    "\n",
    "# pakai EBV_clip jika ada (lebih stabil), fallback EBV\n",
    "EBV_TRAIN_SER = df_train_meta[\"EBV_clip\"] if \"EBV_clip\" in df_train_meta.columns else df_train_meta[\"EBV\"]\n",
    "EBV_TEST_SER  = df_test_meta[\"EBV_clip\"]  if \"EBV_clip\"  in df_test_meta.columns  else df_test_meta[\"EBV\"]\n",
    "\n",
    "MAG_ZP = float(2.5 * np.log10(3631e6))  # ~23.9\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Output root + WIPE (with safety guard)\n",
    "# ----------------------------\n",
    "LC_CLEAN_DIR = ART_DIR / \"lc_clean_mag\"\n",
    "\n",
    "art_abs = ART_DIR.resolve()\n",
    "lc_abs  = LC_CLEAN_DIR.resolve()\n",
    "\n",
    "# robust safety guard\n",
    "try:\n",
    "    ok_rel = lc_abs.is_relative_to(art_abs)\n",
    "except AttributeError:\n",
    "    # very old python fallback (shouldn't happen on Kaggle py3.12)\n",
    "    ok_rel = str(lc_abs).startswith(str(art_abs) + \"/\") or str(lc_abs).startswith(str(art_abs) + \"\\\\\")\n",
    "\n",
    "if not ok_rel:\n",
    "    raise RuntimeError(f\"Safety guard failed: LC_CLEAN_DIR bukan turunan ART_DIR.\\nART_DIR={art_abs}\\nLC_CLEAN_DIR={lc_abs}\")\n",
    "\n",
    "if REBUILD_MODE == \"wipe_all\":\n",
    "    if LC_CLEAN_DIR.exists():\n",
    "        shutil.rmtree(LC_CLEAN_DIR, ignore_errors=True)\n",
    "    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "elif REBUILD_MODE == \"wipe_parts_only\":\n",
    "    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    raise ValueError(\"REBUILD_MODE must be 'wipe_all' or 'wipe_parts_only'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Atomic writer (tmp -> rename)\n",
    "# ----------------------------\n",
    "def _atomic_write_parquet(df: pd.DataFrame, out_path: Path):\n",
    "    # tmp tetap berakhiran .parquet agar engine tidak rewel\n",
    "    tmp = out_path.with_name(out_path.stem + \".tmp\" + out_path.suffix)  # part_0000.tmp.parquet\n",
    "    try:\n",
    "        df.to_parquet(tmp, index=False)\n",
    "        tmp.replace(out_path)\n",
    "    finally:\n",
    "        # cleanup kalau gagal sebelum replace\n",
    "        if tmp.exists() and (not out_path.exists()):\n",
    "            try:\n",
    "                tmp.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def _atomic_write_csv_gz(df: pd.DataFrame, out_path: Path):\n",
    "    final_path = out_path.with_suffix(\".csv.gz\")\n",
    "    tmp = final_path.with_name(final_path.stem + \".tmp\" + \"\".join(final_path.suffixes))  # keep .csv.gz\n",
    "    try:\n",
    "        df.to_csv(tmp, index=False, compression=\"gzip\")\n",
    "        tmp.replace(final_path)\n",
    "    finally:\n",
    "        if tmp.exists() and (not final_path.exists()):\n",
    "            try:\n",
    "                tmp.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return final_path\n",
    "\n",
    "def write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == \"parquet\":\n",
    "        try:\n",
    "            _atomic_write_parquet(df, out_path)\n",
    "            return \"parquet\", out_path\n",
    "        except Exception as e:\n",
    "            alt = _atomic_write_csv_gz(df, out_path)\n",
    "            return f\"csv.gz (fallback from parquet: {type(e).__name__})\", alt\n",
    "    elif fmt == \"csv.gz\":\n",
    "        alt = _atomic_write_csv_gz(df, out_path)\n",
    "        return \"csv.gz\", alt\n",
    "    else:\n",
    "        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Core cleaning (NaN/negative-safe)\n",
    "# ----------------------------\n",
    "def clean_chunk_to_mag(ch: pd.DataFrame, ebv_ser: pd.Series):\n",
    "    # normalize id/filter safely (tanpa np.char)\n",
    "    oid_ser = ch[\"object_id\"].astype(\"string\").str.strip()\n",
    "    filt_ser = ch[\"filter\"].astype(\"string\").str.strip().str.lower()\n",
    "\n",
    "    # numeric\n",
    "    mjd = ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    err  = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    # sanitize err\n",
    "    err = np.nan_to_num(err, nan=np.float32(ERR_EPS), posinf=np.float32(ERR_EPS), neginf=np.float32(ERR_EPS))\n",
    "    err = np.maximum(err, np.float32(ERR_EPS))\n",
    "\n",
    "    # sanitize flux: inf -> NaN (NaN tetap)\n",
    "    flux = flux.astype(np.float32, copy=False)\n",
    "    flux[~np.isfinite(flux)] = np.float32(np.nan)\n",
    "\n",
    "    # band_id mapping (vectorized masks; aman walau filt_ser ada <NA>)\n",
    "    filt = filt_ser.fillna(\"\").to_numpy(dtype=object, copy=False)\n",
    "    band_id = np.full(len(ch), -1, dtype=np.int8)\n",
    "    for b, bid in BAND2ID.items():\n",
    "        band_id[filt == b] = np.int8(bid)\n",
    "\n",
    "    if np.any(band_id < 0):\n",
    "        bad = pd.Series(filt[band_id < 0]).value_counts().head(10).index.tolist()\n",
    "        raise ValueError(f\"Unknown/invalid filter values encountered (top examples): {bad}\")\n",
    "\n",
    "    # EBV lookup (missing -> 0.0)\n",
    "    ebv = oid_ser.map(ebv_ser).fillna(0.0).to_numpy(dtype=np.float32)\n",
    "    ebv[~np.isfinite(ebv)] = np.float32(0.0)\n",
    "\n",
    "    # R_lambda lookup\n",
    "    rlam = np.zeros(len(ch), dtype=np.float32)\n",
    "    for b, rv in EXT_RLAMBDA.items():\n",
    "        rlam[filt == b] = np.float32(rv)\n",
    "\n",
    "    A = (rlam * ebv).astype(np.float32)\n",
    "    mul = np.power(np.float32(10.0), (np.float32(0.4) * A)).astype(np.float32)\n",
    "\n",
    "    flux_deext = (flux * mul).astype(np.float32)\n",
    "    err_deext  = (err  * mul).astype(np.float32)\n",
    "\n",
    "    # snr (NaN flux -> 0)\n",
    "    okf = np.isfinite(flux_deext)\n",
    "    snr = np.zeros_like(err_deext, dtype=np.float32)\n",
    "    snr[okf] = (flux_deext[okf] / np.maximum(err_deext[okf], np.float32(ERR_EPS))).astype(np.float32)\n",
    "\n",
    "    detected = (snr > np.float32(SNR_DET)).astype(np.int8)\n",
    "\n",
    "    nan_flux_rows = int((~okf).sum())\n",
    "    if nan_flux_rows:\n",
    "        detected[~okf] = np.int8(0)\n",
    "        snr[~okf] = np.float32(0.0)\n",
    "\n",
    "    flux_detlim = (np.float32(DET_SIGMA) * err_deext).astype(np.float32)\n",
    "\n",
    "    flux_for_mag = np.where(\n",
    "        detected == 1,\n",
    "        np.maximum(flux_deext, np.float32(MIN_FLUX_POS_UJY)),\n",
    "        np.maximum(flux_detlim, np.float32(MIN_FLUX_POS_UJY)),\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    mag = (np.float32(MAG_ZP) - np.float32(2.5) * np.log10(flux_for_mag)).astype(np.float32)\n",
    "    mag = np.clip(mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32)\n",
    "\n",
    "    mag_err = (np.float32(1.0857362) * (err_deext / flux_for_mag)).astype(np.float32)\n",
    "    mag_err = np.clip(mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32)\n",
    "\n",
    "    if MAGERR_FLOOR_ND is not None and float(MAGERR_FLOOR_ND) > 0:\n",
    "        mag_err = np.where(\n",
    "            detected == 1,\n",
    "            mag_err,\n",
    "            np.maximum(mag_err, np.float32(MAGERR_FLOOR_ND))\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"object_id\": pd.array(oid_ser.to_numpy(copy=False), dtype=\"string\"),\n",
    "        \"mjd\": mjd.astype(np.float32, copy=False),\n",
    "        \"band_id\": band_id.astype(np.int8, copy=False),\n",
    "        \"mag\": mag.astype(np.float32, copy=False),\n",
    "        \"mag_err\": mag_err.astype(np.float32, copy=False),\n",
    "        \"snr\": snr.astype(np.float32, copy=False),\n",
    "        \"detected\": detected.astype(np.int8, copy=False),\n",
    "    })\n",
    "\n",
    "    dropped_time = 0\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        t = out[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "        keep = np.isfinite(t)\n",
    "        dropped_time = int((~keep).sum())\n",
    "        if dropped_time:\n",
    "            out = out[keep]\n",
    "\n",
    "    if KEEP_FLUX_DEBUG:\n",
    "        out[\"flux_deext\"] = pd.Series(np.nan_to_num(flux_deext, nan=0.0), dtype=\"float32\")\n",
    "        out[\"err_deext\"]  = pd.Series(err_deext, dtype=\"float32\")\n",
    "\n",
    "    return out, dropped_time, nan_flux_rows\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Process split-wise\n",
    "# ----------------------------\n",
    "splits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else list(SPLIT_LIST)\n",
    "\n",
    "summary_rows, manifest_rows = [], []\n",
    "\n",
    "def _wipe_parts_dir(out_dir: Path):\n",
    "    if out_dir.exists():\n",
    "        for pat in [\"part_*.parquet\", \"part_*.csv.gz\", \"*.tmp\", \"*.tmp.parquet\", \"*.tmp.csv.gz\"]:\n",
    "            for f in out_dir.glob(pat):\n",
    "                try:\n",
    "                    f.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def process_split(split_name: str, which: str):\n",
    "    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n",
    "    out_dir = LC_CLEAN_DIR / split_name / which\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if REBUILD_MODE == \"wipe_parts_only\":\n",
    "        _wipe_parts_dir(out_dir)\n",
    "\n",
    "    t0 = time.time()\n",
    "    part_idx = 0\n",
    "    n_rows_total = 0\n",
    "    n_det = 0\n",
    "    n_finite_mag = 0\n",
    "    mag_min = np.inf\n",
    "    mag_max = -np.inf\n",
    "    dropped_time_total = 0\n",
    "    nan_flux_total = 0\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n",
    "        cleaned, dropped_time, nan_flux = clean_chunk_to_mag(ch, ebv_ser)\n",
    "\n",
    "        dropped_time_total += int(dropped_time)\n",
    "        nan_flux_total += int(nan_flux)\n",
    "\n",
    "        n_rows = int(len(cleaned))\n",
    "        n_rows_total += n_rows\n",
    "\n",
    "        det_arr = cleaned[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n",
    "        n_det += int(det_arr.sum())\n",
    "\n",
    "        mag_arr = cleaned[\"mag\"].to_numpy(dtype=np.float32, copy=False)\n",
    "        fin = np.isfinite(mag_arr)\n",
    "        n_finite_mag += int(fin.sum())\n",
    "        if fin.any():\n",
    "            mag_min = float(min(mag_min, float(np.min(mag_arr[fin]))))\n",
    "            mag_max = float(max(mag_max, float(np.max(mag_arr[fin]))))\n",
    "\n",
    "        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n",
    "        used_fmt, final_path = write_part(cleaned, out_path, WRITE_FORMAT)\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"which\": which,\n",
    "            \"part\": int(part_idx),\n",
    "            \"path\": str(final_path),\n",
    "            \"rows\": int(n_rows),\n",
    "            \"format\": str(used_fmt),\n",
    "        })\n",
    "\n",
    "        part_idx += 1\n",
    "        del cleaned, ch\n",
    "        if part_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    summary_rows.append({\n",
    "        \"split\": split_name,\n",
    "        \"which\": which,\n",
    "        \"parts\": int(part_idx),\n",
    "        \"rows\": int(n_rows_total),\n",
    "        \"det_frac_snr_gt_thr\": float(n_det / max(n_rows_total, 1)),\n",
    "        \"finite_mag_frac\": float(n_finite_mag / max(n_rows_total, 1)),\n",
    "        \"mag_min\": (mag_min if np.isfinite(mag_min) else np.nan),\n",
    "        \"mag_max\": (mag_max if np.isfinite(mag_max) else np.nan),\n",
    "        \"dropped_time_rows\": int(dropped_time_total),\n",
    "        \"nan_flux_rows\": int(nan_flux_total),\n",
    "        \"sec\": float(dt),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | \"\n",
    "        f\"det%={100*(n_det/max(n_rows_total,1)):.2f}% | \"\n",
    "        f\"nan_flux={nan_flux_total:,} | drop_time={dropped_time_total:,} | \"\n",
    "        f\"mag_range=[{(mag_min if np.isfinite(mag_min) else np.nan):.2f}, {(mag_max if np.isfinite(mag_max) else np.nan):.2f}] | \"\n",
    "        f\"time={dt:.1f}s\"\n",
    "    )\n",
    "\n",
    "print(f\"[Stage 4] REBUILD_MODE={REBUILD_MODE} | Writing to: {LC_CLEAN_DIR}\")\n",
    "for s in splits_to_use:\n",
    "    process_split(s, \"train\")\n",
    "    process_split(s, \"test\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Save manifests + summary + config\n",
    "# ----------------------------\n",
    "df_parts_manifest = pd.DataFrame(manifest_rows)\n",
    "df_summary  = pd.DataFrame(summary_rows)\n",
    "\n",
    "manifest_path = LC_CLEAN_DIR / \"lc_clean_mag_manifest.csv\"\n",
    "summary_path  = LC_CLEAN_DIR / \"lc_clean_mag_summary.csv\"\n",
    "\n",
    "df_parts_manifest.to_csv(manifest_path, index=False)\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "cfg_path = LC_CLEAN_DIR / \"photometric_config_mag.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "        \"SNR_DET\": float(SNR_DET),\n",
    "        \"DET_SIGMA\": float(DET_SIGMA),\n",
    "        \"ERR_EPS\": float(ERR_EPS),\n",
    "        \"MIN_FLUX_POS_UJY\": float(MIN_FLUX_POS_UJY),\n",
    "        \"MAG_ZP\": float(MAG_ZP),\n",
    "        \"MAG_MIN\": float(MAG_MIN),\n",
    "        \"MAG_MAX\": float(MAG_MAX),\n",
    "        \"MAGERR_FLOOR_DET\": float(MAGERR_FLOOR_DET),\n",
    "        \"MAGERR_FLOOR_ND\": float(MAGERR_FLOOR_ND),\n",
    "        \"MAGERR_CAP\": float(MAGERR_CAP),\n",
    "        \"CHUNKSIZE\": int(CHUNKSIZE),\n",
    "        \"WRITE_FORMAT\": str(WRITE_FORMAT),\n",
    "        \"ONLY_SPLITS\": list(splits_to_use),\n",
    "        \"KEEP_FLUX_DEBUG\": bool(KEEP_FLUX_DEBUG),\n",
    "        \"DROP_BAD_TIME_ROWS\": bool(DROP_BAD_TIME_ROWS),\n",
    "        \"REBUILD_MODE\": str(REBUILD_MODE),\n",
    "        \"EBV_SOURCE\": (\"EBV_clip\" if (\"EBV_clip\" in df_train_meta.columns and \"EBV_clip\" in df_test_meta.columns) else \"EBV\"),\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 4] Done.\")\n",
    "print(f\"- LC_CLEAN_DIR  : {LC_CLEAN_DIR}\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved summary : {summary_path}\")\n",
    "print(f\"- Saved config  : {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Helper for next stages\n",
    "# ----------------------------\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    m = df_parts_manifest[(df_parts_manifest[\"split\"] == split_name) & (df_parts_manifest[\"which\"] == which)].sort_values(\"part\")\n",
    "    return m[\"path\"].astype(str).tolist()\n",
    "\n",
    "globals().update({\n",
    "    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "    \"BAND2ID\": BAND2ID,\n",
    "    \"ID2BAND\": ID2BAND,\n",
    "    \"MAG_ZP\": MAG_ZP,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"lc_clean_mag_manifest\": df_parts_manifest,\n",
    "    \"lc_clean_mag_summary\": df_summary,\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febcf8f",
   "metadata": {
    "papermill": {
     "duration": 0.01226,
     "end_time": "2026-01-03T15:24:57.412140",
     "exception": false,
     "start_time": "2026-01-03T15:24:57.399880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Tokenization (Event-based Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3cef24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:24:57.438014Z",
     "iopub.status.busy": "2026-01-03T15:24:57.437664Z",
     "iopub.status.idle": "2026-01-03T15:30:47.851014Z",
     "shell.execute_reply": "2026-01-03T15:30:47.850023Z"
    },
    "papermill": {
     "duration": 350.435027,
     "end_time": "2026-01-03T15:30:47.858661",
     "exception": false,
     "start_time": "2026-01-03T15:24:57.423634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 5 ROUTING SYNC OK\n",
      "- RUN_DIR      : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76\n",
      "- ART_DIR      : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts\n",
      "- LC_CLEAN_DIR : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/lc_clean_mag\n",
      "- manifest_csv : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
      "- meta_sync    : meta already consistent\n",
      "\n",
      "[Stage 5] split_01/train | expected=155 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=26,324 | len_mean 169.8->145.3 | p95 191.7->191.7 | trunc%=3.9% | time=8.70s | mode=mag\n",
      "\n",
      "[Stage 5] split_01/test | expected=364 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=364 (missing_filled=0) | kept_rows=59,235 | len_mean 162.7->148.3 | p95 193.8->193.8 | trunc%=2.2% | time=9.15s | mode=mag\n",
      "\n",
      "[Stage 5] split_02/train | expected=170 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=170 (missing_filled=0) | kept_rows=25,609 | len_mean 150.6->145.9 | p95 195.5->195.5 | trunc%=1.2% | time=8.58s | mode=mag\n",
      "\n",
      "[Stage 5] split_02/test | expected=414 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=414 (missing_filled=0) | kept_rows=71,229 | len_mean 172.1->149.5 | p95 201.0->201.0 | trunc%=3.4% | time=9.19s | mode=mag\n",
      "\n",
      "[Stage 5] split_03/train | expected=138 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=138 (missing_filled=0) | kept_rows=21,676 | len_mean 157.1->145.1 | p95 191.8->191.8 | trunc%=2.2% | time=7.64s | mode=mag\n",
      "\n",
      "[Stage 5] split_03/test | expected=338 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=338 (missing_filled=0) | kept_rows=53,751 | len_mean 159.0->147.0 | p95 194.3->194.3 | trunc%=2.4% | time=8.94s | mode=mag\n",
      "\n",
      "[Stage 5] split_04/train | expected=145 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=145 (missing_filled=0) | kept_rows=22,898 | len_mean 157.9->139.7 | p95 187.0->187.0 | trunc%=2.8% | time=8.18s | mode=mag\n",
      "\n",
      "[Stage 5] split_04/test | expected=332 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=332 (missing_filled=0) | kept_rows=51,408 | len_mean 154.8->140.9 | p95 195.0->195.0 | trunc%=3.0% | time=8.94s | mode=mag\n",
      "\n",
      "[Stage 5] split_05/train | expected=165 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=165 (missing_filled=0) | kept_rows=25,934 | len_mean 157.2->146.9 | p95 191.6->191.6 | trunc%=2.4% | time=8.55s | mode=mag\n",
      "\n",
      "[Stage 5] split_05/test | expected=375 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=375 (missing_filled=0) | kept_rows=61,179 | len_mean 163.1->146.0 | p95 194.0->194.0 | trunc%=2.7% | time=9.08s | mode=mag\n",
      "\n",
      "[Stage 5] split_06/train | expected=155 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,684 | len_mean 165.7->145.8 | p95 198.0->198.0 | trunc%=3.2% | time=8.01s | mode=mag\n",
      "\n",
      "[Stage 5] split_06/test | expected=374 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=374 (missing_filled=0) | kept_rows=57,620 | len_mean 154.1->143.4 | p95 189.3->189.3 | trunc%=1.9% | time=9.08s | mode=mag\n",
      "\n",
      "[Stage 5] split_07/train | expected=165 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=165 (missing_filled=0) | kept_rows=24,473 | len_mean 148.3->146.9 | p95 191.6->191.6 | trunc%=0.6% | time=8.59s | mode=mag\n",
      "\n",
      "[Stage 5] split_07/test | expected=398 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=398 (missing_filled=0) | kept_rows=65,101 | len_mean 163.6->147.9 | p95 194.1->194.1 | trunc%=2.3% | time=9.15s | mode=mag\n",
      "\n",
      "[Stage 5] split_08/train | expected=162 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=162 (missing_filled=0) | kept_rows=25,571 | len_mean 157.8->144.1 | p95 185.9->185.9 | trunc%=1.9% | time=8.34s | mode=mag\n",
      "\n",
      "[Stage 5] split_08/test | expected=387 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=387 (missing_filled=0) | kept_rows=61,498 | len_mean 158.9->148.4 | p95 194.4->194.4 | trunc%=1.6% | time=9.28s | mode=mag\n",
      "\n",
      "[Stage 5] split_09/train | expected=128 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=128 (missing_filled=0) | kept_rows=19,690 | len_mean 153.8->144.8 | p95 192.3->192.3 | trunc%=1.6% | time=7.54s | mode=mag\n",
      "\n",
      "[Stage 5] split_09/test | expected=289 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=289 (missing_filled=0) | kept_rows=47,239 | len_mean 163.5->147.4 | p95 196.0->196.0 | trunc%=3.1% | time=9.00s | mode=mag\n",
      "\n",
      "[Stage 5] split_10/train | expected=144 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=144 (missing_filled=0) | kept_rows=25,151 | len_mean 174.7->150.1 | p95 204.4->204.4 | trunc%=4.2% | time=8.34s | mode=mag\n",
      "\n",
      "[Stage 5] split_10/test | expected=331 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=331 (missing_filled=0) | kept_rows=51,056 | len_mean 154.2->143.7 | p95 188.0->188.0 | trunc%=2.1% | time=9.06s | mode=mag\n",
      "\n",
      "[Stage 5] split_11/train | expected=146 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=146 (missing_filled=0) | kept_rows=22,927 | len_mean 157.0->146.2 | p95 192.0->192.0 | trunc%=1.4% | time=8.19s | mode=mag\n",
      "\n",
      "[Stage 5] split_11/test | expected=325 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=325 (missing_filled=0) | kept_rows=49,723 | len_mean 153.0->144.9 | p95 189.6->189.6 | trunc%=1.2% | time=8.85s | mode=mag\n",
      "\n",
      "[Stage 5] split_12/train | expected=155 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,546 | len_mean 164.8->149.3 | p95 193.9->193.9 | trunc%=3.2% | time=8.45s | mode=mag\n",
      "\n",
      "[Stage 5] split_12/test | expected=353 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=353 (missing_filled=0) | kept_rows=54,499 | len_mean 154.4->145.4 | p95 185.0->185.0 | trunc%=1.1% | time=8.93s | mode=mag\n",
      "\n",
      "[Stage 5] split_13/train | expected=143 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=143 (missing_filled=0) | kept_rows=23,203 | len_mean 162.3->149.3 | p95 190.9->190.9 | trunc%=2.1% | time=8.09s | mode=mag\n",
      "\n",
      "[Stage 5] split_13/test | expected=379 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=379 (missing_filled=0) | kept_rows=63,653 | len_mean 167.9->149.3 | p95 195.1->195.1 | trunc%=2.9% | time=9.06s | mode=mag\n",
      "\n",
      "[Stage 5] split_14/train | expected=154 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=154 (missing_filled=0) | kept_rows=25,706 | len_mean 166.9->151.1 | p95 198.0->198.0 | trunc%=2.6% | time=8.17s | mode=mag\n",
      "\n",
      "[Stage 5] split_14/test | expected=351 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=351 (missing_filled=0) | kept_rows=58,643 | len_mean 167.1->150.2 | p95 193.0->193.0 | trunc%=2.8% | time=9.06s | mode=mag\n",
      "\n",
      "[Stage 5] split_15/train | expected=158 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=158 (missing_filled=0) | kept_rows=23,972 | len_mean 151.7->143.7 | p95 195.0->195.0 | trunc%=2.5% | time=8.09s | mode=mag\n",
      "\n",
      "[Stage 5] split_15/test | expected=342 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=342 (missing_filled=0) | kept_rows=52,943 | len_mean 154.8->143.9 | p95 193.0->193.0 | trunc%=2.3% | time=9.07s | mode=mag\n",
      "\n",
      "[Stage 5] split_16/train | expected=155 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,173 | len_mean 162.4->149.0 | p95 199.9->199.9 | trunc%=1.9% | time=8.05s | mode=mag\n",
      "\n",
      "[Stage 5] split_16/test | expected=354 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=354 (missing_filled=0) | kept_rows=58,192 | len_mean 164.4->148.1 | p95 193.3->193.3 | trunc%=2.8% | time=8.86s | mode=mag\n",
      "\n",
      "[Stage 5] split_17/train | expected=153 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=153 (missing_filled=0) | kept_rows=22,705 | len_mean 148.4->145.0 | p95 190.4->190.4 | trunc%=1.3% | time=7.94s | mode=mag\n",
      "\n",
      "[Stage 5] split_17/test | expected=351 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=351 (missing_filled=0) | kept_rows=59,482 | len_mean 169.5->147.3 | p95 198.0->198.0 | trunc%=3.4% | time=8.91s | mode=mag\n",
      "\n",
      "[Stage 5] split_18/train | expected=152 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=152 (missing_filled=0) | kept_rows=21,536 | len_mean 141.7->137.8 | p95 191.2->191.2 | trunc%=1.3% | time=8.17s | mode=mag\n",
      "\n",
      "[Stage 5] split_18/test | expected=345 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=345 (missing_filled=0) | kept_rows=53,887 | len_mean 156.2->141.2 | p95 188.8->188.8 | trunc%=2.3% | time=9.04s | mode=mag\n",
      "\n",
      "[Stage 5] split_19/train | expected=147 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=147 (missing_filled=0) | kept_rows=22,087 | len_mean 150.3->142.0 | p95 188.4->188.4 | trunc%=1.4% | time=7.91s | mode=mag\n",
      "\n",
      "[Stage 5] split_19/test | expected=375 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=375 (missing_filled=0) | kept_rows=56,355 | len_mean 150.3->143.1 | p95 191.0->191.0 | trunc%=1.1% | time=9.03s | mode=mag\n",
      "\n",
      "[Stage 5] split_20/train | expected=153 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=153 (missing_filled=0) | kept_rows=23,519 | len_mean 153.7->143.2 | p95 187.8->187.8 | trunc%=1.3% | time=8.64s | mode=mag\n",
      "\n",
      "[Stage 5] split_20/test | expected=358 | L_MAX=256 | TRUNC=smart\n",
      "[Stage 5] OK: built=358 (missing_filled=0) | kept_rows=58,432 | len_mean 163.2->148.5 | p95 190.0->190.0 | trunc%=2.5% | time=8.95s | mode=mag\n",
      "\n",
      "[Stage 5] DONE\n",
      "- token_mode : mag\n",
      "- features   : ['t_rel_log', 'dt_log', 'mag', 'mag_err_log', 'snr_tanh', 'detected']\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/seq_tokens/seq_manifest_train.csv (rows=3,043)\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/seq_tokens/seq_manifest_test.csv  (rows=7,135)\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/seq_tokens/seq_build_stats.csv\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/seq_tokens/seq_config.json\n",
      "\n",
      "[Stage 5] Smoke test object_id=Dornhoth_anwar_melethron\n",
      "- seq_len=184 | X_shape=(184, 6) | bands_unique=[0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v5.2 (PATH+META SYNC HARDENED + MISSING-OBJECT SAFE + BUCKET ROBUST)\n",
    "#\n",
    "# FIX UTAMA v5.2:\n",
    "# - Auto-find STAGE 4 manifest dari run manapun.\n",
    "# - Sync path benar: RUN_DIR/ART_DIR/LC_CLEAN_DIR dari manifest.\n",
    "# - Auto-reload df_train_meta/df_test_meta dari ART_DIR synced jika mismatch.\n",
    "# - Validasi semua part path exists.\n",
    "# - Jika ada object_id tidak muncul di cleaned parts -> build EMPTY sequence (len=1) agar built==expected.\n",
    "# - Bucket writer aman (try/finally close) + cleanup tmp dir rmtree.\n",
    "#\n",
    "# OUTPUT:\n",
    "# - artifacts/seq_tokens/split_XX/{train|test}/shard_*.npz\n",
    "# - artifacts/seq_tokens/seq_manifest_{train|test}.csv\n",
    "# - artifacts/seq_tokens/seq_build_stats.csv\n",
    "# - artifacts/seq_tokens/seq_config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings, time, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "for need in [\"ART_DIR\", \"df_train_meta\", \"df_test_meta\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 0 + STAGE 2 dulu (ART_DIR + meta).\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers\n",
    "# ----------------------------\n",
    "def _safe_string_series(s: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return s.astype(\"string\").str.strip()\n",
    "    except Exception:\n",
    "        return s.astype(str).str.strip()\n",
    "\n",
    "def _find_stage4_manifest(art_dir: Path):\n",
    "    cand = art_dir / \"lc_clean_mag\" / \"lc_clean_mag_manifest.csv\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    if not root.exists():\n",
    "        return None\n",
    "\n",
    "    cands = list(root.glob(\"run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\"))\n",
    "    if not cands:\n",
    "        cands = list(root.glob(\"run_*/**/lc_clean_mag_manifest.csv\"))\n",
    "    if not cands:\n",
    "        return None\n",
    "\n",
    "    cands = sorted(cands, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def _sync_dirs_from_manifest(manifest_csv: Path):\n",
    "    lc_clean_dir = manifest_csv.parent\n",
    "    art_dir_new  = lc_clean_dir.parent\n",
    "    run_dir_new  = art_dir_new.parent\n",
    "    return run_dir_new, art_dir_new, lc_clean_dir\n",
    "\n",
    "def _load_meta_if_needed(art_dir_synced: Path):\n",
    "    \"\"\"\n",
    "    Jika df_train_meta/df_test_meta yg ada di memori mismatch dengan file meta di run synced,\n",
    "    maka reload dari file agar routing split/object_id konsisten.\n",
    "    \"\"\"\n",
    "    global df_train_meta, df_test_meta\n",
    "\n",
    "    tr_pq = art_dir_synced / \"train_meta.parquet\"\n",
    "    te_pq = art_dir_synced / \"test_meta.parquet\"\n",
    "    tr_csv = art_dir_synced / \"train_meta.csv\"\n",
    "    te_csv = art_dir_synced / \"test_meta.csv\"\n",
    "\n",
    "    # kalau file meta tidak ada, pakai yang di memori\n",
    "    if not (tr_pq.exists() or tr_csv.exists()) or not (te_pq.exists() or te_csv.exists()):\n",
    "        return False, \"meta file not found in synced ART_DIR; keep in-memory\"\n",
    "\n",
    "    def _read_meta(pq, csv):\n",
    "        if pq.exists():\n",
    "            return pd.read_parquet(pq).set_index(\"object_id\") if \"object_id\" in pd.read_parquet(pq, columns=None).columns else pd.read_parquet(pq)\n",
    "        else:\n",
    "            return pd.read_csv(csv).set_index(\"object_id\")\n",
    "\n",
    "    # load candidate (safe, minimal)\n",
    "    try:\n",
    "        if tr_pq.exists():\n",
    "            cand_train = pd.read_parquet(tr_pq)\n",
    "        else:\n",
    "            cand_train = pd.read_csv(tr_csv)\n",
    "\n",
    "        if te_pq.exists():\n",
    "            cand_test = pd.read_parquet(te_pq)\n",
    "        else:\n",
    "            cand_test = pd.read_csv(te_csv)\n",
    "\n",
    "        # ensure index=object_id\n",
    "        if \"object_id\" in cand_train.columns:\n",
    "            cand_train = cand_train.set_index(\"object_id\", drop=True)\n",
    "        if \"object_id\" in cand_test.columns:\n",
    "            cand_test = cand_test.set_index(\"object_id\", drop=True)\n",
    "\n",
    "        cand_train.index = cand_train.index.astype(\"string\")\n",
    "        cand_test.index = cand_test.index.astype(\"string\")\n",
    "\n",
    "        # mismatch check: size + a few ids overlap\n",
    "        mem_train_n = int(len(df_train_meta))\n",
    "        mem_test_n  = int(len(df_test_meta))\n",
    "        cand_train_n = int(len(cand_train))\n",
    "        cand_test_n  = int(len(cand_test))\n",
    "\n",
    "        if (mem_train_n != cand_train_n) or (mem_test_n != cand_test_n):\n",
    "            df_train_meta = cand_train\n",
    "            df_test_meta = cand_test\n",
    "            return True, f\"reloaded meta due to size mismatch: mem({mem_train_n},{mem_test_n}) -> file({cand_train_n},{cand_test_n})\"\n",
    "\n",
    "        # also check sample ids\n",
    "        sample_ids = df_train_meta.index[:5].astype(str).tolist()\n",
    "        ok = all((sid in cand_train.index) for sid in sample_ids)\n",
    "        if not ok:\n",
    "            df_train_meta = cand_train\n",
    "            df_test_meta = cand_test\n",
    "            return True, \"reloaded meta due to id mismatch\"\n",
    "\n",
    "        return False, \"meta already consistent\"\n",
    "    except Exception as e:\n",
    "        return False, f\"meta reload skipped (error: {type(e).__name__}: {e})\"\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Locate STAGE 4 output (robust)\n",
    "# ----------------------------\n",
    "manifest_csv = _find_stage4_manifest(ART_DIR)\n",
    "if manifest_csv is None:\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    runs = sorted([p.name for p in root.glob(\"run_*\") if p.is_dir()])[-15:] if root.exists() else []\n",
    "    raise RuntimeError(\n",
    "        \"Output STAGE 4 (lc_clean_mag_manifest.csv) tidak ditemukan.\\n\"\n",
    "        f\"- ART_DIR saat ini: {ART_DIR}\\n\"\n",
    "        f\"- Expected: {ART_DIR/'lc_clean_mag'}\\n\"\n",
    "        f\"- Runs available (last 15): {runs}\\n\"\n",
    "        \"Solusi: pastikan STAGE 4 benar-benar selesai dan menulis artifacts/lc_clean_mag.\"\n",
    "    )\n",
    "\n",
    "RUN_DIR, ART_DIR, LC_CLEAN_DIR = _sync_dirs_from_manifest(manifest_csv)\n",
    "\n",
    "print(\"STAGE 5 ROUTING SYNC OK\")\n",
    "print(f\"- RUN_DIR      : {RUN_DIR}\")\n",
    "print(f\"- ART_DIR      : {ART_DIR}\")\n",
    "print(f\"- LC_CLEAN_DIR : {LC_CLEAN_DIR}\")\n",
    "print(f\"- manifest_csv : {manifest_csv}\")\n",
    "\n",
    "reloaded, msg = _load_meta_if_needed(ART_DIR)\n",
    "print(f\"- meta_sync    : {msg}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load & validate Stage4 manifest\n",
    "# ----------------------------\n",
    "_df_clean_manifest = pd.read_csv(manifest_csv)\n",
    "_df_clean_manifest.columns = [c.strip() for c in _df_clean_manifest.columns]\n",
    "\n",
    "need_cols = {\"split\", \"which\", \"part\", \"path\"}\n",
    "miss = sorted(list(need_cols - set(_df_clean_manifest.columns)))\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Manifest STAGE 4 missing columns: {miss} | cols={list(_df_clean_manifest.columns)}\")\n",
    "\n",
    "paths = _df_clean_manifest[\"path\"].astype(str).tolist()\n",
    "missing_paths = [p for p in paths if not Path(p).exists()]\n",
    "if missing_paths:\n",
    "    ex = missing_paths[:10]\n",
    "    raise RuntimeError(\n",
    "        \"Ada file part STAGE 4 yang hilang (manifest ada tapi file tidak ada).\\n\"\n",
    "        f\"Missing count={len(missing_paths)} | contoh={ex}\\n\"\n",
    "        \"Solusi: rerun STAGE 4 dengan mode rebuild/wipe untuk regenerasi cache.\"\n",
    "    )\n",
    "\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    m = _df_clean_manifest[(_df_clean_manifest[\"split\"] == split_name) & (_df_clean_manifest[\"which\"] == which)]\n",
    "    if m.empty:\n",
    "        return []\n",
    "    return m.sort_values(\"part\")[\"path\"].astype(str).tolist()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Recover SPLIT_LIST + routing ids (force stable)\n",
    "# ----------------------------\n",
    "SPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "splits_in_manifest = sorted(set(_df_clean_manifest[\"split\"].astype(str).tolist()))\n",
    "# pakai intersection agar tidak nyasar split yang tidak ada part-nya\n",
    "SPLITS_TO_CONSIDER = [s for s in SPLIT_LIST if s in splits_in_manifest]\n",
    "\n",
    "train_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "for oid, sp in df_train_meta[\"split\"].astype(str).items():\n",
    "    train_ids_by_split[str(sp)].append(str(oid))\n",
    "\n",
    "test_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "for oid, sp in df_test_meta[\"split\"].astype(str).items():\n",
    "    test_ids_by_split[str(sp)].append(str(oid))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Settings\n",
    "# ----------------------------\n",
    "ONLY_SPLITS = None                 # None=all; contoh: [\"split_01\"] untuk test cepat\n",
    "REBUILD_MODE = \"wipe_all\"          # \"wipe_all\" or \"reuse_if_exists\"\n",
    "\n",
    "COMPRESS_NPZ = False\n",
    "SHARD_MAX_OBJECTS = 1500\n",
    "\n",
    "SNR_TANH_SCALE = 10.0\n",
    "TIME_CLIP_MAX_DAYS = None\n",
    "DROP_BAD_TIME_ROWS = True\n",
    "\n",
    "L_MAX = int(CFG.get(\"L_MAX\", 256)) if \"CFG\" in globals() else 256\n",
    "TRUNC_POLICY = str(CFG.get(\"TRUNC_POLICY\", \"smart\")) if \"CFG\" in globals() else \"smart\"  # smart/head/none\n",
    "KEEP_DET_FRAC = float(CFG.get(\"KEEP_DET_FRAC\", 0.70)) if \"CFG\" in globals() else 0.70\n",
    "KEEP_EDGE = True\n",
    "USE_RESTFRAME_TIME = bool(CFG.get(\"USE_RESTFRAME_TIME\", True)) if \"CFG\" in globals() else True\n",
    "\n",
    "NUM_BUCKETS = 64\n",
    "\n",
    "SEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOKEN_MODE = None\n",
    "FEATURE_NAMES = None\n",
    "FEATURE_DIM = None\n",
    "\n",
    "BASE_COLS = {\"object_id\", \"mjd\", \"band_id\", \"snr\", \"detected\"}\n",
    "MODE_COLS = {\"mag\": {\"mag\", \"mag_err\"}, \"asinh\": {\"flux_asinh\", \"err_log1p\"}}\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Reader for cleaned parts\n",
    "# ----------------------------\n",
    "def _read_clean_part(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Clean part missing: {p}\")\n",
    "\n",
    "    if p.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    elif p.name.endswith(\".csv.gz\"):\n",
    "        df = pd.read_csv(p, compression=\"gzip\")\n",
    "    else:\n",
    "        df = pd.read_csv(p)\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    global TOKEN_MODE, FEATURE_NAMES, FEATURE_DIM\n",
    "    if TOKEN_MODE is None:\n",
    "        cols = set(df.columns)\n",
    "        if BASE_COLS.issubset(cols) and MODE_COLS[\"mag\"].issubset(cols):\n",
    "            TOKEN_MODE = \"mag\"\n",
    "            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"mag\", \"mag_err_log\", \"snr_tanh\", \"detected\"]\n",
    "        elif BASE_COLS.issubset(cols) and MODE_COLS[\"asinh\"].issubset(cols):\n",
    "            TOKEN_MODE = \"asinh\"\n",
    "            FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot detect cleaned schema.\\n\"\n",
    "                f\"Found cols={list(df.columns)}\\n\"\n",
    "                \"Expected MAG or ASINH schema from STAGE 4.\"\n",
    "            )\n",
    "        FEATURE_DIM = len(FEATURE_NAMES)\n",
    "\n",
    "    req = set(BASE_COLS) | set(MODE_COLS[TOKEN_MODE])\n",
    "    miss = sorted(list(req - set(df.columns)))\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"Clean part missing columns: {miss} | file={p}\")\n",
    "\n",
    "    df[\"object_id\"] = _safe_string_series(df[\"object_id\"])\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n",
    "    df[\"snr\"] = pd.to_numeric(df[\"snr\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n",
    "\n",
    "    if TOKEN_MODE == \"mag\":\n",
    "        df[\"mag\"] = pd.to_numeric(df[\"mag\"], errors=\"coerce\").astype(np.float32)\n",
    "        df[\"mag_err\"] = pd.to_numeric(df[\"mag_err\"], errors=\"coerce\").astype(np.float32)\n",
    "    else:\n",
    "        df[\"flux_asinh\"] = pd.to_numeric(df[\"flux_asinh\"], errors=\"coerce\").astype(np.float32)\n",
    "        df[\"err_log1p\"] = pd.to_numeric(df[\"err_log1p\"], errors=\"coerce\").astype(np.float32)\n",
    "\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        df = df[np.isfinite(df[\"mjd\"].to_numpy())]\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Truncation\n",
    "# ----------------------------\n",
    "def _smart_truncate(mjd, det, snr, Lmax: int):\n",
    "    n = len(mjd)\n",
    "    if n <= Lmax:\n",
    "        return np.arange(n, dtype=np.int64)\n",
    "\n",
    "    idx_all = np.arange(n, dtype=np.int64)\n",
    "    keep = set()\n",
    "    if KEEP_EDGE:\n",
    "        keep.add(0); keep.add(n - 1)\n",
    "\n",
    "    det_idx = idx_all[det.astype(bool)]\n",
    "    k_det = int(max(0, min(len(det_idx), int(np.floor(Lmax * KEEP_DET_FRAC)))))\n",
    "    if k_det > 0 and len(det_idx) > 0:\n",
    "        score = np.abs(snr[det_idx])\n",
    "        top = det_idx[np.argsort(-score)[:k_det]]\n",
    "        for i in top.tolist():\n",
    "            keep.add(int(i))\n",
    "\n",
    "    if len(keep) < Lmax:\n",
    "        rem = [i for i in idx_all.tolist() if i not in keep]\n",
    "        need = Lmax - len(keep)\n",
    "        if rem and need > 0:\n",
    "            pick = np.linspace(0, len(rem) - 1, num=need, dtype=int)\n",
    "            for p in pick.tolist():\n",
    "                keep.add(int(rem[p]))\n",
    "\n",
    "    out = np.array(sorted(keep), dtype=np.int64)\n",
    "    if len(out) > Lmax:\n",
    "        pos = np.linspace(0, len(out) - 1, num=Lmax, dtype=int)\n",
    "        out = out[pos]\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Build tokens per object (empty-safe)\n",
    "# ----------------------------\n",
    "def build_empty_tokens():\n",
    "    # token kosong (len=1) agar object tetap ada\n",
    "    X = np.zeros((1, int(FEATURE_DIM)), dtype=np.float32)\n",
    "    B = np.full((1,), -1, dtype=np.int8)\n",
    "    return X, B, 0, 1\n",
    "\n",
    "def build_object_tokens(df_obj: pd.DataFrame, z_val: float = 0.0):\n",
    "    if df_obj is None or df_obj.empty:\n",
    "        return build_empty_tokens()\n",
    "\n",
    "    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n",
    "    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "    order = np.lexsort((band, mjd))\n",
    "    mjd = mjd[order]; band = band[order]; snr = snr[order]; det = det[order]\n",
    "\n",
    "    z = float(z_val) if (z_val is not None and np.isfinite(z_val)) else 0.0\n",
    "    denom = (1.0 + max(z, 0.0)) if USE_RESTFRAME_TIME else 1.0\n",
    "\n",
    "    t0 = mjd[0]\n",
    "    t_rel = (mjd - t0) / np.float32(denom)\n",
    "    dt = np.empty_like(t_rel); dt[0] = 0.0\n",
    "    if len(t_rel) > 1:\n",
    "        dt[1:] = np.maximum(t_rel[1:] - t_rel[:-1], 0.0)\n",
    "\n",
    "    if TIME_CLIP_MAX_DAYS is not None:\n",
    "        mx = np.float32(TIME_CLIP_MAX_DAYS)\n",
    "        t_rel = np.clip(t_rel, 0.0, mx)\n",
    "        dt    = np.clip(dt,    0.0, mx)\n",
    "\n",
    "    t_rel_log = np.log1p(t_rel).astype(np.float32)\n",
    "    dt_log    = np.log1p(dt).astype(np.float32)\n",
    "\n",
    "    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n",
    "    det_f = det.astype(np.float32)\n",
    "\n",
    "    if TOKEN_MODE == \"mag\":\n",
    "        mag = df_obj[\"mag\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        mag_err = df_obj[\"mag_err\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        mag = np.nan_to_num(mag, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        mag_err = np.nan_to_num(mag_err, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        mag_err = np.maximum(mag_err, np.float32(0.0))\n",
    "        mag_err_log = np.log1p(mag_err).astype(np.float32)\n",
    "        X = np.stack([t_rel_log, dt_log, mag, mag_err_log, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "    else:\n",
    "        flux = df_obj[\"flux_asinh\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        elog = df_obj[\"err_log1p\"].to_numpy(dtype=np.float32, copy=False)[order]\n",
    "        flux = np.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        elog = np.nan_to_num(elog, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        X = np.stack([t_rel_log, dt_log, flux, elog, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "\n",
    "    L0 = int(X.shape[0])\n",
    "    if L_MAX and int(L_MAX) > 0 and X.shape[0] > int(L_MAX):\n",
    "        if TRUNC_POLICY == \"smart\":\n",
    "            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n",
    "        elif TRUNC_POLICY == \"head\":\n",
    "            keep = np.arange(int(L_MAX), dtype=np.int64)\n",
    "        else:\n",
    "            keep = np.arange(X.shape[0], dtype=np.int64)\n",
    "\n",
    "        if len(keep) != X.shape[0]:\n",
    "            X = X[keep]\n",
    "            band = band[keep]\n",
    "\n",
    "            # recompute dt_log (stabil)\n",
    "            sel_mjd = mjd[keep]\n",
    "            sel_t = (sel_mjd - sel_mjd[0]) / np.float32(denom)\n",
    "            sel_dt = np.empty_like(sel_t); sel_dt[0] = 0.0\n",
    "            if len(sel_t) > 1:\n",
    "                sel_dt[1:] = np.maximum(sel_t[1:] - sel_t[:-1], 0.0)\n",
    "            X[:, 1] = np.log1p(sel_dt).astype(np.float32)\n",
    "\n",
    "    return X, band.astype(np.int8), L0, int(X.shape[0])\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Shard writer\n",
    "# ----------------------------\n",
    "def save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    obj_arr = np.asarray(object_ids, dtype=\"S\")\n",
    "    if COMPRESS_NPZ:\n",
    "        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "    else:\n",
    "        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Robust builder: bucketize -> groupby object -> shard (missing-safe)\n",
    "# ----------------------------\n",
    "def build_sequences_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 64):\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"pyarrow tidak tersedia. Di Kaggle biasanya ada.\") from e\n",
    "\n",
    "    parts = get_clean_parts(split_name, which)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"Tidak ada cleaned parts untuk {split_name}/{which}. Cek STAGE 4 output.\")\n",
    "\n",
    "    tmp_dir = Path(ART_DIR) / \"tmp_seq_buckets\" / split_name / which\n",
    "    if tmp_dir.exists():\n",
    "        shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    writers = {}\n",
    "    kept_rows = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    def bucket_idx(series_objid: pd.Series) -> np.ndarray:\n",
    "        h = pd.util.hash_pandas_object(series_objid, index=False).to_numpy(dtype=np.uint64, copy=False)\n",
    "        return (h % np.uint64(num_buckets)).astype(np.int16)\n",
    "\n",
    "    try:\n",
    "        # 1) write buckets\n",
    "        for p in parts:\n",
    "            df = _read_clean_part(p)\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            df = df[df[\"object_id\"].isin(expected_ids)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            kept_rows += int(len(df))\n",
    "            bidx = bucket_idx(df[\"object_id\"])\n",
    "            df[\"_b\"] = bidx\n",
    "\n",
    "            for b in np.unique(bidx):\n",
    "                sub = df[df[\"_b\"] == b].drop(columns=[\"_b\"])\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                fp = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n",
    "                table = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "                if int(b) not in writers:\n",
    "                    writers[int(b)] = pq.ParquetWriter(fp, table.schema, compression=\"snappy\")\n",
    "                writers[int(b)].write_table(table)\n",
    "\n",
    "            del df\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        for w in list(writers.values()):\n",
    "            try:\n",
    "                w.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    meta = df_train_meta if which == \"train\" else df_test_meta\n",
    "\n",
    "    manifest_rows = []\n",
    "    shard_idx = 0\n",
    "    batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n",
    "    built_ids = set()\n",
    "\n",
    "    len_before, len_after = [], []\n",
    "\n",
    "    def flush_shard_local():\n",
    "        nonlocal shard_idx, batch_obj_ids, batch_X, batch_B, batch_len, manifest_rows\n",
    "        if not batch_obj_ids:\n",
    "            return\n",
    "        lengths = np.asarray(batch_len, dtype=np.int64)\n",
    "        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(lengths)\n",
    "\n",
    "        Xc = np.concatenate(batch_X, axis=0).astype(np.float32)\n",
    "        Bc = np.concatenate(batch_B, axis=0).astype(np.int8)\n",
    "\n",
    "        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n",
    "        save_shard(shard_path, batch_obj_ids, Xc, Bc, offsets)\n",
    "\n",
    "        for i, oid in enumerate(batch_obj_ids):\n",
    "            manifest_rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(shard_path),\n",
    "                \"start\": int(offsets[i]),\n",
    "                \"length\": int(lengths[i]),\n",
    "            })\n",
    "\n",
    "        shard_idx += 1\n",
    "        batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n",
    "        gc.collect()\n",
    "\n",
    "    # 2) read buckets -> groupby object\n",
    "    for bf in sorted(tmp_dir.glob(\"bucket_*.parquet\")):\n",
    "        dfb = pd.read_parquet(bf)\n",
    "        if dfb.empty:\n",
    "            continue\n",
    "\n",
    "        for oid, g in dfb.groupby(\"object_id\", sort=False):\n",
    "            oid = str(oid)\n",
    "            if oid in built_ids:\n",
    "                continue\n",
    "\n",
    "            z_val = float(meta.loc[oid, \"Z\"]) if (USE_RESTFRAME_TIME and oid in meta.index) else 0.0\n",
    "            X, B, lb, la = build_object_tokens(g, z_val=z_val)\n",
    "\n",
    "            len_before.append(lb)\n",
    "            len_after.append(la)\n",
    "\n",
    "            batch_obj_ids.append(oid)\n",
    "            batch_X.append(X)\n",
    "            batch_B.append(B)\n",
    "            batch_len.append(int(X.shape[0]))\n",
    "            built_ids.add(oid)\n",
    "\n",
    "            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                flush_shard_local()\n",
    "\n",
    "        del dfb\n",
    "        gc.collect()\n",
    "\n",
    "    # 3) fill missing objects (empty sequences)\n",
    "    missing_ids = list(expected_ids - built_ids)\n",
    "    if missing_ids:\n",
    "        for oid in missing_ids:\n",
    "            oid = str(oid)\n",
    "            X, B, lb, la = build_empty_tokens()\n",
    "            len_before.append(lb)\n",
    "            len_after.append(la)\n",
    "\n",
    "            batch_obj_ids.append(oid)\n",
    "            batch_X.append(X)\n",
    "            batch_B.append(B)\n",
    "            batch_len.append(int(X.shape[0]))\n",
    "            built_ids.add(oid)\n",
    "\n",
    "            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                flush_shard_local()\n",
    "\n",
    "    flush_shard_local()\n",
    "\n",
    "    # cleanup tmp\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "    st = {\n",
    "        \"kept_rows\": int(kept_rows),\n",
    "        \"built_objects\": int(len(built_ids)),\n",
    "        \"missing_filled\": int(len(missing_ids)),\n",
    "        \"len_before_mean\": float(np.mean(len_before)) if len_before else 0.0,\n",
    "        \"len_before_p95\": float(np.quantile(len_before, 0.95)) if len_before else 0.0,\n",
    "        \"len_after_mean\": float(np.mean(len_after)) if len_after else 0.0,\n",
    "        \"len_after_p95\": float(np.quantile(len_after, 0.95)) if len_after else 0.0,\n",
    "        \"truncated_frac\": float(np.mean([a < b for a, b in zip(len_after, len_before)])) if len_before else 0.0,\n",
    "        \"time_s\": float(time.time() - t0),\n",
    "    }\n",
    "    return manifest_rows, st\n",
    "\n",
    "# ----------------------------\n",
    "# 11) RUN\n",
    "# ----------------------------\n",
    "splits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLITS_TO_CONSIDER\n",
    "all_manifest_train, all_manifest_test, split_run_stats = [], [], []\n",
    "\n",
    "def expected_set_for(split_name: str, which: str) -> set:\n",
    "    return set(train_ids_by_split.get(split_name, [])) if which == \"train\" else set(test_ids_by_split.get(split_name, []))\n",
    "\n",
    "for split_name in splits_to_run:\n",
    "    for which in [\"train\", \"test\"]:\n",
    "        out_dir = SEQ_DIR / split_name / which\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        expected_ids = expected_set_for(split_name, which)\n",
    "        if len(expected_ids) == 0:\n",
    "            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}.\")\n",
    "\n",
    "        shard_exists = any(out_dir.glob(\"shard_*.npz\"))\n",
    "        if REBUILD_MODE == \"reuse_if_exists\" and shard_exists:\n",
    "            print(f\"\\n[Stage 5] SKIP (exists): {split_name}/{which}\")\n",
    "            continue\n",
    "        else:\n",
    "            for f in out_dir.glob(\"shard_*.npz\"):\n",
    "                try: f.unlink()\n",
    "                except Exception: pass\n",
    "\n",
    "        print(f\"\\n[Stage 5] {split_name}/{which} | expected={len(expected_ids):,} | L_MAX={L_MAX} | TRUNC={TRUNC_POLICY}\")\n",
    "\n",
    "        manifest_rows, st = build_sequences_bucket(\n",
    "            split_name=split_name,\n",
    "            which=which,\n",
    "            expected_ids=expected_ids,\n",
    "            out_dir=out_dir,\n",
    "            num_buckets=NUM_BUCKETS\n",
    "        )\n",
    "\n",
    "        print(f\"[Stage 5] OK: built={st['built_objects']:,} (missing_filled={st['missing_filled']:,}) | \"\n",
    "              f\"kept_rows={st['kept_rows']:,} | \"\n",
    "              f\"len_mean {st['len_before_mean']:.1f}->{st['len_after_mean']:.1f} | \"\n",
    "              f\"p95 {st['len_before_p95']:.1f}->{st['len_after_p95']:.1f} | \"\n",
    "              f\"trunc%={st['truncated_frac']*100:.1f}% | \"\n",
    "              f\"time={st['time_s']:.2f}s | mode={TOKEN_MODE}\")\n",
    "\n",
    "        split_run_stats.append({\"split\": split_name, \"which\": which, **st})\n",
    "\n",
    "        if which == \"train\":\n",
    "            all_manifest_train.extend(manifest_rows)\n",
    "        else:\n",
    "            all_manifest_test.extend(manifest_rows)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Save manifests + stats + config\n",
    "# ----------------------------\n",
    "df_m_train = pd.DataFrame(all_manifest_train).sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n",
    "df_m_test  = pd.DataFrame(all_manifest_test).sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n",
    "\n",
    "mtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\n",
    "mtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\n",
    "df_m_train.to_csv(mtrain_path, index=False)\n",
    "df_m_test.to_csv(mtest_path, index=False)\n",
    "\n",
    "df_stats = pd.DataFrame(split_run_stats)\n",
    "stats_path = SEQ_DIR / \"seq_build_stats.csv\"\n",
    "df_stats.to_csv(stats_path, index=False)\n",
    "\n",
    "cfg = {\n",
    "    \"token_mode\": TOKEN_MODE,\n",
    "    \"feature_names\": FEATURE_NAMES,\n",
    "    \"feature_dim\": int(FEATURE_DIM),\n",
    "    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n",
    "    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n",
    "    \"compress_npz\": bool(COMPRESS_NPZ),\n",
    "    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n",
    "    \"num_buckets\": int(NUM_BUCKETS),\n",
    "    \"L_MAX\": int(L_MAX),\n",
    "    \"TRUNC_POLICY\": str(TRUNC_POLICY),\n",
    "    \"KEEP_DET_FRAC\": float(KEEP_DET_FRAC),\n",
    "    \"USE_RESTFRAME_TIME\": bool(USE_RESTFRAME_TIME),\n",
    "    \"REBUILD_MODE\": str(REBUILD_MODE),\n",
    "    \"RUN_DIR_USED\": str(RUN_DIR),\n",
    "    \"ART_DIR_USED\": str(ART_DIR),\n",
    "    \"LC_CLEAN_DIR_USED\": str(LC_CLEAN_DIR),\n",
    "    \"manifest_csv\": str(manifest_csv),\n",
    "}\n",
    "cfg_path = SEQ_DIR / \"seq_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 5] DONE\")\n",
    "print(f\"- token_mode : {TOKEN_MODE}\")\n",
    "print(f\"- features   : {FEATURE_NAMES}\")\n",
    "print(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\n",
    "print(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\n",
    "print(f\"- Saved: {stats_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Smoke test\n",
    "# ----------------------------\n",
    "def load_sequence(object_id: str, which: str):\n",
    "    object_id = str(object_id).strip()\n",
    "    m = df_m_train if which == \"train\" else df_m_test\n",
    "    row = m[m[\"object_id\"] == object_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n",
    "    r = row.iloc[0]\n",
    "    data = np.load(r[\"shard\"], allow_pickle=False)\n",
    "    start = int(r[\"start\"]); length = int(r[\"length\"])\n",
    "    X = data[\"x\"][start:start+length]\n",
    "    B = data[\"band\"][start:start+length]\n",
    "    return X, B\n",
    "\n",
    "_smoke_oid = str(df_train_meta.index[0])\n",
    "X_sm, B_sm = load_sequence(_smoke_oid, \"train\")\n",
    "print(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\n",
    "print(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Export globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"SEQ_DIR\": SEQ_DIR,\n",
    "    \"seq_manifest_train\": df_m_train,\n",
    "    \"seq_manifest_test\": df_m_test,\n",
    "    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n",
    "    \"SEQ_FEATURE_DIM\": int(FEATURE_DIM),\n",
    "    \"SEQ_TOKEN_MODE\": TOKEN_MODE,\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "    \"load_sequence\": load_sequence,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1365d7",
   "metadata": {
    "papermill": {
     "duration": 0.013781,
     "end_time": "2026-01-03T15:30:47.886233",
     "exception": false,
     "start_time": "2026-01-03T15:30:47.872452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Length Policy (Padding, Truncation, Windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76905f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:30:47.916465Z",
     "iopub.status.busy": "2026-01-03T15:30:47.916050Z",
     "iopub.status.idle": "2026-01-03T15:30:49.008085Z",
     "shell.execute_reply": "2026-01-03T15:30:49.006542Z"
    },
    "papermill": {
     "duration": 1.110451,
     "end_time": "2026-01-03T15:30:49.010183",
     "exception": false,
     "start_time": "2026-01-03T15:30:47.899732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 6] token_mode=mag | score_value_feat=mag | F=6\n",
      "\n",
      "TRAIN length stats\n",
      "- n_objects=3,043 | min=17 | p50=150 | p90=183 | p95=194 | p99=256 | max=256\n",
      "\n",
      "TEST length stats\n",
      "- n_objects=7,135 | min=18 | p50=152 | p90=183 | p95=193 | p99=256 | max=256\n",
      "\n",
      "[Stage 6] MAX_LEN=256 (based on p95=194)\n",
      "\n",
      "[Stage 6] Memmap X sizes approx: train=0.02 GB | test=0.04 GB | dtype=<class 'numpy.float32'>\n",
      "\n",
      "[Stage 6] Building fixed cache (TRAIN)...\n",
      "[Stage 6] TRAIN filled=3,043/3,043 | dup=0 | empty=0 | time=0.20s\n",
      "\n",
      "[Stage 6] Building fixed cache (TEST)...\n",
      "[Stage 6] TEST  filled=7,135/7,135 | dup=0 | empty=0 | time=0.39s\n",
      "\n",
      "[Stage 6] Sanity samples (train):\n",
      "- idx=1360 oid=gwilwileth_adel_amloth orig_len=157 kept=157 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3020 oid=vin_araf_gwador orig_len=151 kept=151 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3025 oid=ylf_alph_mindon orig_len=167 kept=167 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] Sanity samples (test):\n",
      "- idx=3191 oid=rom_bellas_lebdas orig_len=142 kept=142 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7082 oid=nim_nestad_thor orig_len=161 kept=161 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7094 oid=rach_bellas_dol orig_len=156 kept=156 bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] DONE\n",
      "- FIX_DIR: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/fixed_seq\n",
      "- Saved config: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/fixed_seq/length_policy_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n",
    "# ONE CELL, Kaggle CPU-SAFE — REVISI FULL v2.2 (MAG/ASINH COMPAT, HARDENED)\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/fixed_seq/{train|test}_{X|B|M}.dat  (memmap)\n",
    "# - artifacts/fixed_seq/{train|test}_ids.npy\n",
    "# - artifacts/fixed_seq/train_y.npy\n",
    "# - artifacts/fixed_seq/{train|test}_origlen.npy, {train|test}_winstart.npy, {train|test}_winend.npy\n",
    "# - artifacts/fixed_seq/length_policy_config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"seq_manifest_train\", \"seq_manifest_test\", \"SEQ_FEATURE_NAMES\",\n",
    "             \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 -> 5 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "m_train = seq_manifest_train.copy()\n",
    "m_test  = seq_manifest_test.copy()\n",
    "\n",
    "SEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\n",
    "feat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Detect token_mode (MAG vs ASINH)\n",
    "# ----------------------------\n",
    "SEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    if (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"asinh\"\n",
    "    elif (\"mag\" in feat) and (\"mag_err_log\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"mag\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Cannot infer SEQ_TOKEN_MODE from SEQ_FEATURE_NAMES.\\n\"\n",
    "            f\"SEQ_FEATURE_NAMES={SEQ_FEATURE_NAMES}\\n\"\n",
    "            \"Expected either (flux_asinh, err_log1p) or (mag, mag_err_log).\"\n",
    "        )\n",
    "\n",
    "REQ_COMMON = [\"t_rel_log\", \"dt_log\", \"snr_tanh\", \"detected\"]\n",
    "for k in REQ_COMMON:\n",
    "    if k not in feat:\n",
    "        raise ValueError(f\"SEQ_FEATURE_NAMES must include '{k}'. Found: {SEQ_FEATURE_NAMES}\")\n",
    "\n",
    "if SEQ_TOKEN_MODE == \"asinh\":\n",
    "    if \"flux_asinh\" not in feat:\n",
    "        raise ValueError(\"token_mode=asinh requires 'flux_asinh'.\")\n",
    "    SCORE_VALUE_FEAT = \"flux_asinh\"\n",
    "elif SEQ_TOKEN_MODE == \"mag\":\n",
    "    if \"mag\" not in feat:\n",
    "        raise ValueError(\"token_mode=mag requires 'mag'.\")\n",
    "    SCORE_VALUE_FEAT = \"mag\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SEQ_TOKEN_MODE={SEQ_TOKEN_MODE}\")\n",
    "\n",
    "print(f\"[Stage 6] token_mode={SEQ_TOKEN_MODE} | score_value_feat={SCORE_VALUE_FEAT} | F={len(SEQ_FEATURE_NAMES)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings\n",
    "# ----------------------------\n",
    "FORCE_MAX_LEN = None          # e.g. 256 (kalau mau paksa)\n",
    "MAXLEN_CAPS = (256, 384, 512) # CPU-safe choices\n",
    "\n",
    "# Score weights\n",
    "W_SNR = 1.00\n",
    "W_VAL = 0.35\n",
    "W_DET = 0.25\n",
    "\n",
    "# Padding policy\n",
    "PAD_BAND_ID = 0\n",
    "\n",
    "# AUTO: kalau shard punya band negatif (mis. -1 dari empty token), shift band ids otomatis\n",
    "SHIFT_BAND_IDS = False\n",
    "AUTO_SHIFT_IF_NEGATIVE_BANDS = True\n",
    "\n",
    "# Build policy\n",
    "REBUILD_MODE = \"wipe_all\"     # \"wipe_all\" atau \"reuse_if_exists\"\n",
    "DTYPE_X = np.float32          # bisa fp16 kalau disk ketat\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Inspect length distribution -> choose MAX_LEN\n",
    "# ----------------------------\n",
    "def describe_lengths(m: pd.DataFrame, name: str):\n",
    "    L = pd.to_numeric(m[\"length\"], errors=\"coerce\").fillna(0).to_numpy(dtype=np.int32, copy=False)\n",
    "    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n",
    "    print(f\"\\n{name} length stats\")\n",
    "    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[7])} | p95={int(q[8])} | p99={int(q[10])} | max={int(q[-1])}\")\n",
    "    return q\n",
    "\n",
    "q_tr = describe_lengths(m_train, \"TRAIN\")\n",
    "q_te = describe_lengths(m_test,  \"TEST\")\n",
    "\n",
    "p95 = int(max(q_tr[8], q_te[8]))\n",
    "if FORCE_MAX_LEN is not None:\n",
    "    MAX_LEN = int(FORCE_MAX_LEN)\n",
    "else:\n",
    "    if p95 <= 256:\n",
    "        MAX_LEN = 256\n",
    "    elif p95 <= 384:\n",
    "        MAX_LEN = 384\n",
    "    else:\n",
    "        MAX_LEN = 512\n",
    "\n",
    "if MAX_LEN not in MAXLEN_CAPS and FORCE_MAX_LEN is None:\n",
    "    MAX_LEN = int(min(MAXLEN_CAPS, key=lambda x: abs(x - MAX_LEN)))\n",
    "\n",
    "print(f\"\\n[Stage 6] MAX_LEN={MAX_LEN} (based on p95={p95})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Window scoring (adaptive)\n",
    "# ----------------------------\n",
    "def _brightness_proxy_from_mag(mag: np.ndarray) -> np.ndarray:\n",
    "    mag = np.nan_to_num(mag, nan=np.float32(0.0), posinf=np.float32(0.0), neginf=np.float32(0.0)).astype(np.float32, copy=False)\n",
    "    if mag.size == 0:\n",
    "        return np.zeros_like(mag, dtype=np.float32)\n",
    "    med = np.float32(np.median(mag))\n",
    "    br = np.maximum(med - mag, np.float32(0.0))\n",
    "    br = np.log1p(br).astype(np.float32, copy=False)\n",
    "    return br\n",
    "\n",
    "def _score_tokens(X: np.ndarray) -> np.ndarray:\n",
    "    snr = np.abs(X[:, feat[\"snr_tanh\"]]).astype(np.float32, copy=False)\n",
    "    det = X[:, feat[\"detected\"]].astype(np.float32, copy=False)\n",
    "\n",
    "    if SEQ_TOKEN_MODE == \"asinh\":\n",
    "        val = np.abs(X[:, feat[\"flux_asinh\"]]).astype(np.float32, copy=False)\n",
    "    else:\n",
    "        mag = X[:, feat[\"mag\"]].astype(np.float32, copy=False)\n",
    "        val = _brightness_proxy_from_mag(mag)\n",
    "\n",
    "    score = (np.float32(W_SNR) * snr) + (np.float32(W_VAL) * val) + (np.float32(W_DET) * det)\n",
    "    score = np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "    return score\n",
    "\n",
    "def select_best_window(score: np.ndarray, max_len: int) -> tuple[int, int]:\n",
    "    L = int(score.shape[0])\n",
    "    if L <= max_len:\n",
    "        return 0, L\n",
    "\n",
    "    cs = np.empty(L + 1, dtype=np.float32)\n",
    "    cs[0] = 0.0\n",
    "    np.cumsum(score.astype(np.float32, copy=False), out=cs[1:])\n",
    "\n",
    "    ws = cs[max_len:] - cs[:-max_len]\n",
    "    if not np.isfinite(ws).any():\n",
    "        start = (L - max_len) // 2\n",
    "    else:\n",
    "        start = int(np.argmax(ws))\n",
    "    end = start + max_len\n",
    "    return start, end\n",
    "\n",
    "def pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n",
    "    L = int(X.shape[0])\n",
    "    F = int(X.shape[1])\n",
    "\n",
    "    Xp = np.zeros((max_len, F), dtype=DTYPE_X)\n",
    "    Bp = np.full((max_len,), PAD_BAND_ID, dtype=np.int8)\n",
    "    Mp = np.zeros((max_len,), dtype=np.int8)\n",
    "\n",
    "    if L <= 0:\n",
    "        return Xp, Bp, Mp, 0, 0, 0\n",
    "\n",
    "    if L <= max_len:\n",
    "        Xw = X\n",
    "        Bw = B\n",
    "        ws, we = 0, L\n",
    "    else:\n",
    "        sc = _score_tokens(X)\n",
    "        ws, we = select_best_window(sc, max_len=max_len)\n",
    "        Xw = X[ws:we]\n",
    "        Bw = B[ws:we]\n",
    "\n",
    "    lw = int(Xw.shape[0])\n",
    "    Xp[:lw] = Xw.astype(DTYPE_X, copy=False)\n",
    "\n",
    "    if SHIFT_BAND_IDS:\n",
    "        # token band 1..K; pad=0; band=-1 -> 0\n",
    "        Bw16 = Bw.astype(np.int16, copy=False)\n",
    "        Bp[:lw] = (Bw16 + 1).astype(np.int8, copy=False)\n",
    "    else:\n",
    "        Bp[:lw] = Bw.astype(np.int8, copy=False)\n",
    "\n",
    "    Mp[:lw] = 1\n",
    "    return Xp, Bp, Mp, L, int(ws), int(we)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Fixed cache builder setup\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(ART_DIR) / \"fixed_seq\"\n",
    "FIX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# robust ordering\n",
    "train_ids = df_train_meta.index.astype(\"string\").str.strip().astype(str).to_list()\n",
    "\n",
    "# y column robust\n",
    "_y_col = None\n",
    "for cand in [\"target\", \"y\", \"label\", \"class\", \"target_id\"]:\n",
    "    if cand in df_train_meta.columns:\n",
    "        _y_col = cand\n",
    "        break\n",
    "if _y_col is None:\n",
    "    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols={list(df_train_meta.columns)[:30]}\")\n",
    "\n",
    "y_train = pd.to_numeric(df_train_meta[_y_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n",
    "\n",
    "def _try_load_sample_sub_ids():\n",
    "    # 1) df_sub (kalau ada)\n",
    "    if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in globals()[\"df_sub\"].columns:\n",
    "        return globals()[\"df_sub\"][\"object_id\"].astype(str).str.strip().to_list()\n",
    "\n",
    "    # 2) PATHS sample_submission\n",
    "    if \"PATHS\" in globals() and isinstance(PATHS, dict):\n",
    "        keys = [\"SAMPLE_SUB\", \"SAMPLE_SUBMISSION\", \"sample_submission\", \"sample_sub\", \"SAMPLE\"]\n",
    "        for k in keys:\n",
    "            p = PATHS.get(k, None)\n",
    "            if p and Path(p).exists():\n",
    "                df = pd.read_csv(p)\n",
    "                if \"object_id\" in df.columns:\n",
    "                    return df[\"object_id\"].astype(str).str.strip().to_list()\n",
    "    return None\n",
    "\n",
    "test_ids = _try_load_sample_sub_ids()\n",
    "if test_ids is None:\n",
    "    test_ids = df_test_meta.index.astype(\"string\").str.strip().astype(str).to_list()\n",
    "\n",
    "# strict unique ids\n",
    "if len(set(train_ids)) != len(train_ids):\n",
    "    raise RuntimeError(\"train_ids contains duplicates. Check df_train_meta.index.\")\n",
    "if len(set(test_ids)) != len(test_ids):\n",
    "    raise RuntimeError(\"test_ids contains duplicates. Check ordering source (df_sub/sample_sub/df_test_meta).\")\n",
    "\n",
    "train_row = {oid: i for i, oid in enumerate(train_ids)}\n",
    "test_row  = {oid: i for i, oid in enumerate(test_ids)}\n",
    "\n",
    "NTR = len(train_ids)\n",
    "NTE = len(test_ids)\n",
    "F = len(SEQ_FEATURE_NAMES)\n",
    "\n",
    "def _gb(nbytes): return float(nbytes) / (1024**3)\n",
    "size_tr = NTR * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\n",
    "size_te = NTE * MAX_LEN * F * np.dtype(DTYPE_X).itemsize\n",
    "print(f\"\\n[Stage 6] Memmap X sizes approx: train={_gb(size_tr):.2f} GB | test={_gb(size_te):.2f} GB | dtype={DTYPE_X}\")\n",
    "\n",
    "# memmap paths\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "test_X_path  = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path  = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path  = FIX_DIR / \"test_M.dat\"\n",
    "\n",
    "train_len_path = FIX_DIR / \"train_origlen.npy\"\n",
    "train_ws_path  = FIX_DIR / \"train_winstart.npy\"\n",
    "train_we_path  = FIX_DIR / \"train_winend.npy\"\n",
    "test_len_path  = FIX_DIR / \"test_origlen.npy\"\n",
    "test_ws_path   = FIX_DIR / \"test_winstart.npy\"\n",
    "test_we_path   = FIX_DIR / \"test_winend.npy\"\n",
    "\n",
    "# ----------------------------\n",
    "# 4b) Auto shift band ids if needed\n",
    "# ----------------------------\n",
    "if AUTO_SHIFT_IF_NEGATIVE_BANDS and (not SHIFT_BAND_IDS):\n",
    "    try:\n",
    "        # ambil 1 shard train untuk cek min band\n",
    "        sp0 = str(m_train[\"shard\"].astype(str).iloc[0])\n",
    "        d0 = np.load(sp0, allow_pickle=False)\n",
    "        b0 = d0[\"band\"]\n",
    "        bmin = int(np.min(b0)) if b0.size else 0\n",
    "        del d0\n",
    "        if bmin < 0:\n",
    "            SHIFT_BAND_IDS = True\n",
    "            print(f\"[Stage 6] AUTO SHIFT_BAND_IDS=True (detected band min={bmin} in shard sample)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Stage 6] AUTO SHIFT check skipped ({type(e).__name__}: {e})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4c) Rebuild handling\n",
    "# ----------------------------\n",
    "def _all_exist(paths):\n",
    "    return all(Path(p).exists() for p in paths)\n",
    "\n",
    "reuse_paths = [\n",
    "    train_X_path, train_B_path, train_M_path,\n",
    "    test_X_path, test_B_path, test_M_path,\n",
    "    FIX_DIR / \"train_ids.npy\", FIX_DIR / \"test_ids.npy\", FIX_DIR / \"train_y.npy\",\n",
    "    train_len_path, train_ws_path, train_we_path,\n",
    "    test_len_path, test_ws_path, test_we_path,\n",
    "    FIX_DIR / \"length_policy_config.json\"\n",
    "]\n",
    "\n",
    "if REBUILD_MODE == \"reuse_if_exists\" and _all_exist(reuse_paths):\n",
    "    print(\"[Stage 6] REUSE (exists): fixed_seq cache already present.\")\n",
    "    globals().update({\n",
    "        \"FIX_DIR\": FIX_DIR, \"MAX_LEN\": MAX_LEN,\n",
    "        \"FIX_TRAIN_X_PATH\": train_X_path, \"FIX_TRAIN_B_PATH\": train_B_path, \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "        \"FIX_TEST_X_PATH\": test_X_path,  \"FIX_TEST_B_PATH\": test_B_path,  \"FIX_TEST_M_PATH\": test_M_path,\n",
    "        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "        \"FIX_POLICY_CFG_PATH\": FIX_DIR / \"length_policy_config.json\",\n",
    "        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n",
    "    })\n",
    "    raise SystemExit\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Create memmaps\n",
    "# ----------------------------\n",
    "Xtr = np.memmap(train_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTR, MAX_LEN, F))\n",
    "Btr = np.memmap(train_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "Mtr = np.memmap(train_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTE, MAX_LEN, F))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "\n",
    "origlen_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "winstart_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "winend_tr   = np.zeros((NTR,), dtype=np.int32)\n",
    "\n",
    "origlen_te = np.zeros((NTE,), dtype=np.int32)\n",
    "winstart_te = np.zeros((NTE,), dtype=np.int32)\n",
    "winend_te   = np.zeros((NTE,), dtype=np.int32)\n",
    "\n",
    "filled_tr = np.zeros((NTR,), dtype=np.uint8)\n",
    "filled_te = np.zeros((NTE,), dtype=np.uint8)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Fill memmaps per shard (fast path)\n",
    "# ----------------------------\n",
    "def process_manifest_into_memmap(m: pd.DataFrame, which: str):\n",
    "    if which == \"train\":\n",
    "        row_map = train_row\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        origlen, ws_arr, we_arr = origlen_tr, winstart_tr, winend_tr\n",
    "        filled_mask = filled_tr\n",
    "        expected_n = NTR\n",
    "    else:\n",
    "        row_map = test_row\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        origlen, ws_arr, we_arr = origlen_te, winstart_te, winend_te\n",
    "        filled_mask = filled_te\n",
    "        expected_n = NTE\n",
    "\n",
    "    for c in [\"object_id\", \"shard\", \"start\", \"length\"]:\n",
    "        if c not in m.columns:\n",
    "            raise RuntimeError(f\"Manifest missing column '{c}'. cols={list(m.columns)}\")\n",
    "\n",
    "    shard_paths = m[\"shard\"].astype(str).unique().tolist()\n",
    "    miss_sh = [p for p in shard_paths if not Path(p).exists()]\n",
    "    if miss_sh:\n",
    "        raise RuntimeError(f\"Missing shard files ({which}): count={len(miss_sh)} | ex={miss_sh[:5]}\")\n",
    "\n",
    "    filled = 0\n",
    "    dup = 0\n",
    "    empty = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for shard_path in sorted(shard_paths):\n",
    "        g = m[m[\"shard\"].astype(str) == shard_path]\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        data = np.load(shard_path, allow_pickle=False)\n",
    "        x_all = data[\"x\"]\n",
    "        b_all = data[\"band\"]\n",
    "\n",
    "        # mapping object_id -> row index (robust int, na=-1)\n",
    "        oids = g[\"object_id\"].astype(str).to_numpy()\n",
    "        idxs = pd.Series(oids).map(row_map).astype(\"Int64\").to_numpy(dtype=np.int64, na_value=-1)\n",
    "\n",
    "        starts = pd.to_numeric(g[\"start\"], errors=\"coerce\").fillna(-1).to_numpy(dtype=np.int64, copy=False)\n",
    "        lens   = pd.to_numeric(g[\"length\"], errors=\"coerce\").fillna(0).to_numpy(dtype=np.int64, copy=False)\n",
    "\n",
    "        valid = (idxs >= 0) & (starts >= 0) & (lens >= 0)\n",
    "        if not valid.any():\n",
    "            del data\n",
    "            continue\n",
    "\n",
    "        idxs_v = idxs[valid]\n",
    "        starts_v = starts[valid]\n",
    "        lens_v = lens[valid]\n",
    "        oids_v = oids[valid]\n",
    "\n",
    "        for oid, idx, st, ln in zip(oids_v, idxs_v, starts_v, lens_v):\n",
    "            if ln <= 0:\n",
    "                empty += 1\n",
    "                continue\n",
    "            if filled_mask[idx]:\n",
    "                dup += 1\n",
    "                continue\n",
    "\n",
    "            end = int(st + ln)\n",
    "            if st < 0 or end > x_all.shape[0] or end > b_all.shape[0]:\n",
    "                raise RuntimeError(\n",
    "                    f\"[Stage 6] Out-of-range slice in shard={shard_path}\\n\"\n",
    "                    f\"- oid={oid} idx={idx} start={st} len={ln} end={end}\\n\"\n",
    "                    f\"- shard_x_len={x_all.shape[0]} shard_b_len={b_all.shape[0]}\"\n",
    "                )\n",
    "\n",
    "            X = x_all[st:end]\n",
    "            B = b_all[st:end]\n",
    "\n",
    "            Xp, Bp, Mp, L0, ws, we = pad_to_fixed(X, B, max_len=MAX_LEN)\n",
    "\n",
    "            Xmm[idx, :, :] = Xp\n",
    "            Bmm[idx, :] = Bp\n",
    "            Mmm[idx, :] = Mp\n",
    "            origlen[idx] = int(L0)\n",
    "            ws_arr[idx] = int(ws)\n",
    "            we_arr[idx] = int(we)\n",
    "            filled_mask[idx] = 1\n",
    "            filled += 1\n",
    "\n",
    "        del data\n",
    "        if filled % 2000 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    return {\"filled\": int(filled), \"dup_skipped\": int(dup), \"empty_len\": int(empty), \"time_s\": float(elapsed), \"expected\": int(expected_n)}\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed cache (TRAIN)...\")\n",
    "st_tr = process_manifest_into_memmap(m_train, \"train\")\n",
    "print(f\"[Stage 6] TRAIN filled={st_tr['filled']:,}/{st_tr['expected']:,} | dup={st_tr['dup_skipped']:,} | empty={st_tr['empty_len']:,} | time={st_tr['time_s']:.2f}s\")\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed cache (TEST)...\")\n",
    "st_te = process_manifest_into_memmap(m_test, \"test\")\n",
    "print(f\"[Stage 6] TEST  filled={st_te['filled']:,}/{st_te['expected']:,} | dup={st_te['dup_skipped']:,} | empty={st_te['empty_len']:,} | time={st_te['time_s']:.2f}s\")\n",
    "\n",
    "Xtr.flush(); Btr.flush(); Mtr.flush()\n",
    "Xte.flush(); Bte.flush(); Mte.flush()\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Hard sanity: must be 100% filled\n",
    "# ----------------------------\n",
    "miss_tr = np.where(filled_tr == 0)[0]\n",
    "miss_te = np.where(filled_te == 0)[0]\n",
    "if len(miss_tr) > 0:\n",
    "    ex = [train_ids[i] for i in miss_tr[:10]]\n",
    "    raise RuntimeError(f\"[Stage 6] TRAIN missing filled rows: {len(miss_tr):,}/{NTR:,} | ex={ex}\")\n",
    "if len(miss_te) > 0:\n",
    "    ex = [test_ids[i] for i in miss_te[:10]]\n",
    "    raise RuntimeError(f\"[Stage 6] TEST missing filled rows: {len(miss_te):,}/{NTE:,} | ex={ex}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save ids + y + meta arrays\n",
    "# ----------------------------\n",
    "np.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"train_y.npy\",   y_train)\n",
    "\n",
    "np.save(train_len_path, origlen_tr)\n",
    "np.save(train_ws_path,  winstart_tr)\n",
    "np.save(train_we_path,  winend_tr)\n",
    "\n",
    "np.save(test_len_path,  origlen_te)\n",
    "np.save(test_ws_path,   winstart_te)\n",
    "np.save(test_we_path,   winend_te)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Quick sanity samples\n",
    "# ----------------------------\n",
    "def sanity_samples(which: str, n_show: int = 3, seed: int = 2025):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if which == \"train\":\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        ids = train_ids\n",
    "        ol = origlen_tr\n",
    "    else:\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        ids = test_ids\n",
    "        ol = origlen_te\n",
    "\n",
    "    idxs = rng.choice(len(ids), size=min(n_show, len(ids)), replace=False)\n",
    "    print(f\"\\n[Stage 6] Sanity samples ({which}):\")\n",
    "    for i in idxs:\n",
    "        kept = int(Mmm[i].sum())\n",
    "        bands = sorted(set(Bmm[i, :kept].tolist())) if kept > 0 else []\n",
    "        print(f\"- idx={i} oid={ids[i]} orig_len={int(ol[i])} kept={kept} bands_unique={bands}\")\n",
    "\n",
    "sanity_samples(\"train\", 3)\n",
    "sanity_samples(\"test\", 3)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Save config\n",
    "# ----------------------------\n",
    "policy_cfg = {\n",
    "    \"token_mode\": SEQ_TOKEN_MODE,\n",
    "    \"max_len\": int(MAX_LEN),\n",
    "    \"feature_names\": list(SEQ_FEATURE_NAMES),\n",
    "    \"score_weights\": {\"W_SNR\": float(W_SNR), \"W_VAL\": float(W_VAL), \"W_DET\": float(W_DET)},\n",
    "    \"score_value_feat\": SCORE_VALUE_FEAT,\n",
    "    \"window_policy\": \"best_contiguous_window_by_max_sum(score)\",\n",
    "    \"padding\": {\"PAD_BAND_ID\": int(PAD_BAND_ID), \"SHIFT_BAND_IDS\": bool(SHIFT_BAND_IDS)},\n",
    "    \"dtype_X\": str(DTYPE_X),\n",
    "    \"order\": {\n",
    "        \"train\": \"df_train_meta.index\",\n",
    "        \"test\": (\"df_sub.object_id\" if (\"df_sub\" in globals() and isinstance(df_sub, pd.DataFrame) and \"object_id\" in df_sub.columns) else \"df_test_meta.index / sample_submission fallback\"),\n",
    "        \"y_col\": str(_y_col),\n",
    "    },\n",
    "    \"stats\": {\"train\": st_tr, \"test\": st_te},\n",
    "    \"files\": {\n",
    "        \"train_X\": str(train_X_path), \"train_B\": str(train_B_path), \"train_M\": str(train_M_path),\n",
    "        \"test_X\": str(test_X_path),   \"test_B\": str(test_B_path),   \"test_M\": str(test_M_path),\n",
    "        \"train_y\": str(FIX_DIR / \"train_y.npy\"),\n",
    "        \"train_ids\": str(FIX_DIR / \"train_ids.npy\"),\n",
    "        \"test_ids\": str(FIX_DIR / \"test_ids.npy\"),\n",
    "        \"train_origlen\": str(train_len_path), \"train_winstart\": str(train_ws_path), \"train_winend\": str(train_we_path),\n",
    "        \"test_origlen\": str(test_len_path),   \"test_winstart\": str(test_ws_path),   \"test_winend\": str(test_we_path),\n",
    "    }\n",
    "}\n",
    "cfg_path = FIX_DIR / \"length_policy_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(policy_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 6] DONE\")\n",
    "print(f\"- FIX_DIR: {FIX_DIR}\")\n",
    "print(f\"- Saved config: {cfg_path}\")\n",
    "\n",
    "globals().update({\n",
    "    \"FIX_DIR\": FIX_DIR,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"FIX_TRAIN_X_PATH\": train_X_path,\n",
    "    \"FIX_TRAIN_B_PATH\": train_B_path,\n",
    "    \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "    \"FIX_TEST_X_PATH\": test_X_path,\n",
    "    \"FIX_TEST_B_PATH\": test_B_path,\n",
    "    \"FIX_TEST_M_PATH\": test_M_path,\n",
    "    \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "    \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "    \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "    \"FIX_POLICY_CFG_PATH\": cfg_path,\n",
    "    \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n",
    "    \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a913b",
   "metadata": {
    "papermill": {
     "duration": 0.014096,
     "end_time": "2026-01-03T15:30:49.038270",
     "exception": false,
     "start_time": "2026-01-03T15:30:49.024174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Split (Object-Level, Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca2ebd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:30:49.069637Z",
     "iopub.status.busy": "2026-01-03T15:30:49.068843Z",
     "iopub.status.idle": "2026-01-03T15:30:51.113673Z",
     "shell.execute_reply": "2026-01-03T15:30:51.112520Z"
    },
    "papermill": {
     "duration": 2.063821,
     "end_time": "2026-01-03T15:30:51.115788",
     "exception": false,
     "start_time": "2026-01-03T15:30:49.051967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 7] seed=2025 | default_splits=5 | MIN_POS_PER_FOLD=3 | enforce_minpos=True | group_by_split=False | fallback_group=True\n",
      "[Stage 7] Candidate n_splits=5 | N=3,043 pos=148 neg=2,895 pos%=4.863621% | order_source=/kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/fixed_seq/train_ids.npy\n",
      "[Stage 7] FINAL: n_splits=5 | cv_type=StratifiedKFold | min_pos_in_fold=29\n",
      "\n",
      "[Stage 7] CV split OK\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/cv/cv_folds.csv\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/cv/cv_folds.npz\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/cv/cv_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/cv/cv_config.json\n",
      "CV=StratifiedKFold n_splits=5 seed=2025\n",
      "Order source: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/fixed_seq/train_ids.npy\n",
      "Target column: target\n",
      "Total: N=3043 | pos=148 | neg=2895 | pos%=4.863621%\n",
      "Per-fold distribution (val):\n",
      "- fold 0: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 1: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 2: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 3: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n",
      "- fold 4: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v2.2 (HARDENED + HOLDOUT FALLBACK)\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/cv/cv_folds.csv\n",
    "# - artifacts/cv/cv_folds.npz   (train_idx_f + val_idx_f)\n",
    "# - artifacts/cv/cv_report.txt\n",
    "# - artifacts/cv/cv_config.json\n",
    "# - globals: fold_assign, folds, n_splits, CV_DIR\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "for need in [\"df_train_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 2 dulu (df_train_meta & ART_DIR).\")\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CV Settings\n",
    "# ----------------------------\n",
    "DEFAULT_SPLITS = 5\n",
    "FORCE_N_SPLITS = None              # set int kalau mau paksa (mis. 3), else None\n",
    "MIN_POS_PER_FOLD = 3               # stabilitas; 3–10 umum\n",
    "ENFORCE_MIN_POS_PER_FOLD = True    # kalau True: n_splits turun otomatis sampai min_pos>=MIN_POS_PER_FOLD (atau fallback holdout)\n",
    "USE_GROUP_BY_SPLIT = False         # True => prefer StratifiedGroupKFold (groups=df_train_meta[\"split\"])\n",
    "AUTO_FALLBACK_GROUP = True         # True => kalau group-cv tidak bisa, fallback ke StratifiedKFold\n",
    "HOLDOUT_FALLBACK = True            # True => kalau CV tidak mungkin, pakai 1 fold holdout (n_splits=1)\n",
    "HOLDOUT_FRAC = 0.20                # target val fraction untuk holdout\n",
    "\n",
    "print(f\"[Stage 7] seed={SEED} | default_splits={DEFAULT_SPLITS} | MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} \"\n",
    "      f\"| enforce_minpos={ENFORCE_MIN_POS_PER_FOLD} | group_by_split={USE_GROUP_BY_SPLIT} | fallback_group={AUTO_FALLBACK_GROUP}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Helpers\n",
    "# ----------------------------\n",
    "def _decode_ids(arr) -> list:\n",
    "    out = []\n",
    "    for x in arr.tolist():\n",
    "        if isinstance(x, (bytes, bytearray)):\n",
    "            s = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            s = str(x)\n",
    "        out.append(s.strip())\n",
    "    return out\n",
    "\n",
    "def _find_train_ids_npy(art_dir: Path) -> Path | None:\n",
    "    # priority 1: FIX_DIR\n",
    "    if \"FIX_DIR\" in globals():\n",
    "        p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # priority 2: ART_DIR/fixed_seq\n",
    "    p = art_dir / \"fixed_seq\" / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    # priority 3: scan mallorn_run runs (latest mtime)\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    if root.exists():\n",
    "        cands = list(root.glob(\"run_*/artifacts/fixed_seq/train_ids.npy\"))\n",
    "        if cands:\n",
    "            cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "            return cands[0]\n",
    "    return None\n",
    "\n",
    "def _safe_str_index(idx: pd.Index) -> pd.Index:\n",
    "    return pd.Index([str(x).strip() for x in idx], dtype=\"object\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Determine train_ids ordering (prefer fixed cache from STAGE 6)\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "order_source = \"df_train_meta.index\"\n",
    "\n",
    "p_ids = _find_train_ids_npy(ART_DIR)\n",
    "if p_ids is not None:\n",
    "    raw = np.load(p_ids, allow_pickle=False)\n",
    "    train_ids = _decode_ids(raw)\n",
    "    order_source = str(p_ids)\n",
    "else:\n",
    "    train_ids = [str(x).strip() for x in df_train_meta.index.astype(str).tolist()]\n",
    "    order_source = \"df_train_meta.index\"\n",
    "\n",
    "# uniqueness check train_ids\n",
    "if len(train_ids) != len(set(train_ids)):\n",
    "    s = pd.Series(train_ids)\n",
    "    dup = s[s.duplicated()].iloc[:10].tolist()\n",
    "    raise RuntimeError(f\"[Stage 7] train_ids has duplicates (examples): {dup}\")\n",
    "\n",
    "N = len(train_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Build robust mapping: train_id -> row position in df_train_meta\n",
    "#    (tidak mengharuskan df_train_meta.index sudah string)\n",
    "# ----------------------------\n",
    "meta = df_train_meta\n",
    "\n",
    "meta_idx_str = _safe_str_index(meta.index)\n",
    "if meta_idx_str.has_duplicates:\n",
    "    d = pd.Series(meta_idx_str).value_counts()\n",
    "    dup = d[d > 1].index.tolist()[:10]\n",
    "    raise RuntimeError(f\"[Stage 7] df_train_meta index has duplicates after str/strip (examples): {dup}\")\n",
    "\n",
    "pos_map = pd.Series(np.arange(len(meta), dtype=np.int32), index=meta_idx_str)\n",
    "\n",
    "pos_s = pos_map.reindex(train_ids)\n",
    "missing = pos_s[pos_s.isna()].index.tolist()\n",
    "if missing:\n",
    "    ex = missing[:10]\n",
    "    raise RuntimeError(\n",
    "        \"[Stage 7] Some train_ids not found in df_train_meta (after str/strip index).\\n\"\n",
    "        f\"Missing count={len(missing)} | ex={ex}\\n\"\n",
    "        \"Solusi: pastikan df_train_meta memang object-level meta dan index-nya object_id.\"\n",
    "    )\n",
    "\n",
    "pos_idx = pos_s.astype(np.int32).to_numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Robust target column -> y (ordered by train_ids)\n",
    "# ----------------------------\n",
    "target_col = None\n",
    "for cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n",
    "    if cand in meta.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise RuntimeError(f\"[Stage 7] Cannot find target column in df_train_meta. cols(sample)={list(meta.columns)[:40]}\")\n",
    "\n",
    "y_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n",
    "y = y_all[pos_idx]\n",
    "y = (y > 0).astype(np.int8)  # force 0/1\n",
    "\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "if pos == 0 or neg == 0:\n",
    "    raise RuntimeError(f\"[Stage 7] Invalid class distribution: pos={pos}, neg={neg}. Cannot do stratified split.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Optional groups (by split)\n",
    "# ----------------------------\n",
    "groups = None\n",
    "group_col = None\n",
    "\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    for cand in [\"split\", \"split_id\", \"split_name\", \"split_idx\"]:\n",
    "        if cand in meta.columns:\n",
    "            group_col = cand\n",
    "            break\n",
    "\n",
    "    if group_col is None:\n",
    "        if not AUTO_FALLBACK_GROUP:\n",
    "            raise RuntimeError(\"[Stage 7] USE_GROUP_BY_SPLIT=True but no split column found in df_train_meta.\")\n",
    "        print(\"[Stage 7] WARN: split column not found; fallback to StratifiedKFold.\")\n",
    "        USE_GROUP_BY_SPLIT = False\n",
    "    else:\n",
    "        g_all = meta[group_col].astype(str).to_numpy()\n",
    "        groups = g_all[pos_idx]\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Choose n_splits safely + auto-adjust\n",
    "# ----------------------------\n",
    "max_splits_by_pos = pos\n",
    "max_splits_by_neg = neg\n",
    "max_splits_by_minpos = max(1, pos // max(int(MIN_POS_PER_FOLD), 1))\n",
    "\n",
    "n0 = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg, max_splits_by_minpos)\n",
    "if FORCE_N_SPLITS is not None:\n",
    "    n0 = int(FORCE_N_SPLITS)\n",
    "\n",
    "print(f\"[Stage 7] Candidate n_splits={n0} | N={N:,} pos={pos:,} neg={neg:,} pos%={pos/max(N,1)*100:.6f}% | order_source={order_source}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Build folds (sklearn) with robust fallback\n",
    "# ----------------------------\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedGroupKFold\n",
    "    except Exception:\n",
    "        StratifiedGroupKFold = None\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn is not available in this environment.\") from e\n",
    "\n",
    "def _try_split_kfold(k: int, use_group: bool):\n",
    "    fold_assign = np.full(N, -1, dtype=np.int16)\n",
    "    folds = []\n",
    "    per = []\n",
    "\n",
    "    if use_group:\n",
    "        if StratifiedGroupKFold is None:\n",
    "            return (False, \"StratifiedGroupKFold(unavailable)\", None, None, None)\n",
    "        splitter = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "        split_iter = splitter.split(np.zeros(N), y, groups=groups)\n",
    "        cv_type = f\"StratifiedGroupKFold({group_col})\"\n",
    "    else:\n",
    "        splitter = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "        split_iter = splitter.split(np.zeros(N), y)\n",
    "        cv_type = \"StratifiedKFold\"\n",
    "\n",
    "    try:\n",
    "        for fold, (tr_idx, val_idx) in enumerate(split_iter):\n",
    "            fold_assign[val_idx] = fold\n",
    "            yf = y[val_idx]\n",
    "            pf = int((yf == 1).sum())\n",
    "            nf = int((yf == 0).sum())\n",
    "            per.append((len(val_idx), pf, nf))\n",
    "            folds.append({\n",
    "                \"fold\": int(fold),\n",
    "                \"train_idx\": tr_idx.astype(np.int32),\n",
    "                \"val_idx\": val_idx.astype(np.int32),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        return (False, f\"{cv_type} (error: {type(e).__name__})\", None, None, None)\n",
    "\n",
    "    if (fold_assign < 0).any():\n",
    "        return (False, f\"{cv_type} (unassigned)\", None, None, None)\n",
    "\n",
    "    # hard check: each fold must have pos>=1 and neg>=1\n",
    "    for (_, pf, nf) in per:\n",
    "        if pf == 0 or nf == 0:\n",
    "            return (False, f\"{cv_type} (empty class in fold)\", None, None, None)\n",
    "\n",
    "    return (True, cv_type, fold_assign, folds, per)\n",
    "\n",
    "def _make_holdout():\n",
    "    # 1 split holdout with StratifiedShuffleSplit\n",
    "    # choose val size that ensures at least 1 pos and 1 neg in val (and in train)\n",
    "    n_pos = pos\n",
    "    n_neg = neg\n",
    "    n = N\n",
    "\n",
    "    # start from HOLDOUT_FRAC\n",
    "    val_n = int(round(n * float(HOLDOUT_FRAC)))\n",
    "    val_n = max(val_n, 2)  # at least 2 samples\n",
    "    val_n = min(val_n, n - 2)\n",
    "\n",
    "    # ensure possible: need at least 1 pos and 1 neg in val\n",
    "    val_n = max(val_n, 2)\n",
    "    if n_pos == 1 or n_neg == 1:\n",
    "        # still possible but fragile, keep val small\n",
    "        val_n = 2\n",
    "\n",
    "    # loop adjust if impossible\n",
    "    def feasible(vn: int) -> bool:\n",
    "        # need vn >=2 and vn <= n-2\n",
    "        if vn < 2 or vn > n - 2:\n",
    "            return False\n",
    "        # can we place at least 1 pos and 1 neg into val and keep at least 1 pos/neg in train?\n",
    "        return (n_pos >= 2 and n_neg >= 2) or ((n_pos >= 1 and n_neg >= 1) and (n_pos - 1 >= 1) and (n_neg - 1 >= 1))\n",
    "\n",
    "    if not feasible(val_n):\n",
    "        # minimal viable with both classes in train and val requires pos>=2 and neg>=2\n",
    "        if n_pos < 2 or n_neg < 2:\n",
    "            raise RuntimeError(\n",
    "                f\"[Stage 7] Cannot build even holdout split safely. Need pos>=2 and neg>=2. Got pos={n_pos}, neg={n_neg}.\"\n",
    "            )\n",
    "        val_n = 2\n",
    "\n",
    "    test_size = val_n / n\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=SEED)\n",
    "    tr_idx, val_idx = next(splitter.split(np.zeros(n), y))\n",
    "\n",
    "    fold_assign = np.full(n, -1, dtype=np.int16)\n",
    "    fold_assign[val_idx] = 0\n",
    "\n",
    "    yf = y[val_idx]\n",
    "    per = [(len(val_idx), int((yf == 1).sum()), int((yf == 0).sum()))]\n",
    "\n",
    "    folds = [{\n",
    "        \"fold\": 0,\n",
    "        \"train_idx\": tr_idx.astype(np.int32),\n",
    "        \"val_idx\": val_idx.astype(np.int32),\n",
    "    }]\n",
    "    return 1, \"Holdout(StratifiedShuffleSplit)\", fold_assign, folds, per\n",
    "\n",
    "best = None\n",
    "used_group = bool(USE_GROUP_BY_SPLIT)\n",
    "\n",
    "if n0 >= 2:\n",
    "    for k in range(n0, 1, -1):\n",
    "        ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=used_group)\n",
    "\n",
    "        if (not ok) and used_group and AUTO_FALLBACK_GROUP:\n",
    "            ok2, cv_type2, fa2, folds2, per2 = _try_split_kfold(k, use_group=False)\n",
    "            if ok2:\n",
    "                ok, cv_type, fa, folds, per = ok2, cv_type2, fa2, folds2, per2\n",
    "                used_group = False\n",
    "\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n",
    "        if ENFORCE_MIN_POS_PER_FOLD and (min_pos_seen < MIN_POS_PER_FOLD) and (FORCE_N_SPLITS is None):\n",
    "            continue\n",
    "\n",
    "        best = (k, cv_type, fa, folds, per, min_pos_seen)\n",
    "        break\n",
    "\n",
    "# if enforce failed completely, pick first valid (pos>=1 in each fold)\n",
    "if best is None and n0 >= 2:\n",
    "    for k in range(n0, 1, -1):\n",
    "        ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=bool(USE_GROUP_BY_SPLIT))\n",
    "        if (not ok) and USE_GROUP_BY_SPLIT and AUTO_FALLBACK_GROUP:\n",
    "            ok, cv_type, fa, folds, per = _try_split_kfold(k, use_group=False)\n",
    "        if ok:\n",
    "            min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n",
    "            best = (k, cv_type, fa, folds, per, min_pos_seen)\n",
    "            print(f\"[Stage 7] NOTE: Could not satisfy MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}. Using k={k} with min_pos={min_pos_seen}.\")\n",
    "            break\n",
    "\n",
    "# fallback to holdout if still none\n",
    "if best is None:\n",
    "    if HOLDOUT_FALLBACK:\n",
    "        n_splits, cv_type, fold_assign, folds, per = _make_holdout()\n",
    "        min_pos_seen = per[0][1] if per else 0\n",
    "        best = (n_splits, cv_type, fold_assign, folds, per, min_pos_seen)\n",
    "        print(f\"[Stage 7] FALLBACK -> {cv_type} | val_pos={min_pos_seen}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"[Stage 7] Failed to build a valid CV split. Try smaller DEFAULT_SPLITS / FORCE_N_SPLITS, or enable HOLDOUT_FALLBACK.\")\n",
    "\n",
    "n_splits, cv_type, fold_assign, folds, per, min_pos_seen = best\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Report + validation\n",
    "# ----------------------------\n",
    "lines = []\n",
    "lines.append(f\"CV={cv_type} n_splits={n_splits} seed={SEED}\")\n",
    "lines.append(f\"Order source: {order_source}\")\n",
    "lines.append(f\"Target column: {target_col}\")\n",
    "lines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    lines.append(f\"Group col requested: {group_col} | used_group={('Group' in cv_type)}\")\n",
    "lines.append(\"Per-fold distribution (val):\")\n",
    "\n",
    "ok = True\n",
    "for f in range(n_splits):\n",
    "    idx = np.where(fold_assign == f)[0]\n",
    "    yf = y[idx]\n",
    "    pf = int((yf == 1).sum())\n",
    "    nf = int((yf == 0).sum())\n",
    "    lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:9.6f}%\")\n",
    "    if pf == 0 or nf == 0:\n",
    "        ok = False\n",
    "\n",
    "if not ok:\n",
    "    raise RuntimeError(\"[Stage 7] A fold has pos=0 or neg=0 after selection (should not happen).\")\n",
    "\n",
    "if (fold_assign < 0).any():\n",
    "    bad = np.where(fold_assign < 0)[0][:10]\n",
    "    ex = [train_ids[i] for i in bad]\n",
    "    raise RuntimeError(f\"[Stage 7] Unassigned fold entries detected: count={(fold_assign<0).sum()} | ex={ex}\")\n",
    "\n",
    "if min_pos_seen < MIN_POS_PER_FOLD and n_splits >= 2:\n",
    "    lines.append(f\"NOTE: min positives in a fold = {min_pos_seen} (< MIN_POS_PER_FOLD={MIN_POS_PER_FOLD}). \"\n",
    "                 \"Threshold/F1 tuning bisa noisy; pertimbangkan n_splits lebih kecil.\")\n",
    "\n",
    "print(f\"[Stage 7] FINAL: n_splits={n_splits} | cv_type={cv_type} | min_pos_in_fold={min_pos_seen}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Save artifacts\n",
    "# ----------------------------\n",
    "CV_DIR = ART_DIR / \"cv\"\n",
    "CV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\n",
    "folds_csv = CV_DIR / \"cv_folds.csv\"\n",
    "df_folds.to_csv(folds_csv, index=False)\n",
    "\n",
    "npz_path = CV_DIR / \"cv_folds.npz\"\n",
    "npz_kwargs = {}\n",
    "for f in range(n_splits):\n",
    "    npz_kwargs[f\"train_idx_{f}\"] = folds[f][\"train_idx\"]\n",
    "    npz_kwargs[f\"val_idx_{f}\"]   = folds[f][\"val_idx\"]\n",
    "np.savez(npz_path, **npz_kwargs)\n",
    "\n",
    "report_path = CV_DIR / \"cv_report.txt\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "cfg_path = CV_DIR / \"cv_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"seed\": SEED,\n",
    "            \"n_splits\": int(n_splits),\n",
    "            \"cv_type\": cv_type,\n",
    "            \"min_pos_per_fold\": int(MIN_POS_PER_FOLD),\n",
    "            \"enforce_min_pos_per_fold\": bool(ENFORCE_MIN_POS_PER_FOLD),\n",
    "            \"use_group_by_split_requested\": bool(USE_GROUP_BY_SPLIT),\n",
    "            \"auto_fallback_group\": bool(AUTO_FALLBACK_GROUP),\n",
    "            \"holdout_fallback\": bool(HOLDOUT_FALLBACK),\n",
    "            \"order_source\": order_source,\n",
    "            \"target_col\": target_col,\n",
    "            \"group_col\": group_col,\n",
    "            \"artifacts\": {\n",
    "                \"folds_csv\": str(folds_csv),\n",
    "                \"folds_npz\": str(npz_path),\n",
    "                \"report_txt\": str(report_path),\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n[Stage 7] CV split OK\")\n",
    "print(f\"- Saved: {folds_csv}\")\n",
    "print(f\"- Saved: {npz_path}\")\n",
    "print(f\"- Saved: {report_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "\n",
    "# print tail report lines\n",
    "tail_n = min(len(lines), n_splits + 6)\n",
    "print(\"\\n\".join(lines[-tail_n:]))\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Export globals for next stage\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"CV_DIR\": CV_DIR,\n",
    "    \"n_splits\": int(n_splits),\n",
    "    \"train_ids_ordered\": train_ids,\n",
    "    \"y_ordered\": y,\n",
    "    \"fold_assign\": fold_assign,\n",
    "    \"folds\": folds,\n",
    "    \"CV_FOLDS_CSV\": folds_csv,\n",
    "    \"CV_FOLDS_NPZ\": npz_path,\n",
    "    \"CV_CFG_PATH\": cfg_path,\n",
    "    \"CV_TYPE\": cv_type,\n",
    "    \"CV_ORDER_SOURCE\": order_source,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978011c",
   "metadata": {
    "papermill": {
     "duration": 0.014302,
     "end_time": "2026-01-03T15:30:51.144688",
     "exception": false,
     "start_time": "2026-01-03T15:30:51.130386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model (CPU-Safe Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e86f5d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:30:51.175967Z",
     "iopub.status.busy": "2026-01-03T15:30:51.175514Z",
     "iopub.status.idle": "2026-01-03T17:21:10.926677Z",
     "shell.execute_reply": "2026-01-03T17:21:10.925656Z"
    },
    "papermill": {
     "duration": 6619.769964,
     "end_time": "2026-01-03T17:21:10.928860",
     "exception": false,
     "start_time": "2026-01-03T15:30:51.158896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8] Building AGG sequence features (one-time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/2308144241.py:258: RuntimeWarning: Mean of empty slice\n",
      "  mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8] AGG built: shape=(3043, 31) | time=0.2s\n",
      "[Stage 8] TRAIN CONFIG (CPU)\n",
      "- N=3,043 | pos=148 | neg=2,895 | pos%=4.863621%\n",
      "- token_mode=mag | val_feat=mag | g_dim=38 | use_agg_seq=True\n",
      "- SHIFT_BAND_IDS(from stage6)=False | PAD_BAND_ID(from stage6)=0\n",
      "- Model: d_model=160 heads=4 layers=3 dropout=0.12\n",
      "- Batch=16 grad_accum=2 epochs=14 lr=0.0005\n",
      "- balance_mode=sampler | label_smoothing=0.03\n",
      "- CKPT_DIR=/kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/checkpoints\n",
      "- OOF_DIR =/kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof\n",
      "- LOG_DIR =/kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/logs\n",
      "\n",
      "[Stage 8] FOLD 0/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17/2308144241.py:366: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  Xt = torch.from_numpy(X.astype(np.float32, copy=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=4.19e-04 | opt_steps=  77 | train_loss=0.68123 | val_loss=0.52205 | val_auc=0.68181 | f1@0.5=0.0615 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.97e-04 | opt_steps=  77 | train_loss=0.61258 | val_loss=0.73800 | val_auc=0.70150 | f1@0.5=0.1620 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=4.80e-04 | opt_steps=  77 | train_loss=0.54667 | val_loss=0.54998 | val_auc=0.75596 | f1@0.5=0.1756 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.49e-04 | opt_steps=  77 | train_loss=0.48130 | val_loss=0.41873 | val_auc=0.81911 | f1@0.5=0.2460 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.05e-04 | opt_steps=  77 | train_loss=0.43914 | val_loss=0.47571 | val_auc=0.83074 | f1@0.5=0.2212 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=3.52e-04 | opt_steps=  77 | train_loss=0.40878 | val_loss=0.33423 | val_auc=0.84231 | f1@0.5=0.2714 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=2.93e-04 | opt_steps=  77 | train_loss=0.39504 | val_loss=0.43668 | val_auc=0.84790 | f1@0.5=0.2434 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=2.31e-04 | opt_steps=  77 | train_loss=0.38102 | val_loss=0.41976 | val_auc=0.84347 | f1@0.5=0.2599 | best_ep=7 | pat=3\n",
      "  epoch 09 | lr=1.70e-04 | opt_steps=  77 | train_loss=0.36524 | val_loss=0.38303 | val_auc=0.84439 | f1@0.5=0.2857 | best_ep=7 | pat=2\n",
      "  epoch 10 | lr=1.14e-04 | opt_steps=  77 | train_loss=0.35977 | val_loss=0.43153 | val_auc=0.84767 | f1@0.5=0.2652 | best_ep=7 | pat=1\n",
      "  epoch 11 | lr=6.71e-05 | opt_steps=  77 | train_loss=0.36650 | val_loss=0.34635 | val_auc=0.84922 | f1@0.5=0.2667 | best_ep=11 | pat=4\n",
      "  epoch 12 | lr=3.10e-05 | opt_steps=  77 | train_loss=0.37017 | val_loss=0.41778 | val_auc=0.85066 | f1@0.5=0.2584 | best_ep=12 | pat=4\n",
      "  epoch 13 | lr=8.52e-06 | opt_steps=  77 | train_loss=0.37697 | val_loss=0.39467 | val_auc=0.85078 | f1@0.5=0.2805 | best_ep=13 | pat=4\n",
      "  epoch 14 | lr=1.00e-06 | opt_steps=  77 | train_loss=0.36985 | val_loss=0.39533 | val_auc=0.85107 | f1@0.5=0.2805 | best_ep=14 | pat=4\n",
      "\n",
      "[Stage 8] FOLD 1/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=4.19e-04 | opt_steps=  77 | train_loss=0.67701 | val_loss=0.65374 | val_auc=0.69522 | f1@0.5=0.1548 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.97e-04 | opt_steps=  77 | train_loss=0.59427 | val_loss=0.50821 | val_auc=0.69827 | f1@0.5=0.1545 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=4.80e-04 | opt_steps=  77 | train_loss=0.55002 | val_loss=0.76821 | val_auc=0.77271 | f1@0.5=0.1584 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.49e-04 | opt_steps=  77 | train_loss=0.45819 | val_loss=0.55853 | val_auc=0.79447 | f1@0.5=0.1803 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.05e-04 | opt_steps=  77 | train_loss=0.42630 | val_loss=0.50609 | val_auc=0.80432 | f1@0.5=0.1959 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=3.52e-04 | opt_steps=  77 | train_loss=0.41834 | val_loss=0.46478 | val_auc=0.82406 | f1@0.5=0.2312 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=2.93e-04 | opt_steps=  77 | train_loss=0.39245 | val_loss=0.44289 | val_auc=0.82873 | f1@0.5=0.2468 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=2.31e-04 | opt_steps=  77 | train_loss=0.38072 | val_loss=0.45938 | val_auc=0.82907 | f1@0.5=0.2317 | best_ep=8 | pat=4\n",
      "  epoch 09 | lr=1.70e-04 | opt_steps=  77 | train_loss=0.38596 | val_loss=0.47403 | val_auc=0.82464 | f1@0.5=0.2286 | best_ep=8 | pat=3\n",
      "  epoch 10 | lr=1.14e-04 | opt_steps=  77 | train_loss=0.36697 | val_loss=0.46653 | val_auc=0.82418 | f1@0.5=0.2339 | best_ep=8 | pat=2\n",
      "  epoch 11 | lr=6.71e-05 | opt_steps=  77 | train_loss=0.36122 | val_loss=0.44440 | val_auc=0.83069 | f1@0.5=0.2390 | best_ep=11 | pat=4\n",
      "  epoch 12 | lr=3.10e-05 | opt_steps=  77 | train_loss=0.34281 | val_loss=0.44090 | val_auc=0.83069 | f1@0.5=0.2484 | best_ep=12 | pat=4\n",
      "  epoch 13 | lr=8.52e-06 | opt_steps=  77 | train_loss=0.35094 | val_loss=0.44842 | val_auc=0.83184 | f1@0.5=0.2532 | best_ep=13 | pat=4\n",
      "  epoch 14 | lr=1.00e-06 | opt_steps=  77 | train_loss=0.36075 | val_loss=0.45855 | val_auc=0.83189 | f1@0.5=0.2469 | best_ep=14 | pat=4\n",
      "\n",
      "[Stage 8] FOLD 2/4 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=4.19e-04 | opt_steps=  77 | train_loss=0.67582 | val_loss=0.76504 | val_auc=0.63673 | f1@0.5=0.1239 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.97e-04 | opt_steps=  77 | train_loss=0.61456 | val_loss=0.78499 | val_auc=0.71439 | f1@0.5=0.1436 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=4.80e-04 | opt_steps=  77 | train_loss=0.53809 | val_loss=0.41829 | val_auc=0.84997 | f1@0.5=0.2700 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.49e-04 | opt_steps=  77 | train_loss=0.47000 | val_loss=0.41279 | val_auc=0.89044 | f1@0.5=0.2796 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.05e-04 | opt_steps=  77 | train_loss=0.43645 | val_loss=0.46541 | val_auc=0.89303 | f1@0.5=0.2291 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=3.52e-04 | opt_steps=  77 | train_loss=0.41075 | val_loss=0.47324 | val_auc=0.90829 | f1@0.5=0.2478 | best_ep=6 | pat=4\n",
      "  epoch 07 | lr=2.93e-04 | opt_steps=  77 | train_loss=0.41096 | val_loss=0.36859 | val_auc=0.92211 | f1@0.5=0.2924 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=2.31e-04 | opt_steps=  77 | train_loss=0.38050 | val_loss=0.36163 | val_auc=0.91670 | f1@0.5=0.3030 | best_ep=7 | pat=3\n",
      "  epoch 09 | lr=1.70e-04 | opt_steps=  77 | train_loss=0.38718 | val_loss=0.34776 | val_auc=0.92499 | f1@0.5=0.3106 | best_ep=9 | pat=4\n",
      "  epoch 10 | lr=1.14e-04 | opt_steps=  77 | train_loss=0.38865 | val_loss=0.31994 | val_auc=0.92216 | f1@0.5=0.3158 | best_ep=9 | pat=3\n",
      "  epoch 11 | lr=6.71e-05 | opt_steps=  77 | train_loss=0.37182 | val_loss=0.33939 | val_auc=0.92320 | f1@0.5=0.3038 | best_ep=9 | pat=2\n",
      "  epoch 12 | lr=3.10e-05 | opt_steps=  77 | train_loss=0.36505 | val_loss=0.35338 | val_auc=0.92176 | f1@0.5=0.2909 | best_ep=9 | pat=1\n",
      "  epoch 13 | lr=8.52e-06 | opt_steps=  77 | train_loss=0.35974 | val_loss=0.33672 | val_auc=0.92153 | f1@0.5=0.2930 | best_ep=9 | pat=0\n",
      "\n",
      "[Stage 8] FOLD 3/4 | train=2,435 val=608 | pos=119 neg=2,316 | pos_weight=19.4622 | balance_mode=sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=4.19e-04 | opt_steps=  77 | train_loss=0.67914 | val_loss=0.65830 | val_auc=0.68066 | f1@0.5=0.1445 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.97e-04 | opt_steps=  77 | train_loss=0.61610 | val_loss=0.61103 | val_auc=0.71717 | f1@0.5=0.1623 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=4.80e-04 | opt_steps=  77 | train_loss=0.55039 | val_loss=0.60442 | val_auc=0.81925 | f1@0.5=0.1683 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.49e-04 | opt_steps=  77 | train_loss=0.47961 | val_loss=0.51981 | val_auc=0.85927 | f1@0.5=0.2143 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.05e-04 | opt_steps=  77 | train_loss=0.44276 | val_loss=0.63572 | val_auc=0.87577 | f1@0.5=0.1856 | best_ep=5 | pat=4\n",
      "  epoch 06 | lr=3.52e-04 | opt_steps=  77 | train_loss=0.42314 | val_loss=0.37156 | val_auc=0.87475 | f1@0.5=0.2788 | best_ep=5 | pat=3\n",
      "  epoch 07 | lr=2.93e-04 | opt_steps=  77 | train_loss=0.38939 | val_loss=0.39841 | val_auc=0.87874 | f1@0.5=0.2747 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=2.31e-04 | opt_steps=  77 | train_loss=0.38471 | val_loss=0.39396 | val_auc=0.87815 | f1@0.5=0.2793 | best_ep=7 | pat=3\n",
      "  epoch 09 | lr=1.70e-04 | opt_steps=  77 | train_loss=0.37657 | val_loss=0.40205 | val_auc=0.87845 | f1@0.5=0.2732 | best_ep=7 | pat=2\n",
      "  epoch 10 | lr=1.14e-04 | opt_steps=  77 | train_loss=0.37730 | val_loss=0.31212 | val_auc=0.86951 | f1@0.5=0.2941 | best_ep=7 | pat=1\n",
      "  epoch 11 | lr=6.71e-05 | opt_steps=  77 | train_loss=0.37245 | val_loss=0.39832 | val_auc=0.87690 | f1@0.5=0.2793 | best_ep=7 | pat=0\n",
      "\n",
      "[Stage 8] FOLD 4/4 | train=2,435 val=608 | pos=119 neg=2,316 | pos_weight=19.4622 | balance_mode=sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=4.19e-04 | opt_steps=  77 | train_loss=0.67648 | val_loss=0.61854 | val_auc=0.66220 | f1@0.5=0.1424 | best_ep=1 | pat=4\n",
      "  epoch 02 | lr=4.97e-04 | opt_steps=  77 | train_loss=0.60941 | val_loss=0.59179 | val_auc=0.75153 | f1@0.5=0.1633 | best_ep=2 | pat=4\n",
      "  epoch 03 | lr=4.80e-04 | opt_steps=  77 | train_loss=0.51543 | val_loss=0.61123 | val_auc=0.81699 | f1@0.5=0.1751 | best_ep=3 | pat=4\n",
      "  epoch 04 | lr=4.49e-04 | opt_steps=  77 | train_loss=0.44166 | val_loss=0.39812 | val_auc=0.85290 | f1@0.5=0.2400 | best_ep=4 | pat=4\n",
      "  epoch 05 | lr=4.05e-04 | opt_steps=  77 | train_loss=0.44037 | val_loss=0.27203 | val_auc=0.85177 | f1@0.5=0.3333 | best_ep=4 | pat=3\n",
      "  epoch 06 | lr=3.52e-04 | opt_steps=  77 | train_loss=0.41427 | val_loss=0.39185 | val_auc=0.85278 | f1@0.5=0.2471 | best_ep=4 | pat=2\n",
      "  epoch 07 | lr=2.93e-04 | opt_steps=  77 | train_loss=0.39864 | val_loss=0.43244 | val_auc=0.85510 | f1@0.5=0.2527 | best_ep=7 | pat=4\n",
      "  epoch 08 | lr=2.31e-04 | opt_steps=  77 | train_loss=0.40246 | val_loss=0.35443 | val_auc=0.86147 | f1@0.5=0.2897 | best_ep=8 | pat=4\n",
      "  epoch 09 | lr=1.70e-04 | opt_steps=  77 | train_loss=0.38456 | val_loss=0.36540 | val_auc=0.86499 | f1@0.5=0.2800 | best_ep=9 | pat=4\n",
      "  epoch 10 | lr=1.14e-04 | opt_steps=  77 | train_loss=0.37861 | val_loss=0.43155 | val_auc=0.86153 | f1@0.5=0.2500 | best_ep=9 | pat=3\n",
      "  epoch 11 | lr=6.71e-05 | opt_steps=  77 | train_loss=0.38057 | val_loss=0.41581 | val_auc=0.86070 | f1@0.5=0.2514 | best_ep=9 | pat=2\n",
      "  epoch 12 | lr=3.10e-05 | opt_steps=  77 | train_loss=0.37792 | val_loss=0.34202 | val_auc=0.86290 | f1@0.5=0.2817 | best_ep=9 | pat=1\n",
      "  epoch 13 | lr=8.52e-06 | opt_steps=  77 | train_loss=0.38771 | val_loss=0.36691 | val_auc=0.86296 | f1@0.5=0.2685 | best_ep=9 | pat=0\n",
      "\n",
      "[Stage 8] CV TRAIN DONE\n",
      "- elapsed: 110.32 min\n",
      "- OOF saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/oof_prob.npy\n",
      "- OOF saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/oof_prob.csv\n",
      "- fold metrics: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/fold_metrics.json\n",
      "- OOF AUC (rough): 0.86771\n",
      "- OOF F1@0.5 (rough): 0.2784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 8 — Train Multiband Event Transformer (CPU-Safe)\n",
    "# REVISI FULL v3.1 (BOOST + NO-LEAK + OneCycle FIX + SHIFT_BAND_IDS OK)\n",
    "#\n",
    "# Output:\n",
    "# - checkpoints/fold_*.pt\n",
    "# - oof/oof_prob.npy + oof/oof_prob.csv\n",
    "# - oof/fold_metrics.json\n",
    "# - logs/train_cfg_stage8.json + global_feature_spec.json\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, math, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal previous stages\n",
    "# ----------------------------\n",
    "need_min = [\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_train_meta\",\"n_splits\",\"folds\"]\n",
    "for k in need_min:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0a) Resolve train_ids ordering + labels (robust)\n",
    "# ----------------------------\n",
    "def _decode_ids(arr):\n",
    "    out = []\n",
    "    for x in arr.tolist():\n",
    "        if isinstance(x, (bytes, bytearray)):\n",
    "            s = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            s = str(x)\n",
    "        out.append(s.strip())\n",
    "    return out\n",
    "\n",
    "# ordering\n",
    "if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n",
    "    train_ids = [str(x).strip() for x in list(globals()[\"train_ids_ordered\"])]\n",
    "else:\n",
    "    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        raw = np.load(p, allow_pickle=False)\n",
    "        train_ids = _decode_ids(raw if raw.dtype.kind in (\"S\",\"O\") else raw.astype(str))\n",
    "    else:\n",
    "        train_ids = [str(x).strip() for x in df_train_meta.index.astype(str).tolist()]\n",
    "\n",
    "# target column robust\n",
    "target_col = None\n",
    "for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n",
    "    if cand in df_train_meta.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols(sample)={list(df_train_meta.columns)[:40]}\")\n",
    "\n",
    "# robust mapping by stringified index (avoid index dtype mismatch)\n",
    "meta_idx_str = pd.Index([str(x).strip() for x in df_train_meta.index], dtype=\"object\")\n",
    "pos_map = pd.Series(np.arange(len(df_train_meta), dtype=np.int32), index=meta_idx_str)\n",
    "pos_idx = pos_map.reindex(train_ids).to_numpy()\n",
    "if np.isnan(pos_idx).any():\n",
    "    miss = [train_ids[i] for i in np.where(np.isnan(pos_idx))[0][:10]]\n",
    "    raise RuntimeError(f\"Some train_ids not found in df_train_meta.index (string-mapped). ex={miss}\")\n",
    "\n",
    "pos_idx = pos_idx.astype(np.int32)\n",
    "\n",
    "y_all = pd.to_numeric(df_train_meta[target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy()\n",
    "y = y_all[pos_idx]\n",
    "y = (y > 0).astype(np.int8)\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Ensure output dirs exist\n",
    "# ----------------------------\n",
    "if \"RUN_DIR\" in globals() and globals()[\"RUN_DIR\"] is not None:\n",
    "    RUN_DIR = Path(globals()[\"RUN_DIR\"])\n",
    "else:\n",
    "    if \"ART_DIR\" in globals() and globals()[\"ART_DIR\"] is not None:\n",
    "        RUN_DIR = Path(globals()[\"ART_DIR\"]).parent\n",
    "    else:\n",
    "        RUN_DIR = Path(\"/kaggle/working/mallorn_run\")\n",
    "\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", RUN_DIR / \"artifacts\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CKPT_DIR = Path(globals().get(\"CKPT_DIR\", RUN_DIR / \"checkpoints\"))\n",
    "OOF_DIR  = Path(globals().get(\"OOF_DIR\",  RUN_DIR / \"oof\"))\n",
    "LOG_DIR  = Path(globals().get(\"LOG_DIR\",  RUN_DIR / \"logs\"))\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "globals().update({\"RUN_DIR\": RUN_DIR, \"ART_DIR\": ART_DIR, \"CKPT_DIR\": CKPT_DIR, \"OOF_DIR\": OOF_DIR, \"LOG_DIR\": LOG_DIR})\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Torch imports + CPU safety\n",
    "# ----------------------------\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# thread guard\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn metrics tidak tersedia.\") from e\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open memmaps (fixed seq) — NO RAM load\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(globals()[\"FIX_DIR\"])\n",
    "N = len(train_ids)\n",
    "L = int(globals()[\"MAX_LEN\"])\n",
    "SEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "feat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "\n",
    "for p in [train_X_path, train_B_path, train_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "X_mm = np.memmap(train_X_path, dtype=np.float32, mode=\"r\", shape=(N, L, Fdim))\n",
    "B_mm = np.memmap(train_B_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "M_mm = np.memmap(train_M_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "\n",
    "# ----------------------------\n",
    "# 2b) Read Stage6 policy to detect SHIFT_BAND_IDS (important!)\n",
    "# ----------------------------\n",
    "SHIFT_BAND_IDS = False\n",
    "PAD_BAND_ID = 0\n",
    "policy_path = FIX_DIR / \"length_policy_config.json\"\n",
    "if policy_path.exists():\n",
    "    try:\n",
    "        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pol = json.load(f)\n",
    "        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n",
    "        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# detect token mode\n",
    "SEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    if (\"mag\" in feat) and (\"mag_err_log\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"mag\"\n",
    "    elif (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n",
    "        SEQ_TOKEN_MODE = \"asinh\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"Cannot infer token_mode from features: {SEQ_FEATURE_NAMES}\")\n",
    "\n",
    "for k in [\"snr_tanh\",\"detected\"]:\n",
    "    if k not in feat:\n",
    "        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n",
    "\n",
    "VAL_FEAT = \"mag\" if SEQ_TOKEN_MODE == \"mag\" else \"flux_asinh\"\n",
    "if VAL_FEAT not in feat:\n",
    "    raise RuntimeError(f\"Feature '{VAL_FEAT}' missing for token_mode={SEQ_TOKEN_MODE}.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build RAW meta global features\n",
    "# ----------------------------\n",
    "BASE_G_COLS = [\"Z\",\"Z_err\",\"EBV\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\n",
    "for c in BASE_G_COLS:\n",
    "    if c not in df_train_meta.columns:\n",
    "        df_train_meta[c] = 0.0\n",
    "\n",
    "G_meta = df_train_meta.iloc[pos_idx][BASE_G_COLS].copy()\n",
    "for c in BASE_G_COLS:\n",
    "    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "G_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "with open(Path(LOG_DIR)/\"global_meta_cols.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"cols\": BASE_G_COLS}, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 3b) Sequence aggregate features (global + per-band) — BOOST\n",
    "# ----------------------------\n",
    "USE_AGG_SEQ_FEATURES = True\n",
    "N_BANDS = 6\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1.0)\n",
    "\n",
    "def build_agg_seq_features(X_mm, B_mm, M_mm, chunk=2048):\n",
    "    snr_i = feat[\"snr_tanh\"]\n",
    "    det_i = feat[\"detected\"]\n",
    "    val_i = feat[VAL_FEAT]\n",
    "\n",
    "    out_chunks = []\n",
    "    for s in range(0, N, chunk):\n",
    "        e = min(N, s + chunk)\n",
    "        Xc = np.asarray(X_mm[s:e])  # (B,L,F)\n",
    "        Bc = np.asarray(B_mm[s:e])  # (B,L)\n",
    "        Mc = np.asarray(M_mm[s:e])  # (B,L)\n",
    "\n",
    "        real = (Mc == 1)\n",
    "        tok_count = real.sum(axis=1).astype(np.float32)\n",
    "\n",
    "        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32)\n",
    "        det = (Xc[:, :, det_i] > 0.5).astype(np.float32)\n",
    "        val = Xc[:, :, val_i].astype(np.float32)\n",
    "\n",
    "        snr_r = snr * real\n",
    "        det_r = det * real\n",
    "\n",
    "        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n",
    "        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n",
    "        max_abs_snr = np.where(tok_count > 0, (snr_r).max(axis=1), 0.0).astype(np.float32)\n",
    "\n",
    "        if SEQ_TOKEN_MODE == \"mag\":\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            mean_val = np.nan_to_num(np.nanmean(val_r, axis=1).astype(np.float32), nan=0.0)\n",
    "            std_val  = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32),  nan=0.0)\n",
    "            min_val  = np.nan_to_num(np.nanmin(val_r, axis=1).astype(np.float32),  nan=0.0)\n",
    "            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n",
    "        else:\n",
    "            aval = np.abs(val).astype(np.float32)\n",
    "            aval_r = aval * real\n",
    "            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            std_val = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32), nan=0.0)\n",
    "            max_aval = np.where(tok_count > 0, (aval_r).max(axis=1), 0.0).astype(np.float32)\n",
    "            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n",
    "\n",
    "        per_band = []\n",
    "        for b in range(N_BANDS):\n",
    "            bm = (Bc == b) & real\n",
    "            cnt = bm.sum(axis=1).astype(np.float32)\n",
    "            detb = (det * bm).sum(axis=1).astype(np.float32)\n",
    "            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n",
    "\n",
    "            det_frac_b = _safe_div(detb, cnt)\n",
    "            mean_abs_snr_b = _safe_div(snrb, cnt)\n",
    "\n",
    "            if SEQ_TOKEN_MODE == \"mag\":\n",
    "                vb = np.where(bm, val, np.nan)\n",
    "                mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n",
    "            else:\n",
    "                ab = (np.abs(val).astype(np.float32) * bm).sum(axis=1).astype(np.float32)\n",
    "                mean_val_b = _safe_div(ab, cnt).astype(np.float32)\n",
    "\n",
    "            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n",
    "\n",
    "        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n",
    "\n",
    "        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n",
    "        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n",
    "\n",
    "        out_chunks.append(agg)\n",
    "        del Xc, Bc, Mc\n",
    "        if (s // chunk) % 3 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    return np.concatenate(out_chunks, axis=0).astype(np.float32)\n",
    "\n",
    "if USE_AGG_SEQ_FEATURES:\n",
    "    print(\"[Stage 8] Building AGG sequence features (one-time)...\")\n",
    "    t0 = time.time()\n",
    "    G_seq_np = build_agg_seq_features(X_mm, B_mm, M_mm, chunk=2048)\n",
    "    print(f\"[Stage 8] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\n",
    "else:\n",
    "    G_seq_np = np.zeros((N,0), dtype=np.float32)\n",
    "\n",
    "G_raw_np = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\n",
    "g_dim = int(G_raw_np.shape[1])\n",
    "\n",
    "with open(Path(LOG_DIR)/\"global_feature_spec.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"meta_cols\": BASE_G_COLS,\n",
    "            \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n",
    "            \"token_mode\": SEQ_TOKEN_MODE,\n",
    "            \"val_feat\": VAL_FEAT,\n",
    "            \"agg_dim\": int(G_seq_np.shape[1]),\n",
    "            \"total_g_dim\": int(g_dim),\n",
    "            \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "            \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset / Loader (num_workers=0) + optional augmentation\n",
    "# ----------------------------\n",
    "AUG_TOKENDROP_P = 0.05     # 0.0 disable\n",
    "AUG_VALUE_NOISE = 0.01     # 0.0 disable\n",
    "\n",
    "class MemmapSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, idx, X_mm, B_mm, M_mm, G_scaled_np, y=None, train_mode=False):\n",
    "        self.idx = np.asarray(idx, dtype=np.int32)\n",
    "        self.X_mm = X_mm\n",
    "        self.B_mm = B_mm\n",
    "        self.M_mm = M_mm\n",
    "        self.G = G_scaled_np\n",
    "        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n",
    "        self.train_mode = bool(train_mode)\n",
    "        self.rng = np.random.default_rng(SEED + (777 if train_mode else 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "\n",
    "        X = np.asarray(self.X_mm[j])  # (L,F) view-ish\n",
    "        B = np.asarray(self.B_mm[j])  # (L,)\n",
    "        M = np.asarray(self.M_mm[j])  # (L,)\n",
    "        G = np.asarray(self.G[j])     # (g_dim,)\n",
    "\n",
    "        # IMPORTANT: handle SHIFT_BAND_IDS from Stage 6\n",
    "        if SHIFT_BAND_IDS:\n",
    "            # real bands are 1..6, pad is 0. Convert real->0..5\n",
    "            real = (M == 1)\n",
    "            if real.any():\n",
    "                B = B.astype(np.int16, copy=False)\n",
    "                B2 = B.copy()\n",
    "                B2[real] = np.clip(B2[real] - 1, 0, N_BANDS - 1)\n",
    "                B2[~real] = 0\n",
    "                B = B2.astype(np.int8, copy=False)\n",
    "\n",
    "        if self.train_mode:\n",
    "            # token dropout: drop a small fraction of REAL tokens, keep at least 1\n",
    "            if AUG_TOKENDROP_P and AUG_TOKENDROP_P > 0:\n",
    "                real = (M == 1)\n",
    "                nreal = int(real.sum())\n",
    "                if nreal > 1:\n",
    "                    drop = (self.rng.random(M.shape[0]) < AUG_TOKENDROP_P) & real\n",
    "                    if int(drop.sum()) >= nreal:  # would drop all real tokens\n",
    "                        keep_pos = np.where(real)[0][int(self.rng.integers(0, nreal))]\n",
    "                        drop[keep_pos] = False\n",
    "                    if drop.any():\n",
    "                        M = M.copy()\n",
    "                        M[drop] = 0\n",
    "\n",
    "            # small value noise on real tokens\n",
    "            if AUG_VALUE_NOISE and AUG_VALUE_NOISE > 0:\n",
    "                vi = feat[VAL_FEAT]\n",
    "                real = (M == 1)\n",
    "                if real.any():\n",
    "                    X = X.copy()\n",
    "                    noise = self.rng.normal(0.0, AUG_VALUE_NOISE, size=int(real.sum())).astype(np.float32)\n",
    "                    X[real, vi] = (X[real, vi] + noise).astype(np.float32)\n",
    "\n",
    "        Xt = torch.from_numpy(X.astype(np.float32, copy=False))\n",
    "        Bt = torch.from_numpy(B.astype(np.int64, copy=False))\n",
    "        Mt = torch.from_numpy(M.astype(np.int64, copy=False))\n",
    "        Gt = torch.from_numpy(G.astype(np.float32, copy=False))\n",
    "\n",
    "        if self.y is None:\n",
    "            return Xt, Bt, Mt, Gt\n",
    "\n",
    "        yy = float(self.y[j])\n",
    "        return Xt, Bt, Mt, Gt, torch.tensor(yy, dtype=torch.float32)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, sampler=None):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(sampler is None and shuffle),\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Model — stronger pooling\n",
    "# ----------------------------\n",
    "class MultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, max_len, n_bands=6, d_model=160, n_heads=4, n_layers=3, ff_mult=2, dropout=0.12, g_dim=0):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.x_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.band_emb = nn.Embedding(n_bands, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * ff_mult),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "        self.pool_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        g_out = max(32, d_model // 2)\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(g_dim, g_out),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model + g_out, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        # clamp band to 0..n_bands-1\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1)\n",
    "\n",
    "        pad_mask = (mask == 0)  # True=pad\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n",
    "        denom = valid.sum(dim=1).clamp_min(1.0)\n",
    "        pooled_mean = (h * valid).sum(dim=1) / denom\n",
    "\n",
    "        pooled = 0.6 * pooled_attn + 0.4 * pooled_mean\n",
    "        pooled = self.pool_ln(pooled)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        return self.head(z).squeeze(-1)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Training config (CPU safe)\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    \"d_model\": 160,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 3,\n",
    "    \"ff_mult\": 2,\n",
    "    \"dropout\": 0.12,\n",
    "\n",
    "    \"batch_size\": 16,\n",
    "    \"grad_accum\": 2,\n",
    "\n",
    "    \"epochs\": 14,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.02,\n",
    "\n",
    "    \"patience\": 4,            # early stop by AUC\n",
    "    \"max_grad_norm\": 1.0,\n",
    "\n",
    "    # imbalance strategy: \"sampler\" | \"pos_weight\" | \"both\" | \"none\"\n",
    "    \"balance_mode\": \"sampler\",\n",
    "\n",
    "    \"label_smoothing\": 0.03,\n",
    "    \"scheduler\": \"onecycle\",\n",
    "}\n",
    "\n",
    "# auto soften for long seq\n",
    "if L >= 512:\n",
    "    CFG[\"d_model\"] = 128\n",
    "    CFG[\"n_heads\"] = 4\n",
    "    CFG[\"n_layers\"] = 2\n",
    "    CFG[\"batch_size\"] = 12\n",
    "    CFG[\"grad_accum\"] = 2\n",
    "    CFG[\"lr\"] = 4e-4\n",
    "\n",
    "cfg_path = Path(LOG_DIR) / \"train_cfg_stage8.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "\n",
    "pos_all = int((y == 1).sum())\n",
    "neg_all = int((y == 0).sum())\n",
    "print(\"[Stage 8] TRAIN CONFIG (CPU)\")\n",
    "print(f\"- N={N:,} | pos={pos_all:,} | neg={neg_all:,} | pos%={pos_all/max(N,1)*100:.6f}%\")\n",
    "print(f\"- token_mode={SEQ_TOKEN_MODE} | val_feat={VAL_FEAT} | g_dim={g_dim} | use_agg_seq={USE_AGG_SEQ_FEATURES}\")\n",
    "print(f\"- SHIFT_BAND_IDS(from stage6)={SHIFT_BAND_IDS} | PAD_BAND_ID(from stage6)={PAD_BAND_ID}\")\n",
    "print(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\n",
    "print(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\n",
    "print(f\"- balance_mode={CFG['balance_mode']} | label_smoothing={CFG['label_smoothing']}\")\n",
    "print(f\"- CKPT_DIR={CKPT_DIR}\")\n",
    "print(f\"- OOF_DIR ={OOF_DIR}\")\n",
    "print(f\"- LOG_DIR ={LOG_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Helpers\n",
    "# ----------------------------\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def f1_binary(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return float(2 * prec * rec / (prec + rec))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses, logits_all, y_all = [], [], []\n",
    "    for batch in loader:\n",
    "        Xb, Bb, Mb, Gb, yb = batch\n",
    "        Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n",
    "        logit = model(Xb, Bb, Mb, Gb)\n",
    "        loss = criterion(logit, yb)\n",
    "        losses.append(float(loss.item()))\n",
    "        logits_all.append(logit.detach().cpu().numpy())\n",
    "        y_all.append(yb.detach().cpu().numpy())\n",
    "    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n",
    "    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n",
    "    probs = sigmoid_np(logits_all)\n",
    "    pred01 = (probs >= 0.5).astype(np.int8)\n",
    "    f1 = f1_binary(y_all, pred01)\n",
    "    auc = float(roc_auc_score(y_all, probs)) if (len(np.unique(y_all)) == 2) else float(\"nan\")\n",
    "    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1, auc\n",
    "\n",
    "# fold-wise scaler (NO leakage)\n",
    "def fit_scaler_fold(G_raw_np, tr_idx):\n",
    "    X = G_raw_np[tr_idx]\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    std  = X.std(axis=0).astype(np.float32)\n",
    "    std  = np.where(std < 1e-6, 1.0, std).astype(np.float32)\n",
    "    return mean, std\n",
    "\n",
    "def apply_scaler(G_raw_np, mean, std):\n",
    "    return ((G_raw_np - mean) / std).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) CV Train\n",
    "# ----------------------------\n",
    "oof_prob = np.zeros((N,), dtype=np.float32)\n",
    "fold_metrics = []\n",
    "\n",
    "all_idx = np.arange(N, dtype=np.int32)\n",
    "n_splits = int(globals()[\"n_splits\"])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for fold_info in globals()[\"folds\"]:\n",
    "    fold = int(fold_info[\"fold\"])\n",
    "    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n",
    "\n",
    "    val_mask = np.zeros(N, dtype=bool)\n",
    "    val_mask[val_idx] = True\n",
    "    tr_idx = all_idx[~val_mask]\n",
    "\n",
    "    y_tr = y[tr_idx]\n",
    "    pos = int((y_tr == 1).sum())\n",
    "    neg = int((y_tr == 0).sum())\n",
    "    if pos == 0:\n",
    "        raise RuntimeError(f\"[Stage 8] Fold {fold}: no positives in training split.\")\n",
    "\n",
    "    # imbalance knobs\n",
    "    balance_mode = str(CFG.get(\"balance_mode\", \"sampler\")).lower()\n",
    "    use_sampler = balance_mode in (\"sampler\", \"both\")\n",
    "    use_posw    = balance_mode in (\"pos_weight\", \"both\")\n",
    "\n",
    "    pos_weight = float(neg / max(pos, 1))\n",
    "    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\n",
    "\n",
    "    # label smoothing\n",
    "    ls = float(CFG[\"label_smoothing\"])\n",
    "    def smooth(yb):\n",
    "        if ls <= 0:\n",
    "            return yb\n",
    "        return yb * (1.0 - ls) + 0.5 * ls\n",
    "\n",
    "    # criterion\n",
    "    if use_posw:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print(f\"\\n[Stage 8] FOLD {fold}/{n_splits-1} | train={len(tr_idx):,} val={len(val_idx):,} \"\n",
    "          f\"| pos={pos:,} neg={neg:,} | pos_weight={pos_weight:.4f} | balance_mode={balance_mode}\")\n",
    "\n",
    "    # fold-wise scaler (NO leakage)\n",
    "    g_mean, g_std = fit_scaler_fold(G_raw_np, tr_idx)\n",
    "    G_fold_z = apply_scaler(G_raw_np, g_mean, g_std)\n",
    "\n",
    "    # datasets\n",
    "    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_fold_z, y=y, train_mode=True)\n",
    "    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_fold_z, y=y, train_mode=False)\n",
    "\n",
    "    # optional weighted sampler (train only)\n",
    "    sampler = None\n",
    "    if use_sampler:\n",
    "        w = np.ones((len(tr_idx),), dtype=np.float32)\n",
    "        ytr_local = y[tr_idx]\n",
    "        w[ytr_local == 1] = float(neg / max(pos, 1))\n",
    "        w_t = torch.from_numpy(w)\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(weights=w_t, num_samples=len(tr_idx), replacement=True)\n",
    "\n",
    "    dl_tr = make_loader(ds_tr, batch_size=int(CFG[\"batch_size\"]), shuffle=True, sampler=sampler)\n",
    "    dl_va = make_loader(ds_va, batch_size=int(CFG[\"batch_size\"]), shuffle=False)\n",
    "\n",
    "    model = MultibandEventTransformer(\n",
    "        feat_dim=Fdim,\n",
    "        max_len=L,\n",
    "        n_bands=6,\n",
    "        d_model=int(CFG[\"d_model\"]),\n",
    "        n_heads=int(CFG[\"n_heads\"]),\n",
    "        n_layers=int(CFG[\"n_layers\"]),\n",
    "        ff_mult=int(CFG[\"ff_mult\"]),\n",
    "        dropout=float(CFG[\"dropout\"]),\n",
    "        g_dim=g_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(CFG[\"lr\"]), weight_decay=float(CFG[\"weight_decay\"]))\n",
    "\n",
    "    # scheduler (FIX: steps_per_epoch must match optimizer steps when using grad_accum)\n",
    "    scheduler = None\n",
    "    grad_accum = int(CFG[\"grad_accum\"])\n",
    "    if str(CFG.get(\"scheduler\",\"\")).lower() == \"onecycle\":\n",
    "        steps_per_epoch_opt = int(math.ceil(len(dl_tr) / max(grad_accum, 1)))\n",
    "        steps_per_epoch_opt = max(steps_per_epoch_opt, 1)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(CFG[\"lr\"]),\n",
    "            epochs=int(CFG[\"epochs\"]),\n",
    "            steps_per_epoch=steps_per_epoch_opt,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"cos\",\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "\n",
    "    best_val_auc = -1e9\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_probs = None\n",
    "    patience_left = int(CFG[\"patience\"])\n",
    "\n",
    "    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        accum = 0\n",
    "        opt_steps = 0\n",
    "\n",
    "        for batch in dl_tr:\n",
    "            Xb, Bb, Mb, Gb, yb = batch\n",
    "            Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n",
    "\n",
    "            yb_s = smooth(yb)\n",
    "\n",
    "            logit = model(Xb, Bb, Mb, Gb)\n",
    "            loss = criterion(logit, yb_s)\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "            (loss / float(grad_accum)).backward()\n",
    "            accum += 1\n",
    "\n",
    "            if accum == grad_accum:\n",
    "                if CFG[\"max_grad_norm\"] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                opt_steps += 1\n",
    "                accum = 0\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        # remainder step\n",
    "        if accum > 0:\n",
    "            if CFG[\"max_grad_norm\"] is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            opt_steps += 1\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        train_loss = total_loss / max(n_batches, 1)\n",
    "\n",
    "        # validate (use NON-smoothed y)\n",
    "        val_loss, probs, y_val, f1_05, val_auc = eval_model(model, dl_va, criterion)\n",
    "\n",
    "        improved = (val_auc > best_val_auc + 1e-6) or (math.isnan(best_val_auc) and not math.isnan(val_auc))\n",
    "        if (not improved) and (abs(val_auc - best_val_auc) <= 1e-6) and (val_loss < best_val_loss - 1e-6):\n",
    "            improved = True\n",
    "\n",
    "        if improved:\n",
    "            best_val_auc = float(val_auc)\n",
    "            best_val_loss = float(val_loss)\n",
    "            best_epoch = int(epoch)\n",
    "            best_probs = probs.copy()\n",
    "\n",
    "            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"fold\": fold,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"cfg\": CFG,\n",
    "                    \"seq_feature_names\": SEQ_FEATURE_NAMES,\n",
    "                    \"max_len\": L,\n",
    "                    \"token_mode\": SEQ_TOKEN_MODE,\n",
    "                    \"val_feat\": VAL_FEAT,\n",
    "                    \"global_meta_cols\": BASE_G_COLS,\n",
    "                    \"use_agg_seq_features\": bool(USE_AGG_SEQ_FEATURES),\n",
    "                    \"global_scaler\": {\"mean\": g_mean, \"std\": g_std},\n",
    "                    \"pos_weight_train\": float(pos_weight),\n",
    "                    \"balance_mode\": balance_mode,\n",
    "                    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "                    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "            patience_left = int(CFG[\"patience\"])\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"  epoch {epoch:02d} | lr={lr_now:.2e} | opt_steps={opt_steps:4d} | \"\n",
    "              f\"train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | val_auc={val_auc:.5f} | f1@0.5={f1_05:.4f} | \"\n",
    "              f\"best_ep={best_epoch} | pat={patience_left}\")\n",
    "\n",
    "        if patience_left <= 0:\n",
    "            break\n",
    "\n",
    "    if best_probs is None:\n",
    "        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n",
    "\n",
    "    # fill OOF\n",
    "    oof_prob[val_idx] = best_probs.astype(np.float32)\n",
    "\n",
    "    pred01 = (best_probs >= 0.5).astype(np.int8)\n",
    "    best_f1_05 = f1_binary(y[val_idx], pred01)\n",
    "\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold,\n",
    "        \"val_size\": int(len(val_idx)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_auc\": float(best_val_auc),\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"f1_at_0p5\": float(best_f1_05),\n",
    "        \"pos_weight_train\": float(pos_weight),\n",
    "        \"g_dim\": int(g_dim),\n",
    "        \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n",
    "        \"balance_mode\": balance_mode,\n",
    "        \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "    })\n",
    "\n",
    "    del model, opt, ds_tr, ds_va, dl_tr, dl_va, G_fold_z\n",
    "    gc.collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Save OOF artifacts + summary\n",
    "# ----------------------------\n",
    "oof_path_npy = OOF_DIR / \"oof_prob.npy\"\n",
    "np.save(oof_path_npy, oof_prob)\n",
    "\n",
    "df_oof = pd.DataFrame({\"object_id\": train_ids, \"target\": y.astype(int), \"oof_prob\": oof_prob.astype(np.float32)})\n",
    "oof_path_csv = OOF_DIR / \"oof_prob.csv\"\n",
    "df_oof.to_csv(oof_path_csv, index=False)\n",
    "\n",
    "metrics_path = OOF_DIR / \"fold_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed)}, f, indent=2)\n",
    "\n",
    "oof_pred01 = (oof_prob >= 0.5).astype(np.int8)\n",
    "oof_f1_05 = f1_binary(y, oof_pred01)\n",
    "oof_auc = float(roc_auc_score(y, oof_prob)) if (len(np.unique(y)) == 2) else float(\"nan\")\n",
    "\n",
    "print(\"\\n[Stage 8] CV TRAIN DONE\")\n",
    "print(f\"- elapsed: {elapsed/60:.2f} min\")\n",
    "print(f\"- OOF saved: {oof_path_npy}\")\n",
    "print(f\"- OOF saved: {oof_path_csv}\")\n",
    "print(f\"- fold metrics: {metrics_path}\")\n",
    "print(f\"- OOF AUC (rough): {oof_auc:.5f}\")\n",
    "print(f\"- OOF F1@0.5 (rough): {oof_f1_05:.4f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"oof_prob\": oof_prob,\n",
    "    \"OOF_PROB_PATH\": oof_path_npy,\n",
    "    \"OOF_CSV_PATH\": oof_path_csv,\n",
    "    \"FOLD_METRICS_PATH\": metrics_path,\n",
    "    \"TRAIN_CFG_PATH\": cfg_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44560a29",
   "metadata": {
    "papermill": {
     "duration": 0.017571,
     "end_time": "2026-01-03T17:21:10.964179",
     "exception": false,
     "start_time": "2026-01-03T17:21:10.946608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OOF Prediction + Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f8f63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:21:11.000881Z",
     "iopub.status.busy": "2026-01-03T17:21:11.000145Z",
     "iopub.status.idle": "2026-01-03T17:21:11.307116Z",
     "shell.execute_reply": "2026-01-03T17:21:11.305957Z"
    },
    "papermill": {
     "duration": 0.328209,
     "end_time": "2026-01-03T17:21:11.309566",
     "exception": false,
     "start_time": "2026-01-03T17:21:10.981357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9] Loaded OOF from: csv(oof_prob.csv)\n",
      "[Stage 9] N=3,043 | pos=148 | neg=2,895 | pos%=4.863621% | target_col=target\n",
      "[Stage 9] DONE\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/threshold_tuning.json\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/threshold_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/threshold_table_top500.csv\n",
      "- BEST_THR_F1  =0.697760 | F1=0.362140 (P=0.260355 R=0.594595)\n",
      "- BEST_THR_ACC =0.954360 | ACC=0.953336 BACC=0.536299 F1=0.134146\n",
      "- BEST_THR_BACC=0.520321 | BACC=0.796016 ACC=0.819257 F1=0.293059\n",
      "- BEST_THR_MCC =0.696405 | MCC=0.348674 F1=0.361789 BACC=0.756634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/2323702939.py:173: RuntimeWarning: invalid value encountered in divide\n",
      "  mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v3.1 (ALIGN SUPER ROBUST + MULTI-METRIC + FAST SWEEP)\n",
    "#\n",
    "# Upgrade v3.1 vs v3:\n",
    "# - String-map df_train_meta.index -> row positions (anti dtype mismatch)\n",
    "# - Prefer oof_prob.csv (object_id + oof_prob) for safest alignment\n",
    "# - Clean oof_prob NaN/inf + clip [0,1]\n",
    "# - FAST threshold sweep via sorting + cumulative counts (vectorized)\n",
    "# - Best thresholds for: F1, Accuracy, Balanced Accuracy, MCC (+ Precision/Recall)\n",
    "# - Exports multiple BEST thresholds\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"OOF_DIR\", \"df_train_meta\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n",
    "\n",
    "OOF_DIR = Path(OOF_DIR)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: robust stringify id\n",
    "# ----------------------------\n",
    "def _to_str_list(ids):\n",
    "    out = []\n",
    "    for x in ids:\n",
    "        if isinstance(x, (bytes, np.bytes_)):\n",
    "            out.append(x.decode(\"utf-8\", errors=\"ignore\").strip())\n",
    "        else:\n",
    "            out.append(str(x).strip())\n",
    "    return out\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Detect target column in df_train_meta\n",
    "# ----------------------------\n",
    "def _detect_target_col(df):\n",
    "    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "TARGET_COL = _detect_target_col(df_train_meta)\n",
    "if TARGET_COL is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot detect target column in df_train_meta. \"\n",
    "        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Load OOF (prefer CSV)\n",
    "# ----------------------------\n",
    "def _load_oof():\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n",
    "            ids = df[\"object_id\"].astype(str).str.strip().tolist()\n",
    "            prob = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n",
    "            return ids, prob, \"csv(oof_prob.csv)\"\n",
    "\n",
    "    if \"oof_prob\" in globals():\n",
    "        prob = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        if isinstance(prob, np.ndarray) and prob.ndim != 0:\n",
    "            if \"train_ids_ordered\" in globals():\n",
    "                ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n",
    "                return ids, prob, \"globals(oof_prob + train_ids_ordered)\"\n",
    "            if len(prob) == len(df_train_meta):\n",
    "                ids = _to_str_list(df_train_meta.index.tolist())\n",
    "                return ids, prob, \"globals(oof_prob + df_train_meta.index)\"\n",
    "\n",
    "    pnpy = OOF_DIR / \"oof_prob.npy\"\n",
    "    if pnpy.exists():\n",
    "        prob = _as_1d_float32(np.load(pnpy, allow_pickle=False))\n",
    "        if \"train_ids_ordered\" in globals():\n",
    "            ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n",
    "            return ids, prob, \"npy(oof_prob.npy + train_ids_ordered)\"\n",
    "        if len(prob) == len(df_train_meta):\n",
    "            ids = _to_str_list(df_train_meta.index.tolist())\n",
    "            return ids, prob, \"npy(oof_prob.npy + df_train_meta.index)\"\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob not found (csv/globals/npy). Jalankan STAGE 8 dulu.\")\n",
    "\n",
    "train_ids, oof_prob, src = _load_oof()\n",
    "\n",
    "if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "    raise TypeError(f\"Invalid oof_prob. Type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}\")\n",
    "\n",
    "# sanitize prob\n",
    "oof_prob = np.nan_to_num(oof_prob, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32)\n",
    "oof_prob = np.clip(oof_prob, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# SUPER ROBUST alignment: string-map df_train_meta.index -> positions\n",
    "# ----------------------------\n",
    "meta_ids = _to_str_list(df_train_meta.index.tolist())\n",
    "pos_map = pd.Series(np.arange(len(meta_ids), dtype=np.int32), index=pd.Index(meta_ids, dtype=\"object\"))\n",
    "\n",
    "pos_idx = pos_map.reindex(train_ids).to_numpy()\n",
    "if np.isnan(pos_idx).any():\n",
    "    bad = [train_ids[i] for i in np.where(np.isnan(pos_idx))[0][:10]]\n",
    "    raise KeyError(f\"OOF ids not found in df_train_meta.index (string-mapped). ex={bad} | missing_n={int(np.isnan(pos_idx).sum())}\")\n",
    "\n",
    "pos_idx = pos_idx.astype(np.int32)\n",
    "\n",
    "# load y aligned\n",
    "y_raw = pd.to_numeric(df_train_meta[TARGET_COL], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy()\n",
    "y = y_raw[pos_idx]\n",
    "y = (y > 0).astype(np.int8)\n",
    "\n",
    "if len(oof_prob) != len(y):\n",
    "    raise RuntimeError(f\"Length mismatch: oof_prob={len(oof_prob)} vs y={len(y)}\")\n",
    "\n",
    "uy = set(np.unique(y).tolist())\n",
    "if not uy.issubset({0, 1}):\n",
    "    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n",
    "\n",
    "N = int(len(y))\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "\n",
    "print(f\"[Stage 9] Loaded OOF from: {src}\")\n",
    "print(f\"[Stage 9] N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}% | target_col={TARGET_COL}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Metrics helpers (vectorized-safe)\n",
    "# ----------------------------\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1e-12)\n",
    "\n",
    "def _metrics_from_counts(tp, fp, fn, tn):\n",
    "    tp = tp.astype(np.float64)\n",
    "    fp = fp.astype(np.float64)\n",
    "    fn = fn.astype(np.float64)\n",
    "    tn = tn.astype(np.float64)\n",
    "\n",
    "    prec = _safe_div(tp, tp + fp)\n",
    "    rec  = _safe_div(tp, tp + fn)\n",
    "    f1   = _safe_div(2 * prec * rec, prec + rec)\n",
    "\n",
    "    acc  = _safe_div(tp + tn, tp + fp + fn + tn)\n",
    "\n",
    "    tpr  = _safe_div(tp, tp + fn)\n",
    "    tnr  = _safe_div(tn, tn + fp)\n",
    "    bacc = 0.5 * (tpr + tnr)\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n",
    "\n",
    "    return f1, prec, rec, acc, bacc, mcc\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold candidates (grid + quantiles + unique-prob sampling)\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.00, 0.10, 41),\n",
    "    np.linspace(0.10, 0.90, 161),\n",
    "    np.linspace(0.90, 1.00, 41),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\n",
    "quant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n",
    "\n",
    "uniq = np.unique(oof_prob)\n",
    "if len(uniq) > 6000:\n",
    "    take = np.linspace(0, len(uniq) - 1, 6000, dtype=int)\n",
    "    uniq = uniq[take].astype(np.float32)\n",
    "\n",
    "thr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr, uniq]), 0.0, 1.0)).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) FAST sweep via sorting + cumulative counts\n",
    "#    Predict positive if oof_prob >= thr\n",
    "# ----------------------------\n",
    "# sort probabilities descending\n",
    "ord_desc = np.argsort(-oof_prob)\n",
    "p_sorted = oof_prob[ord_desc]\n",
    "y_sorted = y[ord_desc].astype(np.int8)\n",
    "\n",
    "# cumulative pos/neg for prefix k (k predicted positive)\n",
    "pos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\n",
    "neg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\n",
    "\n",
    "pos_total = int(pos_prefix[-1]) if N > 0 else 0\n",
    "neg_total = int(neg_prefix[-1]) if N > 0 else 0\n",
    "\n",
    "# for each thr, k = number of items with prob >= thr\n",
    "# since p_sorted is descending, find leftmost index where p_sorted < thr\n",
    "# k = searchsorted(-p_sorted, -thr, side=\"left\")\n",
    "k = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"left\").astype(np.int64)\n",
    "\n",
    "tp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\n",
    "fp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\n",
    "fn = (pos_total - tp).astype(np.int64)\n",
    "tn = (neg_total - fp).astype(np.int64)\n",
    "\n",
    "f1, prec, rec, acc, bacc, mmc = _metrics_from_counts(tp, fp, fn, tn)\n",
    "pos_pred = k.astype(np.int64)\n",
    "\n",
    "thr_table = pd.DataFrame({\n",
    "    \"thr\": thr_candidates.astype(np.float32),\n",
    "    \"f1\": f1.astype(np.float32),\n",
    "    \"precision\": prec.astype(np.float32),\n",
    "    \"recall\": rec.astype(np.float32),\n",
    "    \"accuracy\": acc.astype(np.float32),\n",
    "    \"balanced_accuracy\": bacc.astype(np.float32),\n",
    "    \"mcc\": mmc.astype(np.float32),\n",
    "    \"tp\": tp.astype(np.int64),\n",
    "    \"fp\": fp.astype(np.int64),\n",
    "    \"fn\": fn.astype(np.int64),\n",
    "    \"tn\": tn.astype(np.int64),\n",
    "    \"pos_pred\": pos_pred.astype(np.int64),\n",
    "})\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Pick best thresholds with tie-breakers\n",
    "# ----------------------------\n",
    "def _pick_best(df, primary, tie_cols):\n",
    "    sort_cols = [primary] + tie_cols\n",
    "    asc = [False] * len(sort_cols)\n",
    "    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n",
    "\n",
    "best_f1_row  = _pick_best(thr_table, \"f1\", [\"recall\", \"precision\", \"balanced_accuracy\"])\n",
    "best_acc_row = _pick_best(thr_table, \"accuracy\", [\"balanced_accuracy\", \"f1\"])\n",
    "best_bac_row = _pick_best(thr_table, \"balanced_accuracy\", [\"accuracy\", \"f1\"])\n",
    "best_mcc_row = _pick_best(thr_table, \"mcc\", [\"f1\", \"balanced_accuracy\"])\n",
    "\n",
    "def _row_to_dict(r):\n",
    "    return {\n",
    "        \"thr\": float(r[\"thr\"]),\n",
    "        \"f1\": float(r[\"f1\"]),\n",
    "        \"precision\": float(r[\"precision\"]),\n",
    "        \"recall\": float(r[\"recall\"]),\n",
    "        \"accuracy\": float(r[\"accuracy\"]),\n",
    "        \"balanced_accuracy\": float(r[\"balanced_accuracy\"]),\n",
    "        \"mcc\": float(r[\"mcc\"]),\n",
    "        \"tp\": int(r[\"tp\"]), \"fp\": int(r[\"fp\"]), \"fn\": int(r[\"fn\"]), \"tn\": int(r[\"tn\"]),\n",
    "        \"pos_pred\": int(r[\"pos_pred\"]),\n",
    "    }\n",
    "\n",
    "def _eval_at(thr):\n",
    "    thr = float(thr)\n",
    "    k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"left\"))\n",
    "    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n",
    "    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n",
    "    fn0 = int(pos_total - tp0)\n",
    "    tn0 = int(neg_total - fp0)\n",
    "\n",
    "    prec0 = tp0 / max(tp0 + fp0, 1)\n",
    "    rec0  = tp0 / max(tp0 + fn0, 1)\n",
    "    f10 = 0.0 if (tp0 == 0 or (prec0 + rec0) == 0) else (2 * prec0 * rec0 / (prec0 + rec0))\n",
    "    acc0 = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n",
    "    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n",
    "    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n",
    "    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n",
    "\n",
    "    return {\n",
    "        \"thr\": thr,\n",
    "        \"f1\": float(f10),\n",
    "        \"precision\": float(prec0),\n",
    "        \"recall\": float(rec0),\n",
    "        \"accuracy\": float(acc0),\n",
    "        \"balanced_accuracy\": float(bacc0),\n",
    "        \"mcc\": float(mcc0),\n",
    "        \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0,\n",
    "        \"pos_pred\": int(k0),\n",
    "    }\n",
    "\n",
    "base05 = _eval_at(0.5)\n",
    "\n",
    "BEST_THR_F1   = float(best_f1_row[\"thr\"])\n",
    "BEST_THR_ACC  = float(best_acc_row[\"thr\"])\n",
    "BEST_THR_BACC = float(best_bac_row[\"thr\"])\n",
    "BEST_THR_MCC  = float(best_mcc_row[\"thr\"])\n",
    "\n",
    "best_f1_full  = _eval_at(BEST_THR_F1)\n",
    "best_acc_full = _eval_at(BEST_THR_ACC)\n",
    "best_bac_full = _eval_at(BEST_THR_BACC)\n",
    "best_mcc_full = _eval_at(BEST_THR_MCC)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Save artifacts\n",
    "# ----------------------------\n",
    "out_json = OOF_DIR / \"threshold_tuning.json\"\n",
    "out_txt  = OOF_DIR / \"threshold_report.txt\"\n",
    "out_csv  = OOF_DIR / \"threshold_table_top500.csv\"\n",
    "\n",
    "payload = {\n",
    "    \"source\": src,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"n_objects\": int(N),\n",
    "    \"pos\": int(pos),\n",
    "    \"neg\": int(neg),\n",
    "    \"baseline_thr_0p5\": base05,\n",
    "    \"best_thr_f1\": best_f1_full,\n",
    "    \"best_thr_accuracy\": best_acc_full,\n",
    "    \"best_thr_balanced_accuracy\": best_bac_full,\n",
    "    \"best_thr_mcc\": best_mcc_full,\n",
    "}\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).head(500).to_csv(out_csv, index=False)\n",
    "\n",
    "top_f1 = thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).head(10).reset_index(drop=True)\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Threshold Tuning Report (v3.1)\")\n",
    "lines.append(f\"- source={src}\")\n",
    "lines.append(f\"- target_col={TARGET_COL}\")\n",
    "lines.append(f\"- N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"- F1={base05['f1']:.6f} | P={base05['precision']:.6f} | R={base05['recall']:.6f} | \"\n",
    "             f\"ACC={base05['accuracy']:.6f} | BACC={base05['balanced_accuracy']:.6f} | MCC={base05['mcc']:.6f}\")\n",
    "lines.append(f\"- tp={base05['tp']} fp={base05['fp']} fn={base05['fn']} tn={base05['tn']} | pos_pred={base05['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F1   @ thr={best_f1_full['thr']:.6f} | F1={best_f1_full['f1']:.6f} | P={best_f1_full['precision']:.6f} | R={best_f1_full['recall']:.6f} | pos_pred={best_f1_full['pos_pred']}\")\n",
    "lines.append(f\"BEST-ACC  @ thr={best_acc_full['thr']:.6f} | ACC={best_acc_full['accuracy']:.6f} | BACC={best_acc_full['balanced_accuracy']:.6f} | F1={best_acc_full['f1']:.6f}\")\n",
    "lines.append(f\"BEST-BACC @ thr={best_bac_full['thr']:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f} | ACC={best_bac_full['accuracy']:.6f} | F1={best_bac_full['f1']:.6f}\")\n",
    "lines.append(f\"BEST-MCC  @ thr={best_mcc_full['thr']:.6f} | MCC={best_mcc_full['mcc']:.6f} | F1={best_mcc_full['f1']:.6f} | BACC={best_mcc_full['balanced_accuracy']:.6f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 by F1:\")\n",
    "for i in range(len(top_f1)):\n",
    "    r = top_f1.iloc[i]\n",
    "    lines.append(f\"{i+1:02d}. thr={float(r['thr']):.6f} | f1={float(r['f1']):.6f} | P={float(r['precision']):.6f} | \"\n",
    "                 f\"R={float(r['recall']):.6f} | ACC={float(r['accuracy']):.6f} | BACC={float(r['balanced_accuracy']):.6f} | \"\n",
    "                 f\"MCC={float(r['mcc']):.6f} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print(\"[Stage 9] DONE\")\n",
    "print(f\"- Saved: {out_json}\")\n",
    "print(f\"- Saved: {out_txt}\")\n",
    "print(f\"- Saved: {out_csv}\")\n",
    "print(f\"- BEST_THR_F1  ={BEST_THR_F1:.6f} | F1={best_f1_full['f1']:.6f} (P={best_f1_full['precision']:.6f} R={best_f1_full['recall']:.6f})\")\n",
    "print(f\"- BEST_THR_ACC ={BEST_THR_ACC:.6f} | ACC={best_acc_full['accuracy']:.6f} BACC={best_acc_full['balanced_accuracy']:.6f} F1={best_acc_full['f1']:.6f}\")\n",
    "print(f\"- BEST_THR_BACC={BEST_THR_BACC:.6f} | BACC={best_bac_full['balanced_accuracy']:.6f} ACC={best_bac_full['accuracy']:.6f} F1={best_bac_full['f1']:.6f}\")\n",
    "print(f\"- BEST_THR_MCC ={BEST_THR_MCC:.6f} | MCC={best_mcc_full['mcc']:.6f} F1={best_mcc_full['f1']:.6f} BACC={best_mcc_full['balanced_accuracy']:.6f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"train_ids_oof\": train_ids,\n",
    "    \"oof_prob\": oof_prob,\n",
    "    \"BEST_THR\": BEST_THR_F1,          # default tetap F1\n",
    "    \"BEST_THR_F1\": BEST_THR_F1,\n",
    "    \"BEST_THR_ACC\": BEST_THR_ACC,\n",
    "    \"BEST_THR_BACC\": BEST_THR_BACC,\n",
    "    \"BEST_THR_MCC\": BEST_THR_MCC,\n",
    "    \"thr_table\": thr_table,\n",
    "    \"THR_JSON_PATH\": out_json,\n",
    "    \"THR_REPORT_PATH\": out_txt,\n",
    "    \"THR_TABLE_CSV_PATH\": out_csv,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ec086",
   "metadata": {
    "papermill": {
     "duration": 0.017418,
     "end_time": "2026-01-03T17:21:11.344913",
     "exception": false,
     "start_time": "2026-01-03T17:21:11.327495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Inference (Fold Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b20aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:21:11.383175Z",
     "iopub.status.busy": "2026-01-03T17:21:11.382274Z",
     "iopub.status.idle": "2026-01-03T17:26:56.764993Z",
     "shell.execute_reply": "2026-01-03T17:26:56.764008Z"
    },
    "papermill": {
     "duration": 345.404547,
     "end_time": "2026-01-03T17:26:56.767189",
     "exception": false,
     "start_time": "2026-01-03T17:21:11.362642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 10] Build TEST global features: token_mode=mag | val_feat=mag\n",
      "[Stage 10] Building AGG seq features for TEST (one-time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/937044818.py:438: RuntimeWarning: Mean of empty slice\n",
      "  mean_val_b = np.nanmean(val_b, axis=1).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 10] AGG built: shape=(7135, 31) | time=1.0s\n",
      "[Stage 10] Test inference: N_test=7,135 | folds=5 | batch=64 | ensemble=mean_logits\n",
      "  fold 0: d_model=160 n_heads=4 g_dim=38 | logit_mean=-3.344331 | prob_mean=0.222912 | prob_std=0.265004\n",
      "  fold 1: d_model=160 n_heads=4 g_dim=38 | logit_mean=-2.963255 | prob_mean=0.244801 | prob_std=0.285930\n",
      "  fold 2: d_model=160 n_heads=4 g_dim=38 | logit_mean=-2.969931 | prob_mean=0.258786 | prob_std=0.281893\n",
      "  fold 3: d_model=160 n_heads=4 g_dim=38 | logit_mean=-3.050544 | prob_mean=0.240600 | prob_std=0.264052\n",
      "  fold 4: d_model=160 n_heads=4 g_dim=38 | logit_mean=-2.945759 | prob_mean=0.245993 | prob_std=0.272362\n",
      "\n",
      "[Stage 10] DONE\n",
      "- Saved logits folds: /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/preds/test_logit_folds.npy\n",
      "- Saved logits ens  : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/preds/test_logit_ens.npy\n",
      "- Saved probs folds : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/preds/test_prob_folds.npy\n",
      "- Saved probs ens   : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/preds/test_prob_ens.npy\n",
      "- Saved csv         : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/preds/test_prob_ens.csv\n",
      "- Saved config      : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/artifacts/preds/test_infer_config.json\n",
      "- ens prob mean=0.239069 | std=0.269149 | min=0.000000 | max=0.993899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL, Kaggle CPU-SAFE)\n",
    "# REVISI FULL v3.2 (MATCH STAGE8 FORWARD + BUILD TEST AGG FEATS + LOGIT ENSEMBLE + ID ALIGN HARD)\n",
    "#\n",
    "# Fix utama vs v3.1 kamu:\n",
    "# - Forward model sama dengan STAGE 8 (x_proj includes GELU, pooling mix attn+mean)\n",
    "# - Build G_test raw = meta_cols + agg_seq_feats (dari memmap) persis STAGE 8\n",
    "# - Align df_test_meta.index pakai string-map (anti dtype mismatch)\n",
    "# - Apply fold scaler dari ckpt (NO leakage, consistent)\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, re, math, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True.*\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"ART_DIR\",\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_test_meta\",\"CKPT_DIR\",\"n_splits\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n",
    "\n",
    "# Torch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Thread guard (CPU)\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FIX_DIR = Path(FIX_DIR)\n",
    "ART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR = Path(CKPT_DIR)\n",
    "\n",
    "OUT_DIR = ART_DIR / \"preds\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# helper: normalize id robustly\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _load_ids_npy(path: Path):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    xs = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n",
    "    return [_norm_id(z) for z in xs]\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load TEST ordering (must match STAGE 6)\n",
    "# ----------------------------\n",
    "test_ids_path = FIX_DIR / \"test_ids.npy\"\n",
    "if not test_ids_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "test_ids = _load_ids_npy(test_ids_path)\n",
    "NTE = len(test_ids)\n",
    "if NTE <= 0:\n",
    "    raise RuntimeError(\"test_ids kosong (NTE=0). Pastikan STAGE 6 sukses membuat test_ids.npy.\")\n",
    "\n",
    "# Align df_test_meta.index via string-map (HARD)\n",
    "df_test_meta = df_test_meta.copy(deep=False)\n",
    "meta_ids = [_norm_id(z) for z in df_test_meta.index.tolist()]\n",
    "df_test_meta.index = pd.Index(meta_ids, name=df_test_meta.index.name)\n",
    "\n",
    "pos_map = pd.Series(np.arange(len(meta_ids), dtype=np.int32), index=pd.Index(meta_ids, dtype=\"object\"))\n",
    "pos_idx = pos_map.reindex(test_ids).to_numpy()\n",
    "if np.isnan(pos_idx).any():\n",
    "    bad = [test_ids[i] for i in np.where(np.isnan(pos_idx))[0][:10]]\n",
    "    raise KeyError(f\"Some test_ids not found in df_test_meta.index (string-mapped). ex={bad} | missing_n={int(np.isnan(pos_idx).sum())}\")\n",
    "pos_idx = pos_idx.astype(np.int32)\n",
    "\n",
    "if len(set(test_ids)) != len(test_ids):\n",
    "    s = pd.Series(test_ids)\n",
    "    dup = s[s.duplicated()].head(10).tolist()\n",
    "    raise ValueError(f\"Duplicate object_id in test_ids ordering (examples): {dup}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open fixed-length TEST memmaps\n",
    "# ----------------------------\n",
    "SEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "L = int(MAX_LEN)\n",
    "\n",
    "test_X_path = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path = FIX_DIR / \"test_M.dat\"\n",
    "for p in [test_X_path, test_B_path, test_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=np.float32, mode=\"r\", shape=(NTE, L, Fdim))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "\n",
    "feat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "# detect token mode (same as STAGE 8)\n",
    "if (\"mag\" in feat) and (\"mag_err_log\" in feat):\n",
    "    SEQ_TOKEN_MODE = \"mag\"\n",
    "    VAL_FEAT = \"mag\"\n",
    "elif (\"flux_asinh\" in feat) and (\"err_log1p\" in feat):\n",
    "    SEQ_TOKEN_MODE = \"asinh\"\n",
    "    VAL_FEAT = \"flux_asinh\"\n",
    "else:\n",
    "    raise RuntimeError(f\"Cannot infer token_mode from SEQ_FEATURE_NAMES. Found={SEQ_FEATURE_NAMES}\")\n",
    "\n",
    "for k in [\"snr_tanh\",\"detected\"]:\n",
    "    if k not in feat:\n",
    "        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n",
    "if VAL_FEAT not in feat:\n",
    "    raise RuntimeError(f\"Feature '{VAL_FEAT}' missing for token_mode={SEQ_TOKEN_MODE}.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Checkpoints (fold_*.pt)\n",
    "# ----------------------------\n",
    "ckpts = []\n",
    "for f in range(int(n_splits)):\n",
    "    p = CKPT_DIR / f\"fold_{f}.pt\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n",
    "    ckpts.append(p)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Safe/compat checkpoint loader\n",
    "# ----------------------------\n",
    "def torch_load_compat(path: Path):\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "        if isinstance(obj, dict) and (\"model_state\" in obj or \"cfg\" in obj or \"global_scaler\" in obj):\n",
    "            return obj\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        return torch.load(path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "def extract_state_and_meta(ckpt_obj):\n",
    "    if isinstance(ckpt_obj, dict) and \"model_state\" in ckpt_obj and isinstance(ckpt_obj[\"model_state\"], dict):\n",
    "        return ckpt_obj[\"model_state\"], ckpt_obj\n",
    "    if isinstance(ckpt_obj, dict):\n",
    "        any_tensor = any(hasattr(v, \"shape\") for v in ckpt_obj.values())\n",
    "        if any_tensor:\n",
    "            return ckpt_obj, {}\n",
    "        return ckpt_obj, ckpt_obj\n",
    "    raise RuntimeError(f\"Unsupported ckpt object type: {type(ckpt_obj)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Infer architecture from state_dict\n",
    "# ----------------------------\n",
    "def infer_from_state(sd: dict):\n",
    "    keys = set(sd.keys())\n",
    "\n",
    "    if \"band_emb.weight\" not in sd:\n",
    "        raise RuntimeError(\"state_dict missing band_emb.weight.\")\n",
    "    n_bands = int(sd[\"band_emb.weight\"].shape[0])\n",
    "    d_model = int(sd[\"band_emb.weight\"].shape[1])\n",
    "\n",
    "    if \"pos_emb\" not in sd:\n",
    "        raise RuntimeError(\"state_dict missing pos_emb.\")\n",
    "    max_len_ckpt = int(sd[\"pos_emb\"].shape[1])\n",
    "\n",
    "    xproj_is_seq = (\"x_proj.0.weight\" in keys)\n",
    "    if xproj_is_seq:\n",
    "        feat_dim = int(sd[\"x_proj.0.weight\"].shape[1])\n",
    "    else:\n",
    "        if \"x_proj.weight\" not in sd:\n",
    "            raise RuntimeError(\"state_dict missing x_proj.weight or x_proj.0.weight.\")\n",
    "        feat_dim = int(sd[\"x_proj.weight\"].shape[1])\n",
    "\n",
    "    # g_proj\n",
    "    if \"g_proj.0.weight\" in sd:\n",
    "        g_dim = int(sd[\"g_proj.0.weight\"].shape[1])\n",
    "        g_hidden = int(sd[\"g_proj.0.weight\"].shape[0])\n",
    "    else:\n",
    "        g_dim = 0\n",
    "        g_hidden = max(32, d_model // 2)\n",
    "\n",
    "    # encoder layers\n",
    "    layer_ids = set()\n",
    "    for k in keys:\n",
    "        m = re.match(r\"encoder\\.layers\\.(\\d+)\\.\", k)\n",
    "        if m:\n",
    "            layer_ids.add(int(m.group(1)))\n",
    "    n_layers = (max(layer_ids) + 1) if layer_ids else 0\n",
    "    if n_layers <= 0:\n",
    "        raise RuntimeError(\"Cannot infer n_layers from state_dict (encoder.layers.* not found).\")\n",
    "\n",
    "    # dim_ff from linear1\n",
    "    k_lin1 = \"encoder.layers.0.linear1.weight\"\n",
    "    if k_lin1 in sd:\n",
    "        dim_ff = int(sd[k_lin1].shape[0])\n",
    "    else:\n",
    "        lin1_keys = [k for k in keys if k.endswith(\"linear1.weight\")]\n",
    "        if not lin1_keys:\n",
    "            raise RuntimeError(\"Cannot infer dim_feedforward (linear1.weight not found).\")\n",
    "        dim_ff = int(sd[sorted(lin1_keys)[0]].shape[0])\n",
    "\n",
    "    has_pool_ln = (\"pool_ln.weight\" in keys and \"pool_ln.bias\" in keys)\n",
    "\n",
    "    # head final idx\n",
    "    head_w_idx = []\n",
    "    for k in keys:\n",
    "        m = re.match(r\"head\\.(\\d+)\\.weight\", k)\n",
    "        if m:\n",
    "            head_w_idx.append(int(m.group(1)))\n",
    "    if not head_w_idx:\n",
    "        raise RuntimeError(\"Cannot infer head structure (head.*.weight not found).\")\n",
    "    head_final_idx = max(sorted(set(head_w_idx)))\n",
    "\n",
    "    return {\n",
    "        \"n_bands\": n_bands,\n",
    "        \"d_model\": d_model,\n",
    "        \"max_len_ckpt\": max_len_ckpt,\n",
    "        \"feat_dim\": feat_dim,\n",
    "        \"g_dim\": g_dim,\n",
    "        \"g_hidden\": g_hidden,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"dim_ff\": dim_ff,\n",
    "        \"has_pool_ln\": has_pool_ln,\n",
    "        \"xproj_is_seq\": xproj_is_seq,\n",
    "        \"head_final_idx\": head_final_idx,\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Build model that matches STAGE 8 forward\n",
    "# ----------------------------\n",
    "class FlexMultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, max_len, n_bands, d_model, n_heads, n_layers, dim_ff, dropout,\n",
    "                 g_dim, g_hidden, xproj_is_seq=True, has_pool_ln=True, head_final_idx=3):\n",
    "        super().__init__()\n",
    "        self.n_bands = int(n_bands)\n",
    "        self.max_len = int(max_len)\n",
    "        self.d_model = int(d_model)\n",
    "\n",
    "        # IMPORTANT: match STAGE 8 (Linear + GELU + Dropout)\n",
    "        if xproj_is_seq:\n",
    "            self.x_proj = nn.Sequential(\n",
    "                nn.Linear(int(feat_dim), int(d_model)),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(float(dropout)),\n",
    "            )\n",
    "        else:\n",
    "            # still include GELU+Dropout for forward match\n",
    "            self.x_proj = nn.Sequential(\n",
    "                nn.Linear(int(feat_dim), int(d_model)),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(float(dropout)),\n",
    "            )\n",
    "\n",
    "        self.band_emb = nn.Embedding(int(n_bands), int(d_model))\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, int(max_len), int(d_model)))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=int(d_model),\n",
    "            nhead=int(n_heads),\n",
    "            dim_feedforward=int(dim_ff),\n",
    "            dropout=float(dropout),\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=int(n_layers))\n",
    "\n",
    "        self.attn = nn.Linear(int(d_model), 1)\n",
    "\n",
    "        self.has_pool_ln = bool(has_pool_ln)\n",
    "        if self.has_pool_ln:\n",
    "            self.pool_ln = nn.LayerNorm(int(d_model))\n",
    "\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(int(g_dim), int(g_hidden)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "        )\n",
    "\n",
    "        in_head = int(d_model + g_hidden)\n",
    "        if head_final_idx == 3:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, int(d_model)),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(float(dropout)),\n",
    "                nn.Linear(int(d_model), 1),\n",
    "            )\n",
    "        elif head_final_idx == 2:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, int(d_model)),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(int(d_model), 1),\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, int(d_model)),\n",
    "                nn.Linear(int(d_model), 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        pad_mask = (mask == 0)  # True=pad\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        # attn pooling\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        # mean pooling on valid tokens\n",
    "        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n",
    "        denom = valid.sum(dim=1).clamp_min(1.0)\n",
    "        pooled_mean = (h * valid).sum(dim=1) / denom\n",
    "\n",
    "        pooled = 0.6 * pooled_attn + 0.4 * pooled_mean  # match STAGE 8\n",
    "        if self.has_pool_ln:\n",
    "            pooled = self.pool_ln(pooled)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        return self.head(z).squeeze(-1)  # logit\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Build TEST global features (meta + agg seq feats) — match STAGE 8\n",
    "# ----------------------------\n",
    "BASE_G_COLS_DEFAULT = [\"Z\",\"Z_err\",\"EBV\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\n",
    "N_BANDS = 6\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1.0)\n",
    "\n",
    "def build_agg_seq_features_memmap(Xmm, Bmm, Mmm, chunk=512):\n",
    "    \"\"\"\n",
    "    Match STAGE 8 agg:\n",
    "      glob: [tok_count, det_frac, mean_abs_snr, max_abs_snr] (4)\n",
    "      val stats:\n",
    "        mag  : [mean_mag, std_mag, min_mag] (3)\n",
    "        asinh: [mean_abs_flux, std_flux, max_abs_flux] (3)\n",
    "      per-band (b=0..5): [cnt, det_frac_b, mean_abs_snr_b, mean_val_b] (4*6=24)\n",
    "    total agg_dim=31\n",
    "    \"\"\"\n",
    "    snr_i = feat[\"snr_tanh\"]\n",
    "    det_i = feat[\"detected\"]\n",
    "    val_i = feat[VAL_FEAT]\n",
    "\n",
    "    out = np.zeros((NTE, 31), dtype=np.float32)\n",
    "\n",
    "    for start in range(0, NTE, int(chunk)):\n",
    "        end = min(NTE, start + int(chunk))\n",
    "        Xc = np.asarray(Xmm[start:end])  # (B,L,F)\n",
    "        Bc = np.asarray(Bmm[start:end])  # (B,L)\n",
    "        Mc = np.asarray(Mmm[start:end])  # (B,L)\n",
    "\n",
    "        real = (Mc == 1)\n",
    "        tok_count = real.sum(axis=1).astype(np.float32)\n",
    "\n",
    "        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32)\n",
    "        det = (Xc[:, :, det_i] > 0.5).astype(np.float32)\n",
    "        val = Xc[:, :, val_i].astype(np.float32)\n",
    "\n",
    "        snr_r = snr * real\n",
    "        det_r = det * real\n",
    "\n",
    "        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n",
    "        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n",
    "        max_abs_snr = np.where(tok_count > 0, (snr * real).max(axis=1), 0.0).astype(np.float32)\n",
    "\n",
    "        if SEQ_TOKEN_MODE == \"mag\":\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            mean_val = np.nanmean(val_r, axis=1).astype(np.float32)\n",
    "            std_val  = np.nanstd(val_r, axis=1).astype(np.float32)\n",
    "            min_val  = np.nanmin(val_r, axis=1).astype(np.float32)\n",
    "            mean_val = np.nan_to_num(mean_val, nan=0.0).astype(np.float32)\n",
    "            std_val  = np.nan_to_num(std_val,  nan=0.0).astype(np.float32)\n",
    "            min_val  = np.nan_to_num(min_val,  nan=0.0).astype(np.float32)\n",
    "            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1)\n",
    "        else:\n",
    "            aval = np.abs(val)\n",
    "            aval_r = aval * real\n",
    "            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count)\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            std_val = np.nanstd(val_r, axis=1).astype(np.float32)\n",
    "            max_aval = np.where(tok_count > 0, (aval * real).max(axis=1), 0.0).astype(np.float32)\n",
    "            std_val = np.nan_to_num(std_val, nan=0.0).astype(np.float32)\n",
    "            global_val_feats = np.stack([mean_aval.astype(np.float32), std_val, max_aval], axis=1)\n",
    "\n",
    "        per_band = []\n",
    "        for b in range(N_BANDS):\n",
    "            bm = (Bc == b) & real\n",
    "            cnt = bm.sum(axis=1).astype(np.float32)\n",
    "\n",
    "            detb = (det * bm).sum(axis=1).astype(np.float32)\n",
    "            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n",
    "\n",
    "            det_frac_b = _safe_div(detb, cnt)\n",
    "            mean_abs_snr_b = _safe_div(snrb, cnt)\n",
    "\n",
    "            if SEQ_TOKEN_MODE == \"mag\":\n",
    "                val_b = np.where(bm, val, np.nan)\n",
    "                mean_val_b = np.nanmean(val_b, axis=1).astype(np.float32)\n",
    "                mean_val_b = np.nan_to_num(mean_val_b, nan=0.0).astype(np.float32)\n",
    "            else:\n",
    "                aval_b = np.abs(val) * bm\n",
    "                mean_val_b = _safe_div(aval_b.sum(axis=1).astype(np.float32), cnt)\n",
    "\n",
    "            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n",
    "\n",
    "        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n",
    "\n",
    "        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n",
    "        agg = np.concatenate([glob, global_val_feats.astype(np.float32), per_band], axis=1).astype(np.float32)\n",
    "\n",
    "        out[start:end] = agg\n",
    "\n",
    "        del Xc, Bc, Mc\n",
    "        if (start // int(chunk)) % 4 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    return out\n",
    "\n",
    "print(f\"[Stage 10] Build TEST global features: token_mode={SEQ_TOKEN_MODE} | val_feat={VAL_FEAT}\")\n",
    "\n",
    "# meta part (always same order)\n",
    "BASE_G_COLS = BASE_G_COLS_DEFAULT\n",
    "for c in BASE_G_COLS:\n",
    "    if c not in df_test_meta.columns:\n",
    "        df_test_meta[c] = 0.0\n",
    "\n",
    "G_meta = df_test_meta.iloc[pos_idx][BASE_G_COLS].copy()\n",
    "for c in BASE_G_COLS:\n",
    "    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "G_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "# agg part (same as STAGE 8) — compute ONCE\n",
    "print(\"[Stage 10] Building AGG seq features for TEST (one-time)...\")\n",
    "t0 = time.time()\n",
    "G_seq_np = build_agg_seq_features_memmap(Xte, Bte, Mte, chunk=512)\n",
    "print(f\"[Stage 10] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "# raw global\n",
    "G_raw_default = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)  # (NTE, 38)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Inference per fold (logit ensemble)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_logits_batchwise(model, Xmm, Bmm, Mmm, G_raw, mean=None, std=None, batch_size=64):\n",
    "    model.eval()\n",
    "    out = np.zeros((Xmm.shape[0],), dtype=np.float32)\n",
    "    N0 = int(Xmm.shape[0])\n",
    "    for i in range(0, N0, int(batch_size)):\n",
    "        j = min(N0, i + int(batch_size))\n",
    "        Xb = torch.from_numpy(np.asarray(Xmm[i:j]).astype(np.float32, copy=False))\n",
    "        Bb = torch.from_numpy(np.asarray(Bmm[i:j]).astype(np.int64, copy=False))\n",
    "        Mb = torch.from_numpy(np.asarray(Mmm[i:j]).astype(np.int64, copy=False))\n",
    "\n",
    "        Gb = G_raw[i:j]\n",
    "        if mean is not None and std is not None:\n",
    "            Gb = ((Gb - mean) / std).astype(np.float32, copy=False)\n",
    "        Gb = torch.from_numpy(Gb.astype(np.float32, copy=False))\n",
    "\n",
    "        logit = model(Xb.to(device), Bb.to(device), Mb.to(device), Gb.to(device))\n",
    "        out[i:j] = logit.detach().cpu().numpy().astype(np.float32, copy=False)\n",
    "    return out\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "test_logit_folds = np.zeros((NTE, int(n_splits)), dtype=np.float32)\n",
    "\n",
    "print(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} | ensemble=mean_logits\")\n",
    "\n",
    "arch_used = None\n",
    "\n",
    "for fold, ckpt_path in enumerate(ckpts):\n",
    "    ckpt_obj = torch_load_compat(ckpt_path)\n",
    "    sd, meta = extract_state_and_meta(ckpt_obj)\n",
    "\n",
    "    arch = infer_from_state(sd)\n",
    "    if arch_used is None:\n",
    "        arch_used = dict(arch)\n",
    "\n",
    "    cfg = meta.get(\"cfg\", {}) if isinstance(meta, dict) else {}\n",
    "    dropout = float(cfg.get(\"dropout\", 0.0)) if isinstance(cfg, dict) else 0.0\n",
    "\n",
    "    # n_heads: use cfg if valid else pick divisor\n",
    "    n_heads = int(cfg.get(\"n_heads\", 0)) if isinstance(cfg, dict) else 0\n",
    "    if n_heads <= 0 or (arch[\"d_model\"] % n_heads != 0):\n",
    "        for h in [8, 4, 2, 1, 16, 32]:\n",
    "            if arch[\"d_model\"] % h == 0:\n",
    "                n_heads = h\n",
    "                break\n",
    "        if n_heads <= 0:\n",
    "            n_heads = 4\n",
    "\n",
    "    # HARD schema checks\n",
    "    if arch[\"feat_dim\"] != Fdim:\n",
    "        raise RuntimeError(\n",
    "            f\"Fold {fold}: feature_dim mismatch.\\n\"\n",
    "            f\"- ckpt expects feat_dim={arch['feat_dim']}\\n\"\n",
    "            f\"- memmap has Fdim={Fdim}\\n\"\n",
    "            \"Solusi: pastikan STAGE 6 feature list sama saat training ckpt dibuat.\"\n",
    "        )\n",
    "    if arch[\"max_len_ckpt\"] != L:\n",
    "        raise RuntimeError(\n",
    "            f\"Fold {fold}: max_len mismatch.\\n\"\n",
    "            f\"- ckpt max_len={arch['max_len_ckpt']}\\n\"\n",
    "            f\"- memmap MAX_LEN={L}\\n\"\n",
    "        )\n",
    "\n",
    "    # Decide G_raw to match ckpt g_dim\n",
    "    g_dim = int(arch[\"g_dim\"])\n",
    "    if g_dim <= 0:\n",
    "        G_raw = np.zeros((NTE, 0), dtype=np.float32)\n",
    "        g_mean = None\n",
    "        g_std = None\n",
    "    else:\n",
    "        # If ckpt expects 38 dims (7 meta + 31 agg), use default.\n",
    "        # Otherwise crop/pad default to match g_dim (best-effort).\n",
    "        if G_raw_default.shape[1] == g_dim:\n",
    "            G_raw = G_raw_default\n",
    "        elif G_raw_default.shape[1] > g_dim:\n",
    "            G_raw = G_raw_default[:, :g_dim].copy()\n",
    "        else:\n",
    "            pad = np.zeros((NTE, g_dim - G_raw_default.shape[1]), dtype=np.float32)\n",
    "            G_raw = np.concatenate([G_raw_default, pad], axis=1).astype(np.float32)\n",
    "\n",
    "        scaler = meta.get(\"global_scaler\", None) if isinstance(meta, dict) else None\n",
    "        if scaler is not None and isinstance(scaler, dict) and (\"mean\" in scaler) and (\"std\" in scaler):\n",
    "            g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32).reshape(-1)\n",
    "            g_std  = np.asarray(scaler[\"std\"],  dtype=np.float32).reshape(-1)\n",
    "            if g_mean.shape[0] != g_dim or g_std.shape[0] != g_dim:\n",
    "                g_mean = None; g_std = None\n",
    "            else:\n",
    "                g_std = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n",
    "        else:\n",
    "            g_mean = None; g_std = None\n",
    "\n",
    "    model = FlexMultibandEventTransformer(\n",
    "        feat_dim=arch[\"feat_dim\"],\n",
    "        max_len=arch[\"max_len_ckpt\"],\n",
    "        n_bands=arch[\"n_bands\"],\n",
    "        d_model=arch[\"d_model\"],\n",
    "        n_heads=n_heads,\n",
    "        n_layers=arch[\"n_layers\"],\n",
    "        dim_ff=arch[\"dim_ff\"],\n",
    "        dropout=dropout,\n",
    "        g_dim=g_dim,\n",
    "        g_hidden=arch[\"g_hidden\"],\n",
    "        xproj_is_seq=True,                 # match Stage 8 forward\n",
    "        has_pool_ln=arch[\"has_pool_ln\"],\n",
    "        head_final_idx=arch[\"head_final_idx\"],\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "\n",
    "    logits = predict_logits_batchwise(\n",
    "        model, Xte, Bte, Mte, G_raw, mean=g_mean, std=g_std, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    test_logit_folds[:, fold] = logits\n",
    "\n",
    "    probs_tmp = sigmoid_np(logits)\n",
    "    print(f\"  fold {fold}: d_model={arch['d_model']} n_heads={n_heads} g_dim={g_dim} | \"\n",
    "          f\"logit_mean={float(logits.mean()):.6f} | prob_mean={float(probs_tmp.mean()):.6f} | prob_std={float(probs_tmp.std()):.6f}\")\n",
    "\n",
    "    del model, logits, probs_tmp\n",
    "    gc.collect()\n",
    "\n",
    "# ensemble on logits\n",
    "test_logit_ens = test_logit_folds.mean(axis=1).astype(np.float32)\n",
    "test_prob_folds = sigmoid_np(test_logit_folds).astype(np.float32)\n",
    "test_prob_ens   = sigmoid_np(test_logit_ens).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Save artifacts\n",
    "# ----------------------------\n",
    "logit_fold_path = OUT_DIR / \"test_logit_folds.npy\"\n",
    "logit_ens_path  = OUT_DIR / \"test_logit_ens.npy\"\n",
    "prob_fold_path  = OUT_DIR / \"test_prob_folds.npy\"\n",
    "prob_ens_path   = OUT_DIR / \"test_prob_ens.npy\"\n",
    "csv_path        = OUT_DIR / \"test_prob_ens.csv\"\n",
    "cfg_path        = OUT_DIR / \"test_infer_config.json\"\n",
    "\n",
    "np.save(logit_fold_path, test_logit_folds)\n",
    "np.save(logit_ens_path,  test_logit_ens)\n",
    "np.save(prob_fold_path,  test_prob_folds)\n",
    "np.save(prob_ens_path,   test_prob_ens)\n",
    "\n",
    "pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens}).to_csv(csv_path, index=False)\n",
    "\n",
    "infer_cfg = {\n",
    "    \"seed\": int(SEED),\n",
    "    \"n_splits\": int(n_splits),\n",
    "    \"ensemble\": \"mean_logits_then_sigmoid\",\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "    \"max_len\": int(L),\n",
    "    \"feature_dim\": int(Fdim),\n",
    "    \"token_mode\": SEQ_TOKEN_MODE,\n",
    "    \"val_feat\": VAL_FEAT,\n",
    "    \"global_meta_cols\": BASE_G_COLS_DEFAULT,\n",
    "    \"global_agg_dim\": 31,\n",
    "    \"global_default_dim\": int(G_raw_default.shape[1]),\n",
    "    \"ckpt_dir\": str(CKPT_DIR),\n",
    "    \"ckpts\": [str(p) for p in ckpts],\n",
    "    \"arch_inferred_from_first_fold\": arch_used,\n",
    "    \"outputs\": {\n",
    "        \"test_logit_folds\": str(logit_fold_path),\n",
    "        \"test_logit_ens\": str(logit_ens_path),\n",
    "        \"test_prob_folds\": str(prob_fold_path),\n",
    "        \"test_prob_ens\": str(prob_ens_path),\n",
    "        \"test_prob_ens_csv\": str(csv_path),\n",
    "    }\n",
    "}\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(infer_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 10] DONE\")\n",
    "print(f\"- Saved logits folds: {logit_fold_path}\")\n",
    "print(f\"- Saved logits ens  : {logit_ens_path}\")\n",
    "print(f\"- Saved probs folds : {prob_fold_path}\")\n",
    "print(f\"- Saved probs ens   : {prob_ens_path}\")\n",
    "print(f\"- Saved csv         : {csv_path}\")\n",
    "print(f\"- Saved config      : {cfg_path}\")\n",
    "print(f\"- ens prob mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | \"\n",
    "      f\"min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n",
    "\n",
    "# Export globals for submission stage\n",
    "globals().update({\n",
    "    \"test_ids\": test_ids,\n",
    "    \"test_logit_folds\": test_logit_folds,\n",
    "    \"test_logit_ens\": test_logit_ens,\n",
    "    \"test_prob_folds\": test_prob_folds,\n",
    "    \"test_prob_ens\": test_prob_ens,\n",
    "    \"TEST_LOGIT_FOLDS_PATH\": logit_fold_path,\n",
    "    \"TEST_LOGIT_ENS_PATH\": logit_ens_path,\n",
    "    \"TEST_PROB_FOLDS_PATH\": prob_fold_path,\n",
    "    \"TEST_PROB_ENS_PATH\": prob_ens_path,\n",
    "    \"TEST_PROB_CSV_PATH\": csv_path,\n",
    "    \"TEST_INFER_CFG_PATH\": cfg_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed20e9",
   "metadata": {
    "papermill": {
     "duration": 0.0182,
     "end_time": "2026-01-03T17:26:56.803486",
     "exception": false,
     "start_time": "2026-01-03T17:26:56.785286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "737d5715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:26:56.842267Z",
     "iopub.status.busy": "2026-01-03T17:26:56.841929Z",
     "iopub.status.idle": "2026-01-03T17:26:57.523910Z",
     "shell.execute_reply": "2026-01-03T17:26:57.522655Z"
    },
    "papermill": {
     "duration": 0.704711,
     "end_time": "2026-01-03T17:26:57.526071",
     "exception": false,
     "start_time": "2026-01-03T17:26:56.821360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] OOF source=csv | target_col=target\n",
      "[Eval] N=3,043 | pos=148 | neg=2,895 | pos%=4.863621%\n",
      "\n",
      "EVALUATION (OOF) — Precision/Recall/F1\n",
      "- ROC-AUC=0.867708 | PR-AUC=0.299815\n",
      "\n",
      "Baseline @ thr=0.5\n",
      "- F1=0.278388 | P=0.169896 | R=0.770270 | ACC=0.805784\n",
      "  tp=114 fp=557 fn=34 tn=2338 | pos_pred=671\n",
      "\n",
      "BEST-F1  @ thr=0.697926\n",
      "- F1=0.362140 | P=0.260355 | R=0.594595 | ACC=0.898127\n",
      "  tp=88 fp=250 fn=60 tn=2645 | pos_pred=338\n",
      "\n",
      "BEST-F0.5 @ thr=0.859615 (precision-leaning)\n",
      "- F0.5=0.354478 | P=0.391753 | R=0.256757 | F1=0.310204\n",
      "\n",
      "BEST-F2   @ thr=0.696422 (recall-leaning)\n",
      "- F2=0.475427 | P=0.258721 | R=0.601351 | F1=0.361789\n",
      "\n",
      "Top 10 thresholds by F1:\n",
      "01. thr=0.697926 | f1=0.362140 | f0.5=0.293333 | f2=0.473118 | P=0.260355 R=0.594595 | tp=88 fp=250 fn=60 | pos_pred=338\n",
      "02. thr=0.698250 | f1=0.362140 | f0.5=0.293333 | f2=0.473118 | P=0.260355 R=0.594595 | tp=88 fp=250 fn=60 | pos_pred=338\n",
      "03. thr=0.696422 | f1=0.361789 | f0.5=0.291995 | f2=0.475427 | P=0.258721 R=0.601351 | tp=89 fp=255 fn=59 | pos_pred=344\n",
      "04. thr=0.696473 | f1=0.361789 | f0.5=0.291995 | f2=0.475427 | P=0.258721 R=0.601351 | tp=89 fp=255 fn=59 | pos_pred=344\n",
      "05. thr=0.703237 | f1=0.361746 | f0.5=0.293919 | f2=0.470270 | P=0.261261 R=0.587838 | tp=87 fp=246 fn=61 | pos_pred=333\n",
      "06. thr=0.697760 | f1=0.361396 | f0.5=0.292553 | f2=0.472610 | P=0.259587 R=0.594595 | tp=88 fp=251 fn=60 | pos_pred=339\n",
      "07. thr=0.696405 | f1=0.361055 | f0.5=0.291230 | f2=0.474920 | P=0.257971 R=0.601351 | tp=89 fp=256 fn=59 | pos_pred=345\n",
      "08. thr=0.702411 | f1=0.360996 | f0.5=0.293127 | f2=0.469762 | P=0.260479 R=0.587838 | tp=87 fp=247 fn=61 | pos_pred=334\n",
      "09. thr=0.697415 | f1=0.360656 | f0.5=0.291777 | f2=0.472103 | P=0.258824 R=0.594595 | tp=88 fp=252 fn=60 | pos_pred=340\n",
      "10. thr=0.696347 | f1=0.360324 | f0.5=0.290470 | f2=0.474414 | P=0.257225 R=0.601351 | tp=89 fp=257 fn=59 | pos_pred=346\n",
      "\n",
      "Saved:\n",
      "- /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/eval_report.txt\n",
      "- /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/eval_threshold_table.csv\n",
      "- /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/oof/eval_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ONE CELL — EVALUATION (Precision / Recall / F1) + Threshold Sweep (OOF)\n",
    "# REVISI FULL v3 — Robust target detect + robust align + handle dup ids + better thr candidates\n",
    "#\n",
    "# Input minimal:\n",
    "# - df_train_meta (index: object_id, kolom target: target/y/label/class/is_tde/binary_target)\n",
    "# - oof_prob (globals) ATAU file OOF_DIR/oof_prob.npy ATAU OOF_DIR/oof_prob.csv\n",
    "#\n",
    "# Output:\n",
    "# - Print ringkasan metrik\n",
    "# - Save: eval_report.txt + eval_threshold_table.csv + eval_summary.json\n",
    "# - Export globals: BEST_THR_F1, BEST_THR_F05, BEST_THR_F2, thr_table_eval\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal\n",
    "# ----------------------------\n",
    "if \"df_train_meta\" not in globals():\n",
    "    raise RuntimeError(\"Missing df_train_meta. Jalankan stage meta dulu.\")\n",
    "\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "OOF_DIR = Path(globals().get(\"OOF_DIR\", ART_DIR / \"oof\"))\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Utils: id normalize + robust 1D float32\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _sanitize_prob(p):\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    return p.astype(np.float32)\n",
    "\n",
    "# ensure meta index normalized\n",
    "df_train_meta = df_train_meta.copy(deep=False)\n",
    "df_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Detect target column (robust)\n",
    "# ----------------------------\n",
    "def _detect_target_col(df):\n",
    "    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "TARGET_COL = _detect_target_col(df_train_meta)\n",
    "if TARGET_COL is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot detect target column in df_train_meta. \"\n",
    "        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n",
    "    )\n",
    "\n",
    "def _get_y_aligned(ids):\n",
    "    yy = pd.to_numeric(df_train_meta.loc[ids, TARGET_COL], errors=\"coerce\").fillna(0).to_numpy()\n",
    "    yy = (yy.astype(np.float32) > 0).astype(np.int8)\n",
    "    return yy\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load oof_prob (prefer csv for safest alignment)\n",
    "# ----------------------------\n",
    "def load_oof():\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n",
    "            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "            p = _sanitize_prob(_as_1d_float32(df[\"oof_prob\"].to_numpy()))\n",
    "            if len(p) != len(df):\n",
    "                raise RuntimeError(\"oof_prob.csv: length mismatch after parsing.\")\n",
    "            df = df[[\"object_id\"]].copy()\n",
    "            df[\"oof_prob\"] = p\n",
    "            return p, df, \"csv\"\n",
    "\n",
    "    if \"oof_prob\" in globals():\n",
    "        p = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        if isinstance(p, np.ndarray) and p.ndim != 0:\n",
    "            return _sanitize_prob(p), None, \"globals\"\n",
    "\n",
    "    pnpy = OOF_DIR / \"oof_prob.npy\"\n",
    "    if pnpy.exists():\n",
    "        p = _sanitize_prob(_as_1d_float32(np.load(pnpy, allow_pickle=False)))\n",
    "        return p, None, \"npy\"\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob tidak ditemukan (oof_prob.csv / globals oof_prob / oof_prob.npy).\")\n",
    "\n",
    "oof_prob, df_oof_csv, oof_src = load_oof()\n",
    "if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "    raise TypeError(f\"Invalid oof_prob (scalar/unsized). type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Align y (target) to oof order\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "y = None\n",
    "\n",
    "if df_oof_csv is not None:\n",
    "    # Handle duplicate ids: mean per object_id but keep first-seen order\n",
    "    ids_first = pd.unique(df_oof_csv[\"object_id\"].to_numpy())\n",
    "    if len(ids_first) != len(df_oof_csv):\n",
    "        df_mean = df_oof_csv.groupby(\"object_id\", as_index=True)[\"oof_prob\"].mean()\n",
    "        df_oof_csv = pd.DataFrame({\"object_id\": ids_first})\n",
    "        df_oof_csv[\"oof_prob\"] = df_mean.reindex(ids_first).to_numpy(dtype=np.float32)\n",
    "        oof_prob = _sanitize_prob(df_oof_csv[\"oof_prob\"].to_numpy())\n",
    "\n",
    "    train_ids = df_oof_csv[\"object_id\"].tolist()\n",
    "\n",
    "    # drop ids not in meta\n",
    "    ok = np.asarray([oid in df_train_meta.index for oid in train_ids], dtype=bool)\n",
    "    if not ok.all():\n",
    "        bad = [train_ids[i] for i in np.where(~ok)[0][:10]]\n",
    "        print(f\"[WARN] oof_prob.csv contains ids not in df_train_meta: missing_n={int((~ok).sum())} examples={bad}\")\n",
    "        df_oof_csv = df_oof_csv.loc[ok].reset_index(drop=True)\n",
    "        train_ids = df_oof_csv[\"object_id\"].tolist()\n",
    "        oof_prob = _sanitize_prob(df_oof_csv[\"oof_prob\"].to_numpy())\n",
    "\n",
    "    y = _get_y_aligned(train_ids)\n",
    "\n",
    "if y is None and (\"train_ids_ordered\" in globals()):\n",
    "    ids = [_norm_id(z) for z in list(globals()[\"train_ids_ordered\"])]\n",
    "    if len(ids) == len(oof_prob):\n",
    "        missing = [oid for oid in ids if oid not in df_train_meta.index]\n",
    "        if missing:\n",
    "            raise KeyError(f\"train_ids_ordered contains ids not in df_train_meta. ex={missing[:10]} missing_n={len(missing)}\")\n",
    "        train_ids = ids\n",
    "        y = _get_y_aligned(train_ids)\n",
    "\n",
    "if y is None:\n",
    "    if len(oof_prob) != len(df_train_meta):\n",
    "        raise RuntimeError(\n",
    "            f\"Tidak bisa align y. len(oof_prob)={len(oof_prob)} != len(df_train_meta)={len(df_train_meta)} \"\n",
    "            \"dan tidak ada oof_prob.csv (object_id) atau train_ids_ordered.\"\n",
    "        )\n",
    "    train_ids = df_train_meta.index.astype(str).tolist()\n",
    "    y = _get_y_aligned(train_ids)\n",
    "\n",
    "if len(y) != len(oof_prob):\n",
    "    raise RuntimeError(f\"Length mismatch: y={len(y)} vs oof_prob={len(oof_prob)}\")\n",
    "\n",
    "uy = set(np.unique(y).tolist())\n",
    "if not uy.issubset({0, 1}):\n",
    "    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n",
    "\n",
    "N = int(len(y))\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "\n",
    "print(f\"[Eval] OOF source={oof_src} | target_col={TARGET_COL}\")\n",
    "print(f\"[Eval] N={N:,} | pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Metrics: P/R/F1 + Fbeta + AUC optional\n",
    "# ----------------------------\n",
    "def prf_from_pred(y_true, y_pred01):\n",
    "    y_true = np.asarray(y_true, dtype=np.int32)\n",
    "    y_pred01 = np.asarray(y_pred01, dtype=np.int32)\n",
    "\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred01 == 0)).sum())\n",
    "\n",
    "    precision = tp / max(tp + fp, 1)\n",
    "    recall    = tp / max(tp + fn, 1)\n",
    "    f1 = 0.0 if (precision + recall) == 0 else (2.0 * precision * recall / (precision + recall))\n",
    "\n",
    "    return {\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"pos_pred\": int(y_pred01.sum()),\n",
    "        \"acc\": float((tp + tn) / max(len(y_true), 1)),\n",
    "    }\n",
    "\n",
    "def fbeta_from_pr(precision, recall, beta=1.0):\n",
    "    b2 = beta * beta\n",
    "    denom = (b2 * precision + recall)\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    return float((1 + b2) * precision * recall / denom)\n",
    "\n",
    "def eval_at_threshold(prob, y_true, thr):\n",
    "    pred = (prob >= float(thr)).astype(np.int8)\n",
    "    met = prf_from_pred(y_true, pred)\n",
    "    met[\"thr\"] = float(thr)\n",
    "    met[\"f0.5\"] = fbeta_from_pr(met[\"precision\"], met[\"recall\"], beta=0.5)\n",
    "    met[\"f2\"]   = fbeta_from_pr(met[\"precision\"], met[\"recall\"], beta=2.0)\n",
    "    return met\n",
    "\n",
    "roc_auc = None\n",
    "pr_auc = None\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    if (y.max() == 1) and (y.min() == 0):\n",
    "        roc_auc = float(roc_auc_score(y, oof_prob))\n",
    "        pr_auc  = float(average_precision_score(y, oof_prob))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "base = eval_at_threshold(oof_prob, y, 0.5)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Threshold candidates (grid + quantiles + sampled uniques)\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.00, 0.10, 41),\n",
    "    np.linspace(0.10, 0.90, 161),\n",
    "    np.linspace(0.90, 1.00, 41),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\n",
    "quant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n",
    "\n",
    "uniq = np.unique(oof_prob)\n",
    "if len(uniq) > 4000:\n",
    "    take = np.linspace(0, len(uniq)-1, 4000, dtype=int)\n",
    "    uniq = uniq[take].astype(np.float32)\n",
    "\n",
    "thr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr, uniq]), 0.0, 1.0)).astype(np.float32)\n",
    "\n",
    "rows = []\n",
    "best_f1  = base.copy()\n",
    "best_f05 = base.copy()\n",
    "best_f2  = base.copy()\n",
    "\n",
    "for thr in thr_candidates:\n",
    "    met = eval_at_threshold(oof_prob, y, float(thr))\n",
    "    rows.append([\n",
    "        met[\"thr\"], met[\"f1\"], met[\"f0.5\"], met[\"f2\"],\n",
    "        met[\"precision\"], met[\"recall\"], met[\"acc\"],\n",
    "        met[\"tp\"], met[\"fp\"], met[\"fn\"], met[\"tn\"], met[\"pos_pred\"]\n",
    "    ])\n",
    "\n",
    "    # best F1 tie-break: recall higher, then fp lower\n",
    "    if (met[\"f1\"] > best_f1[\"f1\"] + 1e-12) or (\n",
    "        abs(met[\"f1\"] - best_f1[\"f1\"]) <= 1e-12 and (met[\"recall\"] > best_f1[\"recall\"] + 1e-12)\n",
    "    ) or (\n",
    "        abs(met[\"f1\"] - best_f1[\"f1\"]) <= 1e-12 and abs(met[\"recall\"] - best_f1[\"recall\"]) <= 1e-12 and (met[\"fp\"] < best_f1[\"fp\"])\n",
    "    ):\n",
    "        best_f1 = met.copy()\n",
    "\n",
    "    # best F0.5 (precision-leaning)\n",
    "    if (met[\"f0.5\"] > best_f05.get(\"f0.5\", -1.0) + 1e-12):\n",
    "        best_f05 = met.copy()\n",
    "\n",
    "    # best F2 (recall-leaning)\n",
    "    if (met[\"f2\"] > best_f2.get(\"f2\", -1.0) + 1e-12):\n",
    "        best_f2 = met.copy()\n",
    "\n",
    "thr_table = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"thr\",\"f1\",\"f0.5\",\"f2\",\"precision\",\"recall\",\"acc\",\"tp\",\"fp\",\"fn\",\"tn\",\"pos_pred\"]\n",
    ").sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "BEST_THR_F1  = float(best_f1[\"thr\"])\n",
    "BEST_THR_F05 = float(best_f05[\"thr\"])\n",
    "BEST_THR_F2  = float(best_f2[\"thr\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Print report\n",
    "# ----------------------------\n",
    "print(\"\\nEVALUATION (OOF) — Precision/Recall/F1\")\n",
    "if roc_auc is not None:\n",
    "    print(f\"- ROC-AUC={roc_auc:.6f} | PR-AUC={pr_auc:.6f}\")\n",
    "print(\"\\nBaseline @ thr=0.5\")\n",
    "print(f\"- F1={base['f1']:.6f} | P={base['precision']:.6f} | R={base['recall']:.6f} | ACC={base['acc']:.6f}\")\n",
    "print(f\"  tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\")\n",
    "\n",
    "print(f\"\\nBEST-F1  @ thr={BEST_THR_F1:.6f}\")\n",
    "print(f\"- F1={best_f1['f1']:.6f} | P={best_f1['precision']:.6f} | R={best_f1['recall']:.6f} | ACC={best_f1['acc']:.6f}\")\n",
    "print(f\"  tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\")\n",
    "\n",
    "print(f\"\\nBEST-F0.5 @ thr={BEST_THR_F05:.6f} (precision-leaning)\")\n",
    "print(f\"- F0.5={best_f05['f0.5']:.6f} | P={best_f05['precision']:.6f} | R={best_f05['recall']:.6f} | F1={best_f05['f1']:.6f}\")\n",
    "\n",
    "print(f\"\\nBEST-F2   @ thr={BEST_THR_F2:.6f} (recall-leaning)\")\n",
    "print(f\"- F2={best_f2['f2']:.6f} | P={best_f2['precision']:.6f} | R={best_f2['recall']:.6f} | F1={best_f2['f1']:.6f}\")\n",
    "\n",
    "print(\"\\nTop 10 thresholds by F1:\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    print(f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | f0.5={r['f0.5']:.6f} | f2={r['f2']:.6f} | \"\n",
    "          f\"P={r['precision']:.6f} R={r['recall']:.6f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save artifacts\n",
    "# ----------------------------\n",
    "out_txt  = OOF_DIR / \"eval_report.txt\"\n",
    "out_csv  = OOF_DIR / \"eval_threshold_table.csv\"\n",
    "out_json = OOF_DIR / \"eval_summary.json\"\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Evaluation Report (Precision/Recall/F1)\")\n",
    "lines.append(f\"source={oof_src} | target_col={TARGET_COL}\")\n",
    "lines.append(f\"N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.8f}%\")\n",
    "if roc_auc is not None:\n",
    "    lines.append(f\"ROC-AUC={roc_auc:.10f} | PR-AUC={pr_auc:.10f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"F1={base['f1']:.10f} | P={base['precision']:.10f} | R={base['recall']:.10f} | ACC={base['acc']:.10f}\")\n",
    "lines.append(f\"tp={base['tp']} fp={base['fp']} fn={base['fn']} tn={base['tn']} | pos_pred={base['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F1 @ thr={BEST_THR_F1:.10f}\")\n",
    "lines.append(f\"F1={best_f1['f1']:.10f} | P={best_f1['precision']:.10f} | R={best_f1['recall']:.10f} | ACC={best_f1['acc']:.10f}\")\n",
    "lines.append(f\"tp={best_f1['tp']} fp={best_f1['fp']} fn={best_f1['fn']} tn={best_f1['tn']} | pos_pred={best_f1['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F0.5 @ thr={BEST_THR_F05:.10f}\")\n",
    "lines.append(f\"F0.5={best_f05['f0.5']:.10f} | P={best_f05['precision']:.10f} | R={best_f05['recall']:.10f} | F1={best_f05['f1']:.10f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST-F2 @ thr={BEST_THR_F2:.10f}\")\n",
    "lines.append(f\"F2={best_f2['f2']:.10f} | P={best_f2['precision']:.10f} | R={best_f2['recall']:.10f} | F1={best_f2['f1']:.10f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 thresholds by F1:\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    lines.append(f\"{i+1:02d}. thr={r['thr']:.10f} | f1={r['f1']:.10f} | f0.5={r['f0.5']:.10f} | f2={r['f2']:.10f} | \"\n",
    "                 f\"P={r['precision']:.10f} R={r['recall']:.10f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "thr_table.to_csv(out_csv, index=False)\n",
    "\n",
    "payload = {\n",
    "    \"source\": oof_src,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"N\": N, \"pos\": pos, \"neg\": neg,\n",
    "    \"roc_auc\": roc_auc, \"pr_auc\": pr_auc,\n",
    "    \"baseline_thr_0p5\": base,\n",
    "    \"best_f1\": best_f1,\n",
    "    \"best_f0.5\": best_f05,\n",
    "    \"best_f2\": best_f2,\n",
    "    \"paths\": {\"report\": str(out_txt), \"table\": str(out_csv)}\n",
    "}\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(f\"- {out_txt}\")\n",
    "print(f\"- {out_csv}\")\n",
    "print(f\"- {out_json}\")\n",
    "\n",
    "# Export for next stages\n",
    "globals().update({\n",
    "    \"BEST_THR_F1\": BEST_THR_F1,\n",
    "    \"BEST_THR_F05\": BEST_THR_F05,\n",
    "    \"BEST_THR_F2\": BEST_THR_F2,\n",
    "    \"thr_table_eval\": thr_table,\n",
    "    \"EVAL_REPORT_PATH\": out_txt,\n",
    "    \"EVAL_TABLE_PATH\": out_csv,\n",
    "    \"EVAL_SUMMARY_PATH\": out_json,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36cff83",
   "metadata": {
    "papermill": {
     "duration": 0.017963,
     "end_time": "2026-01-03T17:26:57.562318",
     "exception": false,
     "start_time": "2026-01-03T17:26:57.544355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc4a0994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:26:57.601469Z",
     "iopub.status.busy": "2026-01-03T17:26:57.600888Z",
     "iopub.status.idle": "2026-01-03T17:26:57.901820Z",
     "shell.execute_reply": "2026-01-03T17:26:57.900502Z"
    },
    "papermill": {
     "duration": 0.323798,
     "end_time": "2026-01-03T17:26:57.904350",
     "exception": false,
     "start_time": "2026-01-03T17:26:57.580552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11] SUBMISSION READY (BINARY 0/1)\n",
      "- threshold_used=0.697926\n",
      "- rows=7,135 | pos_pred=628 (8.801682%)\n",
      "- wrote: /kaggle/working/submission.csv\n",
      "- copy : /kaggle/working/mallorn_run/run_20260103_152443_143e1c6a76/submissions/submission.csv\n",
      "\n",
      "Preview:\n",
      "                   object_id  prediction\n",
      "    Eluwaith_Mithrim_nothrim           0\n",
      "          Eru_heledir_archam           0\n",
      "           Gonhir_anann_fuin           0\n",
      "Gwathuirim_haradrim_tegilbor           0\n",
      "            achas_minai_maen           0\n",
      "               adab_fae_gath           0\n",
      "             adel_draug_gaur           0\n",
      "     aderthad_cuil_galadhrim           0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 11 — Submission Build (ONE CELL, Kaggle CPU-SAFE) — REVISI FULL v3\n",
    "#\n",
    "# Fix utama v3:\n",
    "# - Cari pred test sesuai STAGE 10 (ART_DIR/preds/test_prob_ens.csv) + baca dari TEST_INFER_CFG_PATH jika ada\n",
    "# - Fallback lebih lengkap (globals / cfg json / csv / npy)\n",
    "# - Strict align ke sample_submission order, prediction harus 0/1\n",
    "#\n",
    "# Output:\n",
    "# - /kaggle/working/submission.csv\n",
    "# - SUB_DIR/submission.csv (copy)\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"SUB_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n",
    "\n",
    "sample_path = Path(PATHS[\"SAMPLE_SUB\"])\n",
    "if not sample_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n",
    "\n",
    "df_sub = pd.read_csv(sample_path)\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _sanitize_prob(p):\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    return p.astype(np.float32)\n",
    "\n",
    "def _load_ids_npy(path: Path):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    ids = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n",
    "    return [_norm_id(x) for x in ids]\n",
    "\n",
    "def _try_load_stage10_cfg_csv():\n",
    "    \"\"\"\n",
    "    If STAGE 10 wrote config json, use it to find the exact csv path.\n",
    "    Returns Path or None.\n",
    "    \"\"\"\n",
    "    p = globals().get(\"TEST_INFER_CFG_PATH\", None)\n",
    "    if p is None:\n",
    "        return None\n",
    "    p = Path(p)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        cfg = json.load(open(p, \"r\", encoding=\"utf-8\"))\n",
    "        out = cfg.get(\"outputs\", {})\n",
    "        csvp = out.get(\"test_prob_ens_csv\", None)\n",
    "        if csvp:\n",
    "            csvp = Path(csvp)\n",
    "            if csvp.exists():\n",
    "                return csvp\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _load_pred_df():\n",
    "    \"\"\"\n",
    "    Return df_pred with columns: object_id, prob\n",
    "    Priority:\n",
    "      A) globals: test_ids + test_prob_ens\n",
    "      B) stage10 config json -> outputs.test_prob_ens_csv\n",
    "      C) csv fallbacks (ART_DIR/preds/test_prob_ens.csv etc)\n",
    "      D) npy fallback: FIX_DIR/test_ids.npy + test_prob_ens.npy\n",
    "    \"\"\"\n",
    "    # ---- A) globals ----\n",
    "    if (\"test_prob_ens\" in globals()) and (globals()[\"test_prob_ens\"] is not None) and \\\n",
    "       (\"test_ids\" in globals()) and (globals()[\"test_ids\"] is not None):\n",
    "        ids = [_norm_id(x) for x in list(globals()[\"test_ids\"])]\n",
    "        prob = _sanitize_prob(_as_1d_float32(globals()[\"test_prob_ens\"]))\n",
    "        if isinstance(prob, np.ndarray) and prob.ndim != 0 and len(ids) == len(prob) and len(ids) > 0:\n",
    "            return pd.DataFrame({\"object_id\": ids, \"prob\": prob})\n",
    "\n",
    "    # ---- B) exact csv path from STAGE 10 config ----\n",
    "    cfg_csv = _try_load_stage10_cfg_csv()\n",
    "    if cfg_csv is not None and cfg_csv.exists():\n",
    "        df = pd.read_csv(cfg_csv)\n",
    "        if \"object_id\" in df.columns and (\"prob\" in df.columns or \"prediction\" in df.columns):\n",
    "            df = df.copy()\n",
    "            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "            colp = \"prob\" if \"prob\" in df.columns else \"prediction\"\n",
    "            prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n",
    "            if len(prob) != len(df):\n",
    "                raise RuntimeError(f\"CSV prob length mismatch: {cfg_csv}\")\n",
    "            return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob})\n",
    "\n",
    "    # ---- C) csv fallback (best if already aligned with object_id) ----\n",
    "    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "    preds_dir = art_dir / \"preds\"\n",
    "\n",
    "    cand_csv = []\n",
    "    if \"TEST_PROB_CSV_PATH\" in globals() and globals()[\"TEST_PROB_CSV_PATH\"] is not None:\n",
    "        cand_csv.append(Path(globals()[\"TEST_PROB_CSV_PATH\"]))\n",
    "    # STAGE 10 default\n",
    "    cand_csv.append(preds_dir / \"test_prob_ens.csv\")\n",
    "    # older / alternative\n",
    "    cand_csv.append(art_dir / \"test_prob_ens.csv\")\n",
    "\n",
    "    for p in cand_csv:\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p)\n",
    "            if \"object_id\" in df.columns and (\"prob\" in df.columns or \"prediction\" in df.columns):\n",
    "                df = df.copy()\n",
    "                df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "                colp = \"prob\" if \"prob\" in df.columns else \"prediction\"\n",
    "                prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n",
    "                if len(prob) != len(df):\n",
    "                    raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n",
    "                return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob})\n",
    "\n",
    "    # ---- D) npy fallback ----\n",
    "    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n",
    "    p_ids = fix_dir / \"test_ids.npy\"\n",
    "    if not p_ids.exists():\n",
    "        raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat fixed_seq/test_ids.npy atau STAGE 10 export test_ids.\")\n",
    "\n",
    "    ids = _load_ids_npy(p_ids)\n",
    "    if len(ids) == 0:\n",
    "        raise RuntimeError(\"test_ids.npy kosong.\")\n",
    "\n",
    "    cand_npy = []\n",
    "    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n",
    "        cand_npy.append(Path(globals()[\"TEST_PROB_ENS_PATH\"]))\n",
    "    cand_npy.append(preds_dir / \"test_prob_ens.npy\")\n",
    "    cand_npy.append(art_dir / \"test_prob_ens.npy\")\n",
    "\n",
    "    prob = None\n",
    "    for p in cand_npy:\n",
    "        if p.exists():\n",
    "            prob = _sanitize_prob(_as_1d_float32(np.load(p, allow_pickle=False)))\n",
    "            break\n",
    "    if prob is None:\n",
    "        raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n",
    "\n",
    "    if not isinstance(prob, np.ndarray) or prob.ndim == 0:\n",
    "        raise TypeError(f\"Invalid test_prob (scalar/unsized). type={type(prob)} ndim={getattr(prob,'ndim',None)}\")\n",
    "\n",
    "    if len(prob) != len(ids):\n",
    "        raise RuntimeError(f\"Length mismatch (NPY): test_prob={len(prob)} vs test_ids={len(ids)}\")\n",
    "\n",
    "    return pd.DataFrame({\"object_id\": ids, \"prob\": prob})\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load prediction df\n",
    "# ----------------------------\n",
    "df_pred = _load_pred_df()\n",
    "if df_pred.empty:\n",
    "    raise RuntimeError(\"df_pred empty (unexpected).\")\n",
    "\n",
    "df_pred[\"object_id\"] = df_pred[\"object_id\"].apply(_norm_id)\n",
    "if df_pred[\"object_id\"].duplicated().any():\n",
    "    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n",
    "\n",
    "p = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "if not np.isfinite(p).all():\n",
    "    bad = int((~np.isfinite(p)).sum())\n",
    "    raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\n",
    "df_pred[\"prob\"] = _sanitize_prob(p)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold selection (priority)\n",
    "# ----------------------------\n",
    "FORCE_THR = None  # set manual if you want, e.g. 0.37\n",
    "if FORCE_THR is not None:\n",
    "    thr = float(FORCE_THR)\n",
    "elif \"BEST_THR_F1\" in globals() and globals()[\"BEST_THR_F1\"] is not None:\n",
    "    thr = float(globals()[\"BEST_THR_F1\"])\n",
    "elif \"BEST_THR\" in globals() and globals()[\"BEST_THR\"] is not None:\n",
    "    thr = float(globals()[\"BEST_THR\"])\n",
    "else:\n",
    "    thr = 0.5\n",
    "thr = float(np.clip(thr, 0.0, 1.0))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Align to sample_submission order + build BINARY prediction (0/1)\n",
    "# ----------------------------\n",
    "df_sub = df_sub.copy()\n",
    "df_sub[\"object_id\"] = df_sub[\"object_id\"].apply(_norm_id)\n",
    "\n",
    "if df_sub[\"object_id\"].duplicated().any():\n",
    "    dup = df_sub.loc[df_sub[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(f\"sample_submission has duplicate object_id (unexpected). examples={dup}\")\n",
    "\n",
    "df_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if df_out[\"prob\"].isna().any():\n",
    "    missing_n = int(df_out[\"prob\"].isna().sum())\n",
    "    miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(\n",
    "        f\"Some sample_submission object_id have no prediction: missing_n={missing_n}. Examples: {miss_ids}\\n\"\n",
    "        \"Biasanya karena mismatch id normalization atau pred df tidak lengkap.\"\n",
    "    )\n",
    "\n",
    "df_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\n",
    "df_out = df_out[[\"object_id\", \"prediction\"]]\n",
    "\n",
    "u = set(np.unique(df_out[\"prediction\"].to_numpy()).tolist())\n",
    "if not u.issubset({0, 1}):\n",
    "    raise RuntimeError(f\"submission prediction contains values outside {{0,1}}: {sorted(list(u))}\")\n",
    "\n",
    "if len(df_out) != len(df_sub):\n",
    "    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n",
    "\n",
    "pos_pred = int(df_out[\"prediction\"].sum())\n",
    "print(\"[Stage 11] SUBMISSION READY (BINARY 0/1)\")\n",
    "print(f\"- threshold_used={thr:.6f}\")\n",
    "print(f\"- rows={len(df_out):,} | pos_pred={pos_pred:,} ({pos_pred/max(len(df_out),1)*100:.6f}%)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Write files\n",
    "# ----------------------------\n",
    "SUB_DIR = Path(SUB_DIR)\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_main = Path(\"/kaggle/working/submission.csv\")\n",
    "out_copy = SUB_DIR / \"submission.csv\"\n",
    "\n",
    "df_out.to_csv(out_main, index=False)\n",
    "df_out.to_csv(out_copy, index=False)\n",
    "\n",
    "print(f\"- wrote: {out_main}\")\n",
    "print(f\"- copy : {out_copy}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(df_out.head(8).to_string(index=False))\n",
    "\n",
    "globals().update({\n",
    "    \"SUBMISSION_PATH\": out_main,\n",
    "    \"SUBMISSION_COPY_PATH\": out_copy,\n",
    "    \"SUBMISSION_MODE\": \"binary\",\n",
    "    \"SUBMISSION_THRESHOLD\": thr,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8925232,
     "sourceId": 14010596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7347.025194,
   "end_time": "2026-01-03T17:27:00.902469",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-03T15:24:33.877275",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
