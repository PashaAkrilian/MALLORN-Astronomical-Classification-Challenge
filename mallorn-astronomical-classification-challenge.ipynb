{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59908ce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-01T18:52:42.318332Z",
     "iopub.status.busy": "2026-01-01T18:52:42.318060Z",
     "iopub.status.idle": "2026-01-01T18:52:43.249892Z",
     "shell.execute_reply": "2026-01-01T18:52:43.248990Z"
    },
    "papermill": {
     "duration": 0.940489,
     "end_time": "2026-01-01T18:52:43.251385",
     "exception": false,
     "start_time": "2026-01-01T18:52:42.310896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mallorn-dataset/sample_submission.csv\n",
      "/kaggle/input/mallorn-dataset/test_log.csv\n",
      "/kaggle/input/mallorn-dataset/train_log.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_17/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_01/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_02/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_08/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_04/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_07/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_15/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_20/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_06/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_19/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_09/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_12/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_16/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_10/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_18/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_03/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_11/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_14/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_05/test_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/train_full_lightcurves.csv\n",
      "/kaggle/input/mallorn-dataset/split_13/test_full_lightcurves.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393806a",
   "metadata": {
    "papermill": {
     "duration": 0.004471,
     "end_time": "2026-01-01T18:52:43.260921",
     "exception": false,
     "start_time": "2026-01-01T18:52:43.256450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Kaggle CPU Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5610e369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:52:43.271714Z",
     "iopub.status.busy": "2026-01-01T18:52:43.271337Z",
     "iopub.status.idle": "2026-01-01T18:52:47.126862Z",
     "shell.execute_reply": "2026-01-01T18:52:47.126098Z"
    },
    "papermill": {
     "duration": 3.862729,
     "end_time": "2026-01-01T18:52:47.128128",
     "exception": false,
     "start_time": "2026-01-01T18:52:43.265399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV OK (Kaggle CPU)\n",
      "- Python: 3.12.12\n",
      "- Numpy:  2.0.2\n",
      "- Pandas: 2.2.2\n",
      "- Torch:  2.8.0+cu126 | CUDA available: False\n",
      "\n",
      "DATA OK\n",
      "- train_log: 3,043 objects | pos(TDE)=148 | neg=2,895 | pos%=4.86%\n",
      "- test_log:  7,135 objects\n",
      "- submission template rows: 7,135\n",
      "- splits detected: 20 folders (split_01..split_20)\n",
      "\n",
      "Saved env snapshot: /kaggle/working/mallorn_run/env_config.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 0 — Kaggle CPU Environment Setup (ONE CELL, SAFE + COHESIVE)\n",
    "# - Kaggle Web Notebook (CPU only)\n",
    "# - No heavy loading (no full lightcurve concat)\n",
    "# - Hard guards: paths exist, splits exist, required files exist\n",
    "# - Safe thread limits to avoid freeze/oversubscription\n",
    "# ============================================================\n",
    "\n",
    "import os, sys, gc, random, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Quiet + deterministic (reduce noisy warnings, keep critical)\n",
    "# ----------------------------\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "SEED = 2025\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CPU thread limits (anti-freeze on Kaggle CPU)\n",
    "# ----------------------------\n",
    "# Prevent BLAS/OMP oversubscription which can make Kaggle CPU notebooks crawl/hang.\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.set_num_threads(2)\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    torch = None  # torch may be unavailable in some environments\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Paths (as you listed)\n",
    "# ----------------------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\n",
    "\n",
    "PATHS = {\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n",
    "    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n",
    "    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n",
    "    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Working directories (writeable on Kaggle)\n",
    "# ----------------------------\n",
    "WORKDIR = Path(\"/kaggle/working\")\n",
    "RUN_DIR = WORKDIR / \"mallorn_run\"\n",
    "ART_DIR = RUN_DIR / \"artifacts\"\n",
    "CKPT_DIR = RUN_DIR / \"checkpoints\"\n",
    "OOF_DIR = RUN_DIR / \"oof\"\n",
    "SUB_DIR = RUN_DIR / \"submissions\"\n",
    "LOG_DIR = RUN_DIR / \"logs\"\n",
    "\n",
    "for d in [RUN_DIR, ART_DIR, CKPT_DIR, OOF_DIR, SUB_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Hard guards: files must exist\n",
    "# ----------------------------\n",
    "def _must_exist(p: Path, what: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n",
    "\n",
    "_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n",
    "_must_exist(PATHS[\"TRAIN_LOG\"], \"train_log.csv\")\n",
    "_must_exist(PATHS[\"TEST_LOG\"],  \"test_log.csv\")\n",
    "\n",
    "# Validate split folders + key files inside\n",
    "missing_splits = [s for s in PATHS[\"SPLITS\"] if not s.exists()]\n",
    "if missing_splits:\n",
    "    # Show first few to help debug, then fail hard\n",
    "    sample = \"\\n\".join(str(x) for x in missing_splits[:5])\n",
    "    raise FileNotFoundError(f\"Some split folders are missing (showing up to 5):\\n{sample}\")\n",
    "\n",
    "# Verify presence of lightcurve csvs per split (train/test)\n",
    "bad = []\n",
    "for sd in PATHS[\"SPLITS\"]:\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        bad.append((sd.name, tr.exists(), te.exists()))\n",
    "if bad:\n",
    "    msg = \"\\n\".join([f\"- {name}: train={tr_ok}, test={te_ok}\" for name, tr_ok, te_ok in bad[:10]])\n",
    "    raise FileNotFoundError(\n",
    "        \"Some split lightcurve files are missing (showing up to 10):\\n\"\n",
    "        f\"{msg}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Load small metadata only (safe on CPU)\n",
    "# ----------------------------\n",
    "# Use dtype hints to reduce parsing warnings and memory.\n",
    "df_sub = pd.read_csv(PATHS[\"SAMPLE_SUB\"])\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission columns must include object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "df_train_log = pd.read_csv(PATHS[\"TRAIN_LOG\"])\n",
    "df_test_log  = pd.read_csv(PATHS[\"TEST_LOG\"])\n",
    "\n",
    "# Basic column sanity (do not assume perfect casing)\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "df_train_log = _norm_cols(df_train_log)\n",
    "df_test_log  = _norm_cols(df_test_log)\n",
    "\n",
    "need_train = {\"object_id\", \"EBV\", \"Z\", \"split\", \"target\"}\n",
    "need_test  = {\"object_id\", \"EBV\", \"Z\", \"split\"}  # Z_err may or may not exist; handle later\n",
    "missing_train = sorted(list(need_train - set(df_train_log.columns)))\n",
    "missing_test  = sorted(list(need_test - set(df_test_log.columns)))\n",
    "\n",
    "if missing_train:\n",
    "    raise ValueError(f\"train_log.csv missing required columns: {missing_train}\")\n",
    "if missing_test:\n",
    "    raise ValueError(f\"test_log.csv missing required columns: {missing_test}\")\n",
    "\n",
    "# Ensure split names align with folders you have (split_01..split_20)\n",
    "# (Keep as string; later we use it for routing.)\n",
    "df_train_log[\"split\"] = df_train_log[\"split\"].astype(str).str.strip()\n",
    "df_test_log[\"split\"]  = df_test_log[\"split\"].astype(str).str.strip()\n",
    "\n",
    "# Object_id uniqueness check\n",
    "if df_train_log[\"object_id\"].duplicated().any():\n",
    "    dup_n = int(df_train_log[\"object_id\"].duplicated().sum())\n",
    "    raise ValueError(f\"train_log.csv has duplicated object_id rows: {dup_n} duplicates found.\")\n",
    "if df_test_log[\"object_id\"].duplicated().any():\n",
    "    dup_n = int(df_test_log[\"object_id\"].duplicated().sum())\n",
    "    raise ValueError(f\"test_log.csv has duplicated object_id rows: {dup_n} duplicates found.\")\n",
    "\n",
    "# Quick target sanity\n",
    "if not set(pd.unique(df_train_log[\"target\"])).issubset({0, 1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(pd.unique(df_train_log['target']).tolist())}\")\n",
    "\n",
    "pos = int((df_train_log[\"target\"] == 1).sum())\n",
    "neg = int((df_train_log[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_log))\n",
    "\n",
    "# Verify all submission object_id exist in test_log (or warn)\n",
    "sub_missing = set(df_sub[\"object_id\"]) - set(df_test_log[\"object_id\"])\n",
    "if sub_missing:\n",
    "    # do not fail hard; Kaggle sample_submission sometimes includes all test ids, so this is serious\n",
    "    # We'll fail hard to avoid silent mismatch.\n",
    "    sample = list(sub_missing)[:5]\n",
    "    raise ValueError(f\"sample_submission has object_id not found in test_log (showing up to 5): {sample}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Summarize environment (minimal, helpful)\n",
    "# ----------------------------\n",
    "print(\"ENV OK (Kaggle CPU)\")\n",
    "print(f\"- Python: {sys.version.split()[0]}\")\n",
    "print(f\"- Numpy:  {np.__version__}\")\n",
    "print(f\"- Pandas: {pd.__version__}\")\n",
    "if torch is not None:\n",
    "    print(f\"- Torch:  {torch.__version__} | CUDA available: {torch.cuda.is_available()}\")\n",
    "else:\n",
    "    print(\"- Torch:  not available\")\n",
    "\n",
    "print(\"\\nDATA OK\")\n",
    "print(f\"- train_log: {len(df_train_log):,} objects | pos(TDE)={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.2f}%\")\n",
    "print(f\"- test_log:  {len(df_test_log):,} objects\")\n",
    "print(f\"- submission template rows: {len(df_sub):,}\")\n",
    "print(f\"- splits detected: {len(PATHS['SPLITS'])} folders (split_01..split_20)\")\n",
    "\n",
    "# Optional: save a tiny config snapshot for reproducibility\n",
    "cfg_path = RUN_DIR / \"env_config.txt\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"SEED={SEED}\\n\")\n",
    "    f.write(f\"DATA_ROOT={DATA_ROOT}\\n\")\n",
    "    f.write(f\"WORKDIR={WORKDIR}\\n\")\n",
    "    f.write(\"THREADS:\\n\")\n",
    "    for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
    "        f.write(f\"  {k}={os.environ.get(k,'')}\\n\")\n",
    "\n",
    "# Keep objects in globals for next stages\n",
    "globals().update({\n",
    "    \"SEED\": SEED,\n",
    "    \"PATHS\": PATHS,\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"CKPT_DIR\": CKPT_DIR,\n",
    "    \"OOF_DIR\": OOF_DIR,\n",
    "    \"SUB_DIR\": SUB_DIR,\n",
    "    \"LOG_DIR\": LOG_DIR,\n",
    "    \"df_sub\": df_sub,\n",
    "    \"df_train_log\": df_train_log,\n",
    "    \"df_test_log\": df_test_log,\n",
    "})\n",
    "\n",
    "gc.collect()\n",
    "print(f\"\\nSaved env snapshot: {cfg_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff195b",
   "metadata": {
    "papermill": {
     "duration": 0.004476,
     "end_time": "2026-01-01T18:52:47.137452",
     "exception": false,
     "start_time": "2026-01-01T18:52:47.132976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Verify Dataset Paths & Split Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724a4d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:52:47.148127Z",
     "iopub.status.busy": "2026-01-01T18:52:47.147844Z",
     "iopub.status.idle": "2026-01-01T18:52:47.723043Z",
     "shell.execute_reply": "2026-01-01T18:52:47.722222Z"
    },
    "papermill": {
     "duration": 0.582389,
     "end_time": "2026-01-01T18:52:47.724321",
     "exception": false,
     "start_time": "2026-01-01T18:52:47.141932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT DISCOVERY OK\n",
      "- DATA_ROOT: /kaggle/input/mallorn-dataset\n",
      "- Splits on disk: 20 (split_01..split_20)\n",
      "\n",
      "OBJECT COUNTS PER SPLIT (from logs)\n",
      "- split_01: train_objects=155 | test_objects=364\n",
      "- split_02: train_objects=170 | test_objects=414\n",
      "- split_03: train_objects=138 | test_objects=338\n",
      "- split_04: train_objects=145 | test_objects=332\n",
      "- split_05: train_objects=165 | test_objects=375\n",
      "- split_06: train_objects=155 | test_objects=374\n",
      "- split_07: train_objects=165 | test_objects=398\n",
      "- split_08: train_objects=162 | test_objects=387\n",
      "- split_09: train_objects=128 | test_objects=289\n",
      "- split_10: train_objects=144 | test_objects=331\n",
      "- split_11: train_objects=146 | test_objects=325\n",
      "- split_12: train_objects=155 | test_objects=353\n",
      "- split_13: train_objects=143 | test_objects=379\n",
      "- split_14: train_objects=154 | test_objects=351\n",
      "- split_15: train_objects=158 | test_objects=342\n",
      "- split_16: train_objects=155 | test_objects=354\n",
      "- split_17: train_objects=153 | test_objects=351\n",
      "- split_18: train_objects=152 | test_objects=345\n",
      "- split_19: train_objects=147 | test_objects=375\n",
      "- split_20: train_objects=153 | test_objects=358\n",
      "\n",
      "LIGHTCURVE FILE SIZES (MB)\n",
      "- split_01: train_full=     1.4 MB | test_full=     3.1 MB\n",
      "- split_02: train_full=     1.3 MB | test_full=     3.7 MB\n",
      "- split_03: train_full=     1.1 MB | test_full=     2.8 MB\n",
      "- split_04: train_full=     1.2 MB | test_full=     2.7 MB\n",
      "- split_05: train_full=     1.3 MB | test_full=     3.1 MB\n",
      "- split_06: train_full=     1.3 MB | test_full=     2.9 MB\n",
      "- split_07: train_full=     1.3 MB | test_full=     3.4 MB\n",
      "- split_08: train_full=     1.3 MB | test_full=     3.2 MB\n",
      "- split_09: train_full=     1.0 MB | test_full=     2.4 MB\n",
      "- split_10: train_full=     1.3 MB | test_full=     2.6 MB\n",
      "- split_11: train_full=     1.2 MB | test_full=     2.6 MB\n",
      "- split_12: train_full=     1.3 MB | test_full=     2.8 MB\n",
      "- split_13: train_full=     1.2 MB | test_full=     3.3 MB\n",
      "- split_14: train_full=     1.3 MB | test_full=     3.0 MB\n",
      "- split_15: train_full=     1.2 MB | test_full=     2.7 MB\n",
      "- split_16: train_full=     1.3 MB | test_full=     3.0 MB\n",
      "- split_17: train_full=     1.2 MB | test_full=     3.1 MB\n",
      "- split_18: train_full=     1.1 MB | test_full=     2.7 MB\n",
      "- split_19: train_full=     1.1 MB | test_full=     2.9 MB\n",
      "- split_20: train_full=     1.2 MB | test_full=     3.0 MB\n",
      "\n",
      "Stage 1 complete: splits ready for split-wise preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Verify Dataset Paths & Split Discovery (ONE CELL, CPU-SAFE)\n",
    "# - Uses globals from STAGE 0: PATHS, df_train_log, df_test_log\n",
    "# - Normalizes split names -> \"split_XX\"\n",
    "# - Verifies: split folders + required files + lightcurve column sanity (nrows only)\n",
    "# - Summarizes: object counts per split + file sizes\n",
    "# ============================================================\n",
    "\n",
    "import os, re, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"df_train_log\", \"df_test_log\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (Kaggle CPU Environment Setup).\")\n",
    "\n",
    "DATA_ROOT = PATHS[\"DATA_ROOT\"]\n",
    "SPLIT_DIRS = {p.name: p for p in PATHS[\"SPLITS\"]}  # split_01..split_20 -> Path\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers\n",
    "# ----------------------------\n",
    "def normalize_split_name(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert various split formats to canonical 'split_XX'.\n",
    "    Accepts: 'split_01', '01', '1', 'split_1', 'SPLIT_01', etc.\n",
    "    \"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower()\n",
    "    # already canonical?\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    # maybe just digits\n",
    "    m = re.fullmatch(r\"(\\d{1,2})\", s)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    # maybe splitXX without underscore\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    # fallback: keep raw, but mark as unknown\n",
    "    return s\n",
    "\n",
    "def must_exist(p: Path, what: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n",
    "\n",
    "def sizeof_mb(p: Path) -> float:\n",
    "    try:\n",
    "        return p.stat().st_size / (1024**2)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def quick_check_lightcurve_csv(p: Path, nrows: int = 5):\n",
    "    \"\"\"\n",
    "    Read tiny head to validate columns without heavy IO.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(p, nrows=nrows)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "REQ_LC_COLS = {\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"}\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Normalize split columns in logs (in-place for later stages)\n",
    "# ----------------------------\n",
    "for df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n",
    "    if \"split\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing 'split' column.\")\n",
    "    df[\"split\"] = df[\"split\"].astype(str).str.strip()\n",
    "    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Verify split values referenced by logs exist on disk\n",
    "# ----------------------------\n",
    "train_splits = set(df_train_log[\"split\"].unique())\n",
    "test_splits  = set(df_test_log[\"split\"].unique())\n",
    "disk_splits  = set(SPLIT_DIRS.keys())\n",
    "\n",
    "bad_train = sorted([s for s in train_splits if s not in disk_splits])\n",
    "bad_test  = sorted([s for s in test_splits  if s not in disk_splits])\n",
    "\n",
    "if bad_train:\n",
    "    raise FileNotFoundError(f\"train_log references split(s) not found on disk: {bad_train[:10]}\")\n",
    "if bad_test:\n",
    "    raise FileNotFoundError(f\"test_log references split(s) not found on disk: {bad_test[:10]}\")\n",
    "\n",
    "# Also verify we actually have 20 splits (as expected)\n",
    "if len(disk_splits) != 20:\n",
    "    # not always fatal, but better to fail early for this competition\n",
    "    raise RuntimeError(f\"Expected 20 split folders, found {len(disk_splits)}: {sorted(list(disk_splits))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Verify required files per split exist\n",
    "# ----------------------------\n",
    "missing_files = []\n",
    "split_file_info = []\n",
    "\n",
    "for split_name in sorted(disk_splits):\n",
    "    sd = SPLIT_DIRS[split_name]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if not tr.exists():\n",
    "        missing_files.append(str(tr))\n",
    "    if not te.exists():\n",
    "        missing_files.append(str(te))\n",
    "    split_file_info.append((split_name, sizeof_mb(tr), sizeof_mb(te)))\n",
    "\n",
    "if missing_files:\n",
    "    sample = \"\\n\".join(missing_files[:10])\n",
    "    raise FileNotFoundError(f\"Some lightcurve files missing (showing up to 10):\\n{sample}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Lightweight column sanity check (read only a few rows per split)\n",
    "# ----------------------------\n",
    "col_issues = []\n",
    "filter_issues = []\n",
    "\n",
    "for split_name in sorted(disk_splits):\n",
    "    sd = SPLIT_DIRS[split_name]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "\n",
    "    dtr = quick_check_lightcurve_csv(tr, nrows=5)\n",
    "    dte = quick_check_lightcurve_csv(te, nrows=5)\n",
    "\n",
    "    # Columns present?\n",
    "    miss_tr = sorted(list(REQ_LC_COLS - set(dtr.columns)))\n",
    "    miss_te = sorted(list(REQ_LC_COLS - set(dte.columns)))\n",
    "    if miss_tr or miss_te:\n",
    "        col_issues.append((split_name, miss_tr, miss_te, list(dtr.columns), list(dte.columns)))\n",
    "\n",
    "    # Filter values sanity (tiny sample)\n",
    "    if \"Filter\" in dtr.columns:\n",
    "        vals = set(dtr[\"Filter\"].astype(str).str.strip().str.lower().unique())\n",
    "        badf = sorted([v for v in vals if v not in {\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"}])\n",
    "        if badf:\n",
    "            filter_issues.append((split_name, \"train\", badf, sorted(list(vals))))\n",
    "    if \"Filter\" in dte.columns:\n",
    "        vals = set(dte[\"Filter\"].astype(str).str.strip().str.lower().unique())\n",
    "        badf = sorted([v for v in vals if v not in {\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"}])\n",
    "        if badf:\n",
    "            filter_issues.append((split_name, \"test\", badf, sorted(list(vals))))\n",
    "\n",
    "if col_issues:\n",
    "    # Print one detailed example then fail hard (structure mismatch)\n",
    "    s, miss_tr, miss_te, cols_tr, cols_te = col_issues[0]\n",
    "    raise ValueError(\n",
    "        \"Lightcurve column mismatch detected.\\n\"\n",
    "        f\"Example split: {s}\\n\"\n",
    "        f\"Missing in train_full_lightcurves.csv: {miss_tr}\\n\"\n",
    "        f\"Missing in test_full_lightcurves.csv : {miss_te}\\n\"\n",
    "        f\"Train columns: {cols_tr}\\n\"\n",
    "        f\"Test columns : {cols_te}\\n\"\n",
    "    )\n",
    "\n",
    "if filter_issues:\n",
    "    # Not always fatal, but usually indicates whitespace/format issues. Fail early.\n",
    "    ex = filter_issues[0]\n",
    "    raise ValueError(\n",
    "        \"Unexpected Filter values detected (example):\\n\"\n",
    "        f\"split={ex[0]} file={ex[1]} bad={ex[2]} all_sampled={ex[3]}\\n\"\n",
    "        \"Fix by stripping/lowercasing Filter during preprocessing.\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Summaries (counts per split, file sizes)\n",
    "# ----------------------------\n",
    "train_counts = df_train_log[\"split\"].value_counts().to_dict()\n",
    "test_counts  = df_test_log[\"split\"].value_counts().to_dict()\n",
    "\n",
    "print(\"SPLIT DISCOVERY OK\")\n",
    "print(f\"- DATA_ROOT: {DATA_ROOT}\")\n",
    "print(f\"- Splits on disk: {len(disk_splits)} (split_01..split_20)\")\n",
    "\n",
    "print(\"\\nOBJECT COUNTS PER SPLIT (from logs)\")\n",
    "for s in sorted(disk_splits):\n",
    "    print(f\"- {s}: train_objects={train_counts.get(s,0):,} | test_objects={test_counts.get(s,0):,}\")\n",
    "\n",
    "print(\"\\nLIGHTCURVE FILE SIZES (MB)\")\n",
    "for s, mb_tr, mb_te in split_file_info:\n",
    "    print(f\"- {s}: train_full={mb_tr:8.1f} MB | test_full={mb_te:8.1f} MB\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Export split index for later stages (routing + loops)\n",
    "# ----------------------------\n",
    "# Useful for downstream: stable split list + mapping\n",
    "SPLIT_LIST = [f\"split_{i:02d}\" for i in range(1, 21)]\n",
    "globals().update({\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SPLIT_DIRS\": SPLIT_DIRS,\n",
    "    \"SPLIT_LIST\": SPLIT_LIST,\n",
    "})\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\nStage 1 complete: splits ready for split-wise preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06633af",
   "metadata": {
    "papermill": {
     "duration": 0.004683,
     "end_time": "2026-01-01T18:52:47.733839",
     "exception": false,
     "start_time": "2026-01-01T18:52:47.729156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and Validate Train/Test Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f78d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:52:47.744893Z",
     "iopub.status.busy": "2026-01-01T18:52:47.744631Z",
     "iopub.status.idle": "2026-01-01T18:52:48.031611Z",
     "shell.execute_reply": "2026-01-01T18:52:48.030981Z"
    },
    "papermill": {
     "duration": 0.294428,
     "end_time": "2026-01-01T18:52:48.032924",
     "exception": false,
     "start_time": "2026-01-01T18:52:47.738496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGS OK (clean + validated)\n",
      "- train objects: 3,043 | pos(TDE)=148 | neg=2,895 | pos%=4.864%\n",
      "- test objects : 7,135\n",
      "- saved: /kaggle/working/mallorn_run/artifacts/train_log_clean.parquet\n",
      "- saved: /kaggle/working/mallorn_run/artifacts/test_log_clean.parquet\n",
      "- saved: /kaggle/working/mallorn_run/artifacts/split_stats.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Load and Validate Train/Test Logs (ONE CELL, CPU-SAFE)\n",
    "# - Kaggle CPU: ringan, tanpa load full lightcurves\n",
    "# - Output:\n",
    "#   * df_train_meta, df_test_meta  (index=object_id, bersih & siap dipakai)\n",
    "#   * id2split_train, id2split_test (routing cepat ke split folder)\n",
    "#   * artifacts/train_log_clean.parquet, artifacts/test_log_clean.parquet\n",
    "# ============================================================\n",
    "\n",
    "import os, re, gc, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0/1 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n",
    "if \"SPLIT_DIRS\" not in globals():\n",
    "    raise RuntimeError(\"Missing `SPLIT_DIRS`. Jalankan STAGE 1 (Verify Dataset Paths & Split Discovery) dulu.\")\n",
    "\n",
    "TRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\n",
    "TEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n",
    "\n",
    "def _normalize_split_name(x: str) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower()\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    m = re.fullmatch(r\"(\\d{1,2})\", s)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s)\n",
    "    if m:\n",
    "        k = int(m.group(1))\n",
    "        return f\"split_{k:02d}\"\n",
    "    return s\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _coerce_numeric(df: pd.DataFrame, col: str) -> None:\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load logs (fresh read untuk konsistensi)\n",
    "# ----------------------------\n",
    "df_train = pd.read_csv(TRAIN_LOG_PATH)\n",
    "df_test  = pd.read_csv(TEST_LOG_PATH)\n",
    "\n",
    "df_train = _norm_cols(df_train)\n",
    "df_test  = _norm_cols(df_test)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Required columns check\n",
    "# ----------------------------\n",
    "req_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\n",
    "req_train  = req_common | {\"target\"}\n",
    "req_test   = req_common\n",
    "\n",
    "miss_train = sorted(list(req_train - set(df_train.columns)))\n",
    "miss_test  = sorted(list(req_test  - set(df_test.columns)))\n",
    "\n",
    "if miss_train:\n",
    "    raise ValueError(f\"train_log.csv missing required columns: {miss_train} | found={list(df_train.columns)}\")\n",
    "if miss_test:\n",
    "    raise ValueError(f\"test_log.csv missing required columns: {miss_test} | found={list(df_test.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Basic cleaning\n",
    "# ----------------------------\n",
    "# object_id string\n",
    "df_train[\"object_id\"] = df_train[\"object_id\"].astype(str).str.strip()\n",
    "df_test[\"object_id\"]  = df_test[\"object_id\"].astype(str).str.strip()\n",
    "\n",
    "# split canonical\n",
    "df_train[\"split\"] = df_train[\"split\"].astype(str).str.strip().map(_normalize_split_name)\n",
    "df_test[\"split\"]  = df_test[\"split\"].astype(str).str.strip().map(_normalize_split_name)\n",
    "\n",
    "# numeric coercion\n",
    "for c in [\"EBV\", \"Z\", \"Z_err\"]:\n",
    "    _coerce_numeric(df_train, c)\n",
    "    _coerce_numeric(df_test, c)\n",
    "\n",
    "# target binary int\n",
    "df_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    bad_n = int(df_train[\"target\"].isna().sum())\n",
    "    raise ValueError(f\"train_log target has NaN after coercion: {bad_n} rows. Cek isi target di train_log.csv.\")\n",
    "df_train[\"target\"] = df_train[\"target\"].astype(int)\n",
    "uniq_t = set(df_train[\"target\"].unique().tolist())\n",
    "if not uniq_t.issubset({0, 1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Duplicates check (hard fail to avoid silent leakage)\n",
    "# ----------------------------\n",
    "if df_train[\"object_id\"].duplicated().any():\n",
    "    dup = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].iloc[:5].tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in train_log (examples): {dup}\")\n",
    "if df_test[\"object_id\"].duplicated().any():\n",
    "    dup = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].iloc[:5].tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in test_log (examples): {dup}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Missing values handling (anti-error downstream)\n",
    "# ----------------------------\n",
    "# Buat flag missing agar tidak hilang informasi\n",
    "for df in [df_train, df_test]:\n",
    "    for c in [\"EBV\", \"Z\"]:\n",
    "        df[f\"{c}_missing\"] = df[c].isna().astype(np.int8)\n",
    "\n",
    "# Isi EBV NaN -> 0.0 (fisiknya bisa dianggap minimal dust bila tidak ada)\n",
    "if df_train[\"EBV\"].isna().any():\n",
    "    df_train[\"EBV\"] = df_train[\"EBV\"].fillna(0.0)\n",
    "if df_test[\"EBV\"].isna().any():\n",
    "    df_test[\"EBV\"] = df_test[\"EBV\"].fillna(0.0)\n",
    "\n",
    "# Isi Z NaN -> median per split (lebih aman daripada global median)\n",
    "# (Kalau tidak ada NaN, ini tidak mengubah apa-apa)\n",
    "def _fill_z_by_split(df: pd.DataFrame):\n",
    "    if not df[\"Z\"].isna().any():\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    med = df.groupby(\"split\")[\"Z\"].median()\n",
    "    global_med = float(df[\"Z\"].median()) if df[\"Z\"].notna().any() else 0.0\n",
    "    def _fill_row(row):\n",
    "        if pd.isna(row[\"Z\"]):\n",
    "            m = med.get(row[\"split\"], np.nan)\n",
    "            return float(m) if not pd.isna(m) else global_med\n",
    "        return row[\"Z\"]\n",
    "    df[\"Z\"] = df.apply(_fill_row, axis=1)\n",
    "    return df\n",
    "\n",
    "df_train = _fill_z_by_split(df_train)\n",
    "df_test  = _fill_z_by_split(df_test)\n",
    "\n",
    "# Untuk train: kalau Z_err tidak ada, buat kolomnya (konsisten untuk model)\n",
    "if \"Z_err\" not in df_train.columns:\n",
    "    df_train[\"Z_err\"] = np.nan\n",
    "# Untuk test: kalau tidak ada Z_err, tetap buat agar pipeline tidak error\n",
    "if \"Z_err\" not in df_test.columns:\n",
    "    df_test[\"Z_err\"] = np.nan\n",
    "\n",
    "# Flag photometric redshift: train spec-z (anggap 0), test photo-z (anggap 1)\n",
    "df_train[\"is_photoz\"] = np.int8(0)\n",
    "df_test[\"is_photoz\"]  = np.int8(1)\n",
    "\n",
    "# Z_err_missing flag + fill NaN Z_err -> 0.0 agar numeric stabil\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"Z_err_missing\"] = df[\"Z_err\"].isna().astype(np.int8)\n",
    "    df[\"Z_err\"] = df[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Split validity check against disk splits\n",
    "# ----------------------------\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "bad_train_s = sorted([s for s in set(df_train[\"split\"].unique()) if s not in disk_splits])\n",
    "bad_test_s  = sorted([s for s in set(df_test[\"split\"].unique())  if s not in disk_splits])\n",
    "if bad_train_s:\n",
    "    raise FileNotFoundError(f\"train_log references unknown split(s): {bad_train_s[:10]}\")\n",
    "if bad_test_s:\n",
    "    raise FileNotFoundError(f\"test_log references unknown split(s): {bad_test_s[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Build meta tables (index=object_id) + routing dicts\n",
    "# ----------------------------\n",
    "# Simpan hanya kolom yang relevan untuk tahap berikutnya (lebih ringan)\n",
    "keep_train = [\"object_id\",\"split\",\"EBV\",\"Z\",\"Z_err\",\"EBV_missing\",\"Z_missing\",\"Z_err_missing\",\"is_photoz\",\"target\"]\n",
    "keep_test  = [\"object_id\",\"split\",\"EBV\",\"Z\",\"Z_err\",\"EBV_missing\",\"Z_missing\",\"Z_err_missing\",\"is_photoz\"]\n",
    "\n",
    "# Jika SpecType ada, simpan juga untuk analisis (tidak wajib untuk model biner)\n",
    "if \"SpecType\" in df_train.columns:\n",
    "    keep_train.append(\"SpecType\")\n",
    "\n",
    "df_train_meta = df_train[keep_train].copy()\n",
    "df_test_meta  = df_test[keep_test].copy()\n",
    "\n",
    "df_train_meta = df_train_meta.set_index(\"object_id\", drop=True).sort_index()\n",
    "df_test_meta  = df_test_meta.set_index(\"object_id\", drop=True).sort_index()\n",
    "\n",
    "id2split_train = df_train_meta[\"split\"].to_dict()\n",
    "id2split_test  = df_test_meta[\"split\"].to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save cleaned logs (fast reuse)\n",
    "# ----------------------------\n",
    "train_out = Path(ART_DIR) / \"train_log_clean.parquet\"\n",
    "test_out  = Path(ART_DIR) / \"test_log_clean.parquet\"\n",
    "df_train_meta.to_parquet(train_out, index=True)\n",
    "df_test_meta.to_parquet(test_out, index=True)\n",
    "\n",
    "# Save split stats (useful debugging)\n",
    "split_stats = pd.DataFrame({\n",
    "    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(sorted(disk_splits)).fillna(0).astype(int),\n",
    "    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(sorted(disk_splits)).fillna(0).astype(int),\n",
    "})\n",
    "split_stats.index.name = \"split\"\n",
    "split_stats_path = Path(ART_DIR) / \"split_stats.csv\"\n",
    "split_stats.to_csv(split_stats_path)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Print summary (minimal but informative)\n",
    "# ----------------------------\n",
    "pos = int((df_train_meta[\"target\"] == 1).sum())\n",
    "neg = int((df_train_meta[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_meta))\n",
    "print(\"LOGS OK (clean + validated)\")\n",
    "print(f\"- train objects: {tot:,} | pos(TDE)={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.3f}%\")\n",
    "print(f\"- test objects : {len(df_test_meta):,}\")\n",
    "print(f\"- saved: {train_out}\")\n",
    "print(f\"- saved: {test_out}\")\n",
    "print(f\"- saved: {split_stats_path}\")\n",
    "\n",
    "# Keep in globals for next stages\n",
    "globals().update({\n",
    "    \"df_train_meta\": df_train_meta,\n",
    "    \"df_test_meta\": df_test_meta,\n",
    "    \"id2split_train\": id2split_train,\n",
    "    \"id2split_test\": id2split_test,\n",
    "    \"split_stats\": split_stats\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeee232",
   "metadata": {
    "papermill": {
     "duration": 0.005061,
     "end_time": "2026-01-01T18:52:48.043387",
     "exception": false,
     "start_time": "2026-01-01T18:52:48.038326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lightcurve Loading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927ca3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:52:48.055249Z",
     "iopub.status.busy": "2026-01-01T18:52:48.054908Z",
     "iopub.status.idle": "2026-01-01T18:52:48.667956Z",
     "shell.execute_reply": "2026-01-01T18:52:48.667350Z"
    },
    "papermill": {
     "duration": 0.620828,
     "end_time": "2026-01-01T18:52:48.669167",
     "exception": false,
     "start_time": "2026-01-01T18:52:48.048339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIGHTCURVE LOADING STRATEGY OK (split-wise + chunked)\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/split_file_manifest.csv\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/object_counts_by_split.csv\n",
      "- Ready for next stage: photometric preprocessing + sequence building (split-wise loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — Lightcurve Loading Strategy (ONE CELL, Kaggle CPU-SAFE)\n",
    "# - Split-wise file mapping + chunked reader utilities (no full concat)\n",
    "# - Builds:\n",
    "#   * SPLIT_FILES: {split_XX: {\"train\": Path, \"test\": Path}}\n",
    "#   * train_ids_by_split / test_ids_by_split: routing object_ids per split\n",
    "#   * iter_lightcurve_chunks(): generator read_csv(chunksize=...)\n",
    "#   * load_object_lightcurve(): debug-safe per-object extraction (streaming)\n",
    "# - Saves:\n",
    "#   * artifacts/split_file_manifest.csv\n",
    "#   * artifacts/object_counts_by_split.csv\n",
    "# ============================================================\n",
    "\n",
    "import gc, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build split file mapping (train/test lightcurves)\n",
    "# ----------------------------\n",
    "SPLIT_FILES = {}\n",
    "for s in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[s]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n",
    "    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n",
    "\n",
    "# Save split file manifest (helps debug path issues)\n",
    "manifest = []\n",
    "for s in SPLIT_LIST:\n",
    "    manifest.append({\n",
    "        \"split\": s,\n",
    "        \"train_path\": str(SPLIT_FILES[s][\"train\"]),\n",
    "        \"test_path\": str(SPLIT_FILES[s][\"test\"]),\n",
    "        \"train_mb\": SPLIT_FILES[s][\"train\"].stat().st_size / (1024**2),\n",
    "        \"test_mb\":  SPLIT_FILES[s][\"test\"].stat().st_size / (1024**2),\n",
    "    })\n",
    "df_manifest = pd.DataFrame(manifest).sort_values(\"split\")\n",
    "manifest_path = Path(ART_DIR) / \"split_file_manifest.csv\"\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build object routing by split (VERY important for split-wise processing)\n",
    "# ----------------------------\n",
    "train_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "test_ids_by_split  = {s: [] for s in SPLIT_LIST}\n",
    "\n",
    "# df_train_meta/df_test_meta index is object_id (from STAGE 2)\n",
    "for oid, row in df_train_meta[[\"split\"]].itertuples():\n",
    "    train_ids_by_split[row].append(oid)\n",
    "for oid, row in df_test_meta[[\"split\"]].itertuples():\n",
    "    test_ids_by_split[row].append(oid)\n",
    "\n",
    "# Object counts per split (save)\n",
    "df_counts = pd.DataFrame({\n",
    "    \"split\": SPLIT_LIST,\n",
    "    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "})\n",
    "counts_path = Path(ART_DIR) / \"object_counts_by_split.csv\"\n",
    "df_counts.to_csv(counts_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Column normalization & dtypes (memory safe)\n",
    "# ----------------------------\n",
    "# We canonicalize to: object_id, mjd, flux, flux_err, filter\n",
    "LC_RENAME = {\n",
    "    \"Time (MJD)\": \"mjd\",\n",
    "    \"Time(MJD)\": \"mjd\",\n",
    "    \"Time\": \"mjd\",\n",
    "    \"Flux\": \"flux\",\n",
    "    \"Flux_err\": \"flux_err\",\n",
    "    \"FluxErr\": \"flux_err\",\n",
    "    \"Filter\": \"filter\",\n",
    "    \"object_id\": \"object_id\",\n",
    "    \"ObjectID\": \"object_id\",\n",
    "}\n",
    "# Read only columns we need (safe). If dataset has extra columns, we ignore them.\n",
    "USECOLS_RAW = [\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"]\n",
    "# Dtypes (object_id str, filter category-like but keep as string to avoid pandas category pitfalls in chunks)\n",
    "DTYPES = {\n",
    "    \"object_id\": \"string\",\n",
    "    \"Flux\": \"float32\",\n",
    "    \"Flux_err\": \"float32\",\n",
    "    \"Filter\": \"string\",\n",
    "    # Time (MJD) sometimes float64; float32 is usually ok for ML. Keep float32 for memory.\n",
    "    \"Time (MJD)\": \"float32\",\n",
    "}\n",
    "\n",
    "def _normalize_lc_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # rename known columns\n",
    "    df = df.rename(columns={c: LC_RENAME.get(c, c) for c in df.columns})\n",
    "    # enforce required canonical columns exist\n",
    "    need = {\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"}\n",
    "    missing = sorted(list(need - set(df.columns)))\n",
    "    if missing:\n",
    "        raise ValueError(f\"Lightcurve chunk missing required columns after rename: {missing}. Found: {list(df.columns)}\")\n",
    "    # trim/normalize filter values\n",
    "    df[\"filter\"] = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n",
    "    # object_id cleanup\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "    return df[[\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]]\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Chunked readers (core strategy on Kaggle CPU)\n",
    "# ----------------------------\n",
    "def iter_lightcurve_chunks(split_name: str, which: str, chunksize: int = 400_000):\n",
    "    \"\"\"\n",
    "    Stream read a split lightcurve CSV in chunks.\n",
    "    which: 'train' or 'test'\n",
    "    yields normalized chunks with canonical columns:\n",
    "      object_id, mjd, flux, flux_err, filter\n",
    "    \"\"\"\n",
    "    if split_name not in SPLIT_FILES:\n",
    "        raise KeyError(f\"Unknown split_name={split_name}.\")\n",
    "    if which not in (\"train\", \"test\"):\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "    p = SPLIT_FILES[split_name][which]\n",
    "\n",
    "    # Use usecols only if they exist; handle header differences robustly:\n",
    "    # Read header once (cheap), then decide usecols/dtypes mapping.\n",
    "    header = pd.read_csv(p, nrows=0)\n",
    "    cols = [c.strip() for c in header.columns]\n",
    "    # map raw names we expect\n",
    "    raw_usecols = [c for c in USECOLS_RAW if c in cols]\n",
    "    # also accept alternative time column names\n",
    "    if \"Time (MJD)\" not in raw_usecols:\n",
    "        for alt in [\"Time(MJD)\", \"Time\"]:\n",
    "            if alt in cols:\n",
    "                raw_usecols = [c if c != \"Time (MJD)\" else alt for c in raw_usecols]\n",
    "                break\n",
    "\n",
    "    if set(raw_usecols) != set([c for c in raw_usecols]):  # no-op guard\n",
    "        pass\n",
    "\n",
    "    # dtypes must match chosen time column key\n",
    "    dtypes = {}\n",
    "    for k, v in DTYPES.items():\n",
    "        if k in cols:\n",
    "            dtypes[k] = v\n",
    "    # if time column is alt, set dtype\n",
    "    for tcol in [\"Time(MJD)\", \"Time\"]:\n",
    "        if tcol in cols and \"Time (MJD)\" not in cols:\n",
    "            dtypes[tcol] = \"float32\"\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        p,\n",
    "        usecols=raw_usecols if raw_usecols else None,\n",
    "        dtype=dtypes if dtypes else None,\n",
    "        chunksize=int(chunksize),\n",
    "    )\n",
    "    for chunk in reader:\n",
    "        yield _normalize_lc_chunk(chunk)\n",
    "\n",
    "def load_split_lightcurves(split_name: str, which: str):\n",
    "    \"\"\"\n",
    "    Convenience: load entire split file (NOT recommended for huge files).\n",
    "    Use only for quick debugging on small splits.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=400_000):\n",
    "        parts.append(ch)\n",
    "    return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"])\n",
    "\n",
    "def load_object_lightcurve(object_id: str, which: str, chunksize: int = 400_000, sort_time: bool = True):\n",
    "    \"\"\"\n",
    "    Debug-safe per-object extraction by streaming the relevant split file.\n",
    "    This scans the split CSV in chunks (OK for occasional use; do NOT do this for all objects).\n",
    "    \"\"\"\n",
    "    object_id = str(object_id).strip()\n",
    "    if which == \"train\":\n",
    "        if object_id not in df_train_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n",
    "        split_name = df_train_meta.loc[object_id, \"split\"]\n",
    "    elif which == \"test\":\n",
    "        if object_id not in df_test_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n",
    "        split_name = df_test_meta.loc[object_id, \"split\"]\n",
    "    else:\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    pieces = []\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=chunksize):\n",
    "        sub = ch[ch[\"object_id\"] == object_id]\n",
    "        if not sub.empty:\n",
    "            pieces.append(sub)\n",
    "    if not pieces:\n",
    "        out = pd.DataFrame(columns=[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"])\n",
    "    else:\n",
    "        out = pd.concat(pieces, ignore_index=True)\n",
    "        if sort_time and len(out) > 1:\n",
    "            out = out.sort_values([\"mjd\", \"filter\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Quick smoke test (lightweight, no heavy IO)\n",
    "# ----------------------------\n",
    "# Take 1 object from a few splits and ensure extraction works.\n",
    "test_splits = [\"split_01\", \"split_08\", \"split_17\"]\n",
    "for s in test_splits:\n",
    "    if len(train_ids_by_split[s]) == 0 or len(test_ids_by_split[s]) == 0:\n",
    "        raise RuntimeError(f\"Split {s} has 0 objects in train/test log (unexpected).\")\n",
    "    oid_tr = train_ids_by_split[s][0]\n",
    "    oid_te = test_ids_by_split[s][0]\n",
    "    df_tr_obj = load_object_lightcurve(oid_tr, \"train\", chunksize=250_000)\n",
    "    df_te_obj = load_object_lightcurve(oid_te, \"test\",  chunksize=250_000)\n",
    "\n",
    "    if df_tr_obj.empty:\n",
    "        raise RuntimeError(f\"Smoke test failed: empty train lightcurve for {oid_tr} in {s}\")\n",
    "    if df_te_obj.empty:\n",
    "        raise RuntimeError(f\"Smoke test failed: empty test lightcurve for {oid_te} in {s}\")\n",
    "\n",
    "    # filter sanity (allow only u,g,r,i,z,y)\n",
    "    badf_tr = sorted(set(df_tr_obj[\"filter\"].unique()) - set([\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]))\n",
    "    badf_te = sorted(set(df_te_obj[\"filter\"].unique()) - set([\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]))\n",
    "    if badf_tr or badf_te:\n",
    "        raise ValueError(f\"Unexpected filter values in smoke test split={s}: train_bad={badf_tr} test_bad={badf_te}\")\n",
    "\n",
    "print(\"LIGHTCURVE LOADING STRATEGY OK (split-wise + chunked)\")\n",
    "print(f\"- Saved: {manifest_path}\")\n",
    "print(f\"- Saved: {counts_path}\")\n",
    "print(\"- Ready for next stage: photometric preprocessing + sequence building (split-wise loop).\")\n",
    "\n",
    "# Export to globals for next stages\n",
    "globals().update({\n",
    "    \"SPLIT_FILES\": SPLIT_FILES,\n",
    "    \"train_ids_by_split\": train_ids_by_split,\n",
    "    \"test_ids_by_split\": test_ids_by_split,\n",
    "    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n",
    "    \"load_object_lightcurve\": load_object_lightcurve,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ca5d8",
   "metadata": {
    "papermill": {
     "duration": 0.004939,
     "end_time": "2026-01-01T18:52:48.679344",
     "exception": false,
     "start_time": "2026-01-01T18:52:48.674405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46aa2146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:52:48.690988Z",
     "iopub.status.busy": "2026-01-01T18:52:48.690721Z",
     "iopub.status.idle": "2026-01-01T18:52:53.971934Z",
     "shell.execute_reply": "2026-01-01T18:52:53.971280Z"
    },
    "papermill": {
     "duration": 5.288837,
     "end_time": "2026-01-01T18:52:53.973292",
     "exception": false,
     "start_time": "2026-01-01T18:52:48.684455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4] Estimating per-band flux scales from sample (900,000 rows max) ...\n",
      "[Stage 4] BAND_SCALE (median |flux| per band):\n",
      "  - u: 0.336883\n",
      "  - g: 0.249368\n",
      "  - r: 0.377052\n",
      "  - i: 0.451290\n",
      "  - z: 0.576561\n",
      "  - y: 0.996014\n",
      "[Stage 4] Building cleaned lightcurve cache (split-wise) ...\n",
      "[Stage 4] split_01/train: parts=1 | rows=26,324 | neg%=38.95% | det%=19.34%\n",
      "[Stage 4] split_01/test: parts=1 | rows=59,235 | neg%=37.74% | det%=23.02%\n",
      "[Stage 4] split_02/train: parts=1 | rows=25,609 | neg%=34.02% | det%=24.45%\n",
      "[Stage 4] split_02/test: parts=1 | rows=71,229 | neg%=36.48% | det%=21.69%\n",
      "[Stage 4] split_03/train: parts=1 | rows=21,676 | neg%=36.82% | det%=21.65%\n",
      "[Stage 4] split_03/test: parts=1 | rows=53,751 | neg%=36.70% | det%=21.90%\n",
      "[Stage 4] split_04/train: parts=1 | rows=22,898 | neg%=38.36% | det%=21.11%\n",
      "[Stage 4] split_04/test: parts=1 | rows=51,408 | neg%=38.16% | det%=21.70%\n",
      "[Stage 4] split_05/train: parts=1 | rows=25,934 | neg%=39.19% | det%=18.33%\n",
      "[Stage 4] split_05/test: parts=1 | rows=61,179 | neg%=38.41% | det%=18.21%\n",
      "[Stage 4] split_06/train: parts=1 | rows=25,684 | neg%=38.07% | det%=18.85%\n",
      "[Stage 4] split_06/test: parts=1 | rows=57,620 | neg%=37.39% | det%=19.94%\n",
      "[Stage 4] split_07/train: parts=1 | rows=24,473 | neg%=36.28% | det%=21.44%\n",
      "[Stage 4] split_07/test: parts=1 | rows=65,101 | neg%=36.96% | det%=19.10%\n",
      "[Stage 4] split_08/train: parts=1 | rows=25,571 | neg%=39.54% | det%=22.80%\n",
      "[Stage 4] split_08/test: parts=1 | rows=61,498 | neg%=37.88% | det%=24.50%\n",
      "[Stage 4] split_09/train: parts=1 | rows=19,690 | neg%=36.18% | det%=21.13%\n",
      "[Stage 4] split_09/test: parts=1 | rows=47,239 | neg%=35.22% | det%=22.70%\n",
      "[Stage 4] split_10/train: parts=1 | rows=25,151 | neg%=40.38% | det%=20.86%\n",
      "[Stage 4] split_10/test: parts=1 | rows=51,056 | neg%=39.39% | det%=21.21%\n",
      "[Stage 4] split_11/train: parts=1 | rows=22,927 | neg%=39.52% | det%=19.59%\n",
      "[Stage 4] split_11/test: parts=1 | rows=49,723 | neg%=39.36% | det%=20.17%\n",
      "[Stage 4] split_12/train: parts=1 | rows=25,546 | neg%=40.10% | det%=19.64%\n",
      "[Stage 4] split_12/test: parts=1 | rows=54,499 | neg%=38.64% | det%=19.29%\n",
      "[Stage 4] split_13/train: parts=1 | rows=23,203 | neg%=37.92% | det%=20.64%\n",
      "[Stage 4] split_13/test: parts=1 | rows=63,653 | neg%=39.26% | det%=19.56%\n",
      "[Stage 4] split_14/train: parts=1 | rows=25,706 | neg%=35.93% | det%=20.36%\n",
      "[Stage 4] split_14/test: parts=1 | rows=58,643 | neg%=36.82% | det%=17.91%\n",
      "[Stage 4] split_15/train: parts=1 | rows=23,972 | neg%=38.02% | det%=19.09%\n",
      "[Stage 4] split_15/test: parts=1 | rows=52,943 | neg%=38.31% | det%=20.03%\n",
      "[Stage 4] split_16/train: parts=1 | rows=25,173 | neg%=36.92% | det%=21.42%\n",
      "[Stage 4] split_16/test: parts=1 | rows=58,192 | neg%=37.85% | det%=20.12%\n",
      "[Stage 4] split_17/train: parts=1 | rows=22,705 | neg%=35.75% | det%=22.09%\n",
      "[Stage 4] split_17/test: parts=1 | rows=59,482 | neg%=38.08% | det%=19.59%\n",
      "[Stage 4] split_18/train: parts=1 | rows=21,536 | neg%=36.63% | det%=23.77%\n",
      "[Stage 4] split_18/test: parts=1 | rows=53,887 | neg%=35.71% | det%=23.88%\n",
      "[Stage 4] split_19/train: parts=1 | rows=22,087 | neg%=36.38% | det%=23.73%\n",
      "[Stage 4] split_19/test: parts=1 | rows=56,355 | neg%=35.52% | det%=24.17%\n",
      "[Stage 4] split_20/train: parts=1 | rows=23,519 | neg%=37.99% | det%=20.45%\n",
      "[Stage 4] split_20/test: parts=1 | rows=58,432 | neg%=38.29% | det%=19.65%\n",
      "\n",
      "[Stage 4] Done.\n",
      "- Saved manifest: /kaggle/working/mallorn_run/artifacts/lc_clean/lc_clean_manifest.csv\n",
      "- Saved summary : /kaggle/working/mallorn_run/artifacts/lc_clean/lc_clean_summary.csv\n",
      "- Saved config  : /kaggle/working/mallorn_run/artifacts/lc_clean/photometric_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Photometric Cleaning (De-extinction + Negative Flux Safe Transform)\n",
    "# ONE CELL, Kaggle CPU-SAFE, split-wise + chunked\n",
    "#\n",
    "# Prasyarat (sudah ada dari stage sebelumnya):\n",
    "# - iter_lightcurve_chunks (STAGE 3)\n",
    "# - df_train_meta, df_test_meta (STAGE 2)\n",
    "# - ART_DIR, SPLIT_LIST (STAGE 0/1)\n",
    "#\n",
    "# Output:\n",
    "# - /kaggle/working/mallorn_run/artifacts/lc_clean/split_XX/{train|test}/part_*.parquet (atau .csv.gz fallback)\n",
    "# - manifest CSV per split + summary stats\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, math, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings (CPU-safe defaults)\n",
    "# ----------------------------\n",
    "CHUNKSIZE = 350_000          # adjust if needed (bigger = faster but more RAM)\n",
    "SNR_DET = 3.0                # simple detection proxy\n",
    "ERR_EPS = 1e-6               # avoid div-by-zero\n",
    "SCALE_SAMPLE_ROWS_TOTAL = 900_000  # sampling to estimate per-band scales (lightweight)\n",
    "SCALE_SAMPLES_PER_BAND = 60_000    # cap samples stored per band for median\n",
    "WRITE_FORMAT = \"parquet\"     # \"parquet\" recommended; auto-fallback to \"csv.gz\" if parquet fails\n",
    "\n",
    "# For debugging: set a subset of splits to process, e.g. [\"split_01\",\"split_02\"]\n",
    "ONLY_SPLITS = None  # None = process all 20 splits\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Extinction coefficients (R_lambda for LSST-like bands)\n",
    "#    NOTE: If kamu punya koefisien resmi dari notebook Using_the_Data, ganti nilai di sini.\n",
    "#    A_lambda = R_lambda * EBV\n",
    "#    flux_deext = flux * 10^(0.4 * A_lambda)\n",
    "# ----------------------------\n",
    "EXT_RLAMBDA = {\n",
    "    \"u\": 4.8,\n",
    "    \"g\": 3.6,\n",
    "    \"r\": 2.7,\n",
    "    \"i\": 2.1,\n",
    "    \"z\": 1.6,\n",
    "    \"y\": 1.3,\n",
    "}\n",
    "\n",
    "BAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\n",
    "ID2BAND = {v: k for k, v in BAND2ID.items()}\n",
    "\n",
    "# EBV mapping (Series with index object_id) for fast vectorized map\n",
    "EBV_TRAIN_SER = df_train_meta[\"EBV\"]\n",
    "EBV_TEST_SER  = df_test_meta[\"EBV\"]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Estimate per-band flux scale (robust, sample-based) for safe transforms\n",
    "#    We estimate on RAW flux (not de-extincted) to keep this stage fast.\n",
    "# ----------------------------\n",
    "def estimate_band_scale_from_sample(which: str, splits, total_rows=SCALE_SAMPLE_ROWS_TOTAL, per_band_cap=SCALE_SAMPLES_PER_BAND):\n",
    "    \"\"\"\n",
    "    Stream sample abs(flux) from split files to estimate median abs flux per band.\n",
    "    Returns dict: {band: scale} with positive float values.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(2025)\n",
    "    samples = {b: [] for b in BAND2ID.keys()}\n",
    "    seen = 0\n",
    "\n",
    "    for s in splits:\n",
    "        if seen >= total_rows:\n",
    "            break\n",
    "        for ch in iter_lightcurve_chunks(s, which, chunksize=CHUNKSIZE):\n",
    "            # ch columns: object_id,mjd,flux,flux_err,filter\n",
    "            f = ch[\"filter\"].to_numpy(dtype=\"U1\", copy=False)\n",
    "            x = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n",
    "            ax = np.abs(x)\n",
    "\n",
    "            # sample a small fraction to keep memory low\n",
    "            n = len(ch)\n",
    "            if n == 0:\n",
    "                continue\n",
    "            take = int(min(5000, n))  # small per-chunk sample\n",
    "            idx = rng.choice(n, size=take, replace=False)\n",
    "            f_s = f[idx]\n",
    "            ax_s = ax[idx]\n",
    "\n",
    "            for b in BAND2ID.keys():\n",
    "                mask = (f_s == b)\n",
    "                if not np.any(mask):\n",
    "                    continue\n",
    "                vals = ax_s[mask]\n",
    "                if vals.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # append with cap\n",
    "                cur = samples[b]\n",
    "                if len(cur) < per_band_cap:\n",
    "                    need = per_band_cap - len(cur)\n",
    "                    if vals.size > need:\n",
    "                        vals = vals[:need]\n",
    "                    cur.extend(vals.tolist())\n",
    "                # else already full; keep\n",
    "\n",
    "            seen += n\n",
    "            if seen >= total_rows:\n",
    "                break\n",
    "\n",
    "    band_scale = {}\n",
    "    for b in BAND2ID.keys():\n",
    "        arr = np.asarray(samples[b], dtype=np.float32)\n",
    "        arr = arr[np.isfinite(arr)]\n",
    "        arr = arr[arr > 0]\n",
    "        if arr.size == 0:\n",
    "            band_scale[b] = 1.0\n",
    "        else:\n",
    "            med = float(np.median(arr))\n",
    "            # clamp to avoid too small\n",
    "            band_scale[b] = max(med, 1e-3)\n",
    "    return band_scale\n",
    "\n",
    "splits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\n",
    "print(f\"[Stage 4] Estimating per-band flux scales from sample ({SCALE_SAMPLE_ROWS_TOTAL:,} rows max) ...\")\n",
    "BAND_SCALE = estimate_band_scale_from_sample(\"train\", splits_to_use)\n",
    "print(\"[Stage 4] BAND_SCALE (median |flux| per band):\")\n",
    "for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n",
    "    print(f\"  - {b}: {BAND_SCALE[b]:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Chunk photometric cleaning (de-extinction + safe transforms)\n",
    "# ----------------------------\n",
    "def clean_photometric_chunk(ch: pd.DataFrame, ebv_ser: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input chunk columns: object_id,mjd,flux,flux_err,filter (from iter_lightcurve_chunks)\n",
    "    Output columns (float32/int8):\n",
    "      object_id, mjd, band_id, flux_deext, err_deext, flux_asinh, err_log1p, snr, detected\n",
    "    \"\"\"\n",
    "    # Map EBV and extinction coefficient\n",
    "    ebv = ch[\"object_id\"].map(ebv_ser).fillna(0.0).to_numpy(dtype=np.float32)\n",
    "    filt = ch[\"filter\"].to_numpy(dtype=\"U1\", copy=False)\n",
    "\n",
    "    r = pd.Series(filt).map(EXT_RLAMBDA).fillna(0.0).to_numpy(dtype=np.float32)\n",
    "    A = r * ebv  # A_lambda\n",
    "\n",
    "    # de-extinction multiplier: 10^(0.4*A)\n",
    "    mul = np.power(10.0, (0.4 * A).astype(np.float32)).astype(np.float32)\n",
    "\n",
    "    flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    err  = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    # clamp err\n",
    "    err = np.maximum(err, np.float32(ERR_EPS))\n",
    "\n",
    "    flux_deext = (flux * mul).astype(np.float32)\n",
    "    err_deext  = (err  * mul).astype(np.float32)\n",
    "\n",
    "    # per-band scale\n",
    "    scale = pd.Series(filt).map(BAND_SCALE).fillna(1.0).to_numpy(dtype=np.float32)\n",
    "    scale = np.maximum(scale, np.float32(1e-3))\n",
    "\n",
    "    # safe transforms\n",
    "    flux_asinh = np.arcsinh(flux_deext / scale).astype(np.float32)\n",
    "    err_scaled = (err_deext / scale).astype(np.float32)\n",
    "    err_log1p  = np.log1p(err_scaled).astype(np.float32)\n",
    "\n",
    "    snr = (flux_deext / np.maximum(err_deext, np.float32(ERR_EPS))).astype(np.float32)\n",
    "    detected = (snr > np.float32(SNR_DET)).astype(np.int8)\n",
    "\n",
    "    # band_id\n",
    "    band_id = pd.Series(filt).map(BAND2ID).fillna(-1).astype(np.int16).to_numpy()\n",
    "    if np.any(band_id < 0):\n",
    "        bad = sorted(set(pd.Series(filt)[band_id < 0].tolist()))\n",
    "        raise ValueError(f\"Unknown filter values encountered: {bad}\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"object_id\": ch[\"object_id\"].astype(\"string\").to_numpy(),\n",
    "        \"mjd\": ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False),\n",
    "        \"band_id\": band_id.astype(np.int16),\n",
    "        \"flux_deext\": flux_deext,\n",
    "        \"err_deext\": err_deext,\n",
    "        \"flux_asinh\": flux_asinh,\n",
    "        \"err_log1p\": err_log1p,\n",
    "        \"snr\": snr,\n",
    "        \"detected\": detected,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Writer (parquet preferred; fallback csv.gz)\n",
    "# ----------------------------\n",
    "def write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == \"parquet\":\n",
    "        try:\n",
    "            df.to_parquet(out_path, index=False)\n",
    "            return \"parquet\"\n",
    "        except Exception as e:\n",
    "            # fallback\n",
    "            alt = out_path.with_suffix(\".csv.gz\")\n",
    "            df.to_csv(alt, index=False, compression=\"gzip\")\n",
    "            return f\"csv.gz (fallback from parquet: {type(e).__name__})\"\n",
    "    elif fmt == \"csv.gz\":\n",
    "        df.to_csv(out_path.with_suffix(\".csv.gz\"), index=False, compression=\"gzip\")\n",
    "        return \"csv.gz\"\n",
    "    else:\n",
    "        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Process all splits split-wise (stream -> clean -> write parts)\n",
    "# ----------------------------\n",
    "LC_CLEAN_DIR = Path(ART_DIR) / \"lc_clean\"\n",
    "LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary_rows = []\n",
    "manifest_rows = []\n",
    "\n",
    "def process_split(split_name: str, which: str):\n",
    "    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n",
    "    out_dir = LC_CLEAN_DIR / split_name / which\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    part_idx = 0\n",
    "    n_rows_total = 0\n",
    "    n_neg_flux = 0\n",
    "    n_det = 0\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n",
    "        # clean\n",
    "        cleaned = clean_photometric_chunk(ch, ebv_ser)\n",
    "\n",
    "        # simple stats\n",
    "        n_rows = int(len(cleaned))\n",
    "        n_rows_total += n_rows\n",
    "        n_neg_flux += int((cleaned[\"flux_deext\"].to_numpy() < 0).sum())\n",
    "        n_det += int(cleaned[\"detected\"].to_numpy().sum())\n",
    "\n",
    "        # write part\n",
    "        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n",
    "        used_fmt = write_part(cleaned, out_path, WRITE_FORMAT)\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"which\": which,\n",
    "            \"part\": part_idx,\n",
    "            \"path\": str(out_path if used_fmt.startswith(\"parquet\") else out_path.with_suffix(\".csv.gz\")),\n",
    "            \"rows\": n_rows,\n",
    "            \"format\": used_fmt,\n",
    "        })\n",
    "\n",
    "        part_idx += 1\n",
    "\n",
    "        # free memory\n",
    "        del cleaned, ch\n",
    "        if part_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"split\": split_name,\n",
    "        \"which\": which,\n",
    "        \"parts\": part_idx,\n",
    "        \"rows\": n_rows_total,\n",
    "        \"neg_flux_frac\": (n_neg_flux / max(n_rows_total, 1)),\n",
    "        \"det_frac_snr_gt_3\": (n_det / max(n_rows_total, 1)),\n",
    "    })\n",
    "\n",
    "    print(f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | neg%={100*(n_neg_flux/max(n_rows_total,1)):.2f}% | det%={100*(n_det/max(n_rows_total,1)):.2f}%\")\n",
    "\n",
    "print(\"[Stage 4] Building cleaned lightcurve cache (split-wise) ...\")\n",
    "for s in splits_to_use:\n",
    "    process_split(s, \"train\")\n",
    "    process_split(s, \"test\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Save manifests + summary\n",
    "# ----------------------------\n",
    "df_manifest = pd.DataFrame(manifest_rows)\n",
    "df_summary  = pd.DataFrame(summary_rows)\n",
    "\n",
    "manifest_path = LC_CLEAN_DIR / \"lc_clean_manifest.csv\"\n",
    "summary_path  = LC_CLEAN_DIR / \"lc_clean_summary.csv\"\n",
    "\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "# Save coefficients & scales for reproducibility\n",
    "cfg_path = LC_CLEAN_DIR / \"photometric_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "        \"BAND_SCALE\": BAND_SCALE,\n",
    "        \"SNR_DET\": SNR_DET,\n",
    "        \"ERR_EPS\": ERR_EPS,\n",
    "        \"CHUNKSIZE\": CHUNKSIZE,\n",
    "        \"WRITE_FORMAT\": WRITE_FORMAT,\n",
    "        \"ONLY_SPLITS\": splits_to_use,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 4] Done.\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved summary : {summary_path}\")\n",
    "print(f\"- Saved config  : {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Export globals for next stages\n",
    "# ----------------------------\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    \"\"\"Return list of part file paths for cleaned split.\"\"\"\n",
    "    m = df_manifest[(df_manifest[\"split\"] == split_name) & (df_manifest[\"which\"] == which)].sort_values(\"part\")\n",
    "    return m[\"path\"].tolist()\n",
    "\n",
    "globals().update({\n",
    "    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "    \"BAND2ID\": BAND2ID,\n",
    "    \"ID2BAND\": ID2BAND,\n",
    "    \"BAND_SCALE\": BAND_SCALE,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"lc_clean_manifest\": df_manifest,\n",
    "    \"lc_clean_summary\": df_summary,\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5da98",
   "metadata": {
    "papermill": {
     "duration": 0.005587,
     "end_time": "2026-01-01T18:52:53.984780",
     "exception": false,
     "start_time": "2026-01-01T18:52:53.979193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Tokenization (Event-based Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d52fb1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:52:53.998529Z",
     "iopub.status.busy": "2026-01-01T18:52:53.998269Z",
     "iopub.status.idle": "2026-01-01T18:53:06.673406Z",
     "shell.execute_reply": "2026-01-01T18:53:06.672476Z"
    },
    "papermill": {
     "duration": 12.683809,
     "end_time": "2026-01-01T18:53:06.674755",
     "exception": false,
     "start_time": "2026-01-01T18:52:53.990946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 5] Building sequences: split_01/train | expected_objects=155\n",
      "[Stage 5] OK: split_01/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_01/test | expected_objects=364\n",
      "[Stage 5] OK: split_01/test built_objects=364 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_02/train | expected_objects=170\n",
      "[Stage 5] OK: split_02/train built_objects=170 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_02/test | expected_objects=414\n",
      "[Stage 5] OK: split_02/test built_objects=414 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_03/train | expected_objects=138\n",
      "[Stage 5] OK: split_03/train built_objects=138 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_03/test | expected_objects=338\n",
      "[Stage 5] OK: split_03/test built_objects=338 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_04/train | expected_objects=145\n",
      "[Stage 5] OK: split_04/train built_objects=145 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_04/test | expected_objects=332\n",
      "[Stage 5] OK: split_04/test built_objects=332 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_05/train | expected_objects=165\n",
      "[Stage 5] OK: split_05/train built_objects=165 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_05/test | expected_objects=375\n",
      "[Stage 5] OK: split_05/test built_objects=375 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_06/train | expected_objects=155\n",
      "[Stage 5] OK: split_06/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_06/test | expected_objects=374\n",
      "[Stage 5] OK: split_06/test built_objects=374 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_07/train | expected_objects=165\n",
      "[Stage 5] OK: split_07/train built_objects=165 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_07/test | expected_objects=398\n",
      "[Stage 5] OK: split_07/test built_objects=398 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_08/train | expected_objects=162\n",
      "[Stage 5] OK: split_08/train built_objects=162 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_08/test | expected_objects=387\n",
      "[Stage 5] OK: split_08/test built_objects=387 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_09/train | expected_objects=128\n",
      "[Stage 5] OK: split_09/train built_objects=128 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_09/test | expected_objects=289\n",
      "[Stage 5] OK: split_09/test built_objects=289 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_10/train | expected_objects=144\n",
      "[Stage 5] OK: split_10/train built_objects=144 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_10/test | expected_objects=331\n",
      "[Stage 5] OK: split_10/test built_objects=331 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_11/train | expected_objects=146\n",
      "[Stage 5] OK: split_11/train built_objects=146 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_11/test | expected_objects=325\n",
      "[Stage 5] OK: split_11/test built_objects=325 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_12/train | expected_objects=155\n",
      "[Stage 5] OK: split_12/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_12/test | expected_objects=353\n",
      "[Stage 5] OK: split_12/test built_objects=353 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_13/train | expected_objects=143\n",
      "[Stage 5] OK: split_13/train built_objects=143 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_13/test | expected_objects=379\n",
      "[Stage 5] OK: split_13/test built_objects=379 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_14/train | expected_objects=154\n",
      "[Stage 5] OK: split_14/train built_objects=154 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_14/test | expected_objects=351\n",
      "[Stage 5] OK: split_14/test built_objects=351 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_15/train | expected_objects=158\n",
      "[Stage 5] OK: split_15/train built_objects=158 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_15/test | expected_objects=342\n",
      "[Stage 5] OK: split_15/test built_objects=342 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_16/train | expected_objects=155\n",
      "[Stage 5] OK: split_16/train built_objects=155 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_16/test | expected_objects=354\n",
      "[Stage 5] OK: split_16/test built_objects=354 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_17/train | expected_objects=153\n",
      "[Stage 5] OK: split_17/train built_objects=153 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_17/test | expected_objects=351\n",
      "[Stage 5] OK: split_17/test built_objects=351 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_18/train | expected_objects=152\n",
      "[Stage 5] OK: split_18/train built_objects=152 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_18/test | expected_objects=345\n",
      "[Stage 5] OK: split_18/test built_objects=345 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_19/train | expected_objects=147\n",
      "[Stage 5] OK: split_19/train built_objects=147 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_19/test | expected_objects=375\n",
      "[Stage 5] OK: split_19/test built_objects=375 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_20/train | expected_objects=153\n",
      "[Stage 5] OK: split_20/train built_objects=153 | shards=1\n",
      "\n",
      "[Stage 5] Building sequences: split_20/test | expected_objects=358\n",
      "[Stage 5] OK: split_20/test built_objects=358 | shards=1\n",
      "\n",
      "[Stage 5] DONE\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_manifest_train.csv (rows=3,043)\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_manifest_test.csv  (rows=7,135)\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/seq_tokens/seq_config.json\n",
      "\n",
      "[Stage 5] Smoke test object_id=Dornhoth_anwar_melethron\n",
      "- seq_len=184 | X_shape=(184, 6) | bands_unique=[0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL, Kaggle CPU-SAFE)\n",
    "#\n",
    "# Tujuan:\n",
    "# - Mengubah cleaned lightcurve (STAGE 4) -> token sequence per object_id\n",
    "# - Token berbasis event/observasi: 1 baris observasi = 1 token\n",
    "# - Output disimpan dalam shard .npz per split & (train/test)\n",
    "#\n",
    "# Input (dari stage sebelumnya):\n",
    "# - LC_CLEAN_DIR, get_clean_parts, lc_clean_manifest (STAGE 4)\n",
    "# - df_train_meta, df_test_meta, train_ids_by_split, test_ids_by_split, SPLIT_LIST (STAGE 2/3)\n",
    "# - ART_DIR\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/seq_tokens/split_XX/{train|test}/shard_*.npz\n",
    "# - artifacts/seq_tokens/seq_manifest_{train|test}.csv   (mapping object_id -> shard + slice)\n",
    "# - artifacts/seq_tokens/seq_config.json                 (feature spec)\n",
    "#\n",
    "# Catatan:\n",
    "# - Default mencoba \"streaming contiguous object blocks\" (hemat disk, cepat).\n",
    "# - Jika dataset TIDAK contiguous per object_id, otomatis fallback bucket-hash (lebih berat tapi aman).\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, math, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"LC_CLEAN_DIR\", \"get_clean_parts\", \"lc_clean_manifest\",\n",
    "             \"df_train_meta\", \"df_test_meta\", \"train_ids_by_split\", \"test_ids_by_split\",\n",
    "             \"SPLIT_LIST\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 dulu.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings (CPU-safe)\n",
    "# ----------------------------\n",
    "ONLY_SPLITS = None                 # None = proses semua; atau set list [\"split_01\",\"split_02\"] untuk debug\n",
    "COMPRESS_NPZ = False               # True lebih kecil disk tapi jauh lebih lambat di CPU\n",
    "SHARD_MAX_OBJECTS = 1500           # jumlah object per shard file\n",
    "SNR_TANH_SCALE = 10.0              # snr_tanh = tanh(snr / scale)\n",
    "TIME_CLIP_MAX_DAYS = None          # None = no clip; atau mis. 2000.0\n",
    "DROP_BAD_TIME_ROWS = True          # drop rows with NaN/inf mjd\n",
    "FALLBACK_NUM_BUCKETS = 64          # dipakai jika fallback (hash bucket) diperlukan\n",
    "\n",
    "SEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURE_NAMES = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\n",
    "FEATURE_DIM = len(FEATURE_NAMES)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Robust readers for cleaned parts (parquet or csv.gz)\n",
    "# ----------------------------\n",
    "REQ_CLEAN_COLS = {\"object_id\",\"mjd\",\"band_id\",\"flux_asinh\",\"err_log1p\",\"snr\",\"detected\"}\n",
    "\n",
    "def _read_clean_part(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Clean part missing: {p}\")\n",
    "    if p.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    elif p.name.endswith(\".csv.gz\"):\n",
    "        df = pd.read_csv(p, compression=\"gzip\")\n",
    "    else:\n",
    "        # allow plain .csv\n",
    "        df = pd.read_csv(p)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    missing = sorted(list(REQ_CLEAN_COLS - set(df.columns)))\n",
    "    if missing:\n",
    "        raise ValueError(f\"Clean part missing columns {missing}. Found: {list(df.columns)} | file={p}\")\n",
    "    # enforce dtypes lightly\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n",
    "    for c in [\"flux_asinh\",\"err_log1p\",\"snr\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        df = df[np.isfinite(df[\"mjd\"].to_numpy())]\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build tokens for one object (sort by time inside object)\n",
    "# ----------------------------\n",
    "def build_object_tokens(df_obj: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df_obj columns: mjd, band_id, flux_asinh, err_log1p, snr, detected\n",
    "    returns:\n",
    "      X: (L, FEATURE_DIM) float32\n",
    "      B: (L,) int8 band_id\n",
    "    \"\"\"\n",
    "    if df_obj.empty:\n",
    "        return None, None\n",
    "\n",
    "    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n",
    "    flux = df_obj[\"flux_asinh\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    elog = df_obj[\"err_log1p\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "    # sort by mjd, tie-break by band\n",
    "    order = np.lexsort((band, mjd))\n",
    "    mjd = mjd[order]\n",
    "    band = band[order]\n",
    "    flux = flux[order]\n",
    "    elog = elog[order]\n",
    "    snr  = snr[order]\n",
    "    det  = det[order]\n",
    "\n",
    "    # time features\n",
    "    t0 = mjd[0]\n",
    "    t_rel = mjd - t0\n",
    "    # dt: first token dt=0\n",
    "    dt = np.empty_like(t_rel)\n",
    "    dt[0] = 0.0\n",
    "    if len(t_rel) > 1:\n",
    "        dt[1:] = np.maximum(mjd[1:] - mjd[:-1], 0.0)\n",
    "\n",
    "    if TIME_CLIP_MAX_DAYS is not None:\n",
    "        t_rel = np.clip(t_rel, 0.0, float(TIME_CLIP_MAX_DAYS))\n",
    "        dt    = np.clip(dt,    0.0, float(TIME_CLIP_MAX_DAYS))\n",
    "\n",
    "    t_rel_log = np.log1p(t_rel).astype(np.float32)\n",
    "    dt_log    = np.log1p(dt).astype(np.float32)\n",
    "\n",
    "    # robust snr transform\n",
    "    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n",
    "\n",
    "    # detected -> float32\n",
    "    det_f = det.astype(np.float32)\n",
    "\n",
    "    # sanitize other features\n",
    "    flux = np.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    elog = np.nan_to_num(elog, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    X = np.stack([t_rel_log, dt_log, flux, elog, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "    B = band.astype(np.int8)\n",
    "\n",
    "    return X, B\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Shard writer\n",
    "# ----------------------------\n",
    "def save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    obj_arr = np.asarray(object_ids, dtype=\"S\")  # bytes to store in npz\n",
    "    if COMPRESS_NPZ:\n",
    "        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "    else:\n",
    "        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Streaming builder (expects contiguous blocks by object_id across the cleaned parts)\n",
    "# ----------------------------\n",
    "def build_sequences_streaming(split_name: str, which: str, expected_ids: set, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      manifest_rows (list of dict), n_objects_built (int), fallback_needed (bool)\n",
    "    \"\"\"\n",
    "    parts = get_clean_parts(split_name, which)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"No cleaned parts for {split_name}/{which}. Pastikan STAGE 4 sukses.\")\n",
    "\n",
    "    # shard accumulators\n",
    "    manifest_rows = []\n",
    "    shard_idx = 0\n",
    "    batch_obj_ids = []\n",
    "    batch_X_list = []\n",
    "    batch_B_list = []\n",
    "    batch_lengths = []\n",
    "\n",
    "    # streaming buffers for current object\n",
    "    cur_oid = None\n",
    "    cur_buf = []  # list of DataFrames (small blocks)\n",
    "    seen_done = set()  # object_ids that are finalized\n",
    "    fallback_needed = False\n",
    "\n",
    "    def flush_object(oid, buf_blocks):\n",
    "        nonlocal batch_obj_ids, batch_X_list, batch_B_list, batch_lengths\n",
    "        if oid is None or not buf_blocks:\n",
    "            return\n",
    "        df_obj = pd.concat(buf_blocks, ignore_index=True)\n",
    "        # drop rows not expected (safety), but ideally none\n",
    "        if oid not in expected_ids:\n",
    "            return\n",
    "        X, B = build_object_tokens(df_obj)\n",
    "        if X is None:\n",
    "            return\n",
    "        batch_obj_ids.append(oid)\n",
    "        batch_X_list.append(X)\n",
    "        batch_B_list.append(B)\n",
    "        batch_lengths.append(X.shape[0])\n",
    "\n",
    "    def flush_shard():\n",
    "        nonlocal shard_idx, batch_obj_ids, batch_X_list, batch_B_list, batch_lengths, manifest_rows\n",
    "        if not batch_obj_ids:\n",
    "            return\n",
    "        # build concat + offsets\n",
    "        lengths = np.asarray(batch_lengths, dtype=np.int64)\n",
    "        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(lengths)\n",
    "        X_concat = np.concatenate(batch_X_list, axis=0).astype(np.float32)\n",
    "        B_concat = np.concatenate(batch_B_list, axis=0).astype(np.int8)\n",
    "\n",
    "        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n",
    "        save_shard(shard_path, batch_obj_ids, X_concat, B_concat, offsets)\n",
    "\n",
    "        # manifest entries\n",
    "        for i, oid in enumerate(batch_obj_ids):\n",
    "            start = int(offsets[i])\n",
    "            length = int(lengths[i])\n",
    "            manifest_rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(shard_path),\n",
    "                \"start\": start,\n",
    "                \"length\": length\n",
    "            })\n",
    "\n",
    "        shard_idx += 1\n",
    "        # reset batch\n",
    "        batch_obj_ids = []\n",
    "        batch_X_list = []\n",
    "        batch_B_list = []\n",
    "        batch_lengths = []\n",
    "        gc.collect()\n",
    "\n",
    "    # iterate parts sequentially (preserve original order)\n",
    "    for pi, p in enumerate(parts):\n",
    "        df = _read_clean_part(p)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # IMPORTANT: use original row order (do NOT sort by object_id)\n",
    "        oids = df[\"object_id\"].to_numpy(dtype=object, copy=False)\n",
    "\n",
    "        # detect contiguous segments where object_id constant\n",
    "        # boundaries: start indices of segments\n",
    "        change = np.empty(len(oids), dtype=bool)\n",
    "        change[0] = True\n",
    "        change[1:] = oids[1:] != oids[:-1]\n",
    "        seg_starts = np.flatnonzero(change)\n",
    "        seg_ends = np.append(seg_starts[1:], len(oids))\n",
    "\n",
    "        for s_idx, e_idx in zip(seg_starts, seg_ends):\n",
    "            oid = str(oids[s_idx])\n",
    "            block = df.iloc[s_idx:e_idx]\n",
    "\n",
    "            # If we see an oid that already finalized earlier and it's not the current ongoing oid,\n",
    "            # then file is not contiguous-by-object => need fallback\n",
    "            if (oid in seen_done) and (oid != cur_oid):\n",
    "                fallback_needed = True\n",
    "                break\n",
    "\n",
    "            if cur_oid is None:\n",
    "                cur_oid = oid\n",
    "                cur_buf = [block]\n",
    "            elif oid == cur_oid:\n",
    "                cur_buf.append(block)\n",
    "            else:\n",
    "                # finalize previous\n",
    "                flush_object(cur_oid, cur_buf)\n",
    "                seen_done.add(cur_oid)\n",
    "\n",
    "                # flush shard if too big\n",
    "                if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                    flush_shard()\n",
    "\n",
    "                # start new\n",
    "                cur_oid = oid\n",
    "                cur_buf = [block]\n",
    "\n",
    "        del df\n",
    "        if fallback_needed:\n",
    "            break\n",
    "\n",
    "        if (pi + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    # finalize last object\n",
    "    if not fallback_needed:\n",
    "        flush_object(cur_oid, cur_buf)\n",
    "        if cur_oid is not None:\n",
    "            seen_done.add(cur_oid)\n",
    "        flush_shard()\n",
    "\n",
    "    built = len(seen_done.intersection(expected_ids))\n",
    "    return manifest_rows, built, fallback_needed\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Fallback: Hash-bucket builder (robust if not contiguous), temp bucket files then delete\n",
    "# ----------------------------\n",
    "def build_sequences_fallback_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 64):\n",
    "    \"\"\"\n",
    "    Robust method:\n",
    "    - Stream read cleaned parts -> write rows to temporary bucket parquet files based on hash(object_id)\n",
    "    - Process each bucket file: groupby object_id -> build tokens -> write shards\n",
    "    - Delete temp bucket files to save disk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Fallback bucketization needs pyarrow. It seems unavailable.\") from e\n",
    "\n",
    "    parts = get_clean_parts(split_name, which)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"No cleaned parts for {split_name}/{which}.\")\n",
    "\n",
    "    tmp_dir = Path(ART_DIR) / \"tmp_buckets\" / split_name / which\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Parquet writers dict\n",
    "    writers = {}\n",
    "    schemas = {}\n",
    "\n",
    "    def _bucket_series(s: pd.Series) -> np.ndarray:\n",
    "        # fast vectorized pandas hash (stable within this env), mod num_buckets\n",
    "        h = pd.util.hash_pandas_object(s, index=False).to_numpy(dtype=np.uint64, copy=False)\n",
    "        return (h % np.uint64(num_buckets)).astype(np.int16)\n",
    "\n",
    "    # write buckets\n",
    "    for p in parts:\n",
    "        df = _read_clean_part(p)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        # keep only expected ids to reduce disk\n",
    "        df = df[df[\"object_id\"].isin(expected_ids)]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        bidx = _bucket_series(df[\"object_id\"])\n",
    "        df[\"_b\"] = bidx\n",
    "\n",
    "        for b in np.unique(bidx):\n",
    "            sub = df[df[\"_b\"] == b].drop(columns=[\"_b\"])\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            file_path = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n",
    "\n",
    "            table = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "            if int(b) not in writers:\n",
    "                schemas[int(b)] = table.schema\n",
    "                writers[int(b)] = pq.ParquetWriter(file_path, table.schema, compression=\"snappy\")\n",
    "            writers[int(b)].write_table(table)\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "    # close writers\n",
    "    for w in writers.values():\n",
    "        w.close()\n",
    "\n",
    "    # now process each bucket file to create shards\n",
    "    manifest_rows = []\n",
    "    shard_idx = 0\n",
    "\n",
    "    batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n",
    "\n",
    "    def flush_shard_local():\n",
    "        nonlocal shard_idx, batch_obj_ids, batch_X_list, batch_B_list, batch_lengths, manifest_rows\n",
    "        if not batch_obj_ids:\n",
    "            return\n",
    "        lengths = np.asarray(batch_lengths, dtype=np.int64)\n",
    "        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(lengths)\n",
    "        X_concat = np.concatenate(batch_X_list, axis=0).astype(np.float32)\n",
    "        B_concat = np.concatenate(batch_B_list, axis=0).astype(np.int8)\n",
    "\n",
    "        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n",
    "        save_shard(shard_path, batch_obj_ids, X_concat, B_concat, offsets)\n",
    "\n",
    "        for i, oid in enumerate(batch_obj_ids):\n",
    "            manifest_rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(shard_path),\n",
    "                \"start\": int(offsets[i]),\n",
    "                \"length\": int(lengths[i])\n",
    "            })\n",
    "\n",
    "        shard_idx += 1\n",
    "        batch_obj_ids, batch_X_list, batch_B_list, batch_lengths = [], [], [], []\n",
    "        gc.collect()\n",
    "\n",
    "    bucket_files = sorted(tmp_dir.glob(\"bucket_*.parquet\"))\n",
    "    built_ids = set()\n",
    "\n",
    "    for bf in bucket_files:\n",
    "        dfb = pd.read_parquet(bf)\n",
    "        dfb.columns = [c.strip() for c in dfb.columns]\n",
    "        if dfb.empty:\n",
    "            bf.unlink(missing_ok=True)\n",
    "            continue\n",
    "\n",
    "        # groupby object_id (robust)\n",
    "        for oid, g in dfb.groupby(\"object_id\", sort=False):\n",
    "            oid = str(oid)\n",
    "            if oid in built_ids:\n",
    "                continue\n",
    "            X, B = build_object_tokens(g)\n",
    "            if X is None:\n",
    "                continue\n",
    "            batch_obj_ids.append(oid)\n",
    "            batch_X_list.append(X)\n",
    "            batch_B_list.append(B)\n",
    "            batch_lengths.append(X.shape[0])\n",
    "            built_ids.add(oid)\n",
    "\n",
    "            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                flush_shard_local()\n",
    "\n",
    "        # delete bucket file to save disk\n",
    "        bf.unlink(missing_ok=True)\n",
    "        del dfb\n",
    "        gc.collect()\n",
    "\n",
    "    flush_shard_local()\n",
    "\n",
    "    # cleanup tmp dir\n",
    "    try:\n",
    "        tmp_dir.rmdir()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return manifest_rows, len(built_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Run tokenization for all splits (train & test)\n",
    "# ----------------------------\n",
    "splits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLIT_LIST\n",
    "\n",
    "all_manifest_train = []\n",
    "all_manifest_test  = []\n",
    "\n",
    "def expected_set_for(split_name: str, which: str) -> set:\n",
    "    if which == \"train\":\n",
    "        return set(train_ids_by_split[split_name])\n",
    "    else:\n",
    "        return set(test_ids_by_split[split_name])\n",
    "\n",
    "for split_name in splits_to_run:\n",
    "    for which in [\"train\", \"test\"]:\n",
    "        out_dir = SEQ_DIR / split_name / which\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        expected_ids = expected_set_for(split_name, which)\n",
    "        if len(expected_ids) == 0:\n",
    "            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}. Cek log/split mapping.\")\n",
    "\n",
    "        print(f\"\\n[Stage 5] Building sequences: {split_name}/{which} | expected_objects={len(expected_ids):,}\")\n",
    "\n",
    "        manifest_rows, built, fallback_needed = build_sequences_streaming(\n",
    "            split_name=split_name,\n",
    "            which=which,\n",
    "            expected_ids=expected_ids,\n",
    "            out_dir=out_dir\n",
    "        )\n",
    "\n",
    "        # If streaming failed or missing many objects, fallback\n",
    "        if fallback_needed or built != len(expected_ids):\n",
    "            print(f\"[Stage 5] Streaming not safe for {split_name}/{which} \"\n",
    "                  f\"(built={built:,} vs expected={len(expected_ids):,}, fallback_needed={fallback_needed}).\")\n",
    "            print(f\"[Stage 5] Switching to robust bucket fallback (temporary files, then cleaned).\")\n",
    "\n",
    "            # Clear any partial outputs in out_dir to avoid mixing\n",
    "            for f in out_dir.glob(\"shard_*.npz\"):\n",
    "                try: f.unlink()\n",
    "                except Exception: pass\n",
    "\n",
    "            manifest_rows, built2 = build_sequences_fallback_bucket(\n",
    "                split_name=split_name,\n",
    "                which=which,\n",
    "                expected_ids=expected_ids,\n",
    "                out_dir=out_dir,\n",
    "                num_buckets=FALLBACK_NUM_BUCKETS\n",
    "            )\n",
    "            if built2 != len(expected_ids):\n",
    "                raise RuntimeError(f\"Fallback still mismatch for {split_name}/{which}: built={built2:,} expected={len(expected_ids):,}\")\n",
    "            built = built2\n",
    "\n",
    "        print(f\"[Stage 5] OK: {split_name}/{which} built_objects={built:,} | shards={len(list(out_dir.glob('shard_*.npz'))):,}\")\n",
    "\n",
    "        if which == \"train\":\n",
    "            all_manifest_train.extend(manifest_rows)\n",
    "        else:\n",
    "            all_manifest_test.extend(manifest_rows)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save manifests + config\n",
    "# ----------------------------\n",
    "df_m_train = pd.DataFrame(all_manifest_train).sort_values([\"split\",\"shard\",\"start\"]).reset_index(drop=True)\n",
    "df_m_test  = pd.DataFrame(all_manifest_test).sort_values([\"split\",\"shard\",\"start\"]).reset_index(drop=True)\n",
    "\n",
    "mtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\n",
    "mtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\n",
    "df_m_train.to_csv(mtrain_path, index=False)\n",
    "df_m_test.to_csv(mtest_path, index=False)\n",
    "\n",
    "cfg = {\n",
    "    \"feature_names\": FEATURE_NAMES,\n",
    "    \"feature_dim\": FEATURE_DIM,\n",
    "    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n",
    "    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n",
    "    \"compress_npz\": bool(COMPRESS_NPZ),\n",
    "    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n",
    "    \"fallback_num_buckets\": int(FALLBACK_NUM_BUCKETS),\n",
    "}\n",
    "cfg_path = SEQ_DIR / \"seq_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 5] DONE\")\n",
    "print(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\n",
    "print(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Smoke test: load one object sequence\n",
    "# ----------------------------\n",
    "def load_sequence(object_id: str, which: str):\n",
    "    \"\"\"Load one object's sequence from manifest + shard.\"\"\"\n",
    "    object_id = str(object_id).strip()\n",
    "    m = df_m_train if which == \"train\" else df_m_test\n",
    "    row = m[m[\"object_id\"] == object_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n",
    "    r = row.iloc[0]\n",
    "    data = np.load(r[\"shard\"], allow_pickle=False)\n",
    "    start = int(r[\"start\"])\n",
    "    length = int(r[\"length\"])\n",
    "    X = data[\"x\"][start:start+length]\n",
    "    B = data[\"band\"][start:start+length]\n",
    "    return X, B\n",
    "\n",
    "# pick one train object to test\n",
    "_smoke_oid = df_train_meta.index[0]\n",
    "X_sm, B_sm = load_sequence(_smoke_oid, \"train\")\n",
    "print(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\n",
    "print(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n",
    "\n",
    "# Export globals for next stages\n",
    "globals().update({\n",
    "    \"SEQ_DIR\": SEQ_DIR,\n",
    "    \"seq_manifest_train\": df_m_train,\n",
    "    \"seq_manifest_test\": df_m_test,\n",
    "    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n",
    "    \"load_sequence\": load_sequence,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de22019",
   "metadata": {
    "papermill": {
     "duration": 0.006815,
     "end_time": "2026-01-01T18:53:06.688840",
     "exception": false,
     "start_time": "2026-01-01T18:53:06.682025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Length Policy (Padding, Truncation, Windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3d9430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:53:06.703929Z",
     "iopub.status.busy": "2026-01-01T18:53:06.703676Z",
     "iopub.status.idle": "2026-01-01T18:53:08.240399Z",
     "shell.execute_reply": "2026-01-01T18:53:08.239565Z"
    },
    "papermill": {
     "duration": 1.546611,
     "end_time": "2026-01-01T18:53:08.242141",
     "exception": false,
     "start_time": "2026-01-01T18:53:06.695530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN length stats\n",
      "- n_objects=3,043 | min=17 | p50=150 | p90=194 | p95=386 | p99=908 | max=1164\n",
      "\n",
      "TEST length stats\n",
      "- n_objects=7,135 | min=18 | p50=152 | p90=193 | p95=542 | p99=990 | max=1186\n",
      "\n",
      "[Stage 6] Chosen MAX_LEN = 512 (based on p95=542)\n",
      "\n",
      "[Stage 6] Building fixed-length cache (TRAIN)...\n",
      "[Stage 6] TRAIN filled: 3,043 rows (expected ordering size=3,043)\n",
      "\n",
      "[Stage 6] Building fixed-length cache (TEST)...\n",
      "[Stage 6] TEST filled:  7,135 rows (expected ordering size=7,135)\n",
      "\n",
      "[Stage 6] Sanity samples (train):\n",
      "- idx=1360 oid=gwilwileth_adel_amloth orig_len=157 kept=157 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3020 oid=vin_araf_gwador orig_len=151 kept=151 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=3025 oid=ylf_alph_mindon orig_len=167 kept=167 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] Sanity samples (test):\n",
      "- idx=3191 oid=rom_bellas_lebdas orig_len=142 kept=142 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7082 oid=nim_nestad_thor orig_len=161 kept=161 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "- idx=7094 oid=rach_bellas_dol orig_len=156 kept=156 | bands_unique=[0, 1, 2, 3, 4, 5]\n",
      "\n",
      "[Stage 6] DONE\n",
      "- Saved fixed cache dir: /kaggle/working/mallorn_run/artifacts/fixed_seq\n",
      "- Saved config: /kaggle/working/mallorn_run/artifacts/fixed_seq/length_policy_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n",
    "# ONE CELL, Kaggle CPU-SAFE, nyambung dengan STAGE 0..5\n",
    "#\n",
    "# Tujuan:\n",
    "# - Tentukan MAX_LEN otomatis (berdasarkan distribusi panjang sequence)\n",
    "# - Terapkan policy truncation/windowing yang informatif (center around peak)\n",
    "# - Buat fixed-length cache (memmap) untuk TRAIN & TEST:\n",
    "#     X: (N, MAX_LEN, F) float32\n",
    "#     B: (N, MAX_LEN) int8   (band_id)\n",
    "#     M: (N, MAX_LEN) int8   (attention mask: 1=real token, 0=pad)\n",
    "#     y_train: (N,) int8\n",
    "#     ids: (N,) bytes\n",
    "# - Simpan config agar tahap training tidak error karena mismatch shape\n",
    "#\n",
    "# Input:\n",
    "# - seq_manifest_train, seq_manifest_test, SEQ_FEATURE_NAMES, df_train_meta, df_test_meta\n",
    "# - (opsional) df_sub dari STAGE 0 untuk urutan test yang sesuai submission\n",
    "#\n",
    "# Output:\n",
    "# - /kaggle/working/mallorn_run/artifacts/fixed_seq/{train|test}_{X|B|M}.dat\n",
    "# - /kaggle/working/mallorn_run/artifacts/fixed_seq/{train|test}_{ids}.npy\n",
    "# - /kaggle/working/mallorn_run/artifacts/fixed_seq/train_y.npy\n",
    "# - /kaggle/working/mallorn_run/artifacts/fixed_seq/length_policy_config.json\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, math, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"seq_manifest_train\", \"seq_manifest_test\", \"SEQ_FEATURE_NAMES\",\n",
    "             \"df_train_meta\", \"df_test_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 -> 4 -> 5 dulu.\")\n",
    "\n",
    "m_train = seq_manifest_train.copy()\n",
    "m_test  = seq_manifest_test.copy()\n",
    "\n",
    "# feature indices (must match STAGE 5)\n",
    "feat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n",
    "REQ_FEATS = [\"t_rel_log\", \"dt_log\", \"flux_asinh\", \"err_log1p\", \"snr_tanh\", \"detected\"]\n",
    "for k in REQ_FEATS:\n",
    "    if k not in feat:\n",
    "        raise ValueError(f\"SEQ_FEATURE_NAMES must include '{k}'. Found: {SEQ_FEATURE_NAMES}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Inspect length distribution -> choose MAX_LEN (CPU-friendly)\n",
    "# ----------------------------\n",
    "def describe_lengths(m: pd.DataFrame, name: str):\n",
    "    L = m[\"length\"].to_numpy(dtype=np.int32, copy=False)\n",
    "    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n",
    "    print(f\"\\n{name} length stats\")\n",
    "    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[8])} | p95={int(q[9])} | p99={int(q[10])} | max={int(q[-1])}\")\n",
    "    return q\n",
    "\n",
    "q_tr = describe_lengths(m_train, \"TRAIN\")\n",
    "q_te = describe_lengths(m_test,  \"TEST\")\n",
    "\n",
    "# auto pick based on max(p95_train, p95_test) rounded up to multiple of 32, capped for CPU\n",
    "p95 = int(max(q_tr[9], q_te[9]))\n",
    "def round_up(x, base=32):\n",
    "    return int(base * math.ceil(x / base))\n",
    "\n",
    "# CPU-safe caps:\n",
    "# - if p95 <= 256 => 256\n",
    "# - elif <= 384 => 384\n",
    "# - else <= 512 => 512\n",
    "if p95 <= 256:\n",
    "    MAX_LEN = 256\n",
    "elif p95 <= 384:\n",
    "    MAX_LEN = 384\n",
    "else:\n",
    "    MAX_LEN = 512\n",
    "\n",
    "# If you want to force smaller on CPU, set here:\n",
    "FORCE_MAX_LEN = None  # e.g. 256\n",
    "if FORCE_MAX_LEN is not None:\n",
    "    MAX_LEN = int(FORCE_MAX_LEN)\n",
    "\n",
    "print(f\"\\n[Stage 6] Chosen MAX_LEN = {MAX_LEN} (based on p95={p95})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Windowing / truncation policy (informative on TDE-like peaks)\n",
    "# ----------------------------\n",
    "# Policy:\n",
    "# - compute token score = w1*|snr_tanh| + w2*|flux_asinh| + w3*detected\n",
    "# - center = argmax(score)\n",
    "# - take window [center - MAX_LEN//2, center + MAX_LEN//2]\n",
    "# - clamp to valid range\n",
    "W_SNR = 1.0\n",
    "W_FLX = 0.35\n",
    "W_DET = 0.25\n",
    "\n",
    "def select_window(X: np.ndarray, max_len: int) -> tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Returns (start, end, center).\n",
    "    X shape (L, F).\n",
    "    \"\"\"\n",
    "    L = int(X.shape[0])\n",
    "    if L <= max_len:\n",
    "        return 0, L, 0\n",
    "\n",
    "    snr = np.abs(X[:, feat[\"snr_tanh\"]])\n",
    "    flx = np.abs(X[:, feat[\"flux_asinh\"]])\n",
    "    det = X[:, feat[\"detected\"]]\n",
    "\n",
    "    score = (W_SNR * snr) + (W_FLX * flx) + (W_DET * det)\n",
    "    # if all zeros (rare), fallback to middle\n",
    "    if not np.isfinite(score).any() or float(score.max()) <= 0.0:\n",
    "        center = L // 2\n",
    "    else:\n",
    "        center = int(np.argmax(score))\n",
    "\n",
    "    half = max_len // 2\n",
    "    start = center - half\n",
    "    start = max(0, start)\n",
    "    start = min(start, L - max_len)\n",
    "    end = start + max_len\n",
    "    return start, end, center\n",
    "\n",
    "def pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      Xp: (max_len, F) float32\n",
    "      Bp: (max_len,) int8\n",
    "      Mp: (max_len,) int8  (1=real token)\n",
    "      orig_len, win_start, win_end\n",
    "    \"\"\"\n",
    "    L = int(X.shape[0])\n",
    "    if L <= 0:\n",
    "        # safety: should not happen\n",
    "        Xp = np.zeros((max_len, X.shape[1]), dtype=np.float32)\n",
    "        Bp = np.zeros((max_len,), dtype=np.int8)\n",
    "        Mp = np.zeros((max_len,), dtype=np.int8)\n",
    "        return Xp, Bp, Mp, 0, 0, 0\n",
    "\n",
    "    s, e, _ = select_window(X, max_len=max_len)\n",
    "    Xw = X[s:e]\n",
    "    Bw = B[s:e]\n",
    "    lw = int(Xw.shape[0])\n",
    "\n",
    "    Xp = np.zeros((max_len, X.shape[1]), dtype=np.float32)\n",
    "    Bp = np.zeros((max_len,), dtype=np.int8)\n",
    "    Mp = np.zeros((max_len,), dtype=np.int8)\n",
    "\n",
    "    Xp[:lw] = Xw.astype(np.float32, copy=False)\n",
    "    Bp[:lw] = Bw.astype(np.int8, copy=False)\n",
    "    Mp[:lw] = 1\n",
    "\n",
    "    return Xp, Bp, Mp, L, s, e\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Fixed cache builder (efficient: process per shard)\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(ART_DIR) / \"fixed_seq\"\n",
    "FIX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Decide ordering for train/test arrays (important!)\n",
    "# - train: df_train_meta.index order (stable)\n",
    "train_ids = df_train_meta.index.to_list()\n",
    "y_train = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# - test: if df_sub exists, follow it (ensures submission order); else df_test_meta.index\n",
    "if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in df_sub.columns:\n",
    "    test_ids = df_sub[\"object_id\"].astype(str).str.strip().to_list()\n",
    "else:\n",
    "    test_ids = df_test_meta.index.to_list()\n",
    "\n",
    "# mapping to row index\n",
    "train_row = {oid: i for i, oid in enumerate(train_ids)}\n",
    "test_row  = {oid: i for i, oid in enumerate(test_ids)}\n",
    "\n",
    "NTR = len(train_ids)\n",
    "NTE = len(test_ids)\n",
    "F = len(SEQ_FEATURE_NAMES)\n",
    "\n",
    "# memmap paths\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "test_X_path  = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path  = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path  = FIX_DIR / \"test_M.dat\"\n",
    "\n",
    "# metadata arrays\n",
    "train_len_path = FIX_DIR / \"train_origlen.npy\"\n",
    "train_win_path = FIX_DIR / \"train_winstart.npy\"\n",
    "test_len_path  = FIX_DIR / \"test_origlen.npy\"\n",
    "test_win_path  = FIX_DIR / \"test_winstart.npy\"\n",
    "\n",
    "# create memmaps\n",
    "Xtr = np.memmap(train_X_path, dtype=np.float32, mode=\"w+\", shape=(NTR, MAX_LEN, F))\n",
    "Btr = np.memmap(train_B_path, dtype=np.int8,   mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "Mtr = np.memmap(train_M_path, dtype=np.int8,   mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=np.float32, mode=\"w+\", shape=(NTE, MAX_LEN, F))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,   mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,   mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "\n",
    "origlen_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "winstart_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "origlen_te = np.zeros((NTE,), dtype=np.int32)\n",
    "winstart_te = np.zeros((NTE,), dtype=np.int32)\n",
    "\n",
    "def process_manifest_into_memmap(m: pd.DataFrame, which: str):\n",
    "    \"\"\"\n",
    "    Process manifest (train/test) into the appropriate memmaps by reading each shard once.\n",
    "    \"\"\"\n",
    "    if which == \"train\":\n",
    "        row_map = train_row\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        origlen, winstart = origlen_tr, winstart_tr\n",
    "        expected_n = NTR\n",
    "    else:\n",
    "        row_map = test_row\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        origlen, winstart = origlen_te, winstart_te\n",
    "        expected_n = NTE\n",
    "\n",
    "    filled = 0\n",
    "    # group by shard to minimize IO\n",
    "    for shard_path, g in m.groupby(\"shard\", sort=False):\n",
    "        shard_path = str(shard_path)\n",
    "        data = np.load(shard_path, allow_pickle=False)\n",
    "        x_all = data[\"x\"]      # (total_tokens, F)\n",
    "        b_all = data[\"band\"]   # (total_tokens,)\n",
    "        # loop rows in this shard group\n",
    "        for _, r in g.iterrows():\n",
    "            oid = str(r[\"object_id\"])\n",
    "            idx = row_map.get(oid, None)\n",
    "            if idx is None:\n",
    "                # object_id not requested in ordering (should not happen, but skip safely)\n",
    "                continue\n",
    "            start = int(r[\"start\"])\n",
    "            length = int(r[\"length\"])\n",
    "            if length <= 0:\n",
    "                continue\n",
    "\n",
    "            X = x_all[start:start+length]\n",
    "            B = b_all[start:start+length]\n",
    "\n",
    "            Xp, Bp, Mp, L0, ws, _ = pad_to_fixed(X, B, max_len=MAX_LEN)\n",
    "\n",
    "            Xmm[idx, :, :] = Xp\n",
    "            Bmm[idx, :] = Bp\n",
    "            Mmm[idx, :] = Mp\n",
    "            origlen[idx] = int(L0)\n",
    "            winstart[idx] = int(ws)\n",
    "            filled += 1\n",
    "\n",
    "        del data\n",
    "        if filled % 2000 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    # sanity: some ids might not exist in manifest (should be 0 if pipeline correct)\n",
    "    return filled, expected_n\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed-length cache (TRAIN)...\")\n",
    "filled_tr, exp_tr = process_manifest_into_memmap(m_train, \"train\")\n",
    "print(f\"[Stage 6] TRAIN filled: {filled_tr:,} rows (expected ordering size={exp_tr:,})\")\n",
    "\n",
    "print(\"\\n[Stage 6] Building fixed-length cache (TEST)...\")\n",
    "filled_te, exp_te = process_manifest_into_memmap(m_test, \"test\")\n",
    "print(f\"[Stage 6] TEST filled:  {filled_te:,} rows (expected ordering size={exp_te:,})\")\n",
    "\n",
    "# Flush memmaps\n",
    "Xtr.flush(); Btr.flush(); Mtr.flush()\n",
    "Xte.flush(); Bte.flush(); Mte.flush()\n",
    "\n",
    "# Save ids + y + meta\n",
    "np.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids, dtype=\"S\"))\n",
    "np.save(FIX_DIR / \"train_y.npy\",   y_train)\n",
    "\n",
    "np.save(train_len_path, origlen_tr)\n",
    "np.save(train_win_path, winstart_tr)\n",
    "np.save(test_len_path,  origlen_te)\n",
    "np.save(test_win_path,  winstart_te)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Sanity checks (anti-error)\n",
    "# ----------------------------\n",
    "def sanity_check(which: str, n_show: int = 3):\n",
    "    if which == \"train\":\n",
    "        Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "        ids = train_ids\n",
    "        ol = origlen_tr\n",
    "    else:\n",
    "        Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "        ids = test_ids\n",
    "        ol = origlen_te\n",
    "\n",
    "    # check masks and shapes\n",
    "    assert Xmm.shape[1] == MAX_LEN and Xmm.shape[2] == F\n",
    "    assert Bmm.shape[1] == MAX_LEN\n",
    "    assert Mmm.shape[1] == MAX_LEN\n",
    "\n",
    "    # show a few random examples\n",
    "    rng = np.random.default_rng(2025)\n",
    "    idxs = rng.choice(len(ids), size=min(n_show, len(ids)), replace=False)\n",
    "    print(f\"\\n[Stage 6] Sanity samples ({which}):\")\n",
    "    for i in idxs:\n",
    "        msum = int(Mmm[i].sum())\n",
    "        print(f\"- idx={i} oid={ids[i]} orig_len={int(ol[i])} kept={msum} | bands_unique={sorted(set(Bmm[i,:msum].tolist()))}\")\n",
    "\n",
    "sanity_check(\"train\", n_show=3)\n",
    "sanity_check(\"test\", n_show=3)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Save config\n",
    "# ----------------------------\n",
    "policy_cfg = {\n",
    "    \"max_len\": int(MAX_LEN),\n",
    "    \"feature_names\": list(SEQ_FEATURE_NAMES),\n",
    "    \"weights\": {\"snr\": float(W_SNR), \"flux\": float(W_FLX), \"detected\": float(W_DET)},\n",
    "    \"snr_tanh_scale_used_in_stage5\": float(globals().get(\"SNR_TANH_SCALE\", 10.0)) if \"SNR_TANH_SCALE\" in globals() else None,\n",
    "    \"train_order\": \"df_train_meta.index\",\n",
    "    \"test_order\": \"df_sub.object_id\" if (\"df_sub\" in globals() and \"object_id\" in df_sub.columns) else \"df_test_meta.index\",\n",
    "    \"files\": {\n",
    "        \"train_X\": str(train_X_path),\n",
    "        \"train_B\": str(train_B_path),\n",
    "        \"train_M\": str(train_M_path),\n",
    "        \"train_y\": str(FIX_DIR / \"train_y.npy\"),\n",
    "        \"train_ids\": str(FIX_DIR / \"train_ids.npy\"),\n",
    "        \"test_X\": str(test_X_path),\n",
    "        \"test_B\": str(test_B_path),\n",
    "        \"test_M\": str(test_M_path),\n",
    "        \"test_ids\": str(FIX_DIR / \"test_ids.npy\"),\n",
    "    }\n",
    "}\n",
    "cfg_path = FIX_DIR / \"length_policy_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(policy_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 6] DONE\")\n",
    "print(f\"- Saved fixed cache dir: {FIX_DIR}\")\n",
    "print(f\"- Saved config: {cfg_path}\")\n",
    "\n",
    "# Export globals for training stage\n",
    "globals().update({\n",
    "    \"FIX_DIR\": FIX_DIR,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"FIX_TRAIN_X_PATH\": train_X_path,\n",
    "    \"FIX_TRAIN_B_PATH\": train_B_path,\n",
    "    \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "    \"FIX_TEST_X_PATH\": test_X_path,\n",
    "    \"FIX_TEST_B_PATH\": test_B_path,\n",
    "    \"FIX_TEST_M_PATH\": test_M_path,\n",
    "    \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "    \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "    \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "    \"FIX_POLICY_CFG_PATH\": cfg_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a3e3d",
   "metadata": {
    "papermill": {
     "duration": 0.0073,
     "end_time": "2026-01-01T18:53:08.257298",
     "exception": false,
     "start_time": "2026-01-01T18:53:08.249998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Split (Object-Level, Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70038415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:53:08.273199Z",
     "iopub.status.busy": "2026-01-01T18:53:08.272918Z",
     "iopub.status.idle": "2026-01-01T18:53:09.812444Z",
     "shell.execute_reply": "2026-01-01T18:53:09.811606Z"
    },
    "papermill": {
     "duration": 1.549189,
     "end_time": "2026-01-01T18:53:09.813651",
     "exception": false,
     "start_time": "2026-01-01T18:53:08.264462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 7] Building StratifiedKFold: n_splits=5 | N=3,043 | pos=148 | neg=2,895 | seed=2025\n",
      "\n",
      "[Stage 7] CV split OK\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_folds.csv\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_folds.npz\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/artifacts/cv/cv_config.json\n",
      "Total: N=3043 | pos=148 | neg=2895 | pos%=4.8636%\n",
      "Per-fold distribution:\n",
      "- fold 0: n=   609 | pos=   30 | neg=   579 | pos%= 4.9261%\n",
      "- fold 1: n=   609 | pos=   30 | neg=   579 | pos%= 4.9261%\n",
      "- fold 2: n=   609 | pos=   30 | neg=   579 | pos%= 4.9261%\n",
      "- fold 3: n=   608 | pos=   29 | neg=   579 | pos%= 4.7697%\n",
      "- fold 4: n=   608 | pos=   29 | neg=   579 | pos%= 4.7697%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL, Kaggle CPU-SAFE)\n",
    "#\n",
    "# Tujuan:\n",
    "# - Buat Stratified K-Fold di level object_id (bukan per baris lightcurve)\n",
    "# - Konsisten dengan urutan TRAIN yang dipakai di STAGE 6 (fixed_seq/train_ids.npy)\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/cv_folds.csv                (object_id -> fold)\n",
    "# - artifacts/cv_folds.npz                (val_idx per fold, untuk training cepat)\n",
    "# - artifacts/cv_report.txt               (ringkasan distribusi kelas per fold)\n",
    "# - globals: fold_assign, folds (list of dict)\n",
    "# ============================================================\n",
    "\n",
    "import gc, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"df_train_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 2 dulu (Load and Validate Train/Test Logs).\")\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "\n",
    "# Prefer using the same ordering as STAGE 6 fixed cache (if available)\n",
    "train_ids = None\n",
    "if \"FIX_DIR\" in globals():\n",
    "    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        train_ids = np.load(p, allow_pickle=False).astype(\"S\").astype(str).tolist()\n",
    "\n",
    "if train_ids is None:\n",
    "    # fallback: df_train_meta index order\n",
    "    train_ids = df_train_meta.index.astype(str).tolist()\n",
    "\n",
    "# y aligned to train_ids\n",
    "y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "N = len(train_ids)\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "if pos == 0 or neg == 0:\n",
    "    raise RuntimeError(f\"Invalid class distribution: pos={pos}, neg={neg}. Tidak bisa Stratified CV.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Choose n_splits safely (anti-error when positives are very few)\n",
    "# ----------------------------\n",
    "DEFAULT_SPLITS = 5\n",
    "# Ensure each fold can contain at least 1 positive and 1 negative\n",
    "max_splits_by_pos = pos\n",
    "max_splits_by_neg = neg\n",
    "n_splits = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg)\n",
    "\n",
    "if n_splits < 2:\n",
    "    raise RuntimeError(\n",
    "        f\"Terlalu sedikit sampel untuk CV stratified. pos={pos}, neg={neg}. \"\n",
    "        \"Minimal butuh >=2 per kelas untuk KFold.\"\n",
    "    )\n",
    "\n",
    "print(f\"[Stage 7] Building StratifiedKFold: n_splits={n_splits} | N={N:,} | pos={pos:,} | neg={neg:,} | seed={SEED}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build folds\n",
    "# ----------------------------\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn tidak tersedia. Pastikan Kaggle environment punya sklearn.\") from e\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_assign = np.full(N, -1, dtype=np.int16)\n",
    "folds = []\n",
    "\n",
    "for fold, (_, val_idx) in enumerate(skf.split(np.zeros(N), y)):\n",
    "    fold_assign[val_idx] = fold\n",
    "    folds.append({\n",
    "        \"fold\": int(fold),\n",
    "        \"val_idx\": val_idx.astype(np.int32),\n",
    "    })\n",
    "\n",
    "if (fold_assign < 0).any():\n",
    "    raise RuntimeError(\"Fold assignment masih ada -1 (ada object belum ter-assign). Ini tidak seharusnya terjadi.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Validate distribution per fold (anti-silent-bug)\n",
    "# ----------------------------\n",
    "lines = []\n",
    "lines.append(f\"StratifiedKFold n_splits={n_splits} seed={SEED}\")\n",
    "lines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.4f}%\")\n",
    "lines.append(\"Per-fold distribution:\")\n",
    "\n",
    "ok = True\n",
    "for f in range(n_splits):\n",
    "    idx = np.where(fold_assign == f)[0]\n",
    "    yf = y[idx]\n",
    "    pf = int((yf == 1).sum())\n",
    "    nf = int((yf == 0).sum())\n",
    "    lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:7.4f}%\")\n",
    "    # basic checks\n",
    "    if pf == 0 or nf == 0:\n",
    "        ok = False\n",
    "\n",
    "if not ok:\n",
    "    raise RuntimeError(\n",
    "        \"Ada fold yang tidak mengandung salah satu kelas (pos=0 atau neg=0). \"\n",
    "        \"Coba turunkan n_splits atau pastikan stratify berjalan benar.\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Save artifacts\n",
    "# ----------------------------\n",
    "ART_DIR = Path(ART_DIR)\n",
    "cv_dir = ART_DIR / \"cv\"\n",
    "cv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\n",
    "folds_csv = cv_dir / \"cv_folds.csv\"\n",
    "df_folds.to_csv(folds_csv, index=False)\n",
    "\n",
    "# Save val_idx per fold in one npz (train_idx can be derived as ~val_idx)\n",
    "npz_path = cv_dir / \"cv_folds.npz\"\n",
    "npz_kwargs = {f\"val_idx_{f}\": folds[f][\"val_idx\"] for f in range(n_splits)}\n",
    "np.savez(npz_path, **npz_kwargs)\n",
    "\n",
    "report_path = cv_dir / \"cv_report.txt\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "cfg_path = cv_dir / \"cv_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"seed\": SEED,\n",
    "            \"n_splits\": int(n_splits),\n",
    "            \"order_source\": \"fixed_seq/train_ids.npy\" if (\"FIX_DIR\" in globals() and (Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\").exists()) else \"df_train_meta.index\",\n",
    "            \"artifacts\": {\n",
    "                \"folds_csv\": str(folds_csv),\n",
    "                \"folds_npz\": str(npz_path),\n",
    "                \"report_txt\": str(report_path),\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n[Stage 7] CV split OK\")\n",
    "print(f\"- Saved: {folds_csv}\")\n",
    "print(f\"- Saved: {npz_path}\")\n",
    "print(f\"- Saved: {report_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "print(\"\\n\".join(lines[-(n_splits+2):]))  # show short tail\n",
    "\n",
    "# Export globals for next stage\n",
    "globals().update({\n",
    "    \"CV_DIR\": cv_dir,\n",
    "    \"n_splits\": n_splits,\n",
    "    \"train_ids_ordered\": train_ids,  # matches fold_assign\n",
    "    \"y_ordered\": y,\n",
    "    \"fold_assign\": fold_assign,\n",
    "    \"folds\": folds,\n",
    "    \"CV_FOLDS_CSV\": folds_csv,\n",
    "    \"CV_FOLDS_NPZ\": npz_path,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe4c55",
   "metadata": {
    "papermill": {
     "duration": 0.007093,
     "end_time": "2026-01-01T18:53:09.828227",
     "exception": false,
     "start_time": "2026-01-01T18:53:09.821134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Multiband Event Transformer (CPU-Safe Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48c41520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:53:09.844867Z",
     "iopub.status.busy": "2026-01-01T18:53:09.843891Z",
     "iopub.status.idle": "2026-01-01T20:50:27.015962Z",
     "shell.execute_reply": "2026-01-01T20:50:27.014590Z"
    },
    "papermill": {
     "duration": 7037.182344,
     "end_time": "2026-01-01T20:50:27.017612",
     "exception": false,
     "start_time": "2026-01-01T18:53:09.835268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8] TRAIN CONFIG (CPU)\n",
      "- N=3,043 | pos=148 | neg=2,895 | pos_weight=19.5608\n",
      "- Model: d_model=128 heads=4 layers=2 dropout=0.1\n",
      "- Batch=16 grad_accum=2 epochs=10 lr=0.0003\n",
      "- Saved cfg: /kaggle/working/mallorn_run/logs/train_cfg.json\n",
      "- Saved global scaler: /kaggle/working/mallorn_run/logs/global_scaler.json\n",
      "\n",
      "[Stage 8] FOLD 0/4 | train=2,434 val=609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17/1596740308.py:138: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  torch.from_numpy(X),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | train_loss=1.35337 | val_loss=1.30779 | f1@0.5=0.0986 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.33828 | val_loss=1.31824 | f1@0.5=0.0000 | best_epoch=1 | patience_left=2\n",
      "  epoch 03 | train_loss=1.31927 | val_loss=1.31393 | f1@0.5=0.0990 | best_epoch=1 | patience_left=1\n",
      "  epoch 04 | train_loss=1.32508 | val_loss=1.32877 | f1@0.5=0.0997 | best_epoch=1 | patience_left=0\n",
      "\n",
      "[Stage 8] FOLD 1/4 | train=2,434 val=609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | train_loss=1.34358 | val_loss=1.67801 | f1@0.5=0.0000 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.29413 | val_loss=1.58700 | f1@0.5=0.1342 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.30394 | val_loss=1.56567 | f1@0.5=0.1504 | best_epoch=3 | patience_left=3\n",
      "  epoch 04 | train_loss=1.23801 | val_loss=1.78326 | f1@0.5=0.0230 | best_epoch=3 | patience_left=2\n",
      "  epoch 05 | train_loss=1.23231 | val_loss=1.22300 | f1@0.5=0.1521 | best_epoch=5 | patience_left=3\n",
      "  epoch 06 | train_loss=1.18833 | val_loss=1.30282 | f1@0.5=0.1581 | best_epoch=5 | patience_left=2\n",
      "  epoch 07 | train_loss=1.15087 | val_loss=5.10798 | f1@0.5=0.0000 | best_epoch=5 | patience_left=1\n",
      "  epoch 08 | train_loss=1.49521 | val_loss=1.18517 | f1@0.5=0.1641 | best_epoch=8 | patience_left=3\n",
      "  epoch 09 | train_loss=1.11428 | val_loss=1.24447 | f1@0.5=0.1914 | best_epoch=8 | patience_left=2\n",
      "  epoch 10 | train_loss=1.15794 | val_loss=1.52796 | f1@0.5=0.2222 | best_epoch=8 | patience_left=1\n",
      "\n",
      "[Stage 8] FOLD 2/4 | train=2,434 val=609\n",
      "  epoch 01 | train_loss=1.33073 | val_loss=1.29146 | f1@0.5=0.1160 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.33216 | val_loss=1.27502 | f1@0.5=0.1214 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.32085 | val_loss=1.23072 | f1@0.5=0.1264 | best_epoch=3 | patience_left=3\n",
      "  epoch 04 | train_loss=1.25474 | val_loss=1.18032 | f1@0.5=0.1695 | best_epoch=4 | patience_left=3\n",
      "  epoch 05 | train_loss=1.20332 | val_loss=1.14509 | f1@0.5=0.2024 | best_epoch=5 | patience_left=3\n",
      "  epoch 06 | train_loss=1.30633 | val_loss=3.68490 | f1@0.5=0.0000 | best_epoch=5 | patience_left=2\n",
      "  epoch 07 | train_loss=1.60173 | val_loss=1.08112 | f1@0.5=0.1655 | best_epoch=7 | patience_left=3\n",
      "  epoch 08 | train_loss=1.08686 | val_loss=1.29846 | f1@0.5=0.1925 | best_epoch=7 | patience_left=2\n",
      "  epoch 09 | train_loss=1.63217 | val_loss=1.03940 | f1@0.5=0.1507 | best_epoch=9 | patience_left=3\n",
      "  epoch 10 | train_loss=1.32925 | val_loss=1.19375 | f1@0.5=0.1333 | best_epoch=9 | patience_left=2\n",
      "\n",
      "[Stage 8] FOLD 3/4 | train=2,435 val=608\n",
      "  epoch 01 | train_loss=1.38558 | val_loss=1.28828 | f1@0.5=0.0000 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.31241 | val_loss=1.23478 | f1@0.5=0.1162 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.30191 | val_loss=1.21420 | f1@0.5=0.1199 | best_epoch=3 | patience_left=3\n",
      "  epoch 04 | train_loss=1.28641 | val_loss=1.12919 | f1@0.5=0.1394 | best_epoch=4 | patience_left=3\n",
      "  epoch 05 | train_loss=1.19427 | val_loss=1.08103 | f1@0.5=0.1586 | best_epoch=5 | patience_left=3\n",
      "  epoch 06 | train_loss=1.10426 | val_loss=1.09843 | f1@0.5=0.1786 | best_epoch=5 | patience_left=2\n",
      "  epoch 07 | train_loss=1.17521 | val_loss=1.44170 | f1@0.5=0.1983 | best_epoch=5 | patience_left=1\n",
      "  epoch 08 | train_loss=1.25278 | val_loss=1.24641 | f1@0.5=0.1793 | best_epoch=5 | patience_left=0\n",
      "\n",
      "[Stage 8] FOLD 4/4 | train=2,435 val=608\n",
      "  epoch 01 | train_loss=1.39007 | val_loss=1.30840 | f1@0.5=0.0000 | best_epoch=1 | patience_left=3\n",
      "  epoch 02 | train_loss=1.32727 | val_loss=1.25011 | f1@0.5=0.1195 | best_epoch=2 | patience_left=3\n",
      "  epoch 03 | train_loss=1.49774 | val_loss=1.83721 | f1@0.5=0.0000 | best_epoch=2 | patience_left=2\n",
      "  epoch 04 | train_loss=1.27608 | val_loss=1.24957 | f1@0.5=0.1135 | best_epoch=4 | patience_left=3\n",
      "  epoch 05 | train_loss=1.34217 | val_loss=1.19123 | f1@0.5=0.1419 | best_epoch=5 | patience_left=3\n",
      "  epoch 06 | train_loss=1.15886 | val_loss=1.29638 | f1@0.5=0.1473 | best_epoch=5 | patience_left=2\n",
      "  epoch 07 | train_loss=1.19235 | val_loss=1.28861 | f1@0.5=0.1438 | best_epoch=5 | patience_left=1\n",
      "  epoch 08 | train_loss=1.21839 | val_loss=1.05910 | f1@0.5=0.1484 | best_epoch=8 | patience_left=3\n",
      "  epoch 09 | train_loss=1.14409 | val_loss=1.33563 | f1@0.5=0.1373 | best_epoch=8 | patience_left=2\n",
      "  epoch 10 | train_loss=1.02743 | val_loss=2.96789 | f1@0.5=0.1053 | best_epoch=8 | patience_left=1\n",
      "\n",
      "[Stage 8] CV TRAIN DONE\n",
      "- elapsed: 117.28 min\n",
      "- OOF saved: /kaggle/working/mallorn_run/oof/oof_prob.npy\n",
      "- OOF saved: /kaggle/working/mallorn_run/oof/oof_prob.csv\n",
      "- fold metrics: /kaggle/working/mallorn_run/oof/fold_metrics.json\n",
      "- OOF f1@0.5 (rough): 0.1359\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 8 — Train Multiband Event Transformer (CPU-Safe Configuration) (ONE CELL)\n",
    "# - Kaggle Web (CPU only): small model, small batch, num_workers=0\n",
    "# - Trains Stratified CV folds from STAGE 7\n",
    "# - Saves best checkpoint per fold + OOF probabilities (for later threshold tuning)\n",
    "#\n",
    "# Requires globals from previous stages:\n",
    "#   FIX_DIR, MAX_LEN, SEQ_FEATURE_NAMES\n",
    "#   df_train_meta\n",
    "#   n_splits, folds, train_ids_ordered, y_ordered\n",
    "#   CKPT_DIR, OOF_DIR, LOG_DIR\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, math, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\n",
    "    \"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\n",
    "    \"df_train_meta\",\n",
    "    \"n_splits\",\"folds\",\"train_ids_ordered\",\"y_ordered\",\n",
    "    \"CKPT_DIR\",\"OOF_DIR\",\"LOG_DIR\",\n",
    "]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Imports (torch) + CPU safety\n",
    "# ----------------------------\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# CPU thread guard (avoid oversubscription)\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open fixed-length memmaps (do NOT load into RAM)\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(FIX_DIR)\n",
    "train_ids = list(train_ids_ordered)\n",
    "y = np.asarray(y_ordered, dtype=np.int8)\n",
    "N = len(train_ids)\n",
    "L = int(MAX_LEN)\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "\n",
    "for p in [train_X_path, train_B_path, train_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "X_mm = np.memmap(train_X_path, dtype=np.float32, mode=\"r\", shape=(N, L, Fdim))\n",
    "B_mm = np.memmap(train_B_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "M_mm = np.memmap(train_M_path, dtype=np.int8,   mode=\"r\", shape=(N, L))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build global features (object-level) aligned to train_ids\n",
    "# ----------------------------\n",
    "# Use only safe, available columns; fill missing with 0\n",
    "G_COLS = [\"Z\", \"Z_err\", \"EBV\", \"Z_missing\", \"Z_err_missing\", \"EBV_missing\", \"is_photoz\"]\n",
    "for c in G_COLS:\n",
    "    if c not in df_train_meta.columns:\n",
    "        df_train_meta[c] = 0.0\n",
    "\n",
    "G = df_train_meta.loc[train_ids, G_COLS].copy()\n",
    "for c in G_COLS:\n",
    "    G[c] = pd.to_numeric(G[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "G_np = G.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "# Simple standardization for numeric stability (avoid leakage: only within train set)\n",
    "# Standardize all cols (including flags) is okay; flags become small values.\n",
    "g_mean = G_np.mean(axis=0).astype(np.float32)\n",
    "g_std  = G_np.std(axis=0).astype(np.float32)\n",
    "g_std  = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n",
    "G_np_z = ((G_np - g_mean) / g_std).astype(np.float32)\n",
    "\n",
    "# Save scaler for later test inference stage\n",
    "scaler_path = Path(LOG_DIR) / \"global_scaler.json\"\n",
    "with open(scaler_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"cols\": G_COLS, \"mean\": g_mean.tolist(), \"std\": g_std.tolist()}, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset / DataLoader (num_workers=0)\n",
    "# ----------------------------\n",
    "class MemmapSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, idx, X_mm, B_mm, M_mm, G_np_z, y=None):\n",
    "        self.idx = np.asarray(idx, dtype=np.int32)\n",
    "        self.X_mm = X_mm\n",
    "        self.B_mm = B_mm\n",
    "        self.M_mm = M_mm\n",
    "        self.G = G_np_z\n",
    "        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "        X = self.X_mm[j]  # (L,F) float32\n",
    "        B = self.B_mm[j]  # (L,) int8\n",
    "        M = self.M_mm[j]  # (L,) int8\n",
    "        G = self.G[j]     # (G,) float32\n",
    "        if self.y is None:\n",
    "            return (\n",
    "                torch.from_numpy(X),\n",
    "                torch.from_numpy(B.astype(np.int64, copy=False)),\n",
    "                torch.from_numpy(M.astype(np.int64, copy=False)),\n",
    "                torch.from_numpy(G),\n",
    "            )\n",
    "        else:\n",
    "            y = self.y[j]\n",
    "            return (\n",
    "                torch.from_numpy(X),\n",
    "                torch.from_numpy(B.astype(np.int64, copy=False)),\n",
    "                torch.from_numpy(M.astype(np.int64, copy=False)),\n",
    "                torch.from_numpy(G),\n",
    "                torch.tensor(float(y), dtype=torch.float32),\n",
    "            )\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Model: Multiband Event Transformer (CPU-safe small)\n",
    "# ----------------------------\n",
    "class MultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, n_bands=6, d_model=128, n_heads=4, n_layers=2, ff_mult=2, dropout=0.10, g_dim=7):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.n_bands = n_bands\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.x_proj = nn.Linear(feat_dim, d_model)\n",
    "        self.band_emb = nn.Embedding(n_bands, d_model)\n",
    "\n",
    "        # learnable positional embedding (fixed MAX_LEN)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, L, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * ff_mult),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # Attention pooling\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "\n",
    "        # Global feature fusion\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(g_dim, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model + (d_model // 2), d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        \"\"\"\n",
    "        X: (B,L,F) float\n",
    "        band_id: (B,L) long in [0..5]\n",
    "        mask: (B,L) long/int (1=real token, 0=pad)\n",
    "        G: (B,g_dim) float\n",
    "        \"\"\"\n",
    "        # ensure correct dtypes\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "\n",
    "        # Transformer expects src_key_padding_mask with True for pads\n",
    "        pad_mask = (mask == 0)  # (B,L) bool\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        # Attention pooling with mask\n",
    "        a = self.attn(h).squeeze(-1)  # (B,L)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)   # (B,L)\n",
    "        pooled = torch.sum(h * w.unsqueeze(-1), dim=1)  # (B,d_model)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        logit = self.head(z).squeeze(-1)  # (B,)\n",
    "        return logit\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Training hyperparams (CPU-safe)\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    \"d_model\": 128,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 2,\n",
    "    \"ff_mult\": 2,\n",
    "    \"dropout\": 0.10,\n",
    "    \"batch_size\": 16,          # keep modest for CPU\n",
    "    \"grad_accum\": 2,           # effective batch = 32\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"patience\": 3,             # early stopping on val_loss\n",
    "    \"max_grad_norm\": 1.0,\n",
    "}\n",
    "cfg_path = Path(LOG_DIR) / \"train_cfg.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Loss: imbalance handling\n",
    "# ----------------------------\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "if pos == 0:\n",
    "    raise RuntimeError(\"No positive samples in training. Tidak bisa training classifier.\")\n",
    "pos_weight = float(neg / max(pos, 1))\n",
    "pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "\n",
    "print(\"[Stage 8] TRAIN CONFIG (CPU)\")\n",
    "print(f\"- N={N:,} | pos={pos:,} | neg={neg:,} | pos_weight={pos_weight:.4f}\")\n",
    "print(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\n",
    "print(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\n",
    "print(f\"- Saved cfg: {cfg_path}\")\n",
    "print(f\"- Saved global scaler: {scaler_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Metrics helpers (no sklearn dependency)\n",
    "# ----------------------------\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def f1_binary(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return float(2 * prec * rec / (prec + rec))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    logits_all = []\n",
    "    y_all = []\n",
    "    for batch in loader:\n",
    "        Xb, Bb, Mb, Gb, yb = batch\n",
    "        Xb = Xb.to(device)\n",
    "        Bb = Bb.to(device)\n",
    "        Mb = Mb.to(device)\n",
    "        Gb = Gb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logit = model(Xb, Bb, Mb, Gb)\n",
    "        loss = criterion(logit, yb)\n",
    "        losses.append(float(loss.item()))\n",
    "        logits_all.append(logit.detach().cpu().numpy())\n",
    "        y_all.append(yb.detach().cpu().numpy())\n",
    "\n",
    "    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n",
    "    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n",
    "\n",
    "    probs = sigmoid_np(logits_all)\n",
    "    pred01 = (probs >= 0.5).astype(np.int8)\n",
    "    f1 = f1_binary(y_all, pred01)\n",
    "    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Train CV\n",
    "# ----------------------------\n",
    "OOF_DIR = Path(OOF_DIR); OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR = Path(CKPT_DIR); CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "oof_prob = np.zeros((N,), dtype=np.float32)\n",
    "fold_metrics = []\n",
    "\n",
    "# Precompute all indices\n",
    "all_idx = np.arange(N, dtype=np.int32)\n",
    "\n",
    "start_time = time.time()\n",
    "for fold_info in folds:\n",
    "    fold = int(fold_info[\"fold\"])\n",
    "    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n",
    "    val_mask = np.zeros(N, dtype=bool)\n",
    "    val_mask[val_idx] = True\n",
    "    tr_idx = all_idx[~val_mask]\n",
    "\n",
    "    print(f\"\\n[Stage 8] FOLD {fold}/{n_splits-1} | train={len(tr_idx):,} val={len(val_idx):,}\")\n",
    "\n",
    "    # Datasets/Loaders\n",
    "    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_np_z, y=y)\n",
    "    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_np_z, y=y)\n",
    "\n",
    "    dl_tr = make_loader(ds_tr, batch_size=CFG[\"batch_size\"], shuffle=True)\n",
    "    dl_va = make_loader(ds_va, batch_size=CFG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Model + optim\n",
    "    model = MultibandEventTransformer(\n",
    "        feat_dim=Fdim,\n",
    "        n_bands=6,\n",
    "        d_model=CFG[\"d_model\"],\n",
    "        n_heads=CFG[\"n_heads\"],\n",
    "        n_layers=CFG[\"n_layers\"],\n",
    "        ff_mult=CFG[\"ff_mult\"],\n",
    "        dropout=CFG[\"dropout\"],\n",
    "        g_dim=G_np_z.shape[1],\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_probs = None\n",
    "    patience_left = int(CFG[\"patience\"])\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        running = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in dl_tr:\n",
    "            Xb, Bb, Mb, Gb, yb = batch\n",
    "            Xb = Xb.to(device)\n",
    "            Bb = Bb.to(device)\n",
    "            Mb = Mb.to(device)\n",
    "            Gb = Gb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logit = model(Xb, Bb, Mb, Gb)\n",
    "            loss = criterion(logit, yb)\n",
    "            loss = loss / float(CFG[\"grad_accum\"])\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % int(CFG[\"grad_accum\"]) == 0:\n",
    "                if CFG[\"max_grad_norm\"] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            running += float(loss.item()) * float(CFG[\"grad_accum\"])\n",
    "            n_batches += 1\n",
    "            step += 1\n",
    "\n",
    "        train_loss = running / max(n_batches, 1)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, probs, y_val, f1_05 = eval_model(model, dl_va)\n",
    "\n",
    "        improved = val_loss < (best_val_loss - 1e-6)\n",
    "        if improved:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_probs = probs.copy()\n",
    "\n",
    "            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"fold\": fold,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"cfg\": CFG,\n",
    "                    \"seq_feature_names\": SEQ_FEATURE_NAMES,\n",
    "                    \"max_len\": L,\n",
    "                    \"global_cols\": G_COLS,\n",
    "                    \"global_scaler\": {\"mean\": g_mean, \"std\": g_std},\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "            patience_left = int(CFG[\"patience\"])\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "\n",
    "        print(f\"  epoch {epoch:02d} | train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | f1@0.5={f1_05:.4f} | best_epoch={best_epoch} | patience_left={patience_left}\")\n",
    "\n",
    "        if patience_left <= 0:\n",
    "            break\n",
    "\n",
    "    if best_probs is None:\n",
    "        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n",
    "\n",
    "    # Fill OOF\n",
    "    oof_prob[val_idx] = best_probs.astype(np.float32)\n",
    "\n",
    "    # Fold summary\n",
    "    pred01 = (best_probs >= 0.5).astype(np.int8)\n",
    "    best_f1_05 = f1_binary(y[val_idx], pred01)\n",
    "\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold,\n",
    "        \"val_size\": int(len(val_idx)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"f1_at_0p5\": float(best_f1_05),\n",
    "    })\n",
    "\n",
    "    # Cleanup\n",
    "    del model, opt, ds_tr, ds_va, dl_tr, dl_va\n",
    "    gc.collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Save OOF artifacts\n",
    "# ----------------------------\n",
    "oof_path_npy = OOF_DIR / \"oof_prob.npy\"\n",
    "np.save(oof_path_npy, oof_prob)\n",
    "\n",
    "# Also save CSV for convenience\n",
    "df_oof = pd.DataFrame({\n",
    "    \"object_id\": train_ids,\n",
    "    \"target\": y.astype(int),\n",
    "    \"oof_prob\": oof_prob.astype(np.float32),\n",
    "})\n",
    "oof_path_csv = OOF_DIR / \"oof_prob.csv\"\n",
    "df_oof.to_csv(oof_path_csv, index=False)\n",
    "\n",
    "metrics_path = OOF_DIR / \"fold_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed)}, f, indent=2)\n",
    "\n",
    "# quick overall f1@0.5 on OOF (not final; threshold tuning comes later)\n",
    "oof_pred01 = (oof_prob >= 0.5).astype(np.int8)\n",
    "oof_f1_05 = f1_binary(y, oof_pred01)\n",
    "\n",
    "print(\"\\n[Stage 8] CV TRAIN DONE\")\n",
    "print(f\"- elapsed: {elapsed/60:.2f} min\")\n",
    "print(f\"- OOF saved: {oof_path_npy}\")\n",
    "print(f\"- OOF saved: {oof_path_csv}\")\n",
    "print(f\"- fold metrics: {metrics_path}\")\n",
    "print(f\"- OOF f1@0.5 (rough): {oof_f1_05:.4f}\")\n",
    "\n",
    "# Export globals for next stages (threshold tuning + test inference)\n",
    "globals().update({\n",
    "    \"oof_prob\": oof_prob,\n",
    "    \"OOF_PROB_PATH\": oof_path_npy,\n",
    "    \"OOF_CSV_PATH\": oof_path_csv,\n",
    "    \"FOLD_METRICS_PATH\": metrics_path,\n",
    "    \"GLOBAL_SCALER_PATH\": scaler_path,\n",
    "    \"TRAIN_CFG_PATH\": cfg_path,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646cb42",
   "metadata": {
    "papermill": {
     "duration": 0.008872,
     "end_time": "2026-01-01T20:50:27.035874",
     "exception": false,
     "start_time": "2026-01-01T20:50:27.027002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OOF Prediction + Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7a4c58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T20:50:27.057597Z",
     "iopub.status.busy": "2026-01-01T20:50:27.056114Z",
     "iopub.status.idle": "2026-01-01T20:50:27.398027Z",
     "shell.execute_reply": "2026-01-01T20:50:27.397073Z"
    },
    "papermill": {
     "duration": 0.355022,
     "end_time": "2026-01-01T20:50:27.399946",
     "exception": false,
     "start_time": "2026-01-01T20:50:27.044924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9] Threshold tuning DONE\n",
      "- Best threshold: 0.660000\n",
      "- Best F1:        0.171206  (P=0.120219 R=0.297297)\n",
      "- Baseline F1@0.5:0.135934  (P=0.074482 R=0.777027)\n",
      "- Saved: /kaggle/working/mallorn_run/oof/threshold_tuning.json\n",
      "- Saved: /kaggle/working/mallorn_run/oof/threshold_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/oof/threshold_table_top200.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL, Kaggle CPU-SAFE)\n",
    "#\n",
    "# Revisi FULL (fix: \"len() of unsized object\")\n",
    "# - Tanpa mengubah alur utama: tetap load oof -> align y -> sweep threshold -> save report\n",
    "# - Tambahan guard:\n",
    "#   * Pastikan oof_prob selalu 1D array (kalau scalar/0-dim, auto-reload dari file)\n",
    "#   * Kalau train_ids_ordered tidak cocok panjangnya, fallback align via oof_prob.csv (jika ada)\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"OOF_DIR\", \"df_train_meta\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n",
    "\n",
    "OOF_DIR = Path(OOF_DIR)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: robust load oof_prob as 1D float32\n",
    "# ----------------------------\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    # handle object dtype that may hold an array inside\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a  # caller decides (usually invalid for OOF)\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _load_oof_prob():\n",
    "    # 1) try from globals\n",
    "    if \"oof_prob\" in globals():\n",
    "        op = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        # if scalar (0-d), treat as invalid -> reload from file\n",
    "        if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "            return op\n",
    "\n",
    "    # 2) try from npy\n",
    "    p = OOF_DIR / \"oof_prob.npy\"\n",
    "    if p.exists():\n",
    "        op = _as_1d_float32(np.load(p, allow_pickle=False))\n",
    "        if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "            return op\n",
    "\n",
    "    # 3) try from csv (as last resort)\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if \"oof_prob\" in df.columns:\n",
    "            op = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n",
    "            if isinstance(op, np.ndarray) and op.ndim != 0:\n",
    "                return op\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob not found (globals/npy/csv). Jalankan STAGE 8 (training) dulu.\")\n",
    "\n",
    "# Load OOF probabilities (guaranteed 1D or raise)\n",
    "oof_prob = _load_oof_prob()\n",
    "\n",
    "# Final guard: if still scalar, fail clearly\n",
    "if not isinstance(oof_prob, np.ndarray) or oof_prob.ndim == 0:\n",
    "    raise TypeError(\n",
    "        f\"Invalid oof_prob (unsized/scalar). Type={type(oof_prob)} ndim={getattr(oof_prob,'ndim',None)}. \"\n",
    "        \"Pastikan STAGE 8 menyimpan oof_prob sebagai array 1D.\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Align y ordering: prefer train_ids_ordered if valid; else fallback to OOF CSV; else df_train_meta order\n",
    "# ----------------------------\n",
    "train_ids = None\n",
    "y = None\n",
    "\n",
    "# Case A: train_ids_ordered exists and matches length\n",
    "if \"train_ids_ordered\" in globals():\n",
    "    _ids = list(globals()[\"train_ids_ordered\"])\n",
    "    if len(_ids) == len(oof_prob):\n",
    "        train_ids = _ids\n",
    "        y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Case B: fallback via oof_prob.csv (most robust alignment)\n",
    "if y is None:\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df_oof = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df_oof.columns) and (\"oof_prob\" in df_oof.columns):\n",
    "            df_oof[\"object_id\"] = df_oof[\"object_id\"].astype(str).str.strip()\n",
    "            # override oof_prob with csv order to guarantee alignment\n",
    "            oof_prob = _as_1d_float32(df_oof[\"oof_prob\"].to_numpy())\n",
    "            if oof_prob.ndim == 0:\n",
    "                raise TypeError(\"oof_prob from csv became scalar (unexpected).\")\n",
    "            train_ids = df_oof[\"object_id\"].tolist()\n",
    "            y = df_train_meta.loc[train_ids, \"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Case C: last fallback: df_train_meta order must match length\n",
    "if y is None:\n",
    "    if len(oof_prob) != len(df_train_meta):\n",
    "        raise RuntimeError(\n",
    "            f\"Cannot align y: len(oof_prob)={len(oof_prob)} != len(df_train_meta)={len(df_train_meta)} \"\n",
    "            \"and train_ids_ordered not usable and oof_prob.csv not available.\"\n",
    "        )\n",
    "    train_ids = df_train_meta.index.astype(str).tolist()\n",
    "    y = df_train_meta[\"target\"].to_numpy(dtype=np.int8, copy=False)\n",
    "\n",
    "# Final length check\n",
    "if len(oof_prob) != len(y):\n",
    "    raise RuntimeError(f\"Length mismatch: oof_prob={len(oof_prob)} vs y={len(y)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) F1 metric (binary)\n",
    "# ----------------------------\n",
    "def f1_binary(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return float(2 * prec * rec / (prec + rec))\n",
    "\n",
    "def precision_recall(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    return float(prec), float(rec), tp, fp, fn\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold sweep (fast + robust)\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.01, 0.10, 19),\n",
    "    np.linspace(0.10, 0.90, 81),\n",
    "    np.linspace(0.90, 0.99, 19),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.01, 0.99, 99, dtype=np.float32)\n",
    "quant_thr = np.quantile(oof_prob, qs).astype(np.float32)\n",
    "thr_candidates = np.unique(np.clip(np.concatenate([grid, quant_thr]), 0.0, 1.0))\n",
    "\n",
    "best = {\"thr\": 0.5, \"f1\": -1.0, \"prec\": 0.0, \"rec\": 0.0, \"tp\": 0, \"fp\": 0, \"fn\": 0, \"pos_pred\": 0}\n",
    "\n",
    "rows = []\n",
    "for thr in thr_candidates:\n",
    "    pred = (oof_prob >= thr).astype(np.int8)\n",
    "    f1 = f1_binary(y, pred)\n",
    "    prec, rec, tp, fp, fn = precision_recall(y, pred)\n",
    "    pos_pred = int(pred.sum())\n",
    "    rows.append((float(thr), float(f1), float(prec), float(rec), int(tp), int(fp), int(fn), pos_pred))\n",
    "\n",
    "    if (f1 > best[\"f1\"] + 1e-12) or (\n",
    "        abs(f1 - best[\"f1\"]) <= 1e-12 and (rec > best[\"rec\"] + 1e-12)\n",
    "    ) or (\n",
    "        abs(f1 - best[\"f1\"]) <= 1e-12 and abs(rec - best[\"rec\"]) <= 1e-12 and (fp < best[\"fp\"])\n",
    "    ):\n",
    "        best.update({\"thr\": float(thr), \"f1\": float(f1), \"prec\": float(prec), \"rec\": float(rec),\n",
    "                     \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn), \"pos_pred\": int(pos_pred)})\n",
    "\n",
    "thr_table = pd.DataFrame(rows, columns=[\"thr\",\"f1\",\"precision\",\"recall\",\"tp\",\"fp\",\"fn\",\"pos_pred\"])\n",
    "thr_table = thr_table.sort_values([\"f1\",\"recall\",\"precision\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "BEST_THR = float(best[\"thr\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Additional sanity: compare to default 0.5\n",
    "# ----------------------------\n",
    "pred05 = (oof_prob >= 0.5).astype(np.int8)\n",
    "f1_05 = f1_binary(y, pred05)\n",
    "prec_05, rec_05, tp_05, fp_05, fn_05 = precision_recall(y, pred05)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Save report\n",
    "# ----------------------------\n",
    "out_json = OOF_DIR / \"threshold_tuning.json\"\n",
    "out_txt  = OOF_DIR / \"threshold_report.txt\"\n",
    "out_csv  = OOF_DIR / \"threshold_table_top200.csv\"\n",
    "\n",
    "payload = {\n",
    "    \"best_threshold\": BEST_THR,\n",
    "    \"best_f1\": best[\"f1\"],\n",
    "    \"best_precision\": best[\"prec\"],\n",
    "    \"best_recall\": best[\"rec\"],\n",
    "    \"best_counts\": {\"tp\": best[\"tp\"], \"fp\": best[\"fp\"], \"fn\": best[\"fn\"], \"pos_pred\": best[\"pos_pred\"]},\n",
    "    \"baseline_thr_0p5\": {\"f1\": f1_05, \"precision\": prec_05, \"recall\": rec_05, \"tp\": tp_05, \"fp\": fp_05, \"fn\": fn_05, \"pos_pred\": int(pred05.sum())},\n",
    "    \"n_objects\": int(len(y)),\n",
    "    \"pos\": int((y == 1).sum()),\n",
    "    \"neg\": int((y == 0).sum()),\n",
    "}\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Threshold Tuning Report\")\n",
    "lines.append(f\"- N={payload['n_objects']} | pos={payload['pos']} | neg={payload['neg']} | pos%={payload['pos']/max(payload['n_objects'],1)*100:.4f}%\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"- F1={f1_05:.6f} | P={prec_05:.6f} | R={rec_05:.6f} | tp={tp_05} fp={fp_05} fn={fn_05} | pos_pred={int(pred05.sum())}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST @ thr={BEST_THR:.6f}\")\n",
    "lines.append(f\"- F1={best['f1']:.6f} | P={best['prec']:.6f} | R={best['rec']:.6f} | tp={best['tp']} fp={best['fp']} fn={best['fn']} | pos_pred={best['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 thresholds by (F1, recall, precision):\")\n",
    "for i in range(min(10, len(thr_table))):\n",
    "    r = thr_table.iloc[i]\n",
    "    lines.append(\n",
    "        f\"{i+1:02d}. thr={r['thr']:.6f} | f1={r['f1']:.6f} | P={r['precision']:.6f} | R={r['recall']:.6f} | tp={int(r['tp'])} fp={int(r['fp'])} fn={int(r['fn'])} | pos_pred={int(r['pos_pred'])}\"\n",
    "    )\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "thr_table.head(200).to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"[Stage 9] Threshold tuning DONE\")\n",
    "print(f\"- Best threshold: {BEST_THR:.6f}\")\n",
    "print(f\"- Best F1:        {best['f1']:.6f}  (P={best['prec']:.6f} R={best['rec']:.6f})\")\n",
    "print(f\"- Baseline F1@0.5:{f1_05:.6f}  (P={prec_05:.6f} R={rec_05:.6f})\")\n",
    "print(f\"- Saved: {out_json}\")\n",
    "print(f\"- Saved: {out_txt}\")\n",
    "print(f\"- Saved: {out_csv}\")\n",
    "\n",
    "# Export globals for next stages (test inference + submission)\n",
    "globals().update({\n",
    "    \"BEST_THR\": BEST_THR,\n",
    "    \"thr_table\": thr_table,\n",
    "    \"THR_JSON_PATH\": out_json,\n",
    "    \"THR_REPORT_PATH\": out_txt,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c525762",
   "metadata": {
    "papermill": {
     "duration": 0.009811,
     "end_time": "2026-01-01T20:50:27.420606",
     "exception": false,
     "start_time": "2026-01-01T20:50:27.410795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Inference (Fold Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689e2f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T20:50:27.444495Z",
     "iopub.status.busy": "2026-01-01T20:50:27.444156Z",
     "iopub.status.idle": "2026-01-01T21:00:08.501023Z",
     "shell.execute_reply": "2026-01-01T21:00:08.500139Z"
    },
    "papermill": {
     "duration": 581.070449,
     "end_time": "2026-01-01T21:00:08.502631",
     "exception": false,
     "start_time": "2026-01-01T20:50:27.432182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 10] Test inference: N_test=7,135 | folds=5 | batch=64 (CPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fold 0: prob_mean=0.539035 | prob_std=0.036142\n",
      "  fold 1: prob_mean=0.414614 | prob_std=0.293742\n",
      "  fold 2: prob_mean=0.374876 | prob_std=0.240770\n",
      "  fold 3: prob_mean=0.316973 | prob_std=0.238685\n",
      "  fold 4: prob_mean=0.330653 | prob_std=0.263848\n",
      "\n",
      "[Stage 10] DONE\n",
      "- Saved fold probs: /kaggle/working/mallorn_run/artifacts/test_prob_fold.npy\n",
      "- Saved ens probs : /kaggle/working/mallorn_run/artifacts/test_prob_ens.npy\n",
      "- Saved csv       : /kaggle/working/mallorn_run/artifacts/test_prob_ens.csv\n",
      "- ens mean=0.395230 | std=0.201383 | min=0.063738 | max=0.717762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL, Kaggle CPU-SAFE)\n",
    "#\n",
    "# Revisi FULL (fix: PyTorch 2.6 `weights_only=True` default -> UnpicklingError)\n",
    "# - TANPA mengubah alur: tetap load fixed TEST -> load ckpt -> predict per fold -> mean ensemble -> save\n",
    "# - Fix utama: torch.load dibuat kompatibel:\n",
    "#     * coba weights_only=True (aman)\n",
    "#     * kalau gagal (ckpt kamu dari STAGE 8 / trusted), fallback weights_only=False\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\n",
    "    \"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\n",
    "    \"df_test_meta\",\n",
    "    \"CKPT_DIR\",\"LOG_DIR\",\n",
    "    \"n_splits\",\n",
    "]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n",
    "\n",
    "# Torch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Thread guard\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FIX_DIR = Path(FIX_DIR)\n",
    "ART_DIR = Path(globals()[\"ART_DIR\"])\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load TEST ordering (must match STAGE 6)\n",
    "# ----------------------------\n",
    "test_ids_path = FIX_DIR / \"test_ids.npy\"\n",
    "if not test_ids_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n",
    "test_ids = np.load(test_ids_path, allow_pickle=False).astype(\"S\").astype(str).tolist()\n",
    "NTE = len(test_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open fixed-length TEST memmaps\n",
    "# ----------------------------\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "L = int(MAX_LEN)\n",
    "\n",
    "test_X_path = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path = FIX_DIR / \"test_M.dat\"\n",
    "\n",
    "for p in [test_X_path, test_B_path, test_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=np.float32, mode=\"r\", shape=(NTE, L, Fdim))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,   mode=\"r\", shape=(NTE, L))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load global scaler + build standardized global features for TEST\n",
    "# ----------------------------\n",
    "scaler_json = Path(globals().get(\"GLOBAL_SCALER_PATH\", Path(LOG_DIR) / \"global_scaler.json\"))\n",
    "if not scaler_json.exists():\n",
    "    raise FileNotFoundError(f\"Missing global scaler json: {scaler_json}. Jalankan STAGE 8 dulu.\")\n",
    "\n",
    "with open(scaler_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    scaler = json.load(f)\n",
    "G_COLS = scaler[\"cols\"]\n",
    "g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32)\n",
    "g_std  = np.asarray(scaler[\"std\"], dtype=np.float32)\n",
    "g_std  = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n",
    "\n",
    "for c in G_COLS:\n",
    "    if c not in df_test_meta.columns:\n",
    "        df_test_meta[c] = 0.0\n",
    "\n",
    "G = df_test_meta.loc[test_ids, G_COLS].copy()\n",
    "for c in G_COLS:\n",
    "    G[c] = pd.to_numeric(G[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "G_np = G.to_numpy(dtype=np.float32, copy=False)\n",
    "G_np_z = ((G_np - g_mean) / g_std).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset/Loader for inference\n",
    "# ----------------------------\n",
    "class TestMemmapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xmm, Bmm, Mmm, G_np_z):\n",
    "        self.Xmm = Xmm\n",
    "        self.Bmm = Bmm\n",
    "        self.Mmm = Mmm\n",
    "        self.G = G_np_z\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Xmm.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        X = self.Xmm[i]\n",
    "        B = self.Bmm[i].astype(np.int64, copy=False)\n",
    "        M = self.Mmm[i].astype(np.int64, copy=False)\n",
    "        G = self.G[i]\n",
    "        return (\n",
    "            torch.from_numpy(X),\n",
    "            torch.from_numpy(B),\n",
    "            torch.from_numpy(M),\n",
    "            torch.from_numpy(G),\n",
    "        )\n",
    "\n",
    "def make_loader(ds, batch_size=64):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Model definition (must match STAGE 8)\n",
    "# ----------------------------\n",
    "class MultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, n_bands=6, d_model=128, n_heads=4, n_layers=2, ff_mult=2, dropout=0.10, g_dim=7, max_len=512):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.n_bands = n_bands\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.x_proj = nn.Linear(feat_dim, d_model)\n",
    "        self.band_emb = nn.Embedding(n_bands, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * ff_mult),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(g_dim, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model + (d_model // 2), d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        h = self.x_proj(X) + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "\n",
    "        pad_mask = (mask == 0)\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        g = self.g_proj(G.to(torch.float32))\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        logit = self.head(z).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs(model, loader):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for batch in loader:\n",
    "        Xb, Bb, Mb, Gb = batch\n",
    "        Xb = Xb.to(device)\n",
    "        Bb = Bb.to(device)\n",
    "        Mb = Mb.to(device)\n",
    "        Gb = Gb.to(device)\n",
    "        logit = model(Xb, Bb, Mb, Gb)\n",
    "        outs.append(logit.detach().cpu().numpy())\n",
    "    logits = np.concatenate(outs, axis=0) if outs else np.zeros((0,), dtype=np.float32)\n",
    "    return sigmoid_np(logits).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Safe/compat checkpoint loader (fix PyTorch 2.6 weights_only default)\n",
    "# ----------------------------\n",
    "def torch_load_compat(path: Path):\n",
    "    \"\"\"\n",
    "    PyTorch 2.6: default weights_only=True (lebih aman) bisa gagal untuk ckpt dict yang berisi numpy objects.\n",
    "    Karena ckpt ini dibuat oleh STAGE 8 di notebook yang sama (trusted), kita fallback ke weights_only=False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "    except TypeError:\n",
    "        # older torch without weights_only arg\n",
    "        return torch.load(path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        # trusted fallback\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "CKPT_DIR = Path(CKPT_DIR)\n",
    "ckpts = []\n",
    "for f in range(int(n_splits)):\n",
    "    p = CKPT_DIR / f\"fold_{f}.pt\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n",
    "    ckpts.append(p)\n",
    "\n",
    "# Small batch for CPU safety (increase if fast enough)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ds_test = TestMemmapDataset(Xte, Bte, Mte, G_np_z)\n",
    "dl_test = make_loader(ds_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_prob_folds = np.zeros((NTE, int(n_splits)), dtype=np.float32)\n",
    "\n",
    "print(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} (CPU)\")\n",
    "for fold, ckpt_path in enumerate(ckpts):\n",
    "    ckpt = torch_load_compat(ckpt_path)\n",
    "\n",
    "    cfg = ckpt.get(\"cfg\", {})\n",
    "    d_model  = int(cfg.get(\"d_model\", 128))\n",
    "    n_heads  = int(cfg.get(\"n_heads\", 4))\n",
    "    n_layers = int(cfg.get(\"n_layers\", 2))\n",
    "    ff_mult  = int(cfg.get(\"ff_mult\", 2))\n",
    "    dropout  = float(cfg.get(\"dropout\", 0.10))\n",
    "\n",
    "    model = MultibandEventTransformer(\n",
    "        feat_dim=Fdim,\n",
    "        n_bands=6,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        ff_mult=ff_mult,\n",
    "        dropout=dropout,\n",
    "        g_dim=G_np_z.shape[1],\n",
    "        max_len=L,\n",
    "    ).to(device)\n",
    "\n",
    "    if \"model_state\" not in ckpt:\n",
    "        raise KeyError(f\"Checkpoint missing 'model_state': {ckpt_path}\")\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "\n",
    "    probs = predict_probs(model, dl_test)\n",
    "    if len(probs) != NTE:\n",
    "        raise RuntimeError(f\"Fold {fold}: probs length mismatch {len(probs)} vs {NTE}\")\n",
    "\n",
    "    test_prob_folds[:, fold] = probs\n",
    "    print(f\"  fold {fold}: prob_mean={float(probs.mean()):.6f} | prob_std={float(probs.std()):.6f}\")\n",
    "\n",
    "    del model, ckpt, probs\n",
    "    gc.collect()\n",
    "\n",
    "# Ensemble mean\n",
    "test_prob_ens = test_prob_folds.mean(axis=1).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Save artifacts\n",
    "# ----------------------------\n",
    "fold_path = ART_DIR / \"test_prob_fold.npy\"\n",
    "ens_path  = ART_DIR / \"test_prob_ens.npy\"\n",
    "csv_path  = ART_DIR / \"test_prob_ens.csv\"\n",
    "\n",
    "np.save(fold_path, test_prob_folds)\n",
    "np.save(ens_path, test_prob_ens)\n",
    "\n",
    "df_pred = pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens})\n",
    "df_pred.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"\\n[Stage 10] DONE\")\n",
    "print(f\"- Saved fold probs: {fold_path}\")\n",
    "print(f\"- Saved ens probs : {ens_path}\")\n",
    "print(f\"- Saved csv       : {csv_path}\")\n",
    "print(f\"- ens mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n",
    "\n",
    "# Export globals for submission\n",
    "globals().update({\n",
    "    \"test_ids\": test_ids,\n",
    "    \"test_prob_folds\": test_prob_folds,\n",
    "    \"test_prob_ens\": test_prob_ens,\n",
    "    \"TEST_PROB_FOLD_PATH\": fold_path,\n",
    "    \"TEST_PROB_ENS_PATH\": ens_path,\n",
    "    \"TEST_PROB_CSV_PATH\": csv_path,\n",
    "})\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0ef8f",
   "metadata": {
    "papermill": {
     "duration": 0.009957,
     "end_time": "2026-01-01T21:00:08.523115",
     "exception": false,
     "start_time": "2026-01-01T21:00:08.513158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee06d662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T21:00:08.546156Z",
     "iopub.status.busy": "2026-01-01T21:00:08.545485Z",
     "iopub.status.idle": "2026-01-01T21:00:08.927672Z",
     "shell.execute_reply": "2026-01-01T21:00:08.926600Z"
    },
    "papermill": {
     "duration": 0.396522,
     "end_time": "2026-01-01T21:00:08.930091",
     "exception": false,
     "start_time": "2026-01-01T21:00:08.533569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11] SUBMISSION READY\n",
      "- mode: prob | threshold_used=0.660000 (only relevant if binary)\n",
      "- wrote: /kaggle/working/submission.csv\n",
      "- copy : /kaggle/working/mallorn_run/submissions/submission.csv\n",
      "- debug proba: /kaggle/working/mallorn_run/submissions/submission_proba.csv\n",
      "- rows: 7,135\n",
      "\n",
      "Preview:\n",
      "                   object_id  prediction\n",
      "    Eluwaith_Mithrim_nothrim    0.089516\n",
      "          Eru_heledir_archam    0.212484\n",
      "           Gonhir_anann_fuin    0.260010\n",
      "Gwathuirim_haradrim_tegilbor    0.640297\n",
      "            achas_minai_maen    0.211878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 11 — Submission Build (ONE CELL, Kaggle CPU-SAFE)\n",
    "#\n",
    "# Revisi FULL (fix: 'NoneType' object is not iterable pada test_ids)\n",
    "# - TANPA mengubah alur utama: tetap ambil preds -> align ke sample -> cek -> tulis submission\n",
    "# - Fix utama:\n",
    "#   * test_ids bisa ada di globals tapi nilainya None -> treat as missing dan fallback load dari file\n",
    "#   * robust loader untuk test_prob_ens (pastikan 1D array) + test_ids (list of str)\n",
    "# ============================================================\n",
    "\n",
    "import gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"SUB_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n",
    "\n",
    "sample_path = Path(PATHS[\"SAMPLE_SUB\"])\n",
    "if not sample_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n",
    "\n",
    "df_sub = pd.read_csv(sample_path)\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: robust loaders\n",
    "# ----------------------------\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _load_test_prob():\n",
    "    # 1) globals\n",
    "    if \"test_prob_ens\" in globals() and globals()[\"test_prob_ens\"] is not None:\n",
    "        tp = _as_1d_float32(globals()[\"test_prob_ens\"])\n",
    "        if isinstance(tp, np.ndarray) and tp.ndim != 0:\n",
    "            return tp\n",
    "\n",
    "    # 2) artifact path var\n",
    "    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n",
    "        p = Path(globals()[\"TEST_PROB_ENS_PATH\"])\n",
    "        if p.exists():\n",
    "            tp = _as_1d_float32(np.load(p, allow_pickle=False))\n",
    "            if tp.ndim != 0:\n",
    "                return tp\n",
    "\n",
    "    # 3) default artifact location\n",
    "    p = Path(globals().get(\"ART_DIR\", \"/kaggle/working\")) / \"test_prob_ens.npy\"\n",
    "    if p.exists():\n",
    "        tp = _as_1d_float32(np.load(p, allow_pickle=False))\n",
    "        if tp.ndim != 0:\n",
    "            return tp\n",
    "\n",
    "    # 4) csv fallback (if exists)\n",
    "    p = Path(globals().get(\"ART_DIR\", \"/kaggle/working\")) / \"test_prob_ens.csv\"\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        if \"prob\" in df.columns:\n",
    "            tp = _as_1d_float32(df[\"prob\"].to_numpy())\n",
    "            if tp.ndim != 0:\n",
    "                return tp\n",
    "\n",
    "    raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n",
    "\n",
    "def _load_test_ids():\n",
    "    # 1) globals (must be non-None and iterable)\n",
    "    if \"test_ids\" in globals() and globals()[\"test_ids\"] is not None:\n",
    "        try:\n",
    "            ids = list(globals()[\"test_ids\"])\n",
    "            if len(ids) > 0:\n",
    "                return [str(x).strip() for x in ids]\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "    # 2) fixed_seq cache\n",
    "    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n",
    "    pids = fix_dir / \"test_ids.npy\"\n",
    "    if pids.exists():\n",
    "        ids = np.load(pids, allow_pickle=False).astype(\"S\").astype(str).tolist()\n",
    "        return [str(x).strip() for x in ids]\n",
    "\n",
    "    # 3) from test_prob_ens.csv if exists (align to that file)\n",
    "    p = Path(globals().get(\"ART_DIR\", \"/kaggle/working\")) / \"test_prob_ens.csv\"\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        if \"object_id\" in df.columns:\n",
    "            return df[\"object_id\"].astype(str).str.strip().tolist()\n",
    "\n",
    "    raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat test_ids.npy atau STAGE 10 export test_ids.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load predictions + ids\n",
    "# ----------------------------\n",
    "test_prob = _load_test_prob()\n",
    "test_ids  = _load_test_ids()\n",
    "\n",
    "# final guards\n",
    "if not isinstance(test_prob, np.ndarray) or test_prob.ndim == 0:\n",
    "    raise TypeError(\n",
    "        f\"Invalid test_prob (unsized/scalar). Type={type(test_prob)} ndim={getattr(test_prob,'ndim',None)}\"\n",
    "    )\n",
    "\n",
    "if len(test_prob) != len(test_ids):\n",
    "    raise RuntimeError(f\"Length mismatch: test_prob={len(test_prob)} vs test_ids={len(test_ids)}\")\n",
    "\n",
    "# Determine threshold (optional)\n",
    "thr = float(globals().get(\"BEST_THR\", 0.5))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build mapping object_id -> prob\n",
    "# ----------------------------\n",
    "df_pred = pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob.astype(np.float32, copy=False)})\n",
    "\n",
    "if df_pred[\"object_id\"].duplicated().any():\n",
    "    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:5].tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n",
    "\n",
    "p = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "if not np.isfinite(p).all():\n",
    "    bad = int((~np.isfinite(p)).sum())\n",
    "    raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\n",
    "p = np.clip(p, 0.0, 1.0)\n",
    "df_pred[\"prob\"] = p\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Align to sample_submission order\n",
    "# ----------------------------\n",
    "df_sub[\"object_id\"] = df_sub[\"object_id\"].astype(str).str.strip()\n",
    "df_pred[\"object_id\"] = df_pred[\"object_id\"].astype(str).str.strip()\n",
    "\n",
    "df_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n",
    "\n",
    "if df_out[\"prob\"].isna().any():\n",
    "    missing_n = int(df_out[\"prob\"].isna().sum())\n",
    "    miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:5].tolist()\n",
    "    raise ValueError(\n",
    "        f\"Some sample_submission object_id have no prediction: {missing_n} missing. \"\n",
    "        f\"Examples: {miss_ids}\"\n",
    "    )\n",
    "\n",
    "# NOTE: default use probability submission\n",
    "SUBMISSION_MODE = \"prob\"  # \"prob\" or \"binary\"\n",
    "if SUBMISSION_MODE == \"binary\":\n",
    "    df_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\n",
    "else:\n",
    "    df_out[\"prediction\"] = df_out[\"prob\"].astype(np.float32)\n",
    "\n",
    "df_out = df_out[[\"object_id\", \"prediction\"]]\n",
    "\n",
    "if df_out[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(\"submission has duplicate object_id (unexpected).\")\n",
    "if len(df_out) != len(df_sub):\n",
    "    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Write submission files\n",
    "# ----------------------------\n",
    "SUB_DIR = Path(SUB_DIR)\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_main = Path(\"/kaggle/working/submission.csv\")\n",
    "out_copy = SUB_DIR / \"submission.csv\"\n",
    "out_proba = SUB_DIR / \"submission_proba.csv\"\n",
    "\n",
    "df_out.to_csv(out_main, index=False)\n",
    "df_out.to_csv(out_copy, index=False)\n",
    "\n",
    "df_dbg = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\")\n",
    "df_dbg = df_dbg.rename(columns={\"prob\": \"prediction\"}).astype({\"prediction\": np.float32})\n",
    "df_dbg.to_csv(out_proba, index=False)\n",
    "\n",
    "print(\"[Stage 11] SUBMISSION READY\")\n",
    "print(f\"- mode: {SUBMISSION_MODE} | threshold_used={thr:.6f} (only relevant if binary)\")\n",
    "print(f\"- wrote: {out_main}\")\n",
    "print(f\"- copy : {out_copy}\")\n",
    "print(f\"- debug proba: {out_proba}\")\n",
    "print(f\"- rows: {len(df_out):,}\")\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(df_out.head(5).to_string(index=False))\n",
    "\n",
    "globals().update({\n",
    "    \"SUBMISSION_PATH\": out_main,\n",
    "    \"SUBMISSION_COPY_PATH\": out_copy,\n",
    "    \"SUBMISSION_MODE\": SUBMISSION_MODE,\n",
    "    \"SUBMISSION_THRESHOLD\": thr,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8925232,
     "sourceId": 14010596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7651.846648,
   "end_time": "2026-01-01T21:00:11.953072",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-01T18:52:40.106424",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
