{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839e7d62",
   "metadata": {
    "papermill": {
     "duration": 0.01837,
     "end_time": "2026-01-07T16:45:02.976429",
     "exception": false,
     "start_time": "2026-01-07T16:45:02.958059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Kaggle CPU Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97658ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:45:03.009465Z",
     "iopub.status.busy": "2026-01-07T16:45:03.009106Z",
     "iopub.status.idle": "2026-01-07T16:45:12.705203Z",
     "shell.execute_reply": "2026-01-07T16:45:12.704093Z"
    },
    "papermill": {
     "duration": 9.714978,
     "end_time": "2026-01-07T16:45:12.707448",
     "exception": false,
     "start_time": "2026-01-07T16:45:02.992470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STAGE0] Lightcurve validation: sample\n",
      "ENV OK (Stage0)\n",
      "- DEVICE: cpu | THREADS: 2\n",
      "- DATA_ROOT: /kaggle/input/mallorn-dataset\n",
      "- Python: 3.12.12\n",
      "- Numpy:  2.0.2\n",
      "- Pandas: 2.2.2\n",
      "- Torch:  2.8.0+cu126 | CUDA: False\n",
      "\n",
      "DATA OK\n",
      "- train_log objects: 3,043 | pos=148 | neg=2,895 | pos%=4.864%\n",
      "- test_log objects:  7,135\n",
      "- sample_submission: 7,135\n",
      "- splits: 20 folders (01..20)\n",
      "- RUN_DIR: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c\n",
      "\n",
      "SPLIT SUMMARY (meta)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>train_n</th>\n",
       "      <th>train_pos</th>\n",
       "      <th>train_pos_pct</th>\n",
       "      <th>test_n</th>\n",
       "      <th>train_Z_med</th>\n",
       "      <th>train_EBV_med</th>\n",
       "      <th>test_has_zerr_pct</th>\n",
       "      <th>test_Zerr_med</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>split_01</td>\n",
       "      <td>155</td>\n",
       "      <td>12</td>\n",
       "      <td>7.741935</td>\n",
       "      <td>364</td>\n",
       "      <td>0.41840</td>\n",
       "      <td>0.037</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.028375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>split_02</td>\n",
       "      <td>170</td>\n",
       "      <td>12</td>\n",
       "      <td>7.058824</td>\n",
       "      <td>414</td>\n",
       "      <td>0.45460</td>\n",
       "      <td>0.035</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.029345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_03</td>\n",
       "      <td>138</td>\n",
       "      <td>3</td>\n",
       "      <td>2.173913</td>\n",
       "      <td>338</td>\n",
       "      <td>0.61845</td>\n",
       "      <td>0.036</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.031825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>split_04</td>\n",
       "      <td>145</td>\n",
       "      <td>12</td>\n",
       "      <td>8.275862</td>\n",
       "      <td>332</td>\n",
       "      <td>0.54870</td>\n",
       "      <td>0.033</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.031035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>split_05</td>\n",
       "      <td>165</td>\n",
       "      <td>6</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>375</td>\n",
       "      <td>0.46510</td>\n",
       "      <td>0.037</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.029070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>split_06</td>\n",
       "      <td>155</td>\n",
       "      <td>9</td>\n",
       "      <td>5.806452</td>\n",
       "      <td>374</td>\n",
       "      <td>0.51600</td>\n",
       "      <td>0.031</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.030275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>split_07</td>\n",
       "      <td>165</td>\n",
       "      <td>6</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>398</td>\n",
       "      <td>0.41170</td>\n",
       "      <td>0.038</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.028475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>split_08</td>\n",
       "      <td>162</td>\n",
       "      <td>3</td>\n",
       "      <td>1.851852</td>\n",
       "      <td>387</td>\n",
       "      <td>0.49780</td>\n",
       "      <td>0.042</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.030050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>split_09</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>2.343750</td>\n",
       "      <td>289</td>\n",
       "      <td>0.53235</td>\n",
       "      <td>0.038</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.030410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>split_10</td>\n",
       "      <td>144</td>\n",
       "      <td>5</td>\n",
       "      <td>3.472222</td>\n",
       "      <td>331</td>\n",
       "      <td>0.69255</td>\n",
       "      <td>0.036</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.030590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      split  train_n  train_pos  train_pos_pct  test_n  train_Z_med  \\\n",
       "0  split_01      155         12       7.741935     364      0.41840   \n",
       "1  split_02      170         12       7.058824     414      0.45460   \n",
       "2  split_03      138          3       2.173913     338      0.61845   \n",
       "3  split_04      145         12       8.275862     332      0.54870   \n",
       "4  split_05      165          6       3.636364     375      0.46510   \n",
       "5  split_06      155          9       5.806452     374      0.51600   \n",
       "6  split_07      165          6       3.636364     398      0.41170   \n",
       "7  split_08      162          3       1.851852     387      0.49780   \n",
       "8  split_09      128          3       2.343750     289      0.53235   \n",
       "9  split_10      144          5       3.472222     331      0.69255   \n",
       "\n",
       "   train_EBV_med  test_has_zerr_pct  test_Zerr_med  \n",
       "0          0.037              100.0       0.028375  \n",
       "1          0.035              100.0       0.029345  \n",
       "2          0.036              100.0       0.031825  \n",
       "3          0.033              100.0       0.031035  \n",
       "4          0.037              100.0       0.029070  \n",
       "5          0.031              100.0       0.030275  \n",
       "6          0.038              100.0       0.028475  \n",
       "7          0.042              100.0       0.030050  \n",
       "8          0.038              100.0       0.030410  \n",
       "9          0.036              100.0       0.030590  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 0 — Environment + Paths + Health Checks (ONE CELL)\n",
    "# REVISI FULL v8.1 (BRUTAL-LB READY + CONSISTENT SIGNAL POLICY)\n",
    "#\n",
    "# v8.1 changes vs your v8.0:\n",
    "# - SIGNAL policy dibuat konsisten dgn saran: flux-safe (asinh_flux) + snr_tanh (bukan mag_ulim)\n",
    "# - Tambah policy penting untuk stage lanjut:\n",
    "#     * DELTA_POLICY=\"per_band\" (hindari delta lintas band)\n",
    "#     * USE_REST_FRAME_TIME=True (t/(1+z))\n",
    "#     * WINDOW_POLICY=\"peak_centered\" + MULTI_WINDOW_K (TTA windows)\n",
    "#     * Calibration & threshold robust knobs\n",
    "# - Health report tambah ringkas statistik Z_err test (domain shift)\n",
    "# ============================================================\n",
    "\n",
    "import os, sys, gc, json, time, random, hashlib, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# Repro / run identity\n",
    "# ----------------------------\n",
    "SEED = 2025\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# Device detect\n",
    "# ----------------------------\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "    _cuda_ok = torch.cuda.is_available()\n",
    "except Exception:\n",
    "    torch = None\n",
    "    TORCH_OK = False\n",
    "    _cuda_ok = False\n",
    "\n",
    "DEVICE = \"cuda\" if _cuda_ok else \"cpu\"\n",
    "\n",
    "# ----------------------------\n",
    "# Thread policy (anti-freeze, tapi brutal mode boleh lebih tinggi)\n",
    "# ----------------------------\n",
    "def _pick_threads(device: str) -> int:\n",
    "    if device == \"cuda\":\n",
    "        return 2\n",
    "    cpu = os.cpu_count() or 4\n",
    "    # sedikit lebih fleksibel untuk feature engineering, tapi tetap cap\n",
    "    return int(min(10, max(2, cpu // 2)))\n",
    "\n",
    "THREADS = _pick_threads(DEVICE)\n",
    "for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
    "    os.environ.setdefault(k, str(THREADS))\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "if TORCH_OK:\n",
    "    torch.manual_seed(SEED)\n",
    "    try:\n",
    "        torch.set_num_threads(THREADS)\n",
    "        torch.set_num_interop_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "def _must_exist(p: Path, what: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"[MISSING] {what}: {p}\")\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _normalize_split(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    if s.isdigit():\n",
    "        return f\"split_{int(s):02d}\"\n",
    "    s = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s.startswith(\"split_\"):\n",
    "        tail = s.split(\"split_\", 1)[1].strip(\"_\")\n",
    "        if tail.isdigit():\n",
    "            return f\"split_{int(tail):02d}\"\n",
    "    return s\n",
    "\n",
    "def _discover_data_root(default_root: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Cari folder di /kaggle/input yang punya:\n",
    "    - train_log.csv, test_log.csv, sample_submission.csv\n",
    "    - split_01..split_20 (minimal split_01 dan split_20)\n",
    "    \"\"\"\n",
    "    if default_root.exists():\n",
    "        return default_root\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    if not base.exists():\n",
    "        return default_root\n",
    "\n",
    "    candidates = []\n",
    "    for d in base.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if (d / \"train_log.csv\").exists() and (d / \"test_log.csv\").exists() and (d / \"sample_submission.csv\").exists():\n",
    "            has_01 = (d / \"split_01\").exists() or (d / \"split_1\").exists()\n",
    "            has_20 = (d / \"split_20\").exists()\n",
    "            if has_01 and has_20:\n",
    "                candidates.append(d)\n",
    "\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    if len(candidates) > 1:\n",
    "        candidates = sorted(candidates, key=lambda x: (not (x / \"split_01\").exists(), x.name))\n",
    "        return candidates[0]\n",
    "    return default_root\n",
    "\n",
    "def _hash_cfg(d: dict) -> str:\n",
    "    s = json.dumps(d, sort_keys=True, ensure_ascii=True)\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:10]\n",
    "\n",
    "def _safe_float(x, default=0.0):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        if np.isfinite(v):\n",
    "            return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return float(default)\n",
    "\n",
    "def _sample_ids_per_split(df_log: pd.DataFrame, split_name: str, n: int, seed: int) -> list:\n",
    "    s = df_log.loc[df_log[\"split\"] == split_name, \"object_id\"].astype(str)\n",
    "    if len(s) == 0:\n",
    "        return []\n",
    "    if len(s) <= n:\n",
    "        return s.tolist()\n",
    "    return s.sample(n=n, random_state=seed).tolist()\n",
    "\n",
    "def _scan_lightcurve_for_ids(csv_path: Path, target_ids: set, chunk_rows: int = 200_000):\n",
    "    \"\"\"\n",
    "    Scan streaming untuk memastikan object_id target benar-benar muncul di file lightcurve.\n",
    "    Kembalikan: found_ids, obs_count_by_id, band_mask_by_id\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    obs_count = {oid: 0 for oid in target_ids}\n",
    "    band_mask = {oid: 0 for oid in target_ids}  # bitmask u,g,r,i,z,y (0..5)\n",
    "\n",
    "    band_to_bit = {\"u\": 1<<0, \"g\": 1<<1, \"r\": 1<<2, \"i\": 1<<3, \"z\": 1<<4, \"y\": 1<<5}\n",
    "\n",
    "    usecols = [\"object_id\", \"Filter\"]\n",
    "    it = pd.read_csv(\n",
    "        csv_path,\n",
    "        usecols=usecols,\n",
    "        dtype={\"object_id\": \"string\"},\n",
    "        chunksize=chunk_rows,\n",
    "        **SAFE_READ_KW\n",
    "    )\n",
    "\n",
    "    for chunk in it:\n",
    "        chunk = _norm_cols(chunk)\n",
    "        if \"object_id\" not in chunk.columns or \"Filter\" not in chunk.columns:\n",
    "            raise ValueError(f\"[BAD LIGHTCURVE COLUMNS] {csv_path.name}: {list(chunk.columns)}\")\n",
    "\n",
    "        f = chunk[\"Filter\"].astype(\"string\").str.strip().str.lower()\n",
    "        oid = chunk[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "        mask = oid.isin(target_ids)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        sub_oid = oid[mask].astype(str).values\n",
    "        sub_flt = f[mask].astype(str).values\n",
    "\n",
    "        for o, flt in zip(sub_oid, sub_flt):\n",
    "            found.add(o)\n",
    "            obs_count[o] = obs_count.get(o, 0) + 1\n",
    "            band_mask[o] = band_mask.get(o, 0) | band_to_bit.get(flt, 0)\n",
    "\n",
    "        if len(found) == len(target_ids):\n",
    "            break\n",
    "\n",
    "    return found, obs_count, band_mask\n",
    "\n",
    "def _full_scan_lightcurve_object_ids(csv_path: Path, chunk_rows: int = 200_000):\n",
    "    found = set()\n",
    "    it = pd.read_csv(\n",
    "        csv_path,\n",
    "        usecols=[\"object_id\"],\n",
    "        dtype={\"object_id\": \"string\"},\n",
    "        chunksize=chunk_rows,\n",
    "        **SAFE_READ_KW\n",
    "    )\n",
    "    for chunk in it:\n",
    "        chunk = _norm_cols(chunk)\n",
    "        ids = chunk[\"object_id\"].astype(\"string\").str.strip()\n",
    "        found.update(ids.dropna().astype(str).unique().tolist())\n",
    "    return found\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS (auto-discovery)\n",
    "# ----------------------------\n",
    "DEFAULT_DATA_ROOT = Path(\"/kaggle/input/mallorn-dataset\")\n",
    "DATA_ROOT = _discover_data_root(DEFAULT_DATA_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SAMPLE_SUB\": DATA_ROOT / \"sample_submission.csv\",\n",
    "    \"TRAIN_LOG\":  DATA_ROOT / \"train_log.csv\",\n",
    "    \"TEST_LOG\":   DATA_ROOT / \"test_log.csv\",\n",
    "    \"SPLITS\":     [DATA_ROOT / f\"split_{i:02d}\" for i in range(1, 21)],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# WORKDIR (versioned run)\n",
    "# ----------------------------\n",
    "WORKDIR = Path(\"/kaggle/working\")\n",
    "BASE_RUN_DIR = WORKDIR / \"mallorn_run\"\n",
    "BASE_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# CFG — Brutal LB defaults (selaras dengan saran)\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    # Run intent\n",
    "    \"RUN_MODE\": \"brutal_lb\",\n",
    "    \"DEVICE_PREFERRED\": \"auto\",             # auto/cpu/cuda\n",
    "\n",
    "    # Multi-seed (ensemble)\n",
    "    \"SEEDS\": [2025, 2026, 2027],\n",
    "    \"CV_REPEATS\": 1,\n",
    "\n",
    "    # Model switches\n",
    "    \"USE_GBDT\": True,                       # baseline kuat (tabular)\n",
    "    \"USE_TRANSFORMER\": True,\n",
    "    \"USE_HYBRID_BLEND\": True,\n",
    "    \"USE_STACKING\": False,\n",
    "\n",
    "    # F1 essentials\n",
    "    \"USE_THRESHOLD_TUNING\": True,\n",
    "    \"THR_STRATEGY\": \"per_fold_median\",      # per_fold_median / global_best\n",
    "    \"USE_PROBA_CALIBRATION\": True,\n",
    "    \"CALIBRATION_METHOD\": \"temperature\",    # temperature / platt / isotonic\n",
    "\n",
    "    # Photometry / signal policy (KUNCI)\n",
    "    \"USE_DEEXTINCTION\": True,\n",
    "    # SIGNAL_CHANNELS: dibuat flux-safe (bukan mag_ulim) supaya Stage6 scoring & Stage8 agg konsisten\n",
    "    \"SIGNAL_CHANNELS\": [\"asinh_flux\", \"snr_tanh\"],\n",
    "\n",
    "    # flux-safe transform details (dipakai stage 4/5)\n",
    "    \"FLUX_SCALE_POLICY\": \"per_object_median_fluxerr\",  # global_constant / per_object_median_fluxerr\n",
    "    \"GLOBAL_FLUX_SCALE\": 1.0,            # dipakai jika policy global_constant\n",
    "    \"MIN_FLUXERR\": 1e-6,\n",
    "\n",
    "    # SNR handling\n",
    "    \"SNR_CLIP\": 30.0,\n",
    "    \"SNR_DET_THR_LIST\": [2.0, 3.0, 4.0],\n",
    "    \"SNR_STRONG_THR\": 5.0,\n",
    "    \"SNR_TANH_K\": 10.0,                  # snr_tanh = tanh(snr / k)\n",
    "\n",
    "    # Time policy (fisika)\n",
    "    \"USE_REST_FRAME_TIME\": True,         # t_rest = t/(1+z)\n",
    "    \"REST_FRAME_EPS\": 1e-6,\n",
    "\n",
    "    # Delta policy (hindari noise lintas band)\n",
    "    \"DELTA_POLICY\": \"per_band\",          # per_band / global (JANGAN global)\n",
    "\n",
    "    # IO\n",
    "    \"CHUNK_ROWS\": 200_000,\n",
    "\n",
    "    # CV\n",
    "    \"N_FOLDS\": 10,\n",
    "    \"CV_STRATIFY\": True,\n",
    "    \"CV_USE_SPLIT_COL\": True,\n",
    "    \"CV_FORCE_POS_EACH_FOLD\": True,\n",
    "\n",
    "    # Sequence length + windowing (kunci untuk transformer)\n",
    "    \"MAX_LEN_LIST\": [384, 512],\n",
    "    \"WINDOW_POLICY\": \"peak_centered\",    # peak_centered / best_contiguous / multi_window\n",
    "    \"TRAIN_RANDOM_CROP\": True,           # augment crop sekitar peak\n",
    "    \"MULTI_WINDOW_K\": 3,                 # TTA: 2-3 window saat inference (kalau CPU kuat)\n",
    "    \"PEAK_SCORE\": \"snr_pos\",             # snr_pos / abs_signal\n",
    "\n",
    "    # Training defaults\n",
    "    \"DEEP_EPOCHS\": 25,\n",
    "    \"DEEP_BS\": 128,\n",
    "    \"DEEP_LR\": 3e-4,\n",
    "    \"DEEP_WEIGHT_DECAY\": 0.02,\n",
    "    \"DEEP_POS_WEIGHT_MODE\": \"auto\",\n",
    "    \"DEEP_USE_EMA\": True,\n",
    "\n",
    "    # Stage0 validation\n",
    "    \"STAGE0_LC_VALIDATE_MODE\": \"sample\", # off/sample/full\n",
    "    \"STAGE0_LC_SAMPLE_PER_SPLIT\": 80,\n",
    "    \"STAGE0_FAIL_FAST_MISSING_RATE\": 0.01,\n",
    "}\n",
    "\n",
    "CFG_HASH = _hash_cfg(CFG)\n",
    "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = BASE_RUN_DIR / f\"run_{RUN_TAG}_{CFG_HASH}\"\n",
    "\n",
    "ART_DIR   = RUN_DIR / \"artifacts\"\n",
    "CACHE_DIR = RUN_DIR / \"cache\"\n",
    "OOF_DIR   = RUN_DIR / \"oof\"\n",
    "SUB_DIR   = RUN_DIR / \"submissions\"\n",
    "LOG_DIR   = RUN_DIR / \"logs\"\n",
    "FEAT_DIR  = CACHE_DIR / \"features\"\n",
    "SEQ_DIR   = CACHE_DIR / \"seq\"\n",
    "\n",
    "for d in [RUN_DIR, ART_DIR, CACHE_DIR, OOF_DIR, SUB_DIR, LOG_DIR, FEAT_DIR, SEQ_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Validate existence (files + split folders)\n",
    "# ----------------------------\n",
    "_must_exist(PATHS[\"SAMPLE_SUB\"], \"sample_submission.csv\")\n",
    "_must_exist(PATHS[\"TRAIN_LOG\"],  \"train_log.csv\")\n",
    "_must_exist(PATHS[\"TEST_LOG\"],   \"test_log.csv\")\n",
    "\n",
    "for sd in PATHS[\"SPLITS\"]:\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    _must_exist(tr, f\"{sd.name}/train_full_lightcurves.csv\")\n",
    "    _must_exist(te, f\"{sd.name}/test_full_lightcurves.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load logs + sample\n",
    "# ----------------------------\n",
    "df_sub = _norm_cols(pd.read_csv(PATHS[\"SAMPLE_SUB\"], dtype={\"object_id\": \"string\"}, **SAFE_READ_KW))\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission must have object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "if df_sub[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(f\"Duplicated object_id in sample_submission: {int(df_sub['object_id'].duplicated().sum())}\")\n",
    "\n",
    "df_train_log = _norm_cols(pd.read_csv(PATHS[\"TRAIN_LOG\"], dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n",
    "df_test_log  = _norm_cols(pd.read_csv(PATHS[\"TEST_LOG\"],  dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n",
    "\n",
    "need_train = {\"object_id\",\"EBV\",\"Z\",\"split\",\"target\"}\n",
    "need_test  = {\"object_id\",\"EBV\",\"Z\",\"split\"}\n",
    "missing_train = sorted(list(need_train - set(df_train_log.columns)))\n",
    "missing_test  = sorted(list(need_test - set(df_test_log.columns)))\n",
    "if missing_train:\n",
    "    raise ValueError(f\"train_log missing: {missing_train}\")\n",
    "if missing_test:\n",
    "    raise ValueError(f\"test_log missing: {missing_test}\")\n",
    "\n",
    "df_train_log[\"split\"] = df_train_log[\"split\"].map(_normalize_split)\n",
    "df_test_log[\"split\"]  = df_test_log[\"split\"].map(_normalize_split)\n",
    "\n",
    "valid_splits = {f\"split_{i:02d}\" for i in range(1, 21)}\n",
    "bad_tr = sorted(set(df_train_log[\"split\"]) - valid_splits)\n",
    "bad_te = sorted(set(df_test_log[\"split\"]) - valid_splits)\n",
    "if bad_tr:\n",
    "    raise ValueError(f\"Invalid split in train_log (examples): {bad_tr[:10]}\")\n",
    "if bad_te:\n",
    "    raise ValueError(f\"Invalid split in test_log  (examples): {bad_te[:10]}\")\n",
    "\n",
    "for col in [\"EBV\", \"Z\"]:\n",
    "    df_train_log[col] = pd.to_numeric(df_train_log[col], errors=\"coerce\")\n",
    "    df_test_log[col]  = pd.to_numeric(df_test_log[col],  errors=\"coerce\")\n",
    "\n",
    "df_train_log[\"EBV_missing\"] = df_train_log[\"EBV\"].isna().astype(\"int8\")\n",
    "df_train_log[\"Z_missing\"]   = df_train_log[\"Z\"].isna().astype(\"int8\")\n",
    "df_test_log[\"EBV_missing\"]  = df_test_log[\"EBV\"].isna().astype(\"int8\")\n",
    "df_test_log[\"Z_missing\"]    = df_test_log[\"Z\"].isna().astype(\"int8\")\n",
    "\n",
    "ebv_med = float(df_train_log[\"EBV\"].median(skipna=True)) if df_train_log[\"EBV\"].notna().any() else 0.0\n",
    "z_med   = float(df_train_log[\"Z\"].median(skipna=True))   if df_train_log[\"Z\"].notna().any()   else 0.0\n",
    "\n",
    "df_train_log[\"EBV\"] = df_train_log[\"EBV\"].fillna(ebv_med)\n",
    "df_train_log[\"Z\"]   = df_train_log[\"Z\"].fillna(z_med)\n",
    "df_test_log[\"EBV\"]  = df_test_log[\"EBV\"].fillna(ebv_med)\n",
    "df_test_log[\"Z\"]    = df_test_log[\"Z\"].fillna(z_med)\n",
    "\n",
    "df_train_log[\"target\"] = pd.to_numeric(df_train_log[\"target\"], errors=\"coerce\")\n",
    "if df_train_log[\"target\"].isna().any():\n",
    "    raise ValueError(f\"train_log target NaN after coercion: {int(df_train_log['target'].isna().sum())}\")\n",
    "u = set(pd.unique(df_train_log[\"target\"]).tolist())\n",
    "if not u.issubset({0, 1}):\n",
    "    raise ValueError(f\"train_log target must be 0/1. Found: {sorted(list(u))}\")\n",
    "\n",
    "# Z_err handling (domain shift info)\n",
    "if \"Z_err\" not in df_test_log.columns:\n",
    "    df_test_log[\"Z_err\"] = np.nan\n",
    "df_test_log[\"Z_err\"] = pd.to_numeric(df_test_log[\"Z_err\"], errors=\"coerce\")\n",
    "df_test_log[\"has_zerr\"] = (~df_test_log[\"Z_err\"].isna()).astype(\"int8\")\n",
    "df_test_log[\"Z_err\"] = df_test_log[\"Z_err\"].fillna(0.0)\n",
    "\n",
    "if \"Z_err\" not in df_train_log.columns:\n",
    "    df_train_log[\"Z_err\"] = 0.0\n",
    "df_train_log[\"Z_err\"] = pd.to_numeric(df_train_log[\"Z_err\"], errors=\"coerce\").fillna(0.0)\n",
    "df_train_log[\"has_zerr\"] = np.zeros(len(df_train_log), dtype=np.int8)\n",
    "\n",
    "if df_train_log[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(f\"Duplicated object_id in train_log: {int(df_train_log['object_id'].duplicated().sum())}\")\n",
    "if df_test_log[\"object_id\"].duplicated().any():\n",
    "    raise ValueError(f\"Duplicated object_id in test_log:  {int(df_test_log['object_id'].duplicated().sum())}\")\n",
    "\n",
    "sub_ids  = df_sub[\"object_id\"].astype(\"string\").str.strip()\n",
    "test_ids = df_test_log[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "if len(sub_ids) != len(test_ids):\n",
    "    raise ValueError(f\"Row mismatch: sample_submission={len(sub_ids)} vs test_log={len(test_ids)}\")\n",
    "\n",
    "s_sub = set(sub_ids.tolist())\n",
    "s_tst = set(test_ids.tolist())\n",
    "if s_sub != s_tst:\n",
    "    missing_in_test = list(s_sub - s_tst)[:5]\n",
    "    missing_in_sub  = list(s_tst - s_sub)[:5]\n",
    "    raise ValueError(\n",
    "        \"sample_submission and test_log object_id set mismatch.\\n\"\n",
    "        f\"- sample not in test_log (up to5): {missing_in_test}\\n\"\n",
    "        f\"- test_log not in sample (up to5): {missing_in_sub}\"\n",
    "    )\n",
    "\n",
    "SUB_ORDER = sub_ids.tolist()\n",
    "OID2SPLIT_TRAIN = dict(zip(df_train_log[\"object_id\"].astype(str), df_train_log[\"split\"].astype(str)))\n",
    "OID2SPLIT_TEST  = dict(zip(df_test_log[\"object_id\"].astype(str),  df_test_log[\"split\"].astype(str)))\n",
    "\n",
    "# ----------------------------\n",
    "# Health report: per split meta summary\n",
    "# ----------------------------\n",
    "split_rows = []\n",
    "for sp in sorted(valid_splits):\n",
    "    tr = df_train_log[df_train_log[\"split\"] == sp]\n",
    "    te = df_test_log[df_test_log[\"split\"] == sp]\n",
    "    pos = int((tr[\"target\"] == 1).sum())\n",
    "    tot = int(len(tr))\n",
    "    split_rows.append({\n",
    "        \"split\": sp,\n",
    "        \"train_n\": tot,\n",
    "        \"train_pos\": pos,\n",
    "        \"train_pos_pct\": (pos / max(tot, 1)) * 100.0,\n",
    "        \"test_n\": int(len(te)),\n",
    "        \"train_Z_med\": _safe_float(tr[\"Z\"].median(), 0.0) if tot else 0.0,\n",
    "        \"train_EBV_med\": _safe_float(tr[\"EBV\"].median(), 0.0) if tot else 0.0,\n",
    "        \"test_has_zerr_pct\": float(te[\"has_zerr\"].mean() * 100.0) if len(te) else 0.0,\n",
    "        \"test_Zerr_med\": _safe_float(te[\"Z_err\"].median(), 0.0) if len(te) else 0.0,\n",
    "    })\n",
    "df_split_summary = pd.DataFrame(split_rows).sort_values(\"split\").reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# FAIL-FAST Lightcurve validation (off/sample/full)\n",
    "# ----------------------------\n",
    "lc_mode = str(CFG.get(\"STAGE0_LC_VALIDATE_MODE\", \"sample\")).lower().strip()\n",
    "lc_sample_n = int(CFG.get(\"STAGE0_LC_SAMPLE_PER_SPLIT\", 80))\n",
    "chunk_rows = int(CFG.get(\"CHUNK_ROWS\", 200_000))\n",
    "fail_rate = float(CFG.get(\"STAGE0_FAIL_FAST_MISSING_RATE\", 0.01))\n",
    "\n",
    "lc_diag = {\n",
    "    \"mode\": lc_mode,\n",
    "    \"sample_per_split\": lc_sample_n,\n",
    "    \"chunk_rows\": chunk_rows,\n",
    "    \"missing_train_ids\": [],\n",
    "    \"missing_test_ids\": [],\n",
    "    \"missing_train_count\": 0,\n",
    "    \"missing_test_count\": 0,\n",
    "    \"missing_train_rate\": 0.0,\n",
    "    \"missing_test_rate\": 0.0,\n",
    "    \"sample_obs_stats\": {},\n",
    "    \"sample_band_coverage\": {}\n",
    "}\n",
    "\n",
    "if lc_mode not in [\"off\", \"sample\", \"full\"]:\n",
    "    raise ValueError(\"CFG['STAGE0_LC_VALIDATE_MODE'] must be one of: off/sample/full\")\n",
    "\n",
    "if lc_mode != \"off\":\n",
    "    print(\"\\n[STAGE0] Lightcurve validation:\", lc_mode)\n",
    "\n",
    "    missing_train = []\n",
    "    missing_test = []\n",
    "    sample_obs_stats = {}\n",
    "    sample_band_cov = {}\n",
    "\n",
    "    for sp in sorted(valid_splits):\n",
    "        split_dir = DATA_ROOT / sp\n",
    "        tr_path = split_dir / \"train_full_lightcurves.csv\"\n",
    "        te_path = split_dir / \"test_full_lightcurves.csv\"\n",
    "\n",
    "        tr_ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(str).tolist()\n",
    "        te_ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(str).tolist()\n",
    "\n",
    "        if lc_mode == \"full\":\n",
    "            found_tr = _full_scan_lightcurve_object_ids(tr_path, chunk_rows=chunk_rows) if len(tr_ids) else set()\n",
    "            found_te = _full_scan_lightcurve_object_ids(te_path, chunk_rows=chunk_rows) if len(te_ids) else set()\n",
    "\n",
    "            miss_tr = sorted(list(set(tr_ids) - found_tr))\n",
    "            miss_te = sorted(list(set(te_ids) - found_te))\n",
    "\n",
    "            if miss_tr:\n",
    "                missing_train.extend([(sp, x) for x in miss_tr[:50]])\n",
    "            if miss_te:\n",
    "                missing_test.extend([(sp, x) for x in miss_te[:50]])\n",
    "\n",
    "            sample_obs_stats[sp] = {\n",
    "                \"full_mode\": True,\n",
    "                \"train_missing\": len(miss_tr),\n",
    "                \"test_missing\": len(miss_te),\n",
    "            }\n",
    "            sample_band_cov[sp] = {\"full_mode\": True}\n",
    "\n",
    "        else:\n",
    "            tr_s = _sample_ids_per_split(df_train_log, sp, lc_sample_n, SEED)\n",
    "            te_s = _sample_ids_per_split(df_test_log,  sp, lc_sample_n, SEED + 1)\n",
    "\n",
    "            band_cov_bits = 0\n",
    "\n",
    "            if len(tr_s):\n",
    "                found, obs_count, band_mask = _scan_lightcurve_for_ids(tr_path, set(tr_s), chunk_rows=chunk_rows)\n",
    "                miss = [x for x in tr_s if x not in found]\n",
    "                missing_train.extend([(sp, x) for x in miss])\n",
    "\n",
    "                counts = [obs_count.get(x, 0) for x in tr_s]\n",
    "                bandbits = [band_mask.get(x, 0) for x in tr_s]\n",
    "                band_cov_bits |= int(np.bitwise_or.reduce(bandbits)) if len(bandbits) else 0\n",
    "\n",
    "                sample_obs_stats.setdefault(sp, {})\n",
    "                sample_obs_stats[sp].update({\n",
    "                    \"train_sample_n\": len(tr_s),\n",
    "                    \"train_sample_missing\": len(miss),\n",
    "                    \"train_sample_obs_min\": int(np.min(counts)) if len(counts) else 0,\n",
    "                    \"train_sample_obs_med\": float(np.median(counts)) if len(counts) else 0.0,\n",
    "                    \"train_sample_obs_p95\": float(np.percentile(counts, 95)) if len(counts) else 0.0,\n",
    "                })\n",
    "\n",
    "            if len(te_s):\n",
    "                found, obs_count, band_mask = _scan_lightcurve_for_ids(te_path, set(te_s), chunk_rows=chunk_rows)\n",
    "                miss = [x for x in te_s if x not in found]\n",
    "                missing_test.extend([(sp, x) for x in miss])\n",
    "\n",
    "                counts = [obs_count.get(x, 0) for x in te_s]\n",
    "                bandbits = [band_mask.get(x, 0) for x in te_s]\n",
    "                band_cov_bits |= int(np.bitwise_or.reduce(bandbits)) if len(bandbits) else 0\n",
    "\n",
    "                sample_obs_stats.setdefault(sp, {})\n",
    "                sample_obs_stats[sp].update({\n",
    "                    \"test_sample_n\": len(te_s),\n",
    "                    \"test_sample_missing\": len(miss),\n",
    "                    \"test_sample_obs_min\": int(np.min(counts)) if len(counts) else 0,\n",
    "                    \"test_sample_obs_med\": float(np.median(counts)) if len(counts) else 0.0,\n",
    "                    \"test_sample_obs_p95\": float(np.percentile(counts, 95)) if len(counts) else 0.0,\n",
    "                })\n",
    "\n",
    "            bit_to_band = [(1<<0,\"u\"),(1<<1,\"g\"),(1<<2,\"r\"),(1<<3,\"i\"),(1<<4,\"z\"),(1<<5,\"y\")]\n",
    "            bands_present = [b for bit,b in bit_to_band if (band_cov_bits & bit)]\n",
    "            sample_band_cov[sp] = {\n",
    "                \"bands_present_in_sample\": bands_present,\n",
    "                \"bands_present_count\": len(bands_present),\n",
    "            }\n",
    "\n",
    "    lc_diag[\"missing_train_ids\"] = missing_train[:200]\n",
    "    lc_diag[\"missing_test_ids\"] = missing_test[:200]\n",
    "    lc_diag[\"missing_train_count\"] = len(missing_train)\n",
    "    lc_diag[\"missing_test_count\"] = len(missing_test)\n",
    "\n",
    "    if lc_mode == \"full\":\n",
    "        lc_diag[\"missing_train_rate\"] = None\n",
    "        lc_diag[\"missing_test_rate\"]  = None\n",
    "    else:\n",
    "        total_train_sample = sum(v.get(\"train_sample_n\", 0) for v in sample_obs_stats.values())\n",
    "        total_test_sample  = sum(v.get(\"test_sample_n\", 0)  for v in sample_obs_stats.values())\n",
    "        lc_diag[\"missing_train_rate\"] = (len(missing_train) / max(total_train_sample, 1))\n",
    "        lc_diag[\"missing_test_rate\"]  = (len(missing_test)  / max(total_test_sample, 1))\n",
    "\n",
    "    lc_diag[\"sample_obs_stats\"] = sample_obs_stats\n",
    "    lc_diag[\"sample_band_coverage\"] = sample_band_cov\n",
    "\n",
    "    if lc_mode == \"sample\":\n",
    "        if lc_diag[\"missing_test_rate\"] > fail_rate or lc_diag[\"missing_train_rate\"] > fail_rate:\n",
    "            raise RuntimeError(\n",
    "                \"[FAIL-FAST] Lightcurve validation indicates missing object_id in lightcurve files.\\n\"\n",
    "                f\"- missing_train_rate={lc_diag['missing_train_rate']:.3%}\\n\"\n",
    "                f\"- missing_test_rate ={lc_diag['missing_test_rate']:.3%}\\n\"\n",
    "                \"Periksa split routing / file path / object_id normalization.\"\n",
    "            )\n",
    "        if len(missing_test) > 0 or len(missing_train) > 0:\n",
    "            print(\"[WARN] Ada sample object_id yang tidak ditemukan di lightcurve file.\")\n",
    "            print(\"       Contoh missing_test:\", missing_test[:5])\n",
    "            print(\"       Contoh missing_train:\", missing_train[:5])\n",
    "\n",
    "# ----------------------------\n",
    "# Basic counts\n",
    "# ----------------------------\n",
    "pos = int((df_train_log[\"target\"] == 1).sum())\n",
    "neg = int((df_train_log[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_log))\n",
    "\n",
    "print(\"ENV OK (Stage0)\")\n",
    "print(f\"- DEVICE: {DEVICE} | THREADS: {THREADS}\")\n",
    "print(f\"- DATA_ROOT: {DATA_ROOT}\")\n",
    "print(f\"- Python: {sys.version.split()[0]}\")\n",
    "print(f\"- Numpy:  {np.__version__}\")\n",
    "print(f\"- Pandas: {pd.__version__}\")\n",
    "if TORCH_OK:\n",
    "    print(f\"- Torch:  {torch.__version__} | CUDA: {_cuda_ok}\")\n",
    "else:\n",
    "    print(\"- Torch:  not available\")\n",
    "\n",
    "print(\"\\nDATA OK\")\n",
    "print(f\"- train_log objects: {tot:,} | pos={pos:,} | neg={neg:,} | pos%={(pos/max(tot,1))*100:.3f}%\")\n",
    "print(f\"- test_log objects:  {len(df_test_log):,}\")\n",
    "print(f\"- sample_submission: {len(df_sub):,}\")\n",
    "print(f\"- splits: {len(PATHS['SPLITS'])} folders (01..20)\")\n",
    "print(f\"- RUN_DIR: {RUN_DIR}\")\n",
    "\n",
    "print(\"\\nSPLIT SUMMARY (meta)\")\n",
    "try:\n",
    "    display(df_split_summary.head(10))\n",
    "except Exception:\n",
    "    print(df_split_summary.head(10).to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# Save diagnostics snapshot\n",
    "# ----------------------------\n",
    "diag = {\n",
    "    \"SEED\": SEED,\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"THREADS\": THREADS,\n",
    "    \"CFG\": CFG,\n",
    "    \"CFG_HASH\": CFG_HASH,\n",
    "    \"RUN_DIR\": str(RUN_DIR),\n",
    "    \"DATA_ROOT\": str(DATA_ROOT),\n",
    "    \"counts\": {\n",
    "        \"train_objects\": int(len(df_train_log)),\n",
    "        \"train_pos\": int(pos),\n",
    "        \"train_neg\": int(neg),\n",
    "        \"test_objects\": int(len(df_test_log)),\n",
    "        \"sample_rows\": int(len(df_sub)),\n",
    "    },\n",
    "    \"split_meta_summary\": df_split_summary.to_dict(orient=\"records\"),\n",
    "    \"lightcurve_validation\": lc_diag,\n",
    "}\n",
    "\n",
    "with open(RUN_DIR / \"config_stage0.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"SEED\": SEED,\n",
    "            \"DEVICE\": DEVICE,\n",
    "            \"THREADS\": THREADS,\n",
    "            \"CFG\": CFG,\n",
    "            \"CFG_HASH\": CFG_HASH,\n",
    "            \"RUN_DIR\": str(RUN_DIR),\n",
    "            \"DATA_ROOT\": str(DATA_ROOT),\n",
    "        },\n",
    "        f, indent=2\n",
    "    )\n",
    "\n",
    "with open(RUN_DIR / \"run_diagnostics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(diag, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# Export globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"SEED\": SEED, \"DEVICE\": DEVICE, \"THREADS\": THREADS,\n",
    "    \"CFG\": CFG, \"CFG_HASH\": CFG_HASH,\n",
    "    \"PATHS\": PATHS, \"DATA_ROOT\": DATA_ROOT, \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR, \"CACHE_DIR\": CACHE_DIR, \"OOF_DIR\": OOF_DIR,\n",
    "    \"SUB_DIR\": SUB_DIR, \"LOG_DIR\": LOG_DIR, \"FEAT_DIR\": FEAT_DIR, \"SEQ_DIR\": SEQ_DIR,\n",
    "    \"df_sub\": df_sub, \"df_train_log\": df_train_log, \"df_test_log\": df_test_log,\n",
    "    \"OID2SPLIT_TRAIN\": OID2SPLIT_TRAIN, \"OID2SPLIT_TEST\": OID2SPLIT_TEST,\n",
    "    \"SUB_ORDER\": SUB_ORDER,\n",
    "    \"df_split_summary\": df_split_summary,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e7a29",
   "metadata": {
    "papermill": {
     "duration": 0.014649,
     "end_time": "2026-01-07T16:45:12.737180",
     "exception": false,
     "start_time": "2026-01-07T16:45:12.722531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Verify Dataset Paths & Split Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5253e900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:45:12.769604Z",
     "iopub.status.busy": "2026-01-07T16:45:12.769230Z",
     "iopub.status.idle": "2026-01-07T16:45:27.505142Z",
     "shell.execute_reply": "2026-01-07T16:45:27.504024Z"
    },
    "papermill": {
     "duration": 14.755688,
     "end_time": "2026-01-07T16:45:27.507470",
     "exception": false,
     "start_time": "2026-01-07T16:45:12.751782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STAGE1] LC_VALIDATE_MODE=sample | HEAD_ROWS=4000 | SAMPLE_ID_PER_SPLIT=80\n",
      "[STAGE1] SNR_DET_THR_LIST=[2.0, 3.0, 4.0] | SNR_STRONG_THR=5.0 | USE_REST=True\n",
      "\n",
      "[STAGE1] Building object_quality (full streaming scan) ...\n",
      "\n",
      "STAGE 1 OK — ROUTING + PROFILING + OBJECT_QUALITY READY\n",
      "- routing saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/split_routing.csv\n",
      "- lc sample stats saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/lc_sample_stats.csv\n",
      "- id warnings saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/lc_id_presence_warnings.csv\n",
      "- object_quality_train: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/object_quality_train.csv\n",
      "- object_quality_test : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/object_quality_test.csv\n",
      "- split_quality_summary: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/split_quality_summary.csv\n",
      "- summary json saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/stage1_summary.json\n",
      "- elapsed: 0.24 min | warn_flux_na_files=31\n",
      "\n",
      "TOP ISSUES (ID missing in sample/adaptive scan)\n",
      "   split  kind  id_missing  id_check_k  id_scan_chunks  id_scan_cap_used  file_mb\n",
      "split_01 train           0          80               1                 6 1.351567\n",
      "split_01  test           0          80               1                 6 3.052048\n",
      "split_02 train           0          80               1                 6 1.326915\n",
      "split_02  test           0          80               1                 6 3.658606\n",
      "split_03 train           0          80               1                 6 1.124688\n",
      "split_03  test           0          80               1                 6 2.770512\n",
      "split_04 train           0          80               1                 6 1.173453\n",
      "split_04  test           0          80               1                 6 2.658786\n",
      "split_05 train           0          80               1                 6 1.322038\n",
      "split_05  test           0          80               1                 6 3.122271\n",
      "\n",
      "TOP PATTERN (highest negative flux fraction in sample head)\n",
      "   split  kind  flux_neg_frac  snr_ge_det_frac  file_mb\n",
      "split_03  test       0.436250         0.443250 2.770512\n",
      "split_12  test       0.429000         0.256000 2.802762\n",
      "split_20 train       0.425356         0.401350 1.215814\n",
      "split_16  test       0.424250         0.215500 3.013560\n",
      "split_06 train       0.423712         0.269635 1.326515\n",
      "split_04  test       0.422606         0.277069 2.658786\n",
      "split_19  test       0.414372         0.266400 2.904259\n",
      "split_12 train       0.413267         0.349937 1.299782\n",
      "split_02 train       0.411456         0.286393 1.326915\n",
      "split_13  test       0.411250         0.346750 3.300406\n",
      "\n",
      "TOP PATTERN (highest high-SNR fraction in sample head)\n",
      "   split  kind  snr_ge_det_frac  snr_ge_strong_frac  snr_abs_p95  file_mb\n",
      "split_03  test         0.443250            0.242000    15.323467 2.770512\n",
      "split_01  test         0.436250            0.237000    17.110734 3.052048\n",
      "split_18 train         0.408852            0.199300    14.096381 1.102348\n",
      "split_07 train         0.406775            0.215336    17.856070 1.257892\n",
      "split_09  test         0.404655            0.207958    20.268570 2.419297\n",
      "split_20 train         0.401350            0.203551    16.973770 1.215814\n",
      "split_14 train         0.398248            0.190238    19.179551 1.316985\n",
      "split_04 train         0.389640            0.207708    15.424138 1.173453\n",
      "split_15  test         0.389017            0.159353    11.692290 2.714948\n",
      "split_02  test         0.382346            0.198300    15.760137 3.658606\n",
      "\n",
      "Stage 1 complete: splits verified + routing/stats + object_quality exported.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Split Routing + LC Profiling + Object Quality Features (ONE CELL)\n",
    "# REVISI FULL v5.1 (BRUTAL-LB READY + ANTI-AGN + REST-FRAME READY)\n",
    "#\n",
    "# v5.1 vs v5.0:\n",
    "# - Object quality tambah fitur kuat:\n",
    "#   * det_count abs + POS/NEG split (anti-AGN)\n",
    "#   * multi-threshold det counts (ikut CFG[\"SNR_DET_THR_LIST\"])\n",
    "#   * snr_pos_max / snr_neg_min\n",
    "#   * flux_max/min/mean/std + ferr_mean (valid numeric rows)\n",
    "#   * per-band det counts (base thr) + n_bands_det\n",
    "#   * timespan_rest + cadence_proxy_rest jika USE_REST_FRAME_TIME=True\n",
    "# - Fix: chunk[\"Filter\"] dinormalisasi sebelum band counts groupby\n",
    "# - Robust: ignore rows object_id yang tidak ada di log index (tidak crash)\n",
    "# ============================================================\n",
    "\n",
    "import re, gc, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "need0 = [\"PATHS\", \"df_train_log\", \"df_test_log\", \"RUN_DIR\", \"LOG_DIR\", \"CFG\", \"SEED\"]\n",
    "for need in need0:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu.\")\n",
    "\n",
    "DATA_ROOT = Path(PATHS[\"DATA_ROOT\"])\n",
    "RUN_DIR = Path(RUN_DIR)\n",
    "LOG_DIR = Path(LOG_DIR)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG_LOCAL = globals().get(\"CFG\", {}) or {}\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "\n",
    "# deterministic split list (ikuti PATHS[\"SPLITS\"])\n",
    "if \"SPLITS\" not in PATHS or not isinstance(PATHS[\"SPLITS\"], (list, tuple)) or len(PATHS[\"SPLITS\"]) == 0:\n",
    "    raise RuntimeError(\"PATHS['SPLITS'] tidak valid. Pastikan STAGE 0 sukses.\")\n",
    "SPLIT_LIST = [Path(p).name for p in PATHS[\"SPLITS\"]]\n",
    "SPLIT_LIST = [s for s in SPLIT_LIST if s.startswith(\"split_\")]\n",
    "SPLIT_LIST = sorted(SPLIT_LIST)  # split_01..split_20\n",
    "\n",
    "VALID_SPLITS = set([f\"split_{i:02d}\" for i in range(1, 21)])\n",
    "if set(SPLIT_LIST) != VALID_SPLITS:\n",
    "    raise RuntimeError(\n",
    "        \"SPLIT_LIST mismatch dengan expected split_01..split_20.\\n\"\n",
    "        f\"Found (first 10): {sorted(list(set(SPLIT_LIST)))[:10]}\"\n",
    "    )\n",
    "\n",
    "SPLIT_DIRS = {Path(p).name: Path(p) for p in PATHS[\"SPLITS\"]}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Safe read config\n",
    "# ----------------------------\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "# micro profiling knobs\n",
    "HEAD_ROWS = int(CFG_LOCAL.get(\"STAGE1_HEAD_ROWS\", CFG_LOCAL.get(\"LC_HEAD_ROWS\", 4000)))\n",
    "\n",
    "# sample ID presence knobs\n",
    "SAMPLE_ID_PER_SPLIT = int(\n",
    "    CFG_LOCAL.get(\n",
    "        \"STAGE1_ID_SAMPLE_PER_SPLIT\",\n",
    "        CFG_LOCAL.get(\"STAGE0_LC_SAMPLE_PER_SPLIT\", CFG_LOCAL.get(\"SAMPLE_ID_PER_SPLIT\", 12))\n",
    "    )\n",
    ")\n",
    "\n",
    "CHUNK_ROWS = int(CFG_LOCAL.get(\"CHUNK_ROWS\", 200_000))\n",
    "\n",
    "# adaptive scan caps\n",
    "MAX_CHUNKS_PER_FILE = int(CFG_LOCAL.get(\"MAX_CHUNKS_PER_FILE\", 6))\n",
    "MAX_CHUNKS_HARD = int(CFG_LOCAL.get(\"MAX_CHUNKS_HARD\", 30))\n",
    "\n",
    "# numeric sanity thresholds\n",
    "MAX_TIME_NA_FRAC = float(CFG_LOCAL.get(\"MAX_TIME_NA_FRAC\", 0.02))\n",
    "MAX_FERR_NA_FRAC = float(CFG_LOCAL.get(\"MAX_FERR_NA_FRAC\", 0.02))\n",
    "MIN_SAMPLE_ROWS  = int(CFG_LOCAL.get(\"MIN_SAMPLE_ROWS\", 200))\n",
    "\n",
    "# ID miss handling\n",
    "ID_MISS_FAIL_FRAC = float(CFG_LOCAL.get(\"ID_MISS_FAIL_FRAC\", 0.80))\n",
    "FAIL_FAST_MISSING_RATE = float(\n",
    "    CFG_LOCAL.get(\"STAGE1_FAIL_FAST_MISSING_RATE\", CFG_LOCAL.get(\"STAGE0_FAIL_FAST_MISSING_RATE\", 0.01))\n",
    ")\n",
    "\n",
    "# Stage1 validate mode\n",
    "LC_VALIDATE_MODE = str(\n",
    "    CFG_LOCAL.get(\"STAGE1_LC_VALIDATE_MODE\", CFG_LOCAL.get(\"STAGE0_LC_VALIDATE_MODE\", \"sample\"))\n",
    ").lower().strip()\n",
    "if LC_VALIDATE_MODE not in [\"off\", \"sample\", \"full\"]:\n",
    "    raise ValueError(\"CFG['STAGE1_LC_VALIDATE_MODE'] must be one of: off/sample/full\")\n",
    "\n",
    "# SNR policy\n",
    "SNR_CLIP = float(CFG_LOCAL.get(\"SNR_CLIP\", 30.0))\n",
    "MIN_FLUXERR = float(CFG_LOCAL.get(\"MIN_FLUXERR\", 1e-6))\n",
    "\n",
    "# det threshold list (brutal): simpan multi-threshold count per object\n",
    "_thr_list = CFG_LOCAL.get(\"SNR_DET_THR_LIST\", None)\n",
    "if isinstance(_thr_list, (list, tuple)) and len(_thr_list) > 0:\n",
    "    SNR_DET_THR_LIST = [float(x) for x in _thr_list]\n",
    "else:\n",
    "    SNR_DET_THR_LIST = [float(CFG_LOCAL.get(\"SNR_DET_THR\", 3.0))]\n",
    "# sanitize\n",
    "SNR_DET_THR_LIST = sorted(list(dict.fromkeys([float(x) for x in SNR_DET_THR_LIST if np.isfinite(float(x)) and float(x) > 0])))\n",
    "if len(SNR_DET_THR_LIST) == 0:\n",
    "    SNR_DET_THR_LIST = [3.0]\n",
    "\n",
    "# base threshold for “per-band det count”\n",
    "SNR_DET_THR = float(SNR_DET_THR_LIST[0])\n",
    "SNR_STRONG_THR = float(CFG_LOCAL.get(\"SNR_STRONG_THR\", 5.0))\n",
    "\n",
    "# rest-frame policy\n",
    "USE_REST = bool(CFG_LOCAL.get(\"USE_REST_FRAME_TIME\", False))\n",
    "REST_EPS = float(CFG_LOCAL.get(\"REST_FRAME_EPS\", 1e-6))\n",
    "\n",
    "# NEW: build full object quality table\n",
    "BUILD_OBJECT_QUALITY = bool(CFG_LOCAL.get(\"STAGE1_BUILD_OBJECT_QUALITY\", True))\n",
    "OBJ_QUALITY_CHUNK_ROWS = int(CFG_LOCAL.get(\"STAGE1_OBJ_QUALITY_CHUNK_ROWS\", CHUNK_ROWS))\n",
    "ZEROOBS_FAIL_RATE = float(CFG_LOCAL.get(\"STAGE1_ZEROOBS_FAIL_RATE\", 0.0005))  # 0.05% default\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Helpers\n",
    "# ----------------------------\n",
    "REQ_LC_COLS = [\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"]\n",
    "REQ_LC_COLS_SET = set([c.strip() for c in REQ_LC_COLS])\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "BANDS = [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]\n",
    "BAND_TO_IDX = {b:i for i,b in enumerate(BANDS)}\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def normalize_split_name(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.isdigit():\n",
    "        return f\"split_{int(s2):02d}\"\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    return s2\n",
    "\n",
    "def sizeof_mb(p: Path) -> float:\n",
    "    try:\n",
    "        return p.stat().st_size / (1024**2)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "_HEADER_CACHE = {}\n",
    "\n",
    "def _get_usecols(csv_path: Path, required_trimmed: set):\n",
    "    key = str(csv_path)\n",
    "    if key in _HEADER_CACHE:\n",
    "        cols0, trim2orig = _HEADER_CACHE[key]\n",
    "    else:\n",
    "        df0 = pd.read_csv(csv_path, nrows=0, **SAFE_READ_KW)\n",
    "        cols0 = list(df0.columns)\n",
    "        trim2orig = {}\n",
    "        for c in cols0:\n",
    "            ct = str(c).strip()\n",
    "            if ct not in trim2orig:\n",
    "                trim2orig[ct] = c\n",
    "        _HEADER_CACHE[key] = (cols0, trim2orig)\n",
    "\n",
    "    missing = sorted(list(required_trimmed - set(trim2orig.keys())))\n",
    "    if missing:\n",
    "        found_trim = sorted(list(trim2orig.keys()))\n",
    "        raise ValueError(\n",
    "            f\"[LC SCHEMA] {csv_path} missing required columns (trim-aware): {missing}\\n\"\n",
    "            f\"Found columns (trimmed, first 50): {found_trim[:50]}\"\n",
    "        )\n",
    "    usecols = [trim2orig[c] for c in required_trimmed]\n",
    "    return usecols\n",
    "\n",
    "def _read_sample_df(p: Path, nrows: int):\n",
    "    usecols = _get_usecols(p, REQ_LC_COLS_SET)\n",
    "    dfh = pd.read_csv(p, usecols=usecols, nrows=nrows, **SAFE_READ_KW)\n",
    "    dfh = _norm_cols(dfh)\n",
    "    return dfh\n",
    "\n",
    "def _numeric_and_filter_stats(dfh: pd.DataFrame):\n",
    "    out = {\"n_sample\": int(len(dfh))}\n",
    "    if len(dfh) == 0:\n",
    "        out.update({k: np.nan for k in [\n",
    "            \"time_na_frac\",\"flux_na_frac\",\"ferr_na_frac\",\n",
    "            \"time_min\",\"time_max\",\"time_span\",\n",
    "            \"flux_neg_frac\",\"flux_p01\",\"flux_p50\",\"flux_p99\",\n",
    "            \"ferr_min\",\"ferr_p50\",\"ferr_p99\",\n",
    "            \"snr_abs_p50\",\"snr_abs_p95\",\"snr_ge_det_frac\",\"snr_ge_strong_frac\",\n",
    "            \"ferr_zero_frac\"\n",
    "        ]})\n",
    "        out.update({\"filter_bad\": \"\", \"filter_sample\": \"\"})\n",
    "        for b in BANDS:\n",
    "            out[f\"frac_{b}\"] = 0.0\n",
    "        out[\"ferr_neg_any\"] = 0\n",
    "        return out\n",
    "\n",
    "    filt = dfh[\"Filter\"].astype(\"string\").str.strip().str.lower()\n",
    "    filt = filt[~filt.isna()]\n",
    "    uniq = sorted(set(filt.tolist()))\n",
    "    bad = sorted([v for v in uniq if v not in ALLOWED_FILTERS])\n",
    "    out[\"filter_bad\"] = \",\".join(bad[:10]) if bad else \"\"\n",
    "    out[\"filter_sample\"] = \",\".join(uniq[:10]) if uniq else \"\"\n",
    "\n",
    "    if len(filt) > 0:\n",
    "        vc = filt.value_counts()\n",
    "        denom = float(vc.sum())\n",
    "        for b in BANDS:\n",
    "            out[f\"frac_{b}\"] = float(vc.get(b, 0) / denom)\n",
    "    else:\n",
    "        for b in BANDS:\n",
    "            out[f\"frac_{b}\"] = 0.0\n",
    "\n",
    "    t = pd.to_numeric(dfh[\"Time (MJD)\"], errors=\"coerce\")\n",
    "    f = pd.to_numeric(dfh[\"Flux\"], errors=\"coerce\")\n",
    "    e = pd.to_numeric(dfh[\"Flux_err\"], errors=\"coerce\")\n",
    "\n",
    "    out[\"time_na_frac\"] = float(t.isna().mean())\n",
    "    out[\"flux_na_frac\"] = float(f.isna().mean())\n",
    "    out[\"ferr_na_frac\"] = float(e.isna().mean())\n",
    "\n",
    "    if (~t.isna()).any():\n",
    "        out[\"time_min\"] = float(t.min())\n",
    "        out[\"time_max\"] = float(t.max())\n",
    "        out[\"time_span\"] = float(t.max() - t.min())\n",
    "    else:\n",
    "        out[\"time_min\"] = np.nan\n",
    "        out[\"time_max\"] = np.nan\n",
    "        out[\"time_span\"] = np.nan\n",
    "\n",
    "    if (~f.isna()).any():\n",
    "        fd = f.dropna()\n",
    "        out[\"flux_neg_frac\"] = float((fd < 0).mean())\n",
    "        out[\"flux_p01\"] = float(np.quantile(fd, 0.01))\n",
    "        out[\"flux_p50\"] = float(np.quantile(fd, 0.50))\n",
    "        out[\"flux_p99\"] = float(np.quantile(fd, 0.99))\n",
    "    else:\n",
    "        out[\"flux_neg_frac\"] = np.nan\n",
    "        out[\"flux_p01\"] = np.nan\n",
    "        out[\"flux_p50\"] = np.nan\n",
    "        out[\"flux_p99\"] = np.nan\n",
    "\n",
    "    if (~e.isna()).any():\n",
    "        ed = e.dropna()\n",
    "        out[\"ferr_min\"] = float(ed.min())\n",
    "        out[\"ferr_p50\"] = float(np.quantile(ed, 0.50))\n",
    "        out[\"ferr_p99\"] = float(np.quantile(ed, 0.99))\n",
    "        out[\"ferr_neg_any\"] = int((ed < 0).any())\n",
    "        out[\"ferr_zero_frac\"] = float((ed <= 0).mean())\n",
    "    else:\n",
    "        out[\"ferr_min\"] = np.nan\n",
    "        out[\"ferr_p50\"] = np.nan\n",
    "        out[\"ferr_p99\"] = np.nan\n",
    "        out[\"ferr_neg_any\"] = 0\n",
    "        out[\"ferr_zero_frac\"] = np.nan\n",
    "\n",
    "    if (~f.isna()).any() and (~e.isna()).any():\n",
    "        ff = f.to_numpy()\n",
    "        ee = e.to_numpy()\n",
    "        m = np.isfinite(ff) & np.isfinite(ee)\n",
    "        if m.any():\n",
    "            ee2 = np.maximum(ee[m], MIN_FLUXERR)\n",
    "            snr = ff[m] / ee2\n",
    "            snr = np.clip(snr, -SNR_CLIP, SNR_CLIP)\n",
    "            snr_abs = np.abs(snr)\n",
    "            out[\"snr_abs_p50\"] = float(np.quantile(snr_abs, 0.50))\n",
    "            out[\"snr_abs_p95\"] = float(np.quantile(snr_abs, 0.95))\n",
    "            out[\"snr_ge_det_frac\"] = float((snr_abs >= SNR_DET_THR).mean())\n",
    "            out[\"snr_ge_strong_frac\"] = float((snr_abs >= SNR_STRONG_THR).mean())\n",
    "        else:\n",
    "            out[\"snr_abs_p50\"] = np.nan\n",
    "            out[\"snr_abs_p95\"] = np.nan\n",
    "            out[\"snr_ge_det_frac\"] = np.nan\n",
    "            out[\"snr_ge_strong_frac\"] = np.nan\n",
    "    else:\n",
    "        out[\"snr_abs_p50\"] = np.nan\n",
    "        out[\"snr_abs_p95\"] = np.nan\n",
    "        out[\"snr_ge_det_frac\"] = np.nan\n",
    "        out[\"snr_ge_strong_frac\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "def _sample_id_presence_adaptive(csv_path: Path, want_ids: set, chunk_rows: int,\n",
    "                                 max_chunks_init: int, max_chunks_hard: int):\n",
    "    if not want_ids:\n",
    "        return 0, set(), 0, max_chunks_init\n",
    "\n",
    "    usecols = _get_usecols(csv_path, {\"object_id\"})\n",
    "    remaining = set(want_ids)\n",
    "    found = set()\n",
    "    total_chunks_read = 0\n",
    "    used_cap = max_chunks_init\n",
    "\n",
    "    cap = max_chunks_init\n",
    "    while True:\n",
    "        nread = 0\n",
    "        for i, chunk in enumerate(pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_rows, **SAFE_READ_KW)):\n",
    "            if i < total_chunks_read:\n",
    "                continue\n",
    "            nread += 1\n",
    "            total_chunks_read += 1\n",
    "            chunk = _norm_cols(chunk)\n",
    "            ids = set(chunk[\"object_id\"].astype(\"string\").dropna().str.strip().astype(str).tolist())\n",
    "            hit = remaining & ids\n",
    "            if hit:\n",
    "                found |= hit\n",
    "                remaining -= hit\n",
    "            if not remaining:\n",
    "                break\n",
    "            if nread >= cap:\n",
    "                break\n",
    "\n",
    "        used_cap = cap\n",
    "        if not remaining:\n",
    "            break\n",
    "        if total_chunks_read >= max_chunks_hard:\n",
    "            break\n",
    "        cap = min(cap * 2, max_chunks_hard)\n",
    "\n",
    "    return len(found), remaining, total_chunks_read, used_cap\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Normalize split col in logs (idempotent)\n",
    "# ----------------------------\n",
    "for df, name in [(df_train_log, \"train_log\"), (df_test_log, \"test_log\")]:\n",
    "    if \"split\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing 'split' column.\")\n",
    "    df[\"split\"] = df[\"split\"].map(normalize_split_name)\n",
    "\n",
    "bad_train_split = sorted(set(df_train_log[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test_log[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log has invalid split values (examples): {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log has invalid split values (examples): {bad_test_split[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Verify disk splits set + required files exist\n",
    "# ----------------------------\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "missing_dirs = sorted(list(VALID_SPLITS - disk_splits))\n",
    "extra_dirs   = sorted(list(disk_splits - VALID_SPLITS))\n",
    "if missing_dirs or extra_dirs:\n",
    "    msg = []\n",
    "    if missing_dirs: msg.append(f\"Missing split folders: {missing_dirs[:10]}\")\n",
    "    if extra_dirs:   msg.append(f\"Unexpected split folders: {extra_dirs[:10]}\")\n",
    "    raise RuntimeError(\"Split folder set mismatch.\\n\" + \"\\n\".join(msg))\n",
    "\n",
    "missing_files = []\n",
    "for sp in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    for kind in [\"train\", \"test\"]:\n",
    "        p = sd / f\"{kind}_full_lightcurves.csv\"\n",
    "        if not p.exists():\n",
    "            missing_files.append(str(p))\n",
    "if missing_files:\n",
    "    raise FileNotFoundError(\"Some lightcurve files missing (showing up to 10):\\n\" + \"\\n\".join(missing_files[:10]))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Build routing manifest\n",
    "# ----------------------------\n",
    "train_counts = df_train_log[\"split\"].value_counts().to_dict()\n",
    "test_counts  = df_test_log[\"split\"].value_counts().to_dict()\n",
    "\n",
    "routing_rows = []\n",
    "for sp in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    routing_rows.append({\n",
    "        \"split\": sp,\n",
    "        \"train_csv\": str(tr),\n",
    "        \"test_csv\": str(te),\n",
    "        \"train_mb\": sizeof_mb(tr),\n",
    "        \"test_mb\": sizeof_mb(te),\n",
    "        \"n_train_objects_log\": int(train_counts.get(sp, 0)),\n",
    "        \"n_test_objects_log\":  int(test_counts.get(sp, 0)),\n",
    "    })\n",
    "\n",
    "df_routing = pd.DataFrame(routing_rows)\n",
    "routing_path = LOG_DIR / \"split_routing.csv\"\n",
    "df_routing.to_csv(routing_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Micro profiling + ID crosscheck\n",
    "# ----------------------------\n",
    "stats_rows = []\n",
    "id_warn_rows = []\n",
    "warn_flux_na_files = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "agg_id_total = 0\n",
    "agg_id_missing = 0\n",
    "\n",
    "print(f\"[STAGE1] LC_VALIDATE_MODE={LC_VALIDATE_MODE} | HEAD_ROWS={HEAD_ROWS} | SAMPLE_ID_PER_SPLIT={SAMPLE_ID_PER_SPLIT}\")\n",
    "print(f\"[STAGE1] SNR_DET_THR_LIST={SNR_DET_THR_LIST} | SNR_STRONG_THR={SNR_STRONG_THR} | USE_REST={USE_REST}\")\n",
    "\n",
    "for sp in SPLIT_LIST:\n",
    "    sd = SPLIT_DIRS[sp]\n",
    "    for kind in [\"train\", \"test\"]:\n",
    "        p = sd / f\"{kind}_full_lightcurves.csv\"\n",
    "\n",
    "        dfh = _read_sample_df(p, nrows=HEAD_ROWS)\n",
    "        if len(dfh) < MIN_SAMPLE_ROWS:\n",
    "            raise ValueError(f\"[LC SAMPLE] Too few rows sampled from {p} (n={len(dfh)}). Possible read issue.\")\n",
    "\n",
    "        st = _numeric_and_filter_stats(dfh)\n",
    "\n",
    "        if st.get(\"filter_bad\", \"\"):\n",
    "            raise ValueError(f\"[LC FILTER] Unexpected Filter values in {p}: {st['filter_bad']} (sample={st.get('filter_sample','')})\")\n",
    "\n",
    "        if st.get(\"time_na_frac\", 0.0) > MAX_TIME_NA_FRAC:\n",
    "            raise ValueError(f\"[LC NUM] Time(MJD) NaN too high in sample: {p} frac={st['time_na_frac']:.4f}\")\n",
    "        if st.get(\"ferr_na_frac\", 0.0) > MAX_FERR_NA_FRAC:\n",
    "            raise ValueError(f\"[LC NUM] Flux_err NaN too high in sample: {p} frac={st['ferr_na_frac']:.4f}\")\n",
    "        if int(st.get(\"ferr_neg_any\", 0)) == 1:\n",
    "            raise ValueError(f\"[LC NUM] Negative Flux_err detected in sample of {p} (should be >=0).\")\n",
    "\n",
    "        if st.get(\"flux_na_frac\", 0.0) > 0:\n",
    "            warn_flux_na_files += 1\n",
    "\n",
    "        id_k = 0\n",
    "        id_found = 0\n",
    "        id_missing = 0\n",
    "        id_scan_chunks = 0\n",
    "        id_scan_cap_used = 0\n",
    "        miss_ids_list = []\n",
    "\n",
    "        if LC_VALIDATE_MODE in [\"sample\", \"full\"]:\n",
    "            if kind == \"train\":\n",
    "                ids = df_train_log.loc[df_train_log[\"split\"] == sp, \"object_id\"].astype(\"string\").dropna().str.strip()\n",
    "            else:\n",
    "                ids = df_test_log.loc[df_test_log[\"split\"] == sp, \"object_id\"].astype(\"string\").dropna().str.strip()\n",
    "\n",
    "            if LC_VALIDATE_MODE == \"full\":\n",
    "                usecols = _get_usecols(p, {\"object_id\"})\n",
    "                found_all = set()\n",
    "                for chunk in pd.read_csv(p, usecols=usecols, chunksize=CHUNK_ROWS, **SAFE_READ_KW):\n",
    "                    chunk = _norm_cols(chunk)\n",
    "                    found_all.update(chunk[\"object_id\"].astype(\"string\").dropna().str.strip().astype(str).unique().tolist())\n",
    "                miss = sorted(list(set(ids.astype(str).tolist()) - found_all))\n",
    "                id_k = int(len(ids))\n",
    "                id_missing = int(len(miss))\n",
    "                id_found = id_k - id_missing\n",
    "                id_scan_chunks = None\n",
    "                id_scan_cap_used = None\n",
    "                miss_ids_list = miss[:20]\n",
    "            else:\n",
    "                id_k = int(min(SAMPLE_ID_PER_SPLIT, len(ids)))\n",
    "                want = set(ids.sample(n=id_k, random_state=SEED + (0 if kind==\"train\" else 7)).astype(str).tolist()) if id_k > 0 else set()\n",
    "                found_n, missing_ids, chunks_read, cap_used = _sample_id_presence_adaptive(\n",
    "                    p, want, CHUNK_ROWS, MAX_CHUNKS_PER_FILE, MAX_CHUNKS_HARD\n",
    "                )\n",
    "                id_found = int(found_n)\n",
    "                id_missing = int(len(missing_ids))\n",
    "                id_scan_chunks = int(chunks_read)\n",
    "                id_scan_cap_used = int(cap_used)\n",
    "                miss_ids_list = list(missing_ids)[:10]\n",
    "\n",
    "                miss_frac = (id_missing / max(id_k, 1)) if id_k else 0.0\n",
    "                agg_id_total += id_k\n",
    "                agg_id_missing += id_missing\n",
    "\n",
    "                if id_k and miss_frac >= ID_MISS_FAIL_FRAC:\n",
    "                    raise ValueError(\n",
    "                        f\"[LC ID] Severe mismatch within adaptive scan: {p} missing {id_missing}/{id_k} \"\n",
    "                        f\"(chunks_read={chunks_read}, hard_cap={MAX_CHUNKS_HARD}). Example missing: {miss_ids_list[:3]}\"\n",
    "                    )\n",
    "\n",
    "                if id_k and id_missing > 0:\n",
    "                    id_warn_rows.append({\n",
    "                        \"split\": sp, \"kind\": kind, \"file\": str(p),\n",
    "                        \"k\": id_k, \"missing\": id_missing,\n",
    "                        \"chunks_read\": chunks_read, \"cap_used\": cap_used,\n",
    "                        \"example_missing\": \",\".join(miss_ids_list[:5]),\n",
    "                    })\n",
    "\n",
    "        row = {\n",
    "            \"split\": sp,\n",
    "            \"kind\": kind,\n",
    "            \"file\": str(p),\n",
    "            \"file_mb\": sizeof_mb(p),\n",
    "            \"n_sample\": st.get(\"n_sample\", 0),\n",
    "            \"time_na_frac\": st.get(\"time_na_frac\", np.nan),\n",
    "            \"flux_na_frac\": st.get(\"flux_na_frac\", np.nan),\n",
    "            \"ferr_na_frac\": st.get(\"ferr_na_frac\", np.nan),\n",
    "            \"time_min\": st.get(\"time_min\", np.nan),\n",
    "            \"time_max\": st.get(\"time_max\", np.nan),\n",
    "            \"time_span\": st.get(\"time_span\", np.nan),\n",
    "            \"flux_neg_frac\": st.get(\"flux_neg_frac\", np.nan),\n",
    "            \"flux_p01\": st.get(\"flux_p01\", np.nan),\n",
    "            \"flux_p50\": st.get(\"flux_p50\", np.nan),\n",
    "            \"flux_p99\": st.get(\"flux_p99\", np.nan),\n",
    "            \"ferr_min\": st.get(\"ferr_min\", np.nan),\n",
    "            \"ferr_p50\": st.get(\"ferr_p50\", np.nan),\n",
    "            \"ferr_p99\": st.get(\"ferr_p99\", np.nan),\n",
    "            \"ferr_zero_frac\": st.get(\"ferr_zero_frac\", np.nan),\n",
    "            \"snr_abs_p50\": st.get(\"snr_abs_p50\", np.nan),\n",
    "            \"snr_abs_p95\": st.get(\"snr_abs_p95\", np.nan),\n",
    "            \"snr_ge_det_frac\": st.get(\"snr_ge_det_frac\", np.nan),\n",
    "            \"snr_ge_strong_frac\": st.get(\"snr_ge_strong_frac\", np.nan),\n",
    "            \"snr_det_thr_base\": float(SNR_DET_THR),\n",
    "            \"snr_strong_thr\": float(SNR_STRONG_THR),\n",
    "            \"filter_sample\": st.get(\"filter_sample\", \"\"),\n",
    "            \"id_check_k\": int(id_k),\n",
    "            \"id_found\": int(id_found),\n",
    "            \"id_missing\": int(id_missing),\n",
    "            \"id_scan_chunks\": id_scan_chunks,\n",
    "            \"id_scan_cap_used\": id_scan_cap_used,\n",
    "        }\n",
    "        for b in BANDS:\n",
    "            row[f\"frac_{b}\"] = st.get(f\"frac_{b}\", 0.0)\n",
    "\n",
    "        stats_rows.append(row)\n",
    "\n",
    "df_lc_stats = pd.DataFrame(stats_rows)\n",
    "lc_stats_path = LOG_DIR / \"lc_sample_stats.csv\"\n",
    "df_lc_stats.to_csv(lc_stats_path, index=False)\n",
    "\n",
    "df_id_warn = pd.DataFrame(id_warn_rows)\n",
    "id_warn_path = LOG_DIR / \"lc_id_presence_warnings.csv\"\n",
    "df_id_warn.to_csv(id_warn_path, index=False)\n",
    "\n",
    "agg_missing_rate = (agg_id_missing / max(agg_id_total, 1)) if (LC_VALIDATE_MODE == \"sample\") else None\n",
    "if LC_VALIDATE_MODE == \"sample\":\n",
    "    if agg_missing_rate > FAIL_FAST_MISSING_RATE:\n",
    "        raise RuntimeError(\n",
    "            \"[FAIL-FAST] Aggregate sample-ID missing rate terlalu tinggi.\\n\"\n",
    "            f\"- agg_missing_rate={agg_missing_rate:.3%} (missing={agg_id_missing}, total_sample={agg_id_total})\\n\"\n",
    "            f\"- threshold={FAIL_FAST_MISSING_RATE:.3%}\\n\"\n",
    "            \"Ini indikasi routing/split/object_id normalization bermasalah.\"\n",
    "        )\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Build object_quality tables (train/test) — streaming scan\n",
    "# ----------------------------\n",
    "objq_train_path = None\n",
    "objq_test_path = None\n",
    "splitq_path = None\n",
    "\n",
    "if BUILD_OBJECT_QUALITY:\n",
    "    print(\"\\n[STAGE1] Building object_quality (full streaming scan) ...\")\n",
    "\n",
    "    _tr = df_train_log.copy()\n",
    "    _te = df_test_log.copy()\n",
    "    _tr[\"object_id\"] = _tr[\"object_id\"].astype(\"string\").str.strip()\n",
    "    _te[\"object_id\"] = _te[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # include has_zerr if exists\n",
    "    keep_tr = [\"split\",\"target\",\"Z\",\"Z_err\",\"EBV\"]\n",
    "    keep_te = [\"split\",\"Z\",\"Z_err\",\"EBV\"]\n",
    "    if \"has_zerr\" in _tr.columns: keep_tr.append(\"has_zerr\")\n",
    "    if \"has_zerr\" in _te.columns: keep_te.append(\"has_zerr\")\n",
    "\n",
    "    obj_train = _tr.set_index(\"object_id\")[keep_tr].copy()\n",
    "    obj_test  = _te.set_index(\"object_id\")[keep_te].copy()\n",
    "\n",
    "    # numeric holders\n",
    "    def _init_obj_table(df):\n",
    "        df = df.copy()\n",
    "\n",
    "        df[\"n_obs_total\"] = np.int32(0)\n",
    "        for b in BANDS:\n",
    "            df[f\"n_{b}\"] = np.int32(0)\n",
    "\n",
    "        # per-band det counts for base det thr\n",
    "        for b in BANDS:\n",
    "            df[f\"det_{b}\"] = np.int32(0)\n",
    "\n",
    "        # aggregate counters\n",
    "        df[\"neg_count\"] = np.int32(0)\n",
    "        df[\"pos_count\"] = np.int32(0)\n",
    "        df[\"ferr0_count\"] = np.int32(0)\n",
    "\n",
    "        # multi-threshold det counts (abs + pos + neg)\n",
    "        for thr in SNR_DET_THR_LIST:\n",
    "            key = int(round(thr * 10))  # 2.0->20, 3.0->30\n",
    "            df[f\"snr_det_abs_{key}\"] = np.int32(0)\n",
    "            df[f\"snr_det_pos_{key}\"] = np.int32(0)\n",
    "            df[f\"snr_det_neg_{key}\"] = np.int32(0)\n",
    "\n",
    "        # strong counts (abs + pos + neg)\n",
    "        df[\"snr_strong_abs\"] = np.int32(0)\n",
    "        df[\"snr_strong_pos\"] = np.int32(0)\n",
    "        df[\"snr_strong_neg\"] = np.int32(0)\n",
    "\n",
    "        # maxima/minima\n",
    "        df[\"snr_abs_max\"] = np.float32(0.0)\n",
    "        df[\"snr_pos_max\"] = np.float32(0.0)\n",
    "        df[\"snr_neg_min\"] = np.float32(0.0)  # negative (<=0), more negative => stronger neg excursion\n",
    "\n",
    "        df[\"flux_max\"] = np.float32(-np.inf)\n",
    "        df[\"flux_min\"] = np.float32(np.inf)\n",
    "\n",
    "        # sums for mean/std\n",
    "        df[\"flux_sum\"] = np.float64(0.0)\n",
    "        df[\"flux_sumsq\"] = np.float64(0.0)\n",
    "        df[\"ferr_sum\"] = np.float64(0.0)\n",
    "        df[\"n_valid\"] = np.int32(0)\n",
    "\n",
    "        # time range\n",
    "        df[\"tmin\"] = np.float64(np.inf)\n",
    "        df[\"tmax\"] = np.float64(-np.inf)\n",
    "\n",
    "        return df\n",
    "\n",
    "    obj_train = _init_obj_table(obj_train)\n",
    "    obj_test  = _init_obj_table(obj_test)\n",
    "\n",
    "    usecols = _get_usecols(SPLIT_DIRS[\"split_01\"] / \"train_full_lightcurves.csv\", REQ_LC_COLS_SET)  # schema assumed same\n",
    "\n",
    "    def _update_obj_stats(obj_df: pd.DataFrame, csv_path: Path, unknown_counter: dict):\n",
    "        idx_master = obj_df.index\n",
    "\n",
    "        for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=OBJ_QUALITY_CHUNK_ROWS, **SAFE_READ_KW):\n",
    "            chunk = _norm_cols(chunk)\n",
    "            chunk[\"object_id\"] = chunk[\"object_id\"].astype(\"string\").str.strip()\n",
    "            chunk = chunk[chunk[\"object_id\"].notna()]\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "\n",
    "            # normalize Filter into chunk column (important!)\n",
    "            chunk[\"Filter\"] = chunk[\"Filter\"].astype(\"string\").str.strip().str.lower().fillna(\"\")\n",
    "            bad = set(chunk[\"Filter\"].unique().tolist()) - ALLOWED_FILTERS - {\"\"}\n",
    "            if bad:\n",
    "                raise ValueError(f\"[LC FILTER] Unexpected Filter values in {csv_path}: {sorted(list(bad))[:10]}\")\n",
    "\n",
    "            # restrict to known object_id (avoid crash if file contains extra ids)\n",
    "            oid = chunk[\"object_id\"].astype(str)\n",
    "            known_mask = oid.isin(idx_master)\n",
    "            if not known_mask.any():\n",
    "                unknown_counter[\"unknown_rows\"] += int(len(chunk))\n",
    "                continue\n",
    "            if (~known_mask).any():\n",
    "                unknown_counter[\"unknown_rows\"] += int((~known_mask).sum())\n",
    "                chunk = chunk.loc[known_mask].copy()\n",
    "                oid = chunk[\"object_id\"].astype(str)\n",
    "\n",
    "            # numeric\n",
    "            t = pd.to_numeric(chunk[\"Time (MJD)\"], errors=\"coerce\")\n",
    "            f = pd.to_numeric(chunk[\"Flux\"], errors=\"coerce\")\n",
    "            e = pd.to_numeric(chunk[\"Flux_err\"], errors=\"coerce\")\n",
    "\n",
    "            # total n per object from ALL rows\n",
    "            n_all = oid.value_counts()\n",
    "            idx_all = n_all.index\n",
    "            obj_df.loc[idx_all, \"n_obs_total\"] = (obj_df.loc[idx_all, \"n_obs_total\"].astype(np.int64) + n_all.astype(np.int64)).astype(np.int32)\n",
    "\n",
    "            # time min/max (numeric coerce)\n",
    "            tmin = t.groupby(oid).min()\n",
    "            tmax = t.groupby(oid).max()\n",
    "            obj_df.loc[idx_all, \"tmin\"] = np.minimum(obj_df.loc[idx_all, \"tmin\"].to_numpy(), tmin.reindex(idx_all).to_numpy())\n",
    "            obj_df.loc[idx_all, \"tmax\"] = np.maximum(obj_df.loc[idx_all, \"tmax\"].to_numpy(), tmax.reindex(idx_all).to_numpy())\n",
    "\n",
    "            # band counts\n",
    "            bc = chunk.groupby([oid, chunk[\"Filter\"]]).size().unstack(fill_value=0)\n",
    "            for b in BANDS:\n",
    "                if b in bc.columns:\n",
    "                    obj_df.loc[bc.index, f\"n_{b}\"] = (obj_df.loc[bc.index, f\"n_{b}\"].astype(np.int64) + bc[b].astype(np.int64)).astype(np.int32)\n",
    "\n",
    "            # valid numeric rows for SNR/flux stats\n",
    "            ff = f.to_numpy()\n",
    "            ee = e.to_numpy()\n",
    "            mm = np.isfinite(ff) & np.isfinite(ee)\n",
    "            if not mm.any():\n",
    "                continue\n",
    "\n",
    "            oid_m = oid.to_numpy()[mm]\n",
    "            flt_m = chunk[\"Filter\"].to_numpy()[mm]\n",
    "            f_m = ff[mm].astype(np.float64, copy=False)\n",
    "            e_raw_m = ee[mm].astype(np.float64, copy=False)\n",
    "\n",
    "            # ferr0\n",
    "            ferr0 = (e_raw_m <= 0).astype(np.int32)\n",
    "            # clip for snr\n",
    "            e_m = np.maximum(e_raw_m, MIN_FLUXERR)\n",
    "            snr = (f_m / e_m).astype(np.float64, copy=False)\n",
    "            snr = np.clip(snr, -SNR_CLIP, SNR_CLIP)\n",
    "            snr_abs = np.abs(snr)\n",
    "\n",
    "            neg = (f_m < 0).astype(np.int32)\n",
    "            pos = (f_m > 0).astype(np.int32)\n",
    "\n",
    "            # build tmp frame\n",
    "            tmp = pd.DataFrame({\n",
    "                \"object_id\": oid_m,\n",
    "                \"filter\": flt_m,\n",
    "                \"flux\": f_m,\n",
    "                \"ferr\": e_raw_m,\n",
    "                \"neg\": neg,\n",
    "                \"pos\": pos,\n",
    "                \"ferr0\": ferr0,\n",
    "                \"snr\": snr,\n",
    "                \"snr_abs\": snr_abs,\n",
    "                \"snr_pos\": np.maximum(snr, 0.0),\n",
    "                \"snr_neg\": np.minimum(snr, 0.0),\n",
    "            })\n",
    "\n",
    "            # multi-threshold det flags (abs + pos + neg)\n",
    "            for thr in SNR_DET_THR_LIST:\n",
    "                key = int(round(thr * 10))\n",
    "                tmp[f\"det_abs_{key}\"] = (tmp[\"snr_abs\"].to_numpy() >= thr).astype(np.int32)\n",
    "                tmp[f\"det_pos_{key}\"] = (tmp[\"snr\"].to_numpy() >= thr).astype(np.int32)\n",
    "                tmp[f\"det_neg_{key}\"] = (tmp[\"snr\"].to_numpy() <= -thr).astype(np.int32)\n",
    "\n",
    "            # strong flags\n",
    "            tmp[\"strong_abs\"] = (tmp[\"snr_abs\"].to_numpy() >= SNR_STRONG_THR).astype(np.int32)\n",
    "            tmp[\"strong_pos\"] = (tmp[\"snr\"].to_numpy() >= SNR_STRONG_THR).astype(np.int32)\n",
    "            tmp[\"strong_neg\"] = (tmp[\"snr\"].to_numpy() <= -SNR_STRONG_THR).astype(np.int32)\n",
    "\n",
    "            # aggregate per object\n",
    "            agg_dict = {\n",
    "                \"neg_count\": (\"neg\", \"sum\"),\n",
    "                \"pos_count\": (\"pos\", \"sum\"),\n",
    "                \"ferr0_count\": (\"ferr0\", \"sum\"),\n",
    "                \"snr_abs_max\": (\"snr_abs\", \"max\"),\n",
    "                \"snr_pos_max\": (\"snr_pos\", \"max\"),\n",
    "                \"snr_neg_min\": (\"snr_neg\", \"min\"),\n",
    "                \"flux_max\": (\"flux\", \"max\"),\n",
    "                \"flux_min\": (\"flux\", \"min\"),\n",
    "                \"flux_sum\": (\"flux\", \"sum\"),\n",
    "                \"ferr_sum\": (\"ferr\", \"sum\"),\n",
    "                \"n_valid\": (\"flux\", \"count\"),\n",
    "            }\n",
    "            agg = tmp.groupby(\"object_id\").agg(**agg_dict)\n",
    "\n",
    "            # sumsq for std\n",
    "            tmp[\"flux2\"] = tmp[\"flux\"].to_numpy() * tmp[\"flux\"].to_numpy()\n",
    "            agg2 = tmp.groupby(\"object_id\").agg(flux_sumsq=(\"flux2\", \"sum\"))\n",
    "\n",
    "            idx = agg.index\n",
    "            obj_df.loc[idx, \"neg_count\"] = (obj_df.loc[idx, \"neg_count\"].astype(np.int64) + agg[\"neg_count\"].astype(np.int64)).astype(np.int32)\n",
    "            obj_df.loc[idx, \"pos_count\"] = (obj_df.loc[idx, \"pos_count\"].astype(np.int64) + agg[\"pos_count\"].astype(np.int64)).astype(np.int32)\n",
    "            obj_df.loc[idx, \"ferr0_count\"] = (obj_df.loc[idx, \"ferr0_count\"].astype(np.int64) + agg[\"ferr0_count\"].astype(np.int64)).astype(np.int32)\n",
    "\n",
    "            obj_df.loc[idx, \"flux_sum\"] = obj_df.loc[idx, \"flux_sum\"].to_numpy() + agg[\"flux_sum\"].to_numpy()\n",
    "            obj_df.loc[idx, \"flux_sumsq\"] = obj_df.loc[idx, \"flux_sumsq\"].to_numpy() + agg2[\"flux_sumsq\"].reindex(idx).to_numpy()\n",
    "            obj_df.loc[idx, \"ferr_sum\"] = obj_df.loc[idx, \"ferr_sum\"].to_numpy() + agg[\"ferr_sum\"].to_numpy()\n",
    "            obj_df.loc[idx, \"n_valid\"] = (obj_df.loc[idx, \"n_valid\"].astype(np.int64) + agg[\"n_valid\"].astype(np.int64)).astype(np.int32)\n",
    "\n",
    "            obj_df.loc[idx, \"snr_abs_max\"] = np.maximum(obj_df.loc[idx, \"snr_abs_max\"].to_numpy(), agg[\"snr_abs_max\"].to_numpy()).astype(np.float32)\n",
    "            obj_df.loc[idx, \"snr_pos_max\"] = np.maximum(obj_df.loc[idx, \"snr_pos_max\"].to_numpy(), agg[\"snr_pos_max\"].to_numpy()).astype(np.float32)\n",
    "            obj_df.loc[idx, \"snr_neg_min\"] = np.minimum(obj_df.loc[idx, \"snr_neg_min\"].to_numpy(), agg[\"snr_neg_min\"].to_numpy()).astype(np.float32)\n",
    "\n",
    "            obj_df.loc[idx, \"flux_max\"] = np.maximum(obj_df.loc[idx, \"flux_max\"].to_numpy(), agg[\"flux_max\"].to_numpy()).astype(np.float32)\n",
    "            obj_df.loc[idx, \"flux_min\"] = np.minimum(obj_df.loc[idx, \"flux_min\"].to_numpy(), agg[\"flux_min\"].to_numpy()).astype(np.float32)\n",
    "\n",
    "            # multi-threshold det counts update\n",
    "            det_aggs = {}\n",
    "            for thr in SNR_DET_THR_LIST:\n",
    "                key = int(round(thr * 10))\n",
    "                det_aggs[f\"det_abs_{key}\"] = (f\"det_abs_{key}\", \"sum\")\n",
    "                det_aggs[f\"det_pos_{key}\"] = (f\"det_pos_{key}\", \"sum\")\n",
    "                det_aggs[f\"det_neg_{key}\"] = (f\"det_neg_{key}\", \"sum\")\n",
    "            det_sum = tmp.groupby(\"object_id\").agg(**det_aggs)\n",
    "\n",
    "            for thr in SNR_DET_THR_LIST:\n",
    "                key = int(round(thr * 10))\n",
    "                obj_df.loc[det_sum.index, f\"snr_det_abs_{key}\"] = (obj_df.loc[det_sum.index, f\"snr_det_abs_{key}\"].astype(np.int64) + det_sum[f\"det_abs_{key}\"].astype(np.int64)).astype(np.int32)\n",
    "                obj_df.loc[det_sum.index, f\"snr_det_pos_{key}\"] = (obj_df.loc[det_sum.index, f\"snr_det_pos_{key}\"].astype(np.int64) + det_sum[f\"det_pos_{key}\"].astype(np.int64)).astype(np.int32)\n",
    "                obj_df.loc[det_sum.index, f\"snr_det_neg_{key}\"] = (obj_df.loc[det_sum.index, f\"snr_det_neg_{key}\"].astype(np.int64) + det_sum[f\"det_neg_{key}\"].astype(np.int64)).astype(np.int32)\n",
    "\n",
    "            # strong counts\n",
    "            strong_sum = tmp.groupby(\"object_id\").agg(\n",
    "                strong_abs=(\"strong_abs\", \"sum\"),\n",
    "                strong_pos=(\"strong_pos\", \"sum\"),\n",
    "                strong_neg=(\"strong_neg\", \"sum\"),\n",
    "            )\n",
    "            obj_df.loc[strong_sum.index, \"snr_strong_abs\"] = (obj_df.loc[strong_sum.index, \"snr_strong_abs\"].astype(np.int64) + strong_sum[\"strong_abs\"].astype(np.int64)).astype(np.int32)\n",
    "            obj_df.loc[strong_sum.index, \"snr_strong_pos\"] = (obj_df.loc[strong_sum.index, \"snr_strong_pos\"].astype(np.int64) + strong_sum[\"strong_pos\"].astype(np.int64)).astype(np.int32)\n",
    "            obj_df.loc[strong_sum.index, \"snr_strong_neg\"] = (obj_df.loc[strong_sum.index, \"snr_strong_neg\"].astype(np.int64) + strong_sum[\"strong_neg\"].astype(np.int64)).astype(np.int32)\n",
    "\n",
    "            # per-band det counts (base thr only; abs>=SNR_DET_THR)\n",
    "            tmp[\"det_base\"] = (tmp[\"snr_abs\"].to_numpy() >= SNR_DET_THR).astype(np.int32)\n",
    "            det_band = tmp[tmp[\"det_base\"] == 1].groupby([\"object_id\",\"filter\"]).size().unstack(fill_value=0)\n",
    "            for b in BANDS:\n",
    "                if b in det_band.columns:\n",
    "                    obj_df.loc[det_band.index, f\"det_{b}\"] = (obj_df.loc[det_band.index, f\"det_{b}\"].astype(np.int64) + det_band[b].astype(np.int64)).astype(np.int32)\n",
    "\n",
    "    unknown_tr = {\"unknown_rows\": 0}\n",
    "    unknown_te = {\"unknown_rows\": 0}\n",
    "\n",
    "    for sp in SPLIT_LIST:\n",
    "        sd = SPLIT_DIRS[sp]\n",
    "        _update_obj_stats(obj_train, sd / \"train_full_lightcurves.csv\", unknown_tr)\n",
    "        _update_obj_stats(obj_test,  sd / \"test_full_lightcurves.csv\", unknown_te)\n",
    "\n",
    "    def _finalize_obj(df: pd.DataFrame, is_train: bool):\n",
    "        df = df.copy()\n",
    "        n = df[\"n_obs_total\"].astype(np.float64).to_numpy()\n",
    "        n_safe = np.maximum(n, 1.0)\n",
    "\n",
    "        # time features\n",
    "        df[\"timespan\"] = (df[\"tmax\"] - df[\"tmin\"]).astype(np.float64)\n",
    "        df.loc[~np.isfinite(df[\"timespan\"]), \"timespan\"] = np.nan\n",
    "\n",
    "        df.loc[np.isinf(df[\"tmin\"]), \"tmin\"] = np.nan\n",
    "        df.loc[np.isinf(df[\"tmax\"]), \"tmax\"] = np.nan\n",
    "        df.loc[np.isinf(df[\"flux_max\"]), \"flux_max\"] = np.nan\n",
    "        df.loc[np.isinf(df[\"flux_min\"]), \"flux_min\"] = np.nan\n",
    "\n",
    "        # ratios\n",
    "        df[\"neg_flux_frac\"] = (df[\"neg_count\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "        df[\"pos_flux_frac\"] = (df[\"pos_count\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "\n",
    "        # multi-threshold fractions\n",
    "        for thr in SNR_DET_THR_LIST:\n",
    "            key = int(round(thr * 10))\n",
    "            df[f\"snr_det_abs_frac_{key}\"] = (df[f\"snr_det_abs_{key}\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "            df[f\"snr_det_pos_frac_{key}\"] = (df[f\"snr_det_pos_{key}\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "            df[f\"snr_det_neg_frac_{key}\"] = (df[f\"snr_det_neg_{key}\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "\n",
    "        df[\"snr_strong_abs_frac\"] = (df[\"snr_strong_abs\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "        df[\"snr_strong_pos_frac\"] = (df[\"snr_strong_pos\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "        df[\"snr_strong_neg_frac\"] = (df[\"snr_strong_neg\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "\n",
    "        # band coverage\n",
    "        band_present = []\n",
    "        for b in BANDS:\n",
    "            band_present.append((df[f\"n_{b}\"].to_numpy() > 0).astype(np.int8))\n",
    "            df[f\"frac_{b}\"] = (df[f\"n_{b}\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "\n",
    "        df[\"n_bands_present\"] = np.clip(np.sum(np.vstack(band_present), axis=0), 0, 6).astype(np.int8)\n",
    "\n",
    "        # det band coverage (base thr)\n",
    "        det_present = []\n",
    "        for b in BANDS:\n",
    "            det_present.append((df[f\"det_{b}\"].to_numpy() > 0).astype(np.int8))\n",
    "            df[f\"det_frac_{b}\"] = (df[f\"det_{b}\"].astype(np.float64) / n_safe).astype(np.float32)\n",
    "        df[\"n_bands_det\"] = np.clip(np.sum(np.vstack(det_present), axis=0), 0, 6).astype(np.int8)\n",
    "\n",
    "        # cadence proxy\n",
    "        df[\"cadence_proxy\"] = (df[\"timespan\"].astype(np.float64) / np.maximum(df[\"n_obs_total\"].astype(np.float64) - 1.0, 1.0)).astype(np.float32)\n",
    "\n",
    "        # flux / err stats from sums\n",
    "        nv = np.maximum(df[\"n_valid\"].astype(np.float64).to_numpy(), 1.0)\n",
    "        df[\"flux_mean\"] = (df[\"flux_sum\"].astype(np.float64).to_numpy() / nv).astype(np.float32)\n",
    "        # var = E[x^2] - mean^2\n",
    "        ex2 = (df[\"flux_sumsq\"].astype(np.float64).to_numpy() / nv)\n",
    "        var = np.maximum(ex2 - (df[\"flux_mean\"].astype(np.float64).to_numpy() ** 2), 0.0)\n",
    "        df[\"flux_std\"] = np.sqrt(var).astype(np.float32)\n",
    "        df[\"ferr_mean\"] = (df[\"ferr_sum\"].astype(np.float64).to_numpy() / nv).astype(np.float32)\n",
    "\n",
    "        # rest-frame\n",
    "        if USE_REST and (\"Z\" in df.columns):\n",
    "            z = pd.to_numeric(df[\"Z\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float64)\n",
    "            denom = np.maximum(1.0 + z, REST_EPS)\n",
    "            df[\"timespan_rest\"] = (df[\"timespan\"].astype(np.float64).to_numpy() / denom).astype(np.float32)\n",
    "            df[\"cadence_proxy_rest\"] = (df[\"cadence_proxy\"].astype(np.float64).to_numpy() / denom).astype(np.float32)\n",
    "            # zerr_rel (domain shift)\n",
    "            if \"Z_err\" in df.columns:\n",
    "                zerr = pd.to_numeric(df[\"Z_err\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float64)\n",
    "                df[\"zerr_rel\"] = (zerr / denom).astype(np.float32)\n",
    "\n",
    "        return df\n",
    "\n",
    "    obj_train_f = _finalize_obj(obj_train, is_train=True)\n",
    "    obj_test_f  = _finalize_obj(obj_test,  is_train=False)\n",
    "\n",
    "    # fail-fast: zero obs objects\n",
    "    z0_tr = int((obj_train_f[\"n_obs_total\"] == 0).sum())\n",
    "    z0_te = int((obj_test_f[\"n_obs_total\"] == 0).sum())\n",
    "    r0_tr = z0_tr / max(len(obj_train_f), 1)\n",
    "    r0_te = z0_te / max(len(obj_test_f), 1)\n",
    "\n",
    "    if (r0_tr > ZEROOBS_FAIL_RATE) or (r0_te > ZEROOBS_FAIL_RATE):\n",
    "        raise RuntimeError(\n",
    "            \"[FAIL-FAST] Terlalu banyak object_id dengan 0 observasi (routing/reading issue).\\n\"\n",
    "            f\"- train_zero_obs: {z0_tr}/{len(obj_train_f)} ({r0_tr:.3%})\\n\"\n",
    "            f\"- test_zero_obs : {z0_te}/{len(obj_test_f)} ({r0_te:.3%})\\n\"\n",
    "            f\"- threshold      : {ZEROOBS_FAIL_RATE:.3%}\\n\"\n",
    "        )\n",
    "\n",
    "    objq_train_path = LOG_DIR / \"object_quality_train.csv\"\n",
    "    objq_test_path  = LOG_DIR / \"object_quality_test.csv\"\n",
    "    obj_train_f.reset_index().to_csv(objq_train_path, index=False)\n",
    "    obj_test_f.reset_index().to_csv(objq_test_path, index=False)\n",
    "\n",
    "    # split-level summary\n",
    "    def _split_summary(df: pd.DataFrame, is_train: bool):\n",
    "        g = df.groupby(\"split\", dropna=False)\n",
    "        rows = []\n",
    "        for sp, d in g:\n",
    "            r = {\n",
    "                \"split\": sp,\n",
    "                \"n_objects\": int(len(d)),\n",
    "                \"n_obs_total_sum\": int(d[\"n_obs_total\"].sum()),\n",
    "                \"n_obs_total_med\": float(d[\"n_obs_total\"].median()),\n",
    "                \"timespan_med\": float(d[\"timespan\"].median(skipna=True)),\n",
    "                \"neg_flux_frac_med\": float(d[\"neg_flux_frac\"].median(skipna=True)),\n",
    "                \"pos_flux_frac_med\": float(d[\"pos_flux_frac\"].median(skipna=True)),\n",
    "                \"snr_abs_max_p95\": float(np.nanpercentile(d[\"snr_abs_max\"].to_numpy(), 95)),\n",
    "                \"snr_pos_max_p95\": float(np.nanpercentile(d[\"snr_pos_max\"].to_numpy(), 95)),\n",
    "                \"snr_neg_min_p05\": float(np.nanpercentile(d[\"snr_neg_min\"].to_numpy(), 5)),\n",
    "                \"n_bands_present_med\": float(d[\"n_bands_present\"].median()),\n",
    "                \"n_bands_det_med\": float(d[\"n_bands_det\"].median()),\n",
    "                \"cadence_proxy_med\": float(d[\"cadence_proxy\"].median(skipna=True)),\n",
    "                \"flux_std_med\": float(d[\"flux_std\"].median(skipna=True)),\n",
    "            }\n",
    "            # include base det fractions\n",
    "            key0 = int(round(SNR_DET_THR * 10))\n",
    "            if f\"snr_det_abs_frac_{key0}\" in d.columns:\n",
    "                r[\"snr_det_abs_frac_med_base\"] = float(d[f\"snr_det_abs_frac_{key0}\"].median(skipna=True))\n",
    "                r[\"snr_det_pos_frac_med_base\"] = float(d[f\"snr_det_pos_frac_{key0}\"].median(skipna=True))\n",
    "                r[\"snr_det_neg_frac_med_base\"] = float(d[f\"snr_det_neg_frac_{key0}\"].median(skipna=True))\n",
    "\n",
    "            for b in BANDS:\n",
    "                r[f\"frac_{b}_med\"] = float(d[f\"frac_{b}\"].median(skipna=True))\n",
    "                r[f\"det_frac_{b}_med\"] = float(d[f\"det_frac_{b}\"].median(skipna=True))\n",
    "\n",
    "            if USE_REST and \"timespan_rest\" in d.columns:\n",
    "                r[\"timespan_rest_med\"] = float(d[\"timespan_rest\"].median(skipna=True))\n",
    "                r[\"cadence_proxy_rest_med\"] = float(d[\"cadence_proxy_rest\"].median(skipna=True))\n",
    "\n",
    "            if is_train and \"target\" in d.columns:\n",
    "                r[\"pos\"] = int((d[\"target\"] == 1).sum())\n",
    "                r[\"pos_pct\"] = float((d[\"target\"] == 1).mean() * 100.0)\n",
    "            rows.append(r)\n",
    "        return pd.DataFrame(rows).sort_values(\"split\").reset_index(drop=True)\n",
    "\n",
    "    split_train = _split_summary(obj_train_f, is_train=True)\n",
    "    split_test  = _split_summary(obj_test_f,  is_train=False)\n",
    "    split_train[\"kind\"] = \"train\"\n",
    "    split_test[\"kind\"]  = \"test\"\n",
    "    df_splitq = pd.concat([split_train, split_test], ignore_index=True)\n",
    "\n",
    "    splitq_path = LOG_DIR / \"split_quality_summary.csv\"\n",
    "    df_splitq.to_csv(splitq_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Summary prints + JSON summary\n",
    "# ----------------------------\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "worst_id_missing = (\n",
    "    df_lc_stats.sort_values([\"id_missing\", \"id_check_k\"], ascending=[False, False])\n",
    "    .head(10)[[\"split\",\"kind\",\"id_missing\",\"id_check_k\",\"id_scan_chunks\",\"id_scan_cap_used\",\"file_mb\"]]\n",
    ")\n",
    "worst_flux_neg = (\n",
    "    df_lc_stats.sort_values(\"flux_neg_frac\", ascending=False)\n",
    "    .head(10)[[\"split\",\"kind\",\"flux_neg_frac\",\"snr_ge_det_frac\",\"file_mb\"]]\n",
    ")\n",
    "worst_snr = (\n",
    "    df_lc_stats.sort_values(\"snr_ge_det_frac\", ascending=False)\n",
    "    .head(10)[[\"split\",\"kind\",\"snr_ge_det_frac\",\"snr_ge_strong_frac\",\"snr_abs_p95\",\"file_mb\"]]\n",
    ")\n",
    "\n",
    "summary = {\n",
    "    \"stage\": \"stage1\",\n",
    "    \"data_root\": str(DATA_ROOT),\n",
    "    \"log_dir\": str(LOG_DIR),\n",
    "    \"lc_validate_mode\": LC_VALIDATE_MODE,\n",
    "    \"head_rows\": HEAD_ROWS,\n",
    "    \"sample_id_per_split\": SAMPLE_ID_PER_SPLIT,\n",
    "    \"chunk_rows\": CHUNK_ROWS,\n",
    "    \"obj_quality_chunk_rows\": OBJ_QUALITY_CHUNK_ROWS,\n",
    "    \"build_object_quality\": bool(BUILD_OBJECT_QUALITY),\n",
    "    \"snr\": {\n",
    "        \"SNR_CLIP\": SNR_CLIP,\n",
    "        \"SNR_DET_THR_LIST\": SNR_DET_THR_LIST,\n",
    "        \"SNR_STRONG_THR\": SNR_STRONG_THR,\n",
    "        \"MIN_FLUXERR\": MIN_FLUXERR,\n",
    "    },\n",
    "    \"rest_frame\": {\n",
    "        \"USE_REST_FRAME_TIME\": USE_REST,\n",
    "        \"REST_EPS\": REST_EPS,\n",
    "    },\n",
    "    \"thresholds\": {\n",
    "        \"MAX_TIME_NA_FRAC\": MAX_TIME_NA_FRAC,\n",
    "        \"MAX_FERR_NA_FRAC\": MAX_FERR_NA_FRAC,\n",
    "        \"ID_MISS_FAIL_FRAC\": ID_MISS_FAIL_FRAC,\n",
    "        \"FAIL_FAST_MISSING_RATE\": FAIL_FAST_MISSING_RATE,\n",
    "        \"MIN_SAMPLE_ROWS\": MIN_SAMPLE_ROWS,\n",
    "        \"ZEROOBS_FAIL_RATE\": ZEROOBS_FAIL_RATE,\n",
    "    },\n",
    "    \"aggregate_id_missing\": {\n",
    "        \"total_sample_ids\": int(agg_id_total),\n",
    "        \"missing_sample_ids\": int(agg_id_missing),\n",
    "        \"missing_rate\": float(agg_missing_rate) if agg_missing_rate is not None else None\n",
    "    },\n",
    "    \"warn_flux_na_files\": int(warn_flux_na_files),\n",
    "    \"routing_csv\": str(routing_path),\n",
    "    \"lc_sample_stats_csv\": str(lc_stats_path),\n",
    "    \"lc_id_presence_warnings_csv\": str(id_warn_path),\n",
    "    \"object_quality_train_csv\": str(objq_train_path) if objq_train_path else None,\n",
    "    \"object_quality_test_csv\": str(objq_test_path) if objq_test_path else None,\n",
    "    \"split_quality_summary_csv\": str(splitq_path) if splitq_path else None,\n",
    "    \"elapsed_sec\": float(elapsed),\n",
    "}\n",
    "\n",
    "summary_path = LOG_DIR / \"stage1_summary.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nSTAGE 1 OK — ROUTING + PROFILING + OBJECT_QUALITY READY\")\n",
    "print(f\"- routing saved: {routing_path}\")\n",
    "print(f\"- lc sample stats saved: {lc_stats_path}\")\n",
    "print(f\"- id warnings saved: {id_warn_path}\")\n",
    "if objq_train_path:\n",
    "    print(f\"- object_quality_train: {objq_train_path}\")\n",
    "    print(f\"- object_quality_test : {objq_test_path}\")\n",
    "    print(f\"- split_quality_summary: {splitq_path}\")\n",
    "print(f\"- summary json saved: {summary_path}\")\n",
    "print(f\"- elapsed: {elapsed/60:.2f} min | warn_flux_na_files={warn_flux_na_files}\")\n",
    "\n",
    "print(\"\\nTOP ISSUES (ID missing in sample/adaptive scan)\")\n",
    "print(worst_id_missing.to_string(index=False))\n",
    "\n",
    "print(\"\\nTOP PATTERN (highest negative flux fraction in sample head)\")\n",
    "print(worst_flux_neg.to_string(index=False))\n",
    "\n",
    "print(\"\\nTOP PATTERN (highest high-SNR fraction in sample head)\")\n",
    "print(worst_snr.to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Export to globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"SPLIT_DIRS\": SPLIT_DIRS,\n",
    "    \"SPLIT_LIST\": SPLIT_LIST,\n",
    "    \"df_split_routing\": df_routing,\n",
    "    \"df_lc_sample_stats\": df_lc_stats,\n",
    "    \"df_lc_id_presence_warnings\": df_id_warn,\n",
    "    \"STAGE1_SUMMARY_PATH\": summary_path,\n",
    "    \"OBJECT_QUALITY_TRAIN_PATH\": str(objq_train_path) if objq_train_path else None,\n",
    "    \"OBJECT_QUALITY_TEST_PATH\": str(objq_test_path) if objq_test_path else None,\n",
    "    \"SPLIT_QUALITY_SUMMARY_PATH\": str(splitq_path) if splitq_path else None,\n",
    "})\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\nStage 1 complete: splits verified + routing/stats + object_quality exported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14bdc2",
   "metadata": {
    "papermill": {
     "duration": 0.014799,
     "end_time": "2026-01-07T16:45:27.537221",
     "exception": false,
     "start_time": "2026-01-07T16:45:27.522422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and Validate Train/Test Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a9cd00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:45:27.570396Z",
     "iopub.status.busy": "2026-01-07T16:45:27.570002Z",
     "iopub.status.idle": "2026-01-07T16:45:41.133605Z",
     "shell.execute_reply": "2026-01-07T16:45:41.132653Z"
    },
    "papermill": {
     "duration": 13.582668,
     "end_time": "2026-01-07T16:45:41.135539",
     "exception": false,
     "start_time": "2026-01-07T16:45:27.552871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2 OK — META READY (clean + folds + enriched)\n",
      "- CV_USE_SPLIT_COL_USED: True | N_FOLDS=10 | split_quota=2\n",
      "- OBJQ_JOINED: True | objq_cols_used=79 | base_det_thr=2.0\n",
      "- train objects: 3,043 | pos=148 | neg=2,895 | pos%=4.864%\n",
      "- test objects : 7,135\n",
      "- saved train  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/train_meta.parquet\n",
      "- saved test   : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/test_meta.parquet\n",
      "- saved stats  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/split_stats.csv\n",
      "- saved folds  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/train_folds.csv\n",
      "- saved meta_feature_cols: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/meta_feature_cols.json\n",
      "- scale_pos_weight (neg/pos): 19.561\n",
      "\n",
      "CLIP RANGES\n",
      "- EBV clip (train): [0.005042, 0.581790]\n",
      "- Z   clip (train): [0.044923, 4.032352]\n",
      "- Zerr clip used  : [0.000000, 0.000000]\n",
      "\n",
      "FOLD BALANCE (count/pos/pos_rate)\n",
      "      count  pos  pos_rate\n",
      "fold                      \n",
      "0       307   29  0.094463\n",
      "1       325   24  0.073846\n",
      "2       301   12  0.039867\n",
      "3       306    8  0.026144\n",
      "4       288   24  0.083333\n",
      "5       299    0  0.000000\n",
      "6       318   15  0.047170\n",
      "7       323   12  0.037152\n",
      "8       266    6  0.022556\n",
      "9       310   18  0.058065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Clean Meta Logs + CV Fold Assignment + Meta Enrichment (ONE CELL)\n",
    "# REVISI FULL v6.1 (STAGE1 v5.1 compatible + safe objq join + SNR alias + robust clipping)\n",
    "#\n",
    "# v6.1 upgrade:\n",
    "# - Join object_quality: drop duplicate cols (split/target/Z/Z_err/EBV/has_zerr) sebelum join (no _oq noise)\n",
    "# - Auto-detect & include ALL useful objq feature cols (multi-threshold + pos/neg + det band + flux stats)\n",
    "# - Alias safety:\n",
    "#   * snr_det_frac <= snr_det_abs_frac_{base_key} jika ada\n",
    "#   * snr_strong_frac <= snr_strong_abs_frac jika ada\n",
    "# - Z_err clip: prefer train photo-z pool, fallback test pool\n",
    "# - Optional fold balancing add mean_log1p_obs when objq exists\n",
    "# ============================================================\n",
    "\n",
    "import re, gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0/1 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"ART_DIR\", \"SPLIT_DIRS\", \"CFG\", \"SEED\", \"LOG_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 & STAGE 1 dulu.\")\n",
    "\n",
    "TRAIN_LOG_PATH = Path(PATHS[\"TRAIN_LOG\"])\n",
    "TEST_LOG_PATH  = Path(PATHS[\"TEST_LOG\"])\n",
    "\n",
    "ART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = int(SEED)\n",
    "N_FOLDS = int(CFG.get(\"N_FOLDS\", 5))\n",
    "CV_USE_SPLIT_COL = bool(CFG.get(\"CV_USE_SPLIT_COL\", True))\n",
    "\n",
    "# deterministic split list (sinkron STAGE 0/1)\n",
    "SPLIT_LIST = [Path(p).name for p in PATHS[\"SPLITS\"]]\n",
    "SPLIT_LIST = sorted([s for s in SPLIT_LIST if s.startswith(\"split_\")])\n",
    "VALID_SPLITS = set(SPLIT_LIST)\n",
    "\n",
    "disk_splits = set(SPLIT_DIRS.keys())\n",
    "if disk_splits != VALID_SPLITS:\n",
    "    miss = sorted(list(VALID_SPLITS - disk_splits))\n",
    "    extra = sorted(list(disk_splits - VALID_SPLITS))\n",
    "    raise RuntimeError(\n",
    "        f\"SPLIT_DIRS mismatch. missing={miss[:5]} extra={extra[:5]} (jalankan ulang STAGE 1)\"\n",
    "    )\n",
    "\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "# fold assignment tuning\n",
    "FOLD_QUOTA = int(CFG.get(\"SPLIT_PER_FOLD_QUOTA\", int(np.ceil(len(SPLIT_LIST)/max(N_FOLDS,1)))))\n",
    "RESTARTS = int(CFG.get(\"SPLIT_ASSIGN_RESTARTS\", 512))\n",
    "RESTARTS_HARD = int(CFG.get(\"SPLIT_ASSIGN_RESTARTS_HARD\", 2048))\n",
    "PENALTY_ZERO_POS = float(CFG.get(\"FOLD_BALANCE_PENALTY_ZERO_POS\", 3.0))\n",
    "\n",
    "# weights objective\n",
    "LAMBDA_COUNT = float(CFG.get(\"FOLD_BALANCE_LAMBDA_COUNT\", 0.25))\n",
    "LAMBDA_QUOTA = float(CFG.get(\"FOLD_BALANCE_LAMBDA_QUOTA\", 0.05))\n",
    "LAMBDA_ZMEAN = float(CFG.get(\"FOLD_BALANCE_LAMBDA_ZMEAN\", 0.15))\n",
    "LAMBDA_EBVMEAN = float(CFG.get(\"FOLD_BALANCE_LAMBDA_EBVMEAN\", 0.10))\n",
    "LAMBDA_OBSMEAN = float(CFG.get(\"FOLD_BALANCE_LAMBDA_OBSMEAN\", 0.10))  # aktif jika objq ada\n",
    "\n",
    "# clipping quantiles\n",
    "QLO = float(CFG.get(\"META_QLO\", 0.001))\n",
    "QHI = float(CFG.get(\"META_QHI\", 0.999))\n",
    "\n",
    "# split prior smoothing\n",
    "PRIOR_ALPHA = float(CFG.get(\"SPLIT_PRIOR_ALPHA\", 10.0))\n",
    "\n",
    "# optional join object_quality from stage1\n",
    "OBJQ_TRAIN = globals().get(\"OBJECT_QUALITY_TRAIN_PATH\", None)\n",
    "OBJQ_TEST  = globals().get(\"OBJECT_QUALITY_TEST_PATH\", None)\n",
    "if not OBJQ_TRAIN:\n",
    "    p = LOG_DIR / \"object_quality_train.csv\"\n",
    "    OBJQ_TRAIN = str(p) if p.exists() else None\n",
    "if not OBJQ_TEST:\n",
    "    p = LOG_DIR / \"object_quality_test.csv\"\n",
    "    OBJQ_TEST = str(p) if p.exists() else None\n",
    "\n",
    "# base snr det thr key (untuk alias snr_det_frac)\n",
    "_thr_list = CFG.get(\"SNR_DET_THR_LIST\", None)\n",
    "if isinstance(_thr_list, (list, tuple)) and len(_thr_list) > 0:\n",
    "    _thr_list = [float(x) for x in _thr_list if np.isfinite(float(x))]\n",
    "    _thr_list = sorted(list(dict.fromkeys(_thr_list)))\n",
    "else:\n",
    "    _thr_list = [float(CFG.get(\"SNR_DET_THR\", 3.0))]\n",
    "BASE_DET_THR = float(_thr_list[0]) if len(_thr_list) else 3.0\n",
    "BASE_DET_KEY = int(round(BASE_DET_THR * 10))  # 3.0->30\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers\n",
    "# ----------------------------\n",
    "def normalize_split_name(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s2 = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    if s2.isdigit():\n",
    "        return f\"split_{int(s2):02d}\"\n",
    "    m = re.fullmatch(r\"split_(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    m = re.fullmatch(r\"split(\\d{1,2})\", s2)\n",
    "    if m:\n",
    "        return f\"split_{int(m.group(1)):02d}\"\n",
    "    return s2\n",
    "\n",
    "def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _coerce_float32(df: pd.DataFrame, col: str):\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "def _safe_clip(series: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    # handle NaN bounds\n",
    "    if not np.isfinite(lo): lo = float(np.nanmin(series.values.astype(float))) if np.isfinite(series.values.astype(float)).any() else 0.0\n",
    "    if not np.isfinite(hi): hi = float(np.nanmax(series.values.astype(float))) if np.isfinite(series.values.astype(float)).any() else 0.0\n",
    "    if hi < lo:\n",
    "        lo, hi = hi, lo\n",
    "    return series.clip(lower=np.float32(lo), upper=np.float32(hi)).astype(\"float32\")\n",
    "\n",
    "def _qclip_bounds(arr: np.ndarray, qlo=0.001, qhi=0.999, default=(0.0, 0.0)):\n",
    "    x = np.asarray(arr, dtype=float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if len(x) == 0:\n",
    "        return float(default[0]), float(default[1])\n",
    "    lo, hi = np.quantile(x, [qlo, qhi])\n",
    "    lo = float(lo); hi = float(hi)\n",
    "    if not np.isfinite(lo): lo = float(default[0])\n",
    "    if not np.isfinite(hi): hi = float(default[1])\n",
    "    if hi < lo: lo, hi = hi, lo\n",
    "    return lo, hi\n",
    "\n",
    "def _load_or_use_global(global_name: str, path: Path) -> pd.DataFrame:\n",
    "    if global_name in globals() and isinstance(globals()[global_name], pd.DataFrame):\n",
    "        return _norm_cols(globals()[global_name].copy())\n",
    "    return _norm_cols(pd.read_csv(path, dtype={\"object_id\":\"string\",\"split\":\"string\"}, **SAFE_READ_KW))\n",
    "\n",
    "def _fill_z(df: pd.DataFrame, split_med: dict, gmed: float):\n",
    "    z = df[\"Z\"].copy()\n",
    "    if z.isna().any():\n",
    "        z = z.fillna(df[\"split\"].map(split_med))\n",
    "        z = z.fillna(np.float32(gmed))\n",
    "    return z.astype(\"float32\")\n",
    "\n",
    "def _read_objq(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, dtype={\"object_id\": \"string\"})\n",
    "    df = _norm_cols(df)\n",
    "    if \"object_id\" not in df.columns:\n",
    "        raise ValueError(f\"object_quality missing object_id: {path}\")\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # drop columns that duplicate meta log content (avoid _oq collisions)\n",
    "    drop_dup = set([\"split\",\"target\",\"Z\",\"Z_err\",\"EBV\",\"has_zerr\",\"is_photoz\"])\n",
    "    keep_cols = [\"object_id\"] + [c for c in df.columns if c not in drop_dup and c != \"object_id\"]\n",
    "\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    # coerce numerics where possible (do not crash if some are strings)\n",
    "    for c in df.columns:\n",
    "        if c == \"object_id\":\n",
    "            continue\n",
    "        if df[c].dtype == \"O\":\n",
    "            # attempt numeric\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "        # if still object, drop it (keep stage2 meta numeric)\n",
    "        if df[c].dtype == \"O\":\n",
    "            df.drop(columns=[c], inplace=True)\n",
    "\n",
    "    return df.set_index(\"object_id\", drop=True)\n",
    "\n",
    "def _assign_splits_to_folds_greedy_multi(sp_stat: pd.DataFrame, n_folds: int, quota: int, seed: int,\n",
    "                                        lam_count: float, lam_quota: float, lam_z: float, lam_ebv: float, lam_obs: float,\n",
    "                                        penalty_zero_pos: float, restarts: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    global_pos_rate = float(sp_stat[\"pos\"].sum() / max(sp_stat[\"n\"].sum(), 1))\n",
    "    target_fold_n = float(sp_stat[\"n\"].sum() / max(n_folds, 1))\n",
    "\n",
    "    wsum = float(sp_stat[\"n\"].sum())\n",
    "    g_z = float((sp_stat[\"n\"] * sp_stat[\"mean_log1pZ\"]).sum() / max(wsum, 1.0))\n",
    "    g_e = float((sp_stat[\"n\"] * sp_stat[\"mean_EBV\"]).sum() / max(wsum, 1.0))\n",
    "    g_o = float((sp_stat[\"n\"] * sp_stat[\"mean_log1pObs\"]).sum() / max(wsum, 1.0)) if \"mean_log1pObs\" in sp_stat.columns else 0.0\n",
    "\n",
    "    base_order = sp_stat.sort_values([\"pos\",\"n\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best = None\n",
    "\n",
    "    for _ in range(max(restarts, 1)):\n",
    "        order = base_order.copy()\n",
    "        order[\"_j\"] = rng.normal(0, 1e-6, size=len(order))\n",
    "        order = order.sort_values([\"pos\",\"n\",\"_j\"], ascending=[False,False,True]).drop(columns=[\"_j\"]).reset_index(drop=True)\n",
    "\n",
    "        fold_n = np.zeros(n_folds, dtype=float)\n",
    "        fold_pos = np.zeros(n_folds, dtype=float)\n",
    "        fold_k = np.zeros(n_folds, dtype=int)\n",
    "        fold_zsum = np.zeros(n_folds, dtype=float)\n",
    "        fold_esum = np.zeros(n_folds, dtype=float)\n",
    "        fold_osum = np.zeros(n_folds, dtype=float)\n",
    "\n",
    "        split2fold = {}\n",
    "\n",
    "        for _, r in order.iterrows():\n",
    "            sp = r[\"split\"]; n = float(r[\"n\"]); p = float(r[\"pos\"])\n",
    "            zmean = float(r[\"mean_log1pZ\"]); eme = float(r[\"mean_EBV\"])\n",
    "            omean = float(r[\"mean_log1pObs\"]) if \"mean_log1pObs\" in r else 0.0\n",
    "\n",
    "            cand = np.where(fold_k < quota)[0]\n",
    "            if len(cand) == 0:\n",
    "                cand = np.arange(n_folds)\n",
    "\n",
    "            scores = []\n",
    "            for f in cand:\n",
    "                n2 = fold_n[f] + n\n",
    "                p2 = fold_pos[f] + p\n",
    "                pr2 = (p2 / n2) if n2 > 0 else global_pos_rate\n",
    "\n",
    "                z2 = (fold_zsum[f] + n * zmean) / max(n2, 1.0)\n",
    "                e2 = (fold_esum[f] + n * eme) / max(n2, 1.0)\n",
    "                o2 = (fold_osum[f] + n * omean) / max(n2, 1.0)\n",
    "\n",
    "                score = abs(pr2 - global_pos_rate) \\\n",
    "                        + lam_count * abs(n2 - target_fold_n) / max(target_fold_n, 1.0) \\\n",
    "                        + lam_z * abs(z2 - g_z) \\\n",
    "                        + lam_ebv * abs(e2 - g_e) \\\n",
    "                        + lam_obs * abs(o2 - g_o) \\\n",
    "                        + lam_quota * (fold_k[f] / max(quota, 1))\n",
    "                scores.append(score)\n",
    "\n",
    "            scores = np.asarray(scores, dtype=float)\n",
    "            best_idx = np.where(scores == scores.min())[0]\n",
    "            choose = int(cand[int(rng.choice(best_idx))]) if len(best_idx) > 1 else int(cand[int(best_idx[0])])\n",
    "\n",
    "            split2fold[sp] = choose\n",
    "            fold_n[choose] += n\n",
    "            fold_pos[choose] += p\n",
    "            fold_k[choose] += 1\n",
    "            fold_zsum[choose] += n * zmean\n",
    "            fold_esum[choose] += n * eme\n",
    "            fold_osum[choose] += n * omean\n",
    "\n",
    "        fold_pr = np.divide(fold_pos, np.maximum(fold_n, 1e-9))\n",
    "        score = float(np.sum(np.abs(fold_pr - global_pos_rate))) \\\n",
    "                + float(lam_count * np.std(fold_n) / max(target_fold_n, 1.0))\n",
    "\n",
    "        zero_pos = int(np.sum(fold_pos == 0))\n",
    "        score += penalty_zero_pos * zero_pos\n",
    "\n",
    "        cand_pack = (score, split2fold, fold_n.copy(), fold_pos.copy(), fold_k.copy(),\n",
    "                     fold_zsum.copy(), fold_esum.copy(), fold_osum.copy(), zero_pos)\n",
    "\n",
    "        if best is None or cand_pack[0] < best[0]:\n",
    "            best = cand_pack\n",
    "\n",
    "        if best is not None and best[-1] == 0 and best[0] < 0.01:\n",
    "            break\n",
    "\n",
    "    return best\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Load logs\n",
    "# ----------------------------\n",
    "df_train = _load_or_use_global(\"df_train_log\", TRAIN_LOG_PATH)\n",
    "df_test  = _load_or_use_global(\"df_test_log\",  TEST_LOG_PATH)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Required columns check\n",
    "# ----------------------------\n",
    "req_common = {\"object_id\", \"split\", \"EBV\", \"Z\"}\n",
    "req_train  = req_common | {\"target\"}\n",
    "req_test   = req_common\n",
    "\n",
    "miss_train = sorted(list(req_train - set(df_train.columns)))\n",
    "miss_test  = sorted(list(req_test  - set(df_test.columns)))\n",
    "if miss_train:\n",
    "    raise ValueError(f\"train_log missing columns: {miss_train} | found={list(df_train.columns)}\")\n",
    "if miss_test:\n",
    "    raise ValueError(f\"test_log missing columns: {miss_test} | found={list(df_test.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Basic cleaning\n",
    "# ----------------------------\n",
    "df_train[\"object_id\"] = df_train[\"object_id\"].astype(\"string\").str.strip()\n",
    "df_test[\"object_id\"]  = df_test[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "df_train[\"split\"] = df_train[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "df_test[\"split\"]  = df_test[\"split\"].astype(\"string\").map(normalize_split_name)\n",
    "\n",
    "bad_train_split = sorted(set(df_train[\"split\"].unique()) - VALID_SPLITS)\n",
    "bad_test_split  = sorted(set(df_test[\"split\"].unique())  - VALID_SPLITS)\n",
    "if bad_train_split:\n",
    "    raise ValueError(f\"train_log invalid split values: {bad_train_split[:10]}\")\n",
    "if bad_test_split:\n",
    "    raise ValueError(f\"test_log invalid split values: {bad_test_split[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Ensure Z_err exists + numeric coercion\n",
    "# ----------------------------\n",
    "if \"Z_err\" not in df_train.columns:\n",
    "    df_train[\"Z_err\"] = np.nan\n",
    "if \"Z_err\" not in df_test.columns:\n",
    "    df_test[\"Z_err\"] = np.nan\n",
    "\n",
    "for c in [\"EBV\",\"Z\",\"Z_err\"]:\n",
    "    _coerce_float32(df_train, c)\n",
    "    _coerce_float32(df_test, c)\n",
    "\n",
    "df_train[\"has_zerr\"] = (~pd.to_numeric(df_train[\"Z_err\"], errors=\"coerce\").isna()).astype(\"int8\")\n",
    "df_test[\"has_zerr\"]  = (~pd.to_numeric(df_test[\"Z_err\"],  errors=\"coerce\").isna()).astype(\"int8\")\n",
    "df_train[\"is_photoz\"] = df_train[\"has_zerr\"].astype(\"int8\")\n",
    "df_test[\"is_photoz\"]  = df_test[\"has_zerr\"].astype(\"int8\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Duplicate / overlap checks\n",
    "# ----------------------------\n",
    "if df_train[\"object_id\"].duplicated().any():\n",
    "    ex = df_train.loc[df_train[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in train_log (examples): {ex}\")\n",
    "if df_test[\"object_id\"].duplicated().any():\n",
    "    ex = df_test.loc[df_test[\"object_id\"].duplicated(), \"object_id\"].head(5).tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in test_log (examples): {ex}\")\n",
    "\n",
    "overlap = set(df_train[\"object_id\"].tolist()) & set(df_test[\"object_id\"].tolist())\n",
    "if overlap:\n",
    "    raise ValueError(f\"object_id overlap train vs test (examples): {list(overlap)[:5]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Target validation\n",
    "# ----------------------------\n",
    "df_train[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\n",
    "if df_train[\"target\"].isna().any():\n",
    "    raise ValueError(f\"train_log target has NaN after coercion: {int(df_train['target'].isna().sum())} rows.\")\n",
    "uniq_t = set(pd.unique(df_train[\"target\"]).tolist())\n",
    "if not uniq_t.issubset({0,1}):\n",
    "    raise ValueError(f\"train_log target must be binary 0/1. Found: {sorted(list(uniq_t))}\")\n",
    "df_train[\"target\"] = df_train[\"target\"].astype(\"int8\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Missing flags + fills (safe)\n",
    "# ----------------------------\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"EBV_missing\"]  = df[\"EBV\"].isna().astype(\"int8\")\n",
    "    df[\"Z_missing\"]    = df[\"Z\"].isna().astype(\"int8\")\n",
    "    df[\"Zerr_missing\"] = df[\"Z_err\"].isna().astype(\"int8\")\n",
    "\n",
    "# fill EBV with train median\n",
    "ebv_med = float(np.nanmedian(df_train[\"EBV\"].values.astype(float))) if np.isfinite(df_train[\"EBV\"].values.astype(float)).any() else 0.0\n",
    "df_train[\"EBV\"] = df_train[\"EBV\"].fillna(np.float32(ebv_med)).astype(\"float32\")\n",
    "df_test[\"EBV\"]  = df_test[\"EBV\"].fillna(np.float32(ebv_med)).astype(\"float32\")\n",
    "\n",
    "# fill Z with split median then global median (from train only)\n",
    "train_split_med = df_train.groupby(\"split\")[\"Z\"].median().to_dict()\n",
    "train_gmed = float(np.nanmedian(df_train[\"Z\"].values.astype(float))) if np.isfinite(df_train[\"Z\"].values.astype(float)).any() else 0.0\n",
    "df_train[\"Z\"] = _fill_z(df_train, train_split_med, train_gmed)\n",
    "df_test[\"Z\"]  = _fill_z(df_test,  train_split_med, train_gmed)\n",
    "\n",
    "# fill Z_err with 0 (keep has_zerr as flag)\n",
    "df_train[\"Z_err\"] = df_train[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "df_test[\"Z_err\"]  = df_test[\"Z_err\"].fillna(np.float32(0.0)).astype(\"float32\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Clipping + derived meta features\n",
    "# ----------------------------\n",
    "EBV_LO, EBV_HI = _qclip_bounds(df_train[\"EBV\"].values, QLO, QHI)\n",
    "Z_LO,   Z_HI   = _qclip_bounds(df_train[\"Z\"].values,   QLO, QHI)\n",
    "\n",
    "df_train[\"EBV_clip\"] = _safe_clip(df_train[\"EBV\"], EBV_LO, EBV_HI)\n",
    "df_test[\"EBV_clip\"]  = _safe_clip(df_test[\"EBV\"],  EBV_LO, EBV_HI)\n",
    "\n",
    "df_train[\"Z_clip\"] = _safe_clip(df_train[\"Z\"], Z_LO, Z_HI)\n",
    "df_test[\"Z_clip\"]  = _safe_clip(df_test[\"Z\"],  Z_LO, Z_HI)\n",
    "\n",
    "# Z_err clip: prefer train photo-z pool, fallback to test photo-z pool\n",
    "ZE_LO = 0.0\n",
    "pool_tr = df_train.loc[df_train[\"has_zerr\"] == 1, \"Z_err\"].values\n",
    "pool_te = df_test.loc[df_test[\"has_zerr\"] == 1, \"Z_err\"].values\n",
    "if np.isfinite(pool_tr.astype(float)).any():\n",
    "    _, ZE_HI = _qclip_bounds(pool_tr, QLO, QHI, default=(0.0, 0.0))\n",
    "elif np.isfinite(pool_te.astype(float)).any():\n",
    "    _, ZE_HI = _qclip_bounds(pool_te, QLO, QHI, default=(0.0, 0.0))\n",
    "else:\n",
    "    ZE_HI = 0.0\n",
    "ZE_HI = max(float(ZE_HI), 0.0)\n",
    "\n",
    "df_train[\"Zerr_clip\"] = _safe_clip(df_train[\"Z_err\"], ZE_LO, ZE_HI)\n",
    "df_test[\"Zerr_clip\"]  = _safe_clip(df_test[\"Z_err\"],  ZE_LO, ZE_HI)\n",
    "\n",
    "eps = np.float32(1e-6)\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"log1pZ\"] = np.log1p(df[\"Z_clip\"]).astype(\"float32\")\n",
    "    df[\"inv_1pz\"] = (1.0 / (1.0 + df[\"Z_clip\"])).astype(\"float32\")\n",
    "    df[\"z2\"] = (df[\"Z_clip\"] * df[\"Z_clip\"]).astype(\"float32\")\n",
    "\n",
    "    df[\"log1pEBV\"] = np.log1p(df[\"EBV_clip\"]).astype(\"float32\")\n",
    "    df[\"ebv_over_1pz\"] = (df[\"EBV_clip\"] / (1.0 + df[\"Z_clip\"] + eps)).astype(\"float32\")\n",
    "    df[\"ebv_x_1pz\"] = (df[\"EBV_clip\"] * (1.0 + df[\"Z_clip\"])).astype(\"float32\")\n",
    "\n",
    "    df[\"log1pZerr\"] = np.log1p(df[\"Zerr_clip\"]).astype(\"float32\")\n",
    "    # safer denom\n",
    "    df[\"zerr_over_1pz\"] = (df[\"Zerr_clip\"] / (1.0 + df[\"Z_clip\"] + eps)).astype(\"float32\")\n",
    "    df[\"zerr_rel_z\"] = (df[\"Zerr_clip\"] / (df[\"Z_clip\"] + eps)).astype(\"float32\")\n",
    "    df[\"high_zerr\"] = (df[\"zerr_over_1pz\"] > np.float32(0.30)).astype(\"int8\")  # tunable\n",
    "\n",
    "split2id = {f\"split_{i:02d}\": i for i in range(1, 21)}\n",
    "df_train[\"split_id\"] = df_train[\"split\"].map(split2id).astype(\"int16\")\n",
    "df_test[\"split_id\"]  = df_test[\"split\"].map(split2id).astype(\"int16\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) JOIN object_quality from STAGE1 (lebih awal, bisa dipakai fold balancing juga)\n",
    "# ----------------------------\n",
    "objq_joined = False\n",
    "oq_train_cols = []\n",
    "oq_test_cols = []\n",
    "\n",
    "if OBJQ_TRAIN and Path(OBJQ_TRAIN).exists():\n",
    "    oq_tr = _read_objq(OBJQ_TRAIN)\n",
    "    oq_train_cols = oq_tr.columns.tolist()\n",
    "    df_train = df_train.set_index(\"object_id\", drop=False).join(oq_tr, how=\"left\").reset_index(drop=True)\n",
    "    objq_joined = True\n",
    "\n",
    "if OBJQ_TEST and Path(OBJQ_TEST).exists():\n",
    "    oq_te = _read_objq(OBJQ_TEST)\n",
    "    oq_test_cols = oq_te.columns.tolist()\n",
    "    df_test = df_test.set_index(\"object_id\", drop=False).join(oq_te, how=\"left\").reset_index(drop=True)\n",
    "    objq_joined = True\n",
    "\n",
    "# unify objq cols intersection (train/test) for meta keep\n",
    "oq_cols_common = sorted(list(set(oq_train_cols) & set(oq_test_cols)))\n",
    "\n",
    "# fill + alias safety\n",
    "if objq_joined and len(oq_cols_common) > 0:\n",
    "    for df in [df_train, df_test]:\n",
    "        # default fills\n",
    "        if \"n_obs_total\" in df.columns:\n",
    "            df[\"n_obs_total\"] = pd.to_numeric(df[\"n_obs_total\"], errors=\"coerce\").fillna(0).astype(\"int32\")\n",
    "            df[\"low_obs\"] = (df[\"n_obs_total\"] < 20).astype(\"int8\")\n",
    "        if \"n_bands_present\" in df.columns:\n",
    "            df[\"n_bands_present\"] = pd.to_numeric(df[\"n_bands_present\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "            df[\"low_bandcov\"] = (df[\"n_bands_present\"] <= 2).astype(\"int8\")\n",
    "\n",
    "        # alias det/strong fractions if stage1 v5.1 naming present\n",
    "        cand_det = f\"snr_det_abs_frac_{BASE_DET_KEY}\"\n",
    "        if (\"snr_det_frac\" not in df.columns) and (cand_det in df.columns):\n",
    "            df[\"snr_det_frac\"] = pd.to_numeric(df[cand_det], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "        if (\"snr_strong_frac\" not in df.columns) and (\"snr_strong_abs_frac\" in df.columns):\n",
    "            df[\"snr_strong_frac\"] = pd.to_numeric(df[\"snr_strong_abs_frac\"], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        # common numeric fills\n",
    "        for c in [\"timespan\",\"cadence_proxy\",\"neg_flux_frac\",\"snr_det_frac\",\"snr_strong_frac\",\n",
    "                  \"snr_abs_max\",\"snr_pos_max\",\"snr_neg_min\",\"flux_mean\",\"flux_std\",\"ferr_mean\",\n",
    "                  \"timespan_rest\",\"cadence_proxy_rest\",\"zerr_rel\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        # per band fracs\n",
    "        for b in [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]:\n",
    "            c = f\"frac_{b}\"\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "            c2 = f\"det_frac_{b}\"\n",
    "            if c2 in df.columns:\n",
    "                df[c2] = pd.to_numeric(df[c2], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Fold assignment (split-aware, balanced incl Z/EBV (+ optional obs))\n",
    "# ----------------------------\n",
    "df_train[\"fold\"] = -1\n",
    "\n",
    "if CV_USE_SPLIT_COL:\n",
    "    sp_pos = df_train.groupby(\"split\")[\"target\"].sum().reindex(SPLIT_LIST).fillna(0).astype(int)\n",
    "    sp_n   = df_train.groupby(\"split\")[\"target\"].count().reindex(SPLIT_LIST).fillna(0).astype(int)\n",
    "\n",
    "    sp_zm  = df_train.groupby(\"split\")[\"log1pZ\"].mean().reindex(SPLIT_LIST).fillna(0.0).astype(\"float32\")\n",
    "    sp_em  = df_train.groupby(\"split\")[\"EBV_clip\"].mean().reindex(SPLIT_LIST).fillna(0.0).astype(\"float32\")\n",
    "\n",
    "    # optional obs summary if objq exists\n",
    "    if \"n_obs_total\" in df_train.columns:\n",
    "        sp_om = df_train.groupby(\"split\")[\"n_obs_total\"].mean().reindex(SPLIT_LIST).fillna(0.0).astype(\"float32\")\n",
    "        sp_om = np.log1p(sp_om).astype(\"float32\")\n",
    "    else:\n",
    "        sp_om = pd.Series(np.zeros(len(SPLIT_LIST), dtype=\"float32\"), index=SPLIT_LIST)\n",
    "\n",
    "    sp_stat = pd.DataFrame({\n",
    "        \"split\": SPLIT_LIST,\n",
    "        \"n\": sp_n.values.astype(int),\n",
    "        \"pos\": sp_pos.values.astype(int),\n",
    "        \"mean_log1pZ\": sp_zm.values.astype(float),\n",
    "        \"mean_EBV\": sp_em.values.astype(float),\n",
    "        \"mean_log1pObs\": sp_om.values.astype(float),\n",
    "    })\n",
    "\n",
    "    best = _assign_splits_to_folds_greedy_multi(\n",
    "        sp_stat, n_folds=N_FOLDS, quota=FOLD_QUOTA, seed=SEED,\n",
    "        lam_count=LAMBDA_COUNT, lam_quota=LAMBDA_QUOTA,\n",
    "        lam_z=LAMBDA_ZMEAN, lam_ebv=LAMBDA_EBVMEAN, lam_obs=LAMBDA_OBSMEAN,\n",
    "        penalty_zero_pos=PENALTY_ZERO_POS, restarts=RESTARTS\n",
    "    )\n",
    "    if best is None:\n",
    "        raise RuntimeError(\"split->fold assignment failed unexpectedly.\")\n",
    "\n",
    "    if best[-1] > 0 and RESTARTS_HARD > RESTARTS:\n",
    "        best2 = _assign_splits_to_folds_greedy_multi(\n",
    "            sp_stat, n_folds=N_FOLDS, quota=FOLD_QUOTA, seed=SEED + 999,\n",
    "            lam_count=LAMBDA_COUNT, lam_quota=LAMBDA_QUOTA,\n",
    "            lam_z=LAMBDA_ZMEAN, lam_ebv=LAMBDA_EBVMEAN, lam_obs=LAMBDA_OBSMEAN,\n",
    "            penalty_zero_pos=PENALTY_ZERO_POS, restarts=RESTARTS_HARD\n",
    "        )\n",
    "        if best2 is not None and best2[0] <= best[0]:\n",
    "            best = best2\n",
    "\n",
    "    score, split2fold, fold_n, fold_pos, fold_k, *_rest, zero_pos = best\n",
    "    df_train[\"fold\"] = df_train[\"split\"].map(split2fold).astype(\"int16\")\n",
    "\n",
    "    uniq_folds = sorted(df_train[\"fold\"].unique().tolist())\n",
    "    if uniq_folds != list(range(N_FOLDS)):\n",
    "        print(f\"[WARN] split->fold tidak memakai semua fold. uniq_folds={uniq_folds}. Fallback StratifiedKFold.\")\n",
    "        CV_USE_SPLIT_COL = False\n",
    "    else:\n",
    "        with open(ART_DIR / \"split2fold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({k:int(v) for k,v in split2fold.items()}, f)\n",
    "\n",
    "if not CV_USE_SPLIT_COL:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    y = df_train[\"target\"].to_numpy()\n",
    "    idx = np.arange(len(df_train))\n",
    "    for fold_id, (_, va_idx) in enumerate(skf.split(idx, y)):\n",
    "        df_train.iloc[va_idx, df_train.columns.get_loc(\"fold\")] = fold_id\n",
    "    df_train[\"fold\"] = df_train[\"fold\"].astype(\"int16\")\n",
    "\n",
    "if (df_train[\"fold\"] < 0).any():\n",
    "    n_bad = int((df_train[\"fold\"] < 0).sum())\n",
    "    raise RuntimeError(f\"Fold assignment gagal: ada {n_bad} baris fold=-1\")\n",
    "\n",
    "fold_tab = (\n",
    "    df_train.groupby(\"fold\")[\"target\"]\n",
    "    .agg([\"count\",\"sum\"])\n",
    "    .rename(columns={\"sum\":\"pos\"})\n",
    "    .reindex(range(N_FOLDS)).fillna(0)\n",
    ")\n",
    "fold_tab[\"pos_rate\"] = fold_tab[\"pos\"] / fold_tab[\"count\"].clip(lower=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 12) OOF split prior (smoothed)\n",
    "# ----------------------------\n",
    "g_pos = float(df_train[\"target\"].sum())\n",
    "g_n = float(len(df_train))\n",
    "g_rate = g_pos / max(g_n, 1.0)\n",
    "\n",
    "sp_all = df_train.groupby(\"split\")[\"target\"].agg([\"count\",\"sum\"]).rename(columns={\"count\":\"n\",\"sum\":\"pos\"})\n",
    "sp_all[\"prior\"] = (sp_all[\"pos\"] + PRIOR_ALPHA * g_rate) / (sp_all[\"n\"] + PRIOR_ALPHA)\n",
    "\n",
    "df_test[\"split_pos_prior\"] = df_test[\"split\"].map(sp_all[\"prior\"]).fillna(g_rate).astype(\"float32\")\n",
    "\n",
    "df_train[\"split_pos_prior_oof\"] = np.float32(g_rate)\n",
    "for f in range(N_FOLDS):\n",
    "    tr_idx = df_train[\"fold\"] != f\n",
    "    sp_f = df_train.loc[tr_idx].groupby(\"split\")[\"target\"].agg([\"count\",\"sum\"]).rename(columns={\"count\":\"n\",\"sum\":\"pos\"})\n",
    "    g_rate_f = float(df_train.loc[tr_idx, \"target\"].sum() / max(tr_idx.sum(), 1))\n",
    "    sp_f[\"prior\"] = (sp_f[\"pos\"] + PRIOR_ALPHA * g_rate_f) / (sp_f[\"n\"] + PRIOR_ALPHA)\n",
    "    m = df_train[\"fold\"] == f\n",
    "    df_train.loc[m, \"split_pos_prior_oof\"] = df_train.loc[m, \"split\"].map(sp_f[\"prior\"]).fillna(g_rate_f).astype(\"float32\")\n",
    "\n",
    "df_train[\"split_pos_prior\"] = df_train[\"split\"].map(sp_all[\"prior\"]).fillna(g_rate).astype(\"float32\")\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Build meta tables (index=object_id)\n",
    "# ----------------------------\n",
    "base_train_cols = [\n",
    "    \"object_id\",\"split\",\"split_id\",\n",
    "    \"EBV\",\"EBV_clip\",\"log1pEBV\",\"ebv_over_1pz\",\"ebv_x_1pz\",\n",
    "    \"Z\",\"Z_clip\",\"log1pZ\",\"inv_1pz\",\"z2\",\n",
    "    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_over_1pz\",\"zerr_rel_z\",\"high_zerr\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n",
    "    \"split_pos_prior\",\"split_pos_prior_oof\",\n",
    "    \"fold\",\"target\"\n",
    "]\n",
    "base_test_cols = [\n",
    "    \"object_id\",\"split\",\"split_id\",\n",
    "    \"EBV\",\"EBV_clip\",\"log1pEBV\",\"ebv_over_1pz\",\"ebv_x_1pz\",\n",
    "    \"Z\",\"Z_clip\",\"log1pZ\",\"inv_1pz\",\"z2\",\n",
    "    \"Z_err\",\"Zerr_clip\",\"log1pZerr\",\"zerr_over_1pz\",\"zerr_rel_z\",\"high_zerr\",\n",
    "    \"EBV_missing\",\"Z_missing\",\"Zerr_missing\",\"has_zerr\",\"is_photoz\",\n",
    "    \"split_pos_prior\"\n",
    "]\n",
    "\n",
    "# choose objq feature columns that exist in BOTH train/test\n",
    "objq_cols = []\n",
    "for c in oq_cols_common:\n",
    "    if c in df_train.columns and c in df_test.columns:\n",
    "        objq_cols.append(c)\n",
    "\n",
    "# ensure alias cols included if created\n",
    "for c in [\"low_obs\",\"low_bandcov\",\"snr_det_frac\",\"snr_strong_frac\"]:\n",
    "    if (c in df_train.columns) and (c in df_test.columns) and (c not in objq_cols):\n",
    "        objq_cols.append(c)\n",
    "\n",
    "keep_train = [c for c in base_train_cols if c in df_train.columns] + objq_cols\n",
    "keep_test  = [c for c in base_test_cols  if c in df_test.columns]  + objq_cols\n",
    "\n",
    "df_train_meta = df_train[keep_train].copy().set_index(\"object_id\", drop=True).sort_index()\n",
    "df_test_meta  = df_test[keep_test].copy().set_index(\"object_id\", drop=True).sort_index()\n",
    "\n",
    "if not df_train_meta.index.is_unique:\n",
    "    raise RuntimeError(\"df_train_meta index (object_id) not unique after processing.\")\n",
    "if not df_test_meta.index.is_unique:\n",
    "    raise RuntimeError(\"df_test_meta index (object_id) not unique after processing.\")\n",
    "\n",
    "id2split_train = df_train_meta[\"split\"].to_dict()\n",
    "id2split_test  = df_test_meta[\"split\"].to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Save artifacts\n",
    "# ----------------------------\n",
    "train_pq = ART_DIR / \"train_meta.parquet\"\n",
    "test_pq  = ART_DIR / \"test_meta.parquet\"\n",
    "train_csv = ART_DIR / \"train_meta.csv\"\n",
    "test_csv  = ART_DIR / \"test_meta.csv\"\n",
    "\n",
    "try:\n",
    "    df_train_meta.to_parquet(train_pq, index=True)\n",
    "    df_test_meta.to_parquet(test_pq, index=True)\n",
    "    saved_train, saved_test = str(train_pq), str(test_pq)\n",
    "except Exception:\n",
    "    df_train_meta.to_csv(train_csv, index=True)\n",
    "    df_test_meta.to_csv(test_csv, index=True)\n",
    "    saved_train, saved_test = str(train_csv), str(test_csv)\n",
    "\n",
    "split_stats = pd.DataFrame({\n",
    "    \"train_objects\": df_train_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n",
    "    \"test_objects\":  df_test_meta[\"split\"].value_counts().reindex(SPLIT_LIST).fillna(0).astype(int),\n",
    "})\n",
    "split_stats.index.name = \"split\"\n",
    "pos_by_split = df_train_meta.groupby(\"split\")[\"target\"].sum().reindex(SPLIT_LIST).fillna(0).astype(int)\n",
    "split_stats[\"train_pos\"] = pos_by_split.values\n",
    "split_stats[\"train_pos_rate\"] = (split_stats[\"train_pos\"] / split_stats[\"train_objects\"].clip(lower=1)).astype(\"float32\")\n",
    "split_stats_path = ART_DIR / \"split_stats.csv\"\n",
    "split_stats.to_csv(split_stats_path)\n",
    "\n",
    "fold_path = ART_DIR / \"train_folds.csv\"\n",
    "df_train_meta.reset_index()[[\"object_id\",\"split\",\"fold\",\"target\"]].to_csv(fold_path, index=False)\n",
    "\n",
    "with open(ART_DIR / \"id2split_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id2split_train, f)\n",
    "with open(ART_DIR / \"id2split_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id2split_test, f)\n",
    "\n",
    "drop_nonfeat = set([\"target\",\"fold\"])\n",
    "meta_feature_cols = [c for c in df_train_meta.columns.tolist() if c not in drop_nonfeat]\n",
    "with open(ART_DIR / \"meta_feature_cols.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta_feature_cols, f, indent=2)\n",
    "\n",
    "pos = int((df_train_meta[\"target\"] == 1).sum())\n",
    "neg = int((df_train_meta[\"target\"] == 0).sum())\n",
    "tot = int(len(df_train_meta))\n",
    "pos_rate = pos / max(tot, 1)\n",
    "scale_pos_weight = float(neg / max(pos, 1))\n",
    "\n",
    "stage2_summary = {\n",
    "    \"stage\": \"stage2\",\n",
    "    \"N_FOLDS\": int(N_FOLDS),\n",
    "    \"CV_USE_SPLIT_COL_USED\": bool(CV_USE_SPLIT_COL),\n",
    "    \"OBJQ_JOINED\": bool(objq_joined),\n",
    "    \"objq_cols_used\": int(len(objq_cols)),\n",
    "    \"counts\": {\n",
    "        \"train\": int(tot),\n",
    "        \"pos\": int(pos),\n",
    "        \"neg\": int(neg),\n",
    "        \"pos_rate\": float(pos_rate),\n",
    "        \"test\": int(len(df_test_meta))\n",
    "    },\n",
    "    \"clip_ranges\": {\n",
    "        \"EBV_train\": [float(EBV_LO), float(EBV_HI)],\n",
    "        \"Z_train\": [float(Z_LO), float(Z_HI)],\n",
    "        \"Zerr_used\": [float(ZE_LO), float(ZE_HI)]\n",
    "    },\n",
    "    \"split_prior\": {\n",
    "        \"alpha\": float(PRIOR_ALPHA),\n",
    "        \"global_pos_rate\": float(g_rate)\n",
    "    },\n",
    "    \"scale_pos_weight\": float(scale_pos_weight),\n",
    "    \"base_det_thr\": float(BASE_DET_THR),\n",
    "    \"artifacts\": {\n",
    "        \"train_meta\": saved_train,\n",
    "        \"test_meta\": saved_test,\n",
    "        \"split_stats\": str(split_stats_path),\n",
    "        \"train_folds\": str(fold_path),\n",
    "        \"id2split_train\": str(ART_DIR / \"id2split_train.json\"),\n",
    "        \"id2split_test\": str(ART_DIR / \"id2split_test.json\"),\n",
    "        \"split2fold\": str(ART_DIR / \"split2fold.json\") if (ART_DIR / \"split2fold.json\").exists() else None,\n",
    "        \"meta_feature_cols\": str(ART_DIR / \"meta_feature_cols.json\"),\n",
    "    }\n",
    "}\n",
    "with open(ART_DIR / \"stage2_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stage2_summary, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 15) Print summary\n",
    "# ----------------------------\n",
    "print(\"STAGE 2 OK — META READY (clean + folds + enriched)\")\n",
    "print(f\"- CV_USE_SPLIT_COL_USED: {CV_USE_SPLIT_COL} | N_FOLDS={N_FOLDS} | split_quota={FOLD_QUOTA}\")\n",
    "print(f\"- OBJQ_JOINED: {objq_joined} | objq_cols_used={len(objq_cols)} | base_det_thr={BASE_DET_THR}\")\n",
    "print(f\"- train objects: {tot:,} | pos={pos:,} | neg={neg:,} | pos%={pos_rate*100:.3f}%\")\n",
    "print(f\"- test objects : {len(df_test_meta):,}\")\n",
    "print(f\"- saved train  : {saved_train}\")\n",
    "print(f\"- saved test   : {saved_test}\")\n",
    "print(f\"- saved stats  : {split_stats_path}\")\n",
    "print(f\"- saved folds  : {fold_path}\")\n",
    "print(f\"- saved meta_feature_cols: {ART_DIR / 'meta_feature_cols.json'}\")\n",
    "print(f\"- scale_pos_weight (neg/pos): {scale_pos_weight:.3f}\")\n",
    "\n",
    "print(\"\\nCLIP RANGES\")\n",
    "print(f\"- EBV clip (train): [{EBV_LO:.6f}, {EBV_HI:.6f}]\")\n",
    "print(f\"- Z   clip (train): [{Z_LO:.6f}, {Z_HI:.6f}]\")\n",
    "print(f\"- Zerr clip used  : [{ZE_LO:.6f}, {ZE_HI:.6f}]\")\n",
    "\n",
    "print(\"\\nFOLD BALANCE (count/pos/pos_rate)\")\n",
    "print(fold_tab.to_string())\n",
    "\n",
    "# ----------------------------\n",
    "# 16) Export globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"df_train_meta\": df_train_meta,\n",
    "    \"df_test_meta\": df_test_meta,\n",
    "    \"id2split_train\": id2split_train,\n",
    "    \"id2split_test\": id2split_test,\n",
    "    \"split_stats\": split_stats,\n",
    "    \"split2id\": split2id,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"CV_USE_SPLIT_COL_USED\": CV_USE_SPLIT_COL,\n",
    "    \"META_FEATURE_COLS_PATH\": str(ART_DIR / \"meta_feature_cols.json\"),\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd42c97e",
   "metadata": {
    "papermill": {
     "duration": 0.015129,
     "end_time": "2026-01-07T16:45:41.166371",
     "exception": false,
     "start_time": "2026-01-07T16:45:41.151242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lightcurve Loading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54baa767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:45:41.199964Z",
     "iopub.status.busy": "2026-01-07T16:45:41.199557Z",
     "iopub.status.idle": "2026-01-07T16:45:43.152696Z",
     "shell.execute_reply": "2026-01-07T16:45:43.151689Z"
    },
    "papermill": {
     "duration": 1.973016,
     "end_time": "2026-01-07T16:45:43.154610",
     "exception": false,
     "start_time": "2026-01-07T16:45:41.181594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJQ][train] drop META-like cols from objq: ['split', 'target', 'Z', 'Z_err', 'EBV', 'has_zerr', 'zerr_rel']\n",
      "[OBJQ][train] drop non-objq cols: ['det_u', 'det_g', 'det_r', 'det_i', 'det_z', 'det_y', 'neg_count', 'pos_count', 'ferr0_count', 'snr_det_abs_20', 'snr_det_pos_20', 'snr_det_neg_20']...\n",
      "[OBJQ][test] drop META-like cols from objq: ['split', 'Z', 'Z_err', 'EBV', 'has_zerr', 'zerr_rel']\n",
      "[OBJQ][test] drop non-objq cols: ['det_u', 'det_g', 'det_r', 'det_i', 'det_z', 'det_y', 'neg_count', 'pos_count', 'ferr0_count', 'snr_det_abs_20', 'snr_det_pos_20', 'snr_det_neg_20']...\n",
      "[OBJQ][train] overlap detected -> overwrite from objq: ['cadence_proxy', 'flux_mean', 'flux_std', 'frac_g', 'frac_i', 'frac_r', 'frac_u', 'frac_y', 'frac_z', 'n_bands_det', 'n_bands_present', 'n_g']...\n",
      "[OBJQ][test] overlap detected -> overwrite from objq: ['cadence_proxy', 'flux_mean', 'flux_std', 'frac_g', 'frac_i', 'frac_r', 'frac_u', 'frac_y', 'frac_z', 'n_bands_det', 'n_bands_present', 'n_g']...\n",
      "[OBJQ] Joined into meta (SAFE overwrite v5.4) + saved updated meta (parquet) in /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts\n",
      "STAGE 3 OK — LIGHTCURVE UTILITIES READY (+OBJQ full if enabled)\n",
      "- Saved manifest: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/split_file_manifest.csv\n",
      "- Saved counts  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/object_counts_by_split.csv\n",
      "- LC_CACHE_DIR  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/cache/lightcurves_npz\n",
      "- OBJQ train    : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/object_quality_train.csv (exists=True)\n",
      "- OBJQ test     : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs/object_quality_test.csv (exists=True)\n",
      "- Smoke splits  : ['split_01', 'split_02']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — Robust Lightcurve Loader Utilities + FULL Object-Quality Build (ONE CELL)\n",
    "# REVISI FULL v5.4 (UPGRADE dari v5.3: filter numeric-safe + OBJQ sanitize hard + smoke test anti-StopIteration)\n",
    "#\n",
    "# Upgrade v5.4 (dibanding v5.3):\n",
    "# - FIX filter \"0..5\" (string/angka) -> map ke u,g,r,i,z,y (tidak kebuang karena invalid)\n",
    "# - OBJQ join makin aman:\n",
    "#   * drop kolom \"Unnamed:*\"\n",
    "#   * drop kolom meta-like (heuristic) walau tidak overlap\n",
    "#   * keep only kolom objq valid (whitelist) biar objq csv kotor tidak nyusup ke meta\n",
    "#   * numeric coercion + dedup aggregator aman\n",
    "# - Smoke test: handle StopIteration (kalau chunk pertama habis terfilter / file kosong)\n",
    "#\n",
    "# Tetap:\n",
    "# - FAST dtype mode (float32 parse) + fallback SAFE mode (coerce)\n",
    "# - FULL Object Quality builder (streaming per split)\n",
    "# - Output: logs/object_quality_train.csv + logs/object_quality_test.csv\n",
    "# - Auto-join ke df_train_meta / df_test_meta + overwrite meta di ART_DIR\n",
    "# - Loader functions: iter_lightcurve_chunks, load_object_lightcurve, load_many_object_lightcurves, NPZ cache\n",
    "#\n",
    "# Output globals:\n",
    "# - OBJECT_QUALITY_TRAIN_PATH, OBJECT_QUALITY_TEST_PATH\n",
    "# - df_train_meta, df_test_meta updated (enriched)\n",
    "# ============================================================\n",
    "\n",
    "import gc, re, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need_prev = [\"SPLIT_DIRS\", \"SPLIT_LIST\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"CFG\", \"SEED\", \"LOG_DIR\"]\n",
    "for need in need_prev:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> STAGE 1 -> STAGE 2 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SEED = int(SEED)\n",
    "\n",
    "# Optional CACHE_DIR (dari STAGE 0). Kalau tidak ada, fallback ke ART_DIR/cache\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", ART_DIR / \"cache\"))\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Core config\n",
    "MIN_FLUXERR = float(CFG.get(\"MIN_FLUXERR\", 1e-6))\n",
    "CHUNK_ROWS_DEFAULT = int(CFG.get(\"CHUNK_ROWS\", 400_000))\n",
    "SNR_CLIP = float(CFG.get(\"SNR_CLIP\", 30.0))\n",
    "SNR_DET_THR = float(CFG.get(\"SNR_DET_THR\", 3.0))\n",
    "SNR_STRONG_THR = float(CFG.get(\"SNR_STRONG_THR\", 5.0))\n",
    "\n",
    "# Object-quality builder config (BRUTAL but CPU-safe)\n",
    "BUILD_OBJECT_QUALITY = bool(CFG.get(\"BUILD_OBJECT_QUALITY\", True))\n",
    "OBJQ_REFRESH = bool(CFG.get(\"OBJQ_REFRESH\", False))\n",
    "OBJQ_CHUNK_ROWS = int(CFG.get(\"OBJQ_CHUNK_ROWS\", max(200_000, CHUNK_ROWS_DEFAULT)))\n",
    "OBJQ_SAVE_PARQUET = bool(CFG.get(\"OBJQ_SAVE_PARQUET\", False))  # default csv for portability\n",
    "OBJQ_ONLY_IF_MISSING = bool(CFG.get(\"OBJQ_ONLY_IF_MISSING\", True))  # don't rebuild if files exist (unless refresh)\n",
    "\n",
    "# konsisten dengan STAGE 0/2\n",
    "SAFE_NA_VALUES = [\"\", \" \", \"NA\", \"NaN\", \"nan\", \"NULL\", \"null\", \"None\", \"none\"]\n",
    "SAFE_READ_KW = dict(low_memory=False, na_values=SAFE_NA_VALUES, keep_default_na=True)\n",
    "\n",
    "REQ_LC_KEYS = [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]\n",
    "ALLOWED_FILTERS = {\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"}\n",
    "ALLOWED_FILTERS_TUP = (\"u\", \"g\", \"r\", \"i\", \"z\", \"y\")\n",
    "FILTER_ORDER = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\n",
    "FILTER2ID = {\"u\":0, \"g\":1, \"r\":2, \"i\":3, \"z\":4, \"y\":5}\n",
    "ID2FILTER = {v:k for k,v in FILTER2ID.items()}\n",
    "\n",
    "# v5.4: support numeric filters \"0..5\"\n",
    "FILTER_NUM2STR = {\"0\":\"u\",\"1\":\"g\",\"2\":\"r\",\"3\":\"i\",\"4\":\"z\",\"5\":\"y\"}\n",
    "\n",
    "# Cache configs\n",
    "_LC_CFG_CACHE = {}  # (split_name, which) -> cfg dict\n",
    "_LC_OBJ_CACHE = {}  # in-memory small cache: (which, object_id) -> df\n",
    "LC_CACHE_DIR = CACHE_DIR / \"lightcurves_npz\"\n",
    "LC_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAX_MEM_CACHE = int(CFG.get(\"LC_MEM_CACHE_MAX\", 64))  # max objects in RAM cache\n",
    "\n",
    "# ----------------------------\n",
    "# Utils: ID normalize (robust)\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_, bytearray)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "# Normalize meta indices early (prevent mismatch)\n",
    "df_train_meta = df_train_meta.copy(deep=False)\n",
    "df_test_meta  = df_test_meta.copy(deep=False)\n",
    "df_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index.tolist()], name=df_train_meta.index.name)\n",
    "df_test_meta.index  = pd.Index([_norm_id(z) for z in df_test_meta.index.tolist()], name=df_test_meta.index.name)\n",
    "\n",
    "if \"split\" not in df_train_meta.columns or \"split\" not in df_test_meta.columns:\n",
    "    raise RuntimeError(\"Missing column `split` in df_train_meta/df_test_meta. Pastikan STAGE 2 membuat routing split.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build split file mapping (train/test lightcurves)\n",
    "# ----------------------------\n",
    "SPLIT_FILES = {}\n",
    "for s in SPLIT_LIST:\n",
    "    sd = Path(SPLIT_DIRS[s])\n",
    "    tr = sd / \"train_full_lightcurves.csv\"\n",
    "    te = sd / \"test_full_lightcurves.csv\"\n",
    "    if (not tr.exists()) or (not te.exists()):\n",
    "        raise FileNotFoundError(f\"Missing lightcurve file(s) in {sd}: train={tr.exists()} test={te.exists()}\")\n",
    "    SPLIT_FILES[s] = {\"train\": tr, \"test\": te}\n",
    "\n",
    "# Save split file manifest\n",
    "manifest = []\n",
    "for s in SPLIT_LIST:\n",
    "    p_tr = SPLIT_FILES[s][\"train\"]\n",
    "    p_te = SPLIT_FILES[s][\"test\"]\n",
    "    manifest.append({\n",
    "        \"split\": s,\n",
    "        \"train_path\": str(p_tr),\n",
    "        \"test_path\": str(p_te),\n",
    "        \"train_mb\": float(p_tr.stat().st_size) / (1024**2),\n",
    "        \"test_mb\":  float(p_te.stat().st_size) / (1024**2),\n",
    "    })\n",
    "df_manifest = pd.DataFrame(manifest).sort_values(\"split\")\n",
    "manifest_path = ART_DIR / \"split_file_manifest.csv\"\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build object routing by split\n",
    "# ----------------------------\n",
    "train_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "test_ids_by_split  = {s: [] for s in SPLIT_LIST}\n",
    "\n",
    "tr_groups = df_train_meta.groupby(\"split\").groups\n",
    "te_groups = df_test_meta.groupby(\"split\").groups\n",
    "\n",
    "for sp, idx in tr_groups.items():\n",
    "    if sp in train_ids_by_split:\n",
    "        train_ids_by_split[sp] = pd.Index(idx).astype(str).map(_norm_id).tolist()\n",
    "\n",
    "for sp, idx in te_groups.items():\n",
    "    if sp in test_ids_by_split:\n",
    "        test_ids_by_split[sp] = pd.Index(idx).astype(str).map(_norm_id).tolist()\n",
    "\n",
    "if sum(len(v) for v in train_ids_by_split.values()) != len(df_train_meta):\n",
    "    raise RuntimeError(\"Routing train_ids_by_split mismatch total vs df_train_meta length.\")\n",
    "if sum(len(v) for v in test_ids_by_split.values()) != len(df_test_meta):\n",
    "    raise RuntimeError(\"Routing test_ids_by_split mismatch total vs df_test_meta length.\")\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    \"split\": SPLIT_LIST,\n",
    "    \"train_objects\": [len(train_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "    \"test_objects\":  [len(test_ids_by_split[s]) for s in SPLIT_LIST],\n",
    "})\n",
    "counts_path = ART_DIR / \"object_counts_by_split.csv\"\n",
    "df_counts.to_csv(counts_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Robust header mapping -> canonical columns (FAST dtype + SAFE fallback)\n",
    "# ----------------------------\n",
    "def _canon_col(x: str) -> str:\n",
    "    s = str(x).strip().lower()\n",
    "    s = s.replace(\"\\ufeff\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    s = s.replace(\"-\", \"_\")\n",
    "    return s\n",
    "\n",
    "def _build_lc_read_cfg(p: Path):\n",
    "    h = pd.read_csv(p, nrows=0, **SAFE_READ_KW)\n",
    "    orig_cols = list(h.columns)\n",
    "\n",
    "    c2o = {}\n",
    "    for c in orig_cols:\n",
    "        k = _canon_col(c)\n",
    "        if k not in c2o:\n",
    "            c2o[k] = c\n",
    "\n",
    "    obj_col = c2o.get(\"object_id\", None)\n",
    "\n",
    "    time_col = None\n",
    "    for k in [\"time_mjd\", \"timemjd\", \"mjd\", \"time\"]:\n",
    "        if k in c2o:\n",
    "            time_col = c2o[k]\n",
    "            break\n",
    "\n",
    "    flux_col = c2o.get(\"flux\", None)\n",
    "\n",
    "    ferr_col = None\n",
    "    for k in [\"flux_err\", \"fluxerr\", \"fluxerror\"]:\n",
    "        if k in c2o:\n",
    "            ferr_col = c2o[k]\n",
    "            break\n",
    "\n",
    "    filt_col = c2o.get(\"filter\", None)\n",
    "\n",
    "    missing = []\n",
    "    if obj_col is None:  missing.append(\"object_id\")\n",
    "    if time_col is None: missing.append(\"Time (MJD)\")\n",
    "    if flux_col is None: missing.append(\"Flux\")\n",
    "    if ferr_col is None: missing.append(\"Flux_err\")\n",
    "    if filt_col is None: missing.append(\"Filter\")\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Missing required lightcurve columns in {p.name}: {missing}. \"\n",
    "            f\"Header sample: {orig_cols[:20]}\"\n",
    "        )\n",
    "\n",
    "    usecols = [obj_col, time_col, flux_col, ferr_col, filt_col]\n",
    "    rename = {obj_col:\"object_id\", time_col:\"mjd\", flux_col:\"flux\", ferr_col:\"flux_err\", filt_col:\"filter\"}\n",
    "\n",
    "    dtype_fast = {\n",
    "        obj_col: \"string\",\n",
    "        filt_col: \"string\",\n",
    "        time_col: \"float32\",\n",
    "        flux_col: \"float32\",\n",
    "        ferr_col: \"float32\",\n",
    "    }\n",
    "    dtype_safe = {obj_col: \"string\", filt_col: \"string\"}\n",
    "\n",
    "    return {\"usecols\": usecols, \"dtype_fast\": dtype_fast, \"dtype_safe\": dtype_safe, \"rename\": rename}\n",
    "\n",
    "def _normalize_lc_chunk(\n",
    "    df: pd.DataFrame,\n",
    "    drop_bad_filter: bool = True,\n",
    "    drop_bad_mjd: bool = True,\n",
    "    drop_bad_fluxerr: bool = True,\n",
    "    encode_filter: bool = False,\n",
    "):\n",
    "    df = df[[\"object_id\",\"mjd\",\"flux\",\"flux_err\",\"filter\"]].copy()\n",
    "\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # v5.4: filter robust (support numeric 0..5)\n",
    "    f = df[\"filter\"].astype(\"string\").str.strip().str.lower()\n",
    "    f = f.replace(FILTER_NUM2STR)  # \"0\"->\"u\", ...\n",
    "    df[\"filter\"] = f\n",
    "\n",
    "    df.loc[~df[\"filter\"].isin(ALLOWED_FILTERS_TUP), \"filter\"] = pd.NA\n",
    "\n",
    "    # numeric coercion (SAFE path)\n",
    "    if df[\"mjd\"].dtype == \"O\" or str(df[\"mjd\"].dtype).startswith(\"string\"):\n",
    "        df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\")\n",
    "    if df[\"flux\"].dtype == \"O\" or str(df[\"flux\"].dtype).startswith(\"string\"):\n",
    "        df[\"flux\"] = pd.to_numeric(df[\"flux\"], errors=\"coerce\")\n",
    "    if df[\"flux_err\"].dtype == \"O\" or str(df[\"flux_err\"].dtype).startswith(\"string\"):\n",
    "        df[\"flux_err\"] = pd.to_numeric(df[\"flux_err\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"mjd\"] = df[\"mjd\"].astype(\"float32\")\n",
    "    df[\"flux\"] = df[\"flux\"].astype(\"float32\")\n",
    "    df[\"flux_err\"] = df[\"flux_err\"].astype(\"float32\")\n",
    "\n",
    "    # Guard flux_err\n",
    "    fe = df[\"flux_err\"]\n",
    "    if drop_bad_fluxerr:\n",
    "        df = df[fe.notna()]\n",
    "        fe = df[\"flux_err\"]\n",
    "        df = df[fe > 0]\n",
    "        fe = df[\"flux_err\"]\n",
    "\n",
    "    if MIN_FLUXERR > 0:\n",
    "        fe = df[\"flux_err\"]\n",
    "        df.loc[fe.notna() & (fe < MIN_FLUXERR), \"flux_err\"] = np.float32(MIN_FLUXERR)\n",
    "\n",
    "    # Drop empty id\n",
    "    df = df[df[\"object_id\"].notna() & (df[\"object_id\"] != \"\")]\n",
    "\n",
    "    if drop_bad_filter:\n",
    "        df = df[df[\"filter\"].notna()]\n",
    "    if drop_bad_mjd:\n",
    "        df = df[df[\"mjd\"].notna()]\n",
    "\n",
    "    df = df[REQ_LC_KEYS]\n",
    "\n",
    "    if encode_filter:\n",
    "        df = df.copy()\n",
    "        df[\"filter_id\"] = df[\"filter\"].map(FILTER2ID).astype(\"int8\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Chunked readers\n",
    "# ----------------------------\n",
    "def iter_lightcurve_chunks(\n",
    "    split_name: str,\n",
    "    which: str,\n",
    "    chunksize: int = None,\n",
    "    drop_bad_filter: bool = True,\n",
    "    drop_bad_mjd: bool = True,\n",
    "    drop_bad_fluxerr: bool = True,\n",
    "    encode_filter: bool = False,\n",
    "):\n",
    "    split_name = str(split_name).strip()\n",
    "    if split_name not in SPLIT_FILES:\n",
    "        raise KeyError(f\"Unknown split_name={split_name}. Known={list(SPLIT_FILES.keys())[:5]}..\")\n",
    "    if which not in (\"train\", \"test\"):\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    if chunksize is None:\n",
    "        chunksize = CHUNK_ROWS_DEFAULT\n",
    "\n",
    "    p = SPLIT_FILES[split_name][which]\n",
    "    key = (split_name, which)\n",
    "    if key not in _LC_CFG_CACHE:\n",
    "        _LC_CFG_CACHE[key] = _build_lc_read_cfg(p)\n",
    "    cfg = _LC_CFG_CACHE[key]\n",
    "\n",
    "    try:\n",
    "        reader = pd.read_csv(\n",
    "            p,\n",
    "            usecols=cfg[\"usecols\"],\n",
    "            dtype=cfg[\"dtype_fast\"],\n",
    "            chunksize=int(chunksize),\n",
    "            engine=\"c\",\n",
    "            memory_map=True,\n",
    "            **SAFE_READ_KW\n",
    "        )\n",
    "        for chunk in reader:\n",
    "            chunk = chunk.rename(columns=cfg[\"rename\"])\n",
    "            yield _normalize_lc_chunk(\n",
    "                chunk,\n",
    "                drop_bad_filter=drop_bad_filter,\n",
    "                drop_bad_mjd=drop_bad_mjd,\n",
    "                drop_bad_fluxerr=drop_bad_fluxerr,\n",
    "                encode_filter=encode_filter,\n",
    "            )\n",
    "    except Exception:\n",
    "        reader = pd.read_csv(\n",
    "            p,\n",
    "            usecols=cfg[\"usecols\"],\n",
    "            dtype=cfg[\"dtype_safe\"],\n",
    "            chunksize=int(chunksize),\n",
    "            engine=\"c\",\n",
    "            memory_map=True,\n",
    "            **SAFE_READ_KW\n",
    "        )\n",
    "        for chunk in reader:\n",
    "            chunk = chunk.rename(columns=cfg[\"rename\"])\n",
    "            yield _normalize_lc_chunk(\n",
    "                chunk,\n",
    "                drop_bad_filter=drop_bad_filter,\n",
    "                drop_bad_mjd=drop_bad_mjd,\n",
    "                drop_bad_fluxerr=drop_bad_fluxerr,\n",
    "                encode_filter=encode_filter,\n",
    "            )\n",
    "\n",
    "def load_object_lightcurve(\n",
    "    object_id: str,\n",
    "    which: str,\n",
    "    chunksize: int = None,\n",
    "    sort_time: bool = True,\n",
    "    max_chunks: int = None,\n",
    "    stop_after_found_block: bool = True,\n",
    "    drop_bad_filter: bool = True,\n",
    "    drop_bad_mjd: bool = True,\n",
    "    drop_bad_fluxerr: bool = True,\n",
    "):\n",
    "    object_id = _norm_id(object_id)\n",
    "\n",
    "    if which == \"train\":\n",
    "        if object_id not in df_train_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_train_meta: {object_id}\")\n",
    "        split_name = str(df_train_meta.loc[object_id, \"split\"]).strip()\n",
    "    elif which == \"test\":\n",
    "        if object_id not in df_test_meta.index:\n",
    "            raise KeyError(f\"object_id not found in df_test_meta: {object_id}\")\n",
    "        split_name = str(df_test_meta.loc[object_id, \"split\"]).strip()\n",
    "    else:\n",
    "        raise ValueError(\"which must be 'train' or 'test'\")\n",
    "\n",
    "    if split_name not in SPLIT_FILES:\n",
    "        raise KeyError(f\"Routing split not found in SPLIT_FILES: split={split_name} object_id={object_id}\")\n",
    "\n",
    "    if chunksize is None:\n",
    "        chunksize = CHUNK_ROWS_DEFAULT\n",
    "\n",
    "    pieces = []\n",
    "    seen = 0\n",
    "    found_any = False\n",
    "    last_hit = False\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(\n",
    "        split_name, which, chunksize=chunksize,\n",
    "        drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd, drop_bad_fluxerr=drop_bad_fluxerr,\n",
    "        encode_filter=False\n",
    "    ):\n",
    "        seen += 1\n",
    "        sub = ch[ch[\"object_id\"] == object_id]\n",
    "        hit = (len(sub) > 0)\n",
    "        if hit:\n",
    "            pieces.append(sub)\n",
    "            found_any = True\n",
    "\n",
    "        if stop_after_found_block and found_any and last_hit and (not hit):\n",
    "            break\n",
    "        last_hit = hit\n",
    "\n",
    "        if max_chunks is not None and seen >= int(max_chunks):\n",
    "            break\n",
    "\n",
    "    if not pieces:\n",
    "        out = pd.DataFrame(columns=REQ_LC_KEYS)\n",
    "    else:\n",
    "        out = pd.concat(pieces, ignore_index=True)\n",
    "        if sort_time and len(out) > 1:\n",
    "            out[\"filter_ord\"] = out[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n",
    "            out = (\n",
    "                out.sort_values([\"mjd\", \"filter_ord\"], kind=\"mergesort\")\n",
    "                   .drop(columns=[\"filter_ord\"])\n",
    "                   .reset_index(drop=True)\n",
    "            )\n",
    "    return out\n",
    "\n",
    "def load_many_object_lightcurves(\n",
    "    split_name: str,\n",
    "    which: str,\n",
    "    object_ids,\n",
    "    chunksize: int = None,\n",
    "    max_chunks: int = None,\n",
    "    sort_time: bool = True,\n",
    "    drop_bad_filter: bool = True,\n",
    "    drop_bad_mjd: bool = True,\n",
    "    drop_bad_fluxerr: bool = True,\n",
    "):\n",
    "    split_name = str(split_name).strip()\n",
    "    if split_name not in SPLIT_FILES:\n",
    "        raise KeyError(f\"Unknown split_name={split_name}\")\n",
    "    if which not in (\"train\",\"test\"):\n",
    "        raise ValueError(\"which must be train/test\")\n",
    "\n",
    "    if chunksize is None:\n",
    "        chunksize = CHUNK_ROWS_DEFAULT\n",
    "\n",
    "    oid_set = set([_norm_id(x) for x in object_ids if _norm_id(x) != \"\"])\n",
    "    if len(oid_set) == 0:\n",
    "        return {}\n",
    "\n",
    "    out = {oid: [] for oid in oid_set}\n",
    "\n",
    "    seen = 0\n",
    "    for ch in iter_lightcurve_chunks(\n",
    "        split_name, which, chunksize=chunksize,\n",
    "        drop_bad_filter=drop_bad_filter, drop_bad_mjd=drop_bad_mjd, drop_bad_fluxerr=drop_bad_fluxerr,\n",
    "        encode_filter=False\n",
    "    ):\n",
    "        seen += 1\n",
    "        m = ch[\"object_id\"].isin(oid_set)\n",
    "        if m.any():\n",
    "            sub = ch.loc[m]\n",
    "            for oid, g in sub.groupby(\"object_id\", sort=False):\n",
    "                out[_norm_id(oid)].append(g)\n",
    "\n",
    "        if max_chunks is not None and seen >= int(max_chunks):\n",
    "            break\n",
    "\n",
    "    final = {}\n",
    "    for oid, parts in out.items():\n",
    "        if not parts:\n",
    "            continue\n",
    "        df = pd.concat(parts, ignore_index=True)\n",
    "        if sort_time and len(df) > 1:\n",
    "            df[\"filter_ord\"] = df[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n",
    "            df = (\n",
    "                df.sort_values([\"mjd\",\"filter_ord\"], kind=\"mergesort\")\n",
    "                  .drop(columns=[\"filter_ord\"])\n",
    "                  .reset_index(drop=True)\n",
    "            )\n",
    "        final[oid] = df\n",
    "\n",
    "    return final\n",
    "\n",
    "# ----------------------------\n",
    "# 5) NPZ cache utilities\n",
    "# ----------------------------\n",
    "def get_lc_cache_path(object_id: str, which: str) -> Path:\n",
    "    object_id = _norm_id(object_id)\n",
    "    which = str(which).strip()\n",
    "    return LC_CACHE_DIR / f\"{which}__{object_id}.npz\"\n",
    "\n",
    "def _mem_cache_put(key, value):\n",
    "    _LC_OBJ_CACHE[key] = value\n",
    "    if len(_LC_OBJ_CACHE) > MAX_MEM_CACHE:\n",
    "        k0 = next(iter(_LC_OBJ_CACHE.keys()))\n",
    "        _LC_OBJ_CACHE.pop(k0, None)\n",
    "\n",
    "def load_object_lightcurve_cached(\n",
    "    object_id: str,\n",
    "    which: str,\n",
    "    chunksize: int = None,\n",
    "    sort_time: bool = True,\n",
    "    max_chunks: int = None,\n",
    "    stop_after_found_block: bool = True,\n",
    "    use_npz_cache: bool = True,\n",
    "    refresh_cache: bool = False,\n",
    "):\n",
    "    object_id = _norm_id(object_id)\n",
    "    which = str(which).strip()\n",
    "\n",
    "    mem_key = (which, object_id)\n",
    "    if mem_key in _LC_OBJ_CACHE and (not refresh_cache):\n",
    "        return _LC_OBJ_CACHE[mem_key].copy()\n",
    "\n",
    "    npz_path = get_lc_cache_path(object_id, which)\n",
    "    if use_npz_cache and npz_path.exists() and (not refresh_cache):\n",
    "        z = np.load(npz_path, allow_pickle=False)\n",
    "        mjd = z[\"mjd\"].astype(\"float32\")\n",
    "        flux = z[\"flux\"].astype(\"float32\")\n",
    "        ferr = z[\"flux_err\"].astype(\"float32\")\n",
    "        filt_id = z[\"filter_id\"].astype(\"int8\")\n",
    "        filt = np.array([ID2FILTER[int(i)] for i in filt_id], dtype=object)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"object_id\": object_id,\n",
    "            \"mjd\": mjd,\n",
    "            \"flux\": flux,\n",
    "            \"flux_err\": ferr,\n",
    "            \"filter\": filt,\n",
    "        })\n",
    "        if sort_time and len(df) > 1:\n",
    "            df[\"filter_ord\"] = df[\"filter\"].map(FILTER_ORDER).astype(\"int16\")\n",
    "            df = df.sort_values([\"mjd\",\"filter_ord\"], kind=\"mergesort\").drop(columns=[\"filter_ord\"]).reset_index(drop=True)\n",
    "        _mem_cache_put(mem_key, df)\n",
    "        return df.copy()\n",
    "\n",
    "    df = load_object_lightcurve(\n",
    "        object_id, which,\n",
    "        chunksize=chunksize,\n",
    "        sort_time=sort_time,\n",
    "        max_chunks=max_chunks,\n",
    "        stop_after_found_block=stop_after_found_block,\n",
    "        drop_bad_filter=True, drop_bad_mjd=True, drop_bad_fluxerr=True\n",
    "    )\n",
    "\n",
    "    if use_npz_cache:\n",
    "        try:\n",
    "            if len(df) > 0:\n",
    "                filt_id = df[\"filter\"].map(FILTER2ID).astype(\"int8\").to_numpy()\n",
    "                np.savez_compressed(\n",
    "                    npz_path,\n",
    "                    mjd=df[\"mjd\"].to_numpy(dtype=\"float32\"),\n",
    "                    flux=df[\"flux\"].to_numpy(dtype=\"float32\"),\n",
    "                    flux_err=df[\"flux_err\"].to_numpy(dtype=\"float32\"),\n",
    "                    filter_id=filt_id\n",
    "                )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    _mem_cache_put(mem_key, df)\n",
    "    return df.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 6) FULL Object-Quality builder (streaming, vectorized)\n",
    "# ----------------------------\n",
    "def _build_object_quality(which: str, out_path: Path, refresh: bool = False, chunksize: int = 400_000):\n",
    "    which = str(which).strip()\n",
    "    if which not in (\"train\",\"test\"):\n",
    "        raise ValueError(\"which must be train/test\")\n",
    "\n",
    "    if out_path.exists() and (not refresh):\n",
    "        df = pd.read_csv(out_path, dtype={\"object_id\":\"string\"}, **SAFE_READ_KW)\n",
    "        if \"object_id\" not in df.columns:\n",
    "            raise RuntimeError(f\"Bad objq file (missing object_id): {out_path}\")\n",
    "        df[\"object_id\"] = df[\"object_id\"].astype(\"string\").map(_norm_id)\n",
    "        return df\n",
    "\n",
    "    idx = df_train_meta.index if which == \"train\" else df_test_meta.index\n",
    "    idx = pd.Index([_norm_id(z) for z in idx.astype(str).tolist()])\n",
    "    n_obj = int(len(idx))\n",
    "\n",
    "    n_obs = np.zeros(n_obj, dtype=np.int32)\n",
    "    mjd_min = np.full(n_obj, np.float32(np.inf), dtype=np.float32)\n",
    "    mjd_max = np.full(n_obj, np.float32(-np.inf), dtype=np.float32)\n",
    "\n",
    "    sum_flux = np.zeros(n_obj, dtype=np.float64)\n",
    "    sum_flux2 = np.zeros(n_obj, dtype=np.float64)\n",
    "    sum_abs_flux = np.zeros(n_obj, dtype=np.float64)\n",
    "    neg_cnt = np.zeros(n_obj, dtype=np.int32)\n",
    "\n",
    "    snr_abs_max = np.full(n_obj, np.float32(0.0), dtype=np.float32)\n",
    "    sum_snr_abs = np.zeros(n_obj, dtype=np.float64)\n",
    "    det_cnt = np.zeros(n_obj, dtype=np.int32)\n",
    "    strong_cnt = np.zeros(n_obj, dtype=np.int32)\n",
    "\n",
    "    band_cnt = np.zeros((6, n_obj), dtype=np.int32)\n",
    "\n",
    "    t0 = time.time()\n",
    "    rows_seen = 0\n",
    "\n",
    "    for sp in SPLIT_LIST:\n",
    "        for ch in iter_lightcurve_chunks(\n",
    "            sp, which,\n",
    "            chunksize=chunksize,\n",
    "            drop_bad_filter=True, drop_bad_mjd=True, drop_bad_fluxerr=True,\n",
    "            encode_filter=True\n",
    "        ):\n",
    "            if ch is None or len(ch) == 0:\n",
    "                continue\n",
    "\n",
    "            oids = ch[\"object_id\"].to_numpy(dtype=object, copy=False)\n",
    "            oid_idx = idx.get_indexer(oids)  # -1 if not found\n",
    "            m = oid_idx >= 0\n",
    "            if not np.any(m):\n",
    "                continue\n",
    "\n",
    "            oid_idx = oid_idx[m].astype(np.int64, copy=False)\n",
    "\n",
    "            mjd = ch[\"mjd\"].to_numpy(dtype=np.float32, copy=False)[m]\n",
    "            flux = ch[\"flux\"].to_numpy(dtype=np.float32, copy=False)[m]\n",
    "            ferr = ch[\"flux_err\"].to_numpy(dtype=np.float32, copy=False)[m]\n",
    "            fid = ch[\"filter_id\"].to_numpy(dtype=np.int8, copy=False)[m]\n",
    "\n",
    "            if MIN_FLUXERR > 0:\n",
    "                ferr = np.maximum(ferr, np.float32(MIN_FLUXERR))\n",
    "\n",
    "            rows_seen += int(len(oid_idx))\n",
    "\n",
    "            bc = np.bincount(oid_idx, minlength=n_obj)\n",
    "            n_obs += bc.astype(np.int32)\n",
    "\n",
    "            fx64 = flux.astype(np.float64)\n",
    "            sum_flux += np.bincount(oid_idx, weights=fx64, minlength=n_obj)\n",
    "            sum_flux2 += np.bincount(oid_idx, weights=(fx64 * fx64), minlength=n_obj)\n",
    "            sum_abs_flux += np.bincount(oid_idx, weights=np.abs(fx64), minlength=n_obj)\n",
    "            neg_cnt += np.bincount(oid_idx, weights=(flux < 0).astype(np.int32), minlength=n_obj).astype(np.int32)\n",
    "\n",
    "            snr = (flux / ferr).astype(np.float32)\n",
    "            snr = np.clip(snr, -np.float32(SNR_CLIP), np.float32(SNR_CLIP))\n",
    "            snr_abs = np.abs(snr).astype(np.float32)\n",
    "\n",
    "            sum_snr_abs += np.bincount(oid_idx, weights=snr_abs.astype(np.float64), minlength=n_obj)\n",
    "            det_cnt += np.bincount(oid_idx, weights=(snr_abs >= np.float32(SNR_DET_THR)).astype(np.int32), minlength=n_obj).astype(np.int32)\n",
    "            strong_cnt += np.bincount(oid_idx, weights=(snr_abs >= np.float32(SNR_STRONG_THR)).astype(np.int32), minlength=n_obj).astype(np.int32)\n",
    "\n",
    "            for b in range(6):\n",
    "                mb = (fid == b)\n",
    "                if np.any(mb):\n",
    "                    band_cnt[b] += np.bincount(oid_idx[mb], minlength=n_obj).astype(np.int32)\n",
    "\n",
    "            order = np.argsort(oid_idx, kind=\"mergesort\")\n",
    "            idx_s = oid_idx[order]\n",
    "            mjd_s = mjd[order]\n",
    "            snr_s = snr_abs[order]\n",
    "\n",
    "            starts = np.r_[0, 1 + np.where(idx_s[1:] != idx_s[:-1])[0]]\n",
    "            uniq = idx_s[starts]\n",
    "\n",
    "            mn = np.minimum.reduceat(mjd_s, starts).astype(np.float32)\n",
    "            mx = np.maximum.reduceat(mjd_s, starts).astype(np.float32)\n",
    "            sx = np.maximum.reduceat(snr_s, starts).astype(np.float32)\n",
    "\n",
    "            mjd_min[uniq] = np.minimum(mjd_min[uniq], mn)\n",
    "            mjd_max[uniq] = np.maximum(mjd_max[uniq], mx)\n",
    "            snr_abs_max[uniq] = np.maximum(snr_abs_max[uniq], sx)\n",
    "\n",
    "    n_obs_f = np.maximum(n_obs.astype(np.float32), np.float32(1.0))\n",
    "    timespan = np.where(\n",
    "        np.isfinite(mjd_min) & np.isfinite(mjd_max),\n",
    "        (mjd_max - mjd_min).astype(np.float32),\n",
    "        np.float32(0.0)\n",
    "    )\n",
    "    cadence_proxy = (timespan / np.maximum(n_obs - 1, 1).astype(np.float32)).astype(np.float32)\n",
    "\n",
    "    flux_mean = (sum_flux / n_obs_f.astype(np.float64)).astype(np.float32)\n",
    "    flux_var = (sum_flux2 / n_obs_f.astype(np.float64) - (sum_flux / n_obs_f.astype(np.float64))**2)\n",
    "    flux_var = np.maximum(flux_var, 0.0)\n",
    "    flux_std = np.sqrt(flux_var).astype(np.float32)\n",
    "\n",
    "    abs_flux_mean = (sum_abs_flux / n_obs_f.astype(np.float64)).astype(np.float32)\n",
    "    neg_flux_frac = (neg_cnt.astype(np.float32) / n_obs_f).astype(np.float32)\n",
    "\n",
    "    snr_abs_mean = (sum_snr_abs / n_obs_f.astype(np.float64)).astype(np.float32)\n",
    "    snr_det_frac = (det_cnt.astype(np.float32) / n_obs_f).astype(np.float32)\n",
    "    snr_strong_frac = (strong_cnt.astype(np.float32) / n_obs_f).astype(np.float32)\n",
    "\n",
    "    n_bands_present = (band_cnt > 0).sum(axis=0).astype(np.int8)\n",
    "    frac_bands = (band_cnt.astype(np.float32) / n_obs_f[None, :]).astype(np.float32)\n",
    "\n",
    "    df_out = pd.DataFrame({\n",
    "        \"object_id\": pd.Series(idx, dtype=\"string\"),\n",
    "        \"n_obs_total\": n_obs.astype(np.int32),\n",
    "        \"n_bands_present\": n_bands_present,\n",
    "        \"timespan\": timespan,\n",
    "        \"cadence_proxy\": cadence_proxy,\n",
    "        \"neg_flux_frac\": neg_flux_frac,\n",
    "        \"flux_mean\": flux_mean,\n",
    "        \"flux_std\": flux_std,\n",
    "        \"abs_flux_mean\": abs_flux_mean,\n",
    "        \"snr_abs_mean\": snr_abs_mean,\n",
    "        \"snr_abs_max\": snr_abs_max.astype(np.float32),\n",
    "        \"snr_det_frac\": snr_det_frac,\n",
    "        \"snr_strong_frac\": snr_strong_frac,\n",
    "    })\n",
    "\n",
    "    for b, name in enumerate([\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]):\n",
    "        df_out[f\"n_{name}\"] = band_cnt[b].astype(np.int32)\n",
    "        df_out[f\"frac_{name}\"] = frac_bands[b].astype(np.float32)\n",
    "\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "    if OBJQ_SAVE_PARQUET:\n",
    "        try:\n",
    "            df_out.to_parquet(out_path.with_suffix(\".parquet\"), index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"[OBJQ] Built {which} object_quality: {out_path} | objects={len(df_out):,} | rows_seen={rows_seen:,} | {elapsed/60:.2f} min\")\n",
    "    return df_out\n",
    "\n",
    "# ----------------------------\n",
    "# 7) SAFE overwrite-join object-quality -> meta (v5.4 sanitize hard)\n",
    "# ----------------------------\n",
    "OBJECT_QUALITY_TRAIN_PATH = LOG_DIR / \"object_quality_train.csv\"\n",
    "OBJECT_QUALITY_TEST_PATH  = LOG_DIR / \"object_quality_test.csv\"\n",
    "\n",
    "META_PROTECT_COLS = set([\n",
    "    \"split\", \"split_id\", \"fold\",\n",
    "    \"target\", \"y\", \"label\", \"class\",\n",
    "    \"EBV\", \"EBV_clip\", \"EBV_used\", \"EBV_missing\", \"log1pEBV\",\n",
    "    \"Z\", \"Z_clip\", \"Z_err\", \"Z_missing\", \"Z_err_missing\", \"log1pZ\", \"log1pZerr\",\n",
    "    \"is_photoz\", \"photoz\", \"redshift\",\n",
    "    \"prior\", \"pos_prior\", \"neg_prior\"\n",
    "])\n",
    "\n",
    "def _drop_unnamed_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    bad = [c for c in df.columns if str(c).startswith(\"Unnamed\")]\n",
    "    return df.drop(columns=bad, errors=\"ignore\") if bad else df\n",
    "\n",
    "def _is_objq_valid_col(c: str) -> bool:\n",
    "    c = str(c)\n",
    "    base = {\n",
    "        \"n_obs_total\",\"n_bands_present\",\"timespan\",\"cadence_proxy\",\n",
    "        \"neg_flux_frac\",\"abs_flux_mean\",\n",
    "        \"flux_mean\",\"flux_std\",\n",
    "        \"snr_abs_mean\",\"snr_abs_max\",\"snr_det_frac\",\"snr_strong_frac\",\n",
    "        \"low_obs\",\"low_bandcov\",\n",
    "    }\n",
    "    if c in base:\n",
    "        return True\n",
    "    if c.startswith(\"n_\"):     # n_u..n_y\n",
    "        return True\n",
    "    if c.startswith(\"frac_\"):  # frac_u..frac_y\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _looks_like_meta_col(c: str) -> bool:\n",
    "    s = str(c).lower()\n",
    "    if s in {x.lower() for x in META_PROTECT_COLS}:\n",
    "        return True\n",
    "    meta_sub = [\n",
    "        \"split\", \"target\", \"label\", \"class\", \"fold\",\n",
    "        \"ebv\", \"redshift\", \"photoz\", \"z_err\", \"zerr\",\n",
    "        \"log1p\", \"_clip\", \"_missing\", \"prior\"\n",
    "    ]\n",
    "    return any(k in s for k in meta_sub)\n",
    "\n",
    "def _clean_objq_df(df: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "    df = _drop_unnamed_cols(df).copy()\n",
    "\n",
    "    if \"object_id\" not in df.columns:\n",
    "        raise RuntimeError(f\"[OBJQ][{tag}] missing object_id col\")\n",
    "\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").map(_norm_id)\n",
    "\n",
    "    drop_metaish = [c for c in df.columns if c != \"object_id\" and _looks_like_meta_col(c)]\n",
    "    if drop_metaish:\n",
    "        print(f\"[OBJQ][{tag}] drop META-like cols from objq: {drop_metaish[:12]}{'...' if len(drop_metaish)>12 else ''}\")\n",
    "        df = df.drop(columns=drop_metaish, errors=\"ignore\")\n",
    "\n",
    "    keep = [\"object_id\"] + [c for c in df.columns if c != \"object_id\" and _is_objq_valid_col(c)]\n",
    "    dropped_other = [c for c in df.columns if c not in keep]\n",
    "    if dropped_other:\n",
    "        print(f\"[OBJQ][{tag}] drop non-objq cols: {dropped_other[:12]}{'...' if len(dropped_other)>12 else ''}\")\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c == \"object_id\":\n",
    "            continue\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def _dedup_objq(df: pd.DataFrame, tag: str):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"object_id\"] = df[\"object_id\"].astype(\"string\").map(_norm_id)\n",
    "\n",
    "    if not df[\"object_id\"].duplicated().any():\n",
    "        return df\n",
    "\n",
    "    print(f\"[WARN][OBJQ][{tag}] duplicated object_id detected -> aggregating\")\n",
    "    num_cols = [c for c in df.columns if c != \"object_id\"]\n",
    "    count_like = set([c for c in num_cols if c.startswith(\"n_\")] + [\"n_obs_total\",\"n_bands_present\",\"low_obs\",\"low_bandcov\"])\n",
    "\n",
    "    agg = {}\n",
    "    for c in num_cols:\n",
    "        agg[c] = \"max\" if c in count_like else \"mean\"\n",
    "\n",
    "    return df.groupby(\"object_id\", as_index=False).agg(agg)\n",
    "\n",
    "def _add_objq_flags(objq_df: pd.DataFrame):\n",
    "    oq = objq_df.copy()\n",
    "    if \"n_obs_total\" in oq.columns:\n",
    "        oq[\"low_obs\"] = (pd.to_numeric(oq[\"n_obs_total\"], errors=\"coerce\").fillna(0).astype(int) < 20).astype(\"int8\")\n",
    "    if \"n_bands_present\" in oq.columns:\n",
    "        oq[\"low_bandcov\"] = (pd.to_numeric(oq[\"n_bands_present\"], errors=\"coerce\").fillna(0).astype(int) <= 2).astype(\"int8\")\n",
    "    return oq\n",
    "\n",
    "def _safe_join_objq_overwrite(df_meta: pd.DataFrame, objq_df: pd.DataFrame, tag=\"train\"):\n",
    "    df_meta = df_meta.copy(deep=False)\n",
    "    df_meta.index = pd.Index([_norm_id(z) for z in df_meta.index.tolist()], name=df_meta.index.name)\n",
    "\n",
    "    oq = objq_df.copy(deep=False)\n",
    "    if \"object_id\" in oq.columns:\n",
    "        oq[\"object_id\"] = oq[\"object_id\"].astype(\"string\").map(_norm_id)\n",
    "        oq = oq.set_index(\"object_id\", drop=True)\n",
    "    else:\n",
    "        oq.index = pd.Index([_norm_id(z) for z in oq.index.tolist()], name=oq.index.name)\n",
    "\n",
    "    overlap = df_meta.columns.intersection(oq.columns)\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"[OBJQ][{tag}] overlap detected -> overwrite from objq: {list(overlap[:12])}{'...' if len(overlap)>12 else ''}\")\n",
    "        df_meta = df_meta.drop(columns=list(overlap), errors=\"ignore\")\n",
    "\n",
    "    out = df_meta.join(oq, how=\"left\")\n",
    "    added_cols = [c for c in oq.columns if c in out.columns]\n",
    "    return out, added_cols\n",
    "\n",
    "objq_train_df = None\n",
    "objq_test_df  = None\n",
    "added_cols_train = []\n",
    "added_cols_test  = []\n",
    "\n",
    "if BUILD_OBJECT_QUALITY:\n",
    "    do_train = True\n",
    "    do_test = True\n",
    "    if OBJQ_ONLY_IF_MISSING and (OBJECT_QUALITY_TRAIN_PATH.exists() and OBJECT_QUALITY_TEST_PATH.exists()) and (not OBJQ_REFRESH):\n",
    "        do_train = do_test = False\n",
    "\n",
    "    if do_train:\n",
    "        objq_train_df = _build_object_quality(\"train\", OBJECT_QUALITY_TRAIN_PATH, refresh=OBJQ_REFRESH, chunksize=OBJQ_CHUNK_ROWS)\n",
    "    else:\n",
    "        objq_train_df = pd.read_csv(OBJECT_QUALITY_TRAIN_PATH, dtype={\"object_id\":\"string\"}, **SAFE_READ_KW)\n",
    "\n",
    "    if do_test:\n",
    "        objq_test_df = _build_object_quality(\"test\", OBJECT_QUALITY_TEST_PATH, refresh=OBJQ_REFRESH, chunksize=OBJQ_CHUNK_ROWS)\n",
    "    else:\n",
    "        objq_test_df = pd.read_csv(OBJECT_QUALITY_TEST_PATH, dtype={\"object_id\":\"string\"}, **SAFE_READ_KW)\n",
    "\n",
    "    objq_train_df = _clean_objq_df(objq_train_df, \"train\")\n",
    "    objq_test_df  = _clean_objq_df(objq_test_df,  \"test\")\n",
    "\n",
    "    objq_train_df = _dedup_objq(objq_train_df, \"train\")\n",
    "    objq_test_df  = _dedup_objq(objq_test_df,  \"test\")\n",
    "\n",
    "    objq_train_df = _add_objq_flags(objq_train_df)\n",
    "    objq_test_df  = _add_objq_flags(objq_test_df)\n",
    "\n",
    "    df_train_meta, added_cols_train = _safe_join_objq_overwrite(df_train_meta, objq_train_df, tag=\"train\")\n",
    "    df_test_meta,  added_cols_test  = _safe_join_objq_overwrite(df_test_meta,  objq_test_df,  tag=\"test\")\n",
    "\n",
    "    for dfm, added in [(df_train_meta, added_cols_train), (df_test_meta, added_cols_test)]:\n",
    "        for c in added:\n",
    "            if c in dfm.columns:\n",
    "                dfm[c] = pd.to_numeric(dfm[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    try:\n",
    "        df_train_meta.to_parquet(ART_DIR / \"train_meta.parquet\", index=True)\n",
    "        df_test_meta.to_parquet(ART_DIR / \"test_meta.parquet\", index=True)\n",
    "        meta_saved = \"parquet\"\n",
    "    except Exception:\n",
    "        df_train_meta.to_csv(ART_DIR / \"train_meta.csv\", index=True)\n",
    "        df_test_meta.to_csv(ART_DIR / \"test_meta.csv\", index=True)\n",
    "        meta_saved = \"csv\"\n",
    "\n",
    "    with open(ART_DIR / \"stage3_objq_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"stage\": \"stage3\",\n",
    "            \"version\": \"v5.4\",\n",
    "            \"BUILD_OBJECT_QUALITY\": bool(BUILD_OBJECT_QUALITY),\n",
    "            \"OBJQ_REFRESH\": bool(OBJQ_REFRESH),\n",
    "            \"OBJQ_CHUNK_ROWS\": int(OBJQ_CHUNK_ROWS),\n",
    "            \"object_quality_train\": str(OBJECT_QUALITY_TRAIN_PATH),\n",
    "            \"object_quality_test\": str(OBJECT_QUALITY_TEST_PATH),\n",
    "            \"meta_overwrite_format\": meta_saved,\n",
    "            \"added_objq_cols_train\": sorted(list(set(added_cols_train))),\n",
    "            \"added_objq_cols_test\": sorted(list(set(added_cols_test))),\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"[OBJQ] Joined into meta (SAFE overwrite v5.4) + saved updated meta ({meta_saved}) in {ART_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Smoke test (schema + find a few objects quickly) — v5.4 safe StopIteration\n",
    "# ----------------------------\n",
    "rng = np.random.default_rng(SEED)\n",
    "candidate_splits = []\n",
    "for s in SPLIT_LIST:\n",
    "    if len(train_ids_by_split.get(s, [])) > 0 and len(test_ids_by_split.get(s, [])) > 0:\n",
    "        candidate_splits.append(s)\n",
    "    if len(candidate_splits) >= 2:\n",
    "        break\n",
    "if len(candidate_splits) == 0:\n",
    "    raise RuntimeError(\"Tidak ada split yang punya train & test objects (unexpected). Cek STAGE 2.\")\n",
    "\n",
    "SMOKE_CHUNK = int(CFG.get(\"SMOKE_CHUNK_ROWS\", 80_000))\n",
    "SMOKE_MAX_CHUNKS = int(CFG.get(\"SMOKE_MAX_CHUNKS\", 6))\n",
    "SMOKE_N_IDS = int(CFG.get(\"SMOKE_N_IDS_PER_SPLIT\", 2))\n",
    "\n",
    "for s in candidate_splits:\n",
    "    try:\n",
    "        ch_tr = next(iter_lightcurve_chunks(s, \"train\", chunksize=SMOKE_CHUNK, encode_filter=False))\n",
    "        ch_te = next(iter_lightcurve_chunks(s, \"test\",  chunksize=SMOKE_CHUNK, encode_filter=False))\n",
    "    except StopIteration:\n",
    "        print(f\"[WARN][SMOKE] split={s} StopIteration (file empty / all rows filtered). Skip.\")\n",
    "        continue\n",
    "\n",
    "    if list(ch_tr.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Train chunk schema mismatch in {s}: {list(ch_tr.columns)}\")\n",
    "    if list(ch_te.columns) != REQ_LC_KEYS:\n",
    "        raise RuntimeError(f\"Test chunk schema mismatch in {s}: {list(ch_te.columns)}\")\n",
    "\n",
    "    tr_ids = train_ids_by_split[s]\n",
    "    te_ids = test_ids_by_split[s]\n",
    "    pick_tr = [tr_ids[i] for i in rng.integers(0, len(tr_ids), size=min(SMOKE_N_IDS, len(tr_ids)))]\n",
    "    pick_te = [te_ids[i] for i in rng.integers(0, len(te_ids), size=min(SMOKE_N_IDS, len(te_ids)))]\n",
    "\n",
    "    got_tr = load_many_object_lightcurves(s, \"train\", pick_tr, chunksize=SMOKE_CHUNK, max_chunks=SMOKE_MAX_CHUNKS, sort_time=False)\n",
    "    got_te = load_many_object_lightcurves(s, \"test\",  pick_te, chunksize=SMOKE_CHUNK, max_chunks=SMOKE_MAX_CHUNKS, sort_time=False)\n",
    "\n",
    "    miss_tr = [x for x in pick_tr if _norm_id(x) not in got_tr]\n",
    "    miss_te = [x for x in pick_te if _norm_id(x) not in got_te]\n",
    "\n",
    "    if miss_tr:\n",
    "        print(f\"[WARN][SMOKE] split={s} train not found within first {SMOKE_MAX_CHUNKS} chunks: {miss_tr[:2]}\")\n",
    "    if miss_te:\n",
    "        print(f\"[WARN][SMOKE] split={s} test not found within first {SMOKE_MAX_CHUNKS} chunks: {miss_te[:2]}\")\n",
    "\n",
    "print(\"STAGE 3 OK — LIGHTCURVE UTILITIES READY (+OBJQ full if enabled)\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved counts  : {counts_path}\")\n",
    "print(f\"- LC_CACHE_DIR  : {LC_CACHE_DIR}\")\n",
    "print(f\"- OBJQ train    : {OBJECT_QUALITY_TRAIN_PATH} (exists={OBJECT_QUALITY_TRAIN_PATH.exists()})\")\n",
    "print(f\"- OBJQ test     : {OBJECT_QUALITY_TEST_PATH} (exists={OBJECT_QUALITY_TEST_PATH.exists()})\")\n",
    "print(f\"- Smoke splits  : {candidate_splits}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Export globals\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"SPLIT_FILES\": SPLIT_FILES,\n",
    "    \"train_ids_by_split\": train_ids_by_split,\n",
    "    \"test_ids_by_split\": test_ids_by_split,\n",
    "    \"iter_lightcurve_chunks\": iter_lightcurve_chunks,\n",
    "    \"load_object_lightcurve\": load_object_lightcurve,\n",
    "    \"load_many_object_lightcurves\": load_many_object_lightcurves,\n",
    "    \"load_object_lightcurve_cached\": load_object_lightcurve_cached,\n",
    "    \"get_lc_cache_path\": get_lc_cache_path,\n",
    "    \"LC_CACHE_DIR\": LC_CACHE_DIR,\n",
    "    \"REQ_LC_KEYS\": REQ_LC_KEYS,\n",
    "    \"ALLOWED_FILTERS\": ALLOWED_FILTERS,\n",
    "    \"FILTER_ORDER\": FILTER_ORDER,\n",
    "    \"FILTER2ID\": FILTER2ID,\n",
    "    \"ID2FILTER\": ID2FILTER,\n",
    "    \"OBJECT_QUALITY_TRAIN_PATH\": str(OBJECT_QUALITY_TRAIN_PATH),\n",
    "    \"OBJECT_QUALITY_TEST_PATH\": str(OBJECT_QUALITY_TEST_PATH),\n",
    "    \"df_train_meta\": df_train_meta,\n",
    "    \"df_test_meta\": df_test_meta,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020d98e",
   "metadata": {
    "papermill": {
     "duration": 0.015322,
     "end_time": "2026-01-07T16:45:43.186018",
     "exception": false,
     "start_time": "2026-01-07T16:45:43.170696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Photometric Cleaning (De-extinction + Negative Flux Safe Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb2f52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:45:43.219267Z",
     "iopub.status.busy": "2026-01-07T16:45:43.218906Z",
     "iopub.status.idle": "2026-01-07T16:45:55.021634Z",
     "shell.execute_reply": "2026-01-07T16:45:55.020622Z"
    },
    "papermill": {
     "duration": 11.822382,
     "end_time": "2026-01-07T16:45:55.024054",
     "exception": false,
     "start_time": "2026-01-07T16:45:43.201672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4] REBUILD_MODE=wipe_all | Writing to: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/lc_clean_mag\n",
      "[Stage 4] WRITE_FORMAT=parquet | CHUNKSIZE=350,000 | DROP_NAN_FLUX_ROWS=False | STRICT_FILTER=False\n",
      "[Stage 4] split_01/train: parts=1 | rows=26,324 | det_pos%=19.34% | bad_filter_drop=0 | nan_flux=11 | drop_nan_flux=0 | drop_time=0 | mag=[19.72,25.96] | asinh_mag=[19.72,28.74] | time=0.2s\n",
      "[Stage 4] split_01/test: parts=1 | rows=59,235 | det_pos%=23.02% | bad_filter_drop=0 | nan_flux=23 | drop_nan_flux=0 | drop_time=0 | mag=[19.61,26.20] | asinh_mag=[19.61,30.19] | time=0.4s\n",
      "[Stage 4] split_02/train: parts=1 | rows=25,609 | det_pos%=24.45% | bad_filter_drop=0 | nan_flux=6 | drop_nan_flux=0 | drop_time=0 | mag=[20.11,26.03] | asinh_mag=[20.10,28.39] | time=0.2s\n",
      "[Stage 4] split_02/test: parts=1 | rows=71,229 | det_pos%=21.69% | bad_filter_drop=0 | nan_flux=8 | drop_nan_flux=0 | drop_time=0 | mag=[18.77,26.32] | asinh_mag=[18.77,30.59] | time=0.5s\n",
      "[Stage 4] split_03/train: parts=1 | rows=21,676 | det_pos%=21.65% | bad_filter_drop=0 | nan_flux=5 | drop_nan_flux=0 | drop_time=0 | mag=[20.17,26.23] | asinh_mag=[20.17,28.07] | time=0.2s\n",
      "[Stage 4] split_03/test: parts=1 | rows=53,751 | det_pos%=21.90% | bad_filter_drop=0 | nan_flux=8 | drop_nan_flux=0 | drop_time=0 | mag=[19.61,26.37] | asinh_mag=[19.61,28.87] | time=0.4s\n",
      "[Stage 4] split_04/train: parts=1 | rows=22,898 | det_pos%=21.11% | bad_filter_drop=0 | nan_flux=12 | drop_nan_flux=0 | drop_time=0 | mag=[20.38,26.16] | asinh_mag=[20.38,28.67] | time=0.2s\n",
      "[Stage 4] split_04/test: parts=1 | rows=51,408 | det_pos%=21.70% | bad_filter_drop=0 | nan_flux=50 | drop_nan_flux=0 | drop_time=0 | mag=[19.66,26.25] | asinh_mag=[19.66,29.48] | time=0.4s\n",
      "[Stage 4] split_05/train: parts=1 | rows=25,934 | det_pos%=18.33% | bad_filter_drop=0 | nan_flux=15 | drop_nan_flux=0 | drop_time=0 | mag=[18.82,26.33] | asinh_mag=[18.82,29.55] | time=0.2s\n",
      "[Stage 4] split_05/test: parts=1 | rows=61,179 | det_pos%=18.21% | bad_filter_drop=0 | nan_flux=42 | drop_nan_flux=0 | drop_time=0 | mag=[19.09,26.23] | asinh_mag=[19.08,29.29] | time=0.4s\n",
      "[Stage 4] split_06/train: parts=1 | rows=25,684 | det_pos%=18.85% | bad_filter_drop=0 | nan_flux=11 | drop_nan_flux=0 | drop_time=0 | mag=[19.80,26.15] | asinh_mag=[19.79,28.91] | time=0.2s\n",
      "[Stage 4] split_06/test: parts=1 | rows=57,620 | det_pos%=19.94% | bad_filter_drop=0 | nan_flux=21 | drop_nan_flux=0 | drop_time=0 | mag=[19.28,26.04] | asinh_mag=[19.27,29.33] | time=0.4s\n",
      "[Stage 4] split_07/train: parts=1 | rows=24,473 | det_pos%=21.44% | bad_filter_drop=0 | nan_flux=306 | drop_nan_flux=0 | drop_time=0 | mag=[20.05,26.32] | asinh_mag=[20.05,28.63] | time=0.2s\n",
      "[Stage 4] split_07/test: parts=1 | rows=65,101 | det_pos%=19.10% | bad_filter_drop=0 | nan_flux=904 | drop_nan_flux=0 | drop_time=0 | mag=[19.97,26.32] | asinh_mag=[19.96,29.03] | time=0.4s\n",
      "[Stage 4] split_08/train: parts=1 | rows=25,571 | det_pos%=22.80% | bad_filter_drop=0 | nan_flux=6 | drop_nan_flux=0 | drop_time=0 | mag=[18.59,26.30] | asinh_mag=[18.59,29.46] | time=0.2s\n",
      "[Stage 4] split_08/test: parts=1 | rows=61,498 | det_pos%=24.50% | bad_filter_drop=0 | nan_flux=47 | drop_nan_flux=0 | drop_time=0 | mag=[19.14,25.96] | asinh_mag=[19.14,30.06] | time=0.4s\n",
      "[Stage 4] split_09/train: parts=1 | rows=19,690 | det_pos%=21.13% | bad_filter_drop=0 | nan_flux=315 | drop_nan_flux=0 | drop_time=0 | mag=[19.83,26.31] | asinh_mag=[19.83,28.21] | time=0.2s\n",
      "[Stage 4] split_09/test: parts=1 | rows=47,239 | det_pos%=22.70% | bad_filter_drop=0 | nan_flux=514 | drop_nan_flux=0 | drop_time=0 | mag=[18.88,26.11] | asinh_mag=[18.88,28.91] | time=0.3s\n",
      "[Stage 4] split_10/train: parts=1 | rows=25,151 | det_pos%=20.86% | bad_filter_drop=0 | nan_flux=7 | drop_nan_flux=0 | drop_time=0 | mag=[19.52,26.20] | asinh_mag=[19.52,30.15] | time=0.2s\n",
      "[Stage 4] split_10/test: parts=1 | rows=51,056 | det_pos%=21.21% | bad_filter_drop=0 | nan_flux=51 | drop_nan_flux=0 | drop_time=0 | mag=[16.62,26.43] | asinh_mag=[16.62,30.10] | time=0.4s\n",
      "[Stage 4] split_11/train: parts=1 | rows=22,927 | det_pos%=19.59% | bad_filter_drop=0 | nan_flux=4 | drop_nan_flux=0 | drop_time=0 | mag=[20.57,26.41] | asinh_mag=[20.54,28.87] | time=0.2s\n",
      "[Stage 4] split_11/test: parts=1 | rows=49,723 | det_pos%=20.17% | bad_filter_drop=0 | nan_flux=12 | drop_nan_flux=0 | drop_time=0 | mag=[20.31,26.42] | asinh_mag=[20.30,29.87] | time=0.3s\n",
      "[Stage 4] split_12/train: parts=1 | rows=25,546 | det_pos%=19.64% | bad_filter_drop=0 | nan_flux=12 | drop_nan_flux=0 | drop_time=0 | mag=[20.40,26.32] | asinh_mag=[20.39,28.87] | time=0.2s\n",
      "[Stage 4] split_12/test: parts=1 | rows=54,499 | det_pos%=19.29% | bad_filter_drop=0 | nan_flux=13 | drop_nan_flux=0 | drop_time=0 | mag=[19.98,26.26] | asinh_mag=[19.98,30.16] | time=0.4s\n",
      "[Stage 4] split_13/train: parts=1 | rows=23,203 | det_pos%=20.64% | bad_filter_drop=0 | nan_flux=2 | drop_nan_flux=0 | drop_time=0 | mag=[19.46,26.05] | asinh_mag=[19.46,28.21] | time=0.2s\n",
      "[Stage 4] split_13/test: parts=1 | rows=63,653 | det_pos%=19.56% | bad_filter_drop=0 | nan_flux=15 | drop_nan_flux=0 | drop_time=0 | mag=[18.30,26.14] | asinh_mag=[18.29,28.60] | time=0.4s\n",
      "[Stage 4] split_14/train: parts=1 | rows=25,706 | det_pos%=20.36% | bad_filter_drop=0 | nan_flux=23 | drop_nan_flux=0 | drop_time=0 | mag=[20.52,26.40] | asinh_mag=[20.55,30.08] | time=0.2s\n",
      "[Stage 4] split_14/test: parts=1 | rows=58,643 | det_pos%=17.91% | bad_filter_drop=0 | nan_flux=43 | drop_nan_flux=0 | drop_time=0 | mag=[20.42,26.04] | asinh_mag=[20.31,30.50] | time=0.4s\n",
      "[Stage 4] split_15/train: parts=1 | rows=23,972 | det_pos%=19.09% | bad_filter_drop=0 | nan_flux=116 | drop_nan_flux=0 | drop_time=0 | mag=[20.11,26.04] | asinh_mag=[20.09,28.90] | time=0.2s\n",
      "[Stage 4] split_15/test: parts=1 | rows=52,943 | det_pos%=20.03% | bad_filter_drop=0 | nan_flux=197 | drop_nan_flux=0 | drop_time=0 | mag=[19.91,26.24] | asinh_mag=[19.90,30.37] | time=0.4s\n",
      "[Stage 4] split_16/train: parts=1 | rows=25,173 | det_pos%=21.42% | bad_filter_drop=0 | nan_flux=3 | drop_nan_flux=0 | drop_time=0 | mag=[20.00,26.26] | asinh_mag=[20.00,30.16] | time=0.2s\n",
      "[Stage 4] split_16/test: parts=1 | rows=58,192 | det_pos%=20.12% | bad_filter_drop=0 | nan_flux=13 | drop_nan_flux=0 | drop_time=0 | mag=[19.57,26.17] | asinh_mag=[19.57,30.53] | time=0.4s\n",
      "[Stage 4] split_17/train: parts=1 | rows=22,705 | det_pos%=22.09% | bad_filter_drop=0 | nan_flux=12 | drop_nan_flux=0 | drop_time=0 | mag=[19.64,26.17] | asinh_mag=[19.59,29.24] | time=0.2s\n",
      "[Stage 4] split_17/test: parts=1 | rows=59,482 | det_pos%=19.59% | bad_filter_drop=0 | nan_flux=21 | drop_nan_flux=0 | drop_time=0 | mag=[19.96,26.42] | asinh_mag=[19.93,29.02] | time=0.4s\n",
      "[Stage 4] split_18/train: parts=1 | rows=21,536 | det_pos%=23.77% | bad_filter_drop=0 | nan_flux=7 | drop_nan_flux=0 | drop_time=0 | mag=[20.48,26.05] | asinh_mag=[20.48,29.63] | time=0.2s\n",
      "[Stage 4] split_18/test: parts=1 | rows=53,887 | det_pos%=23.88% | bad_filter_drop=0 | nan_flux=14 | drop_nan_flux=0 | drop_time=0 | mag=[20.60,26.44] | asinh_mag=[20.49,30.38] | time=0.4s\n",
      "[Stage 4] split_19/train: parts=1 | rows=22,087 | det_pos%=23.73% | bad_filter_drop=0 | nan_flux=8 | drop_nan_flux=0 | drop_time=0 | mag=[19.98,26.30] | asinh_mag=[19.98,28.81] | time=0.2s\n",
      "[Stage 4] split_19/test: parts=1 | rows=56,355 | det_pos%=24.17% | bad_filter_drop=0 | nan_flux=16 | drop_nan_flux=0 | drop_time=0 | mag=[20.30,26.34] | asinh_mag=[20.30,29.23] | time=0.4s\n",
      "[Stage 4] split_20/train: parts=1 | rows=23,519 | det_pos%=20.45% | bad_filter_drop=0 | nan_flux=10 | drop_nan_flux=0 | drop_time=0 | mag=[19.80,25.93] | asinh_mag=[19.80,28.66] | time=0.2s\n",
      "[Stage 4] split_20/test: parts=1 | rows=58,432 | det_pos%=19.65% | bad_filter_drop=0 | nan_flux=10 | drop_nan_flux=0 | drop_time=0 | mag=[19.87,26.36] | asinh_mag=[19.87,30.27] | time=0.4s\n",
      "\n",
      "[Stage 4] Done.\n",
      "- LC_CLEAN_DIR  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/lc_clean_mag\n",
      "- Saved manifest: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
      "- Saved summary : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/lc_clean_mag/lc_clean_mag_summary.csv\n",
      "- Saved config  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/lc_clean_mag/photometric_config_mag.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Photometric Cleaning (FORCE OVERWRITE) — REVISI FULL v7.1\n",
    "# - Dust de-extinction (Rubin DustValues().r_x) for ugrizy\n",
    "# - Output:\n",
    "#   * mag, mag_err (positive-only; non-detect uses DET_SIGMA*err as limit)\n",
    "#   * asinh_mag, asinh_mag_err (Luptitude-like; works for negative flux)\n",
    "#   * snr, snr_abs, snr_asinh (preserve sign via asinh)\n",
    "# - Manifest + summary + config JSON\n",
    "#\n",
    "# v7.1 upgrade (dibanding v7.0):\n",
    "# - filter lebih robust: support \"0..5\" + \"lsst_u\" + unexpected casing; invalid filter bisa drop (atau strict)\n",
    "# - EBV robust: kalau EBV/EBV_clip tidak ada -> auto zeros\n",
    "# - skip write untuk part kosong (hindari rows<=0 error)\n",
    "# - summary tambah counters: dropped_bad_filter_rows, skipped_empty_parts\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings, time, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "for need in [\"iter_lightcurve_chunks\", \"df_train_meta\", \"df_test_meta\", \"ART_DIR\", \"SPLIT_LIST\", \"LOG_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 -> 1 -> 2 -> 3 dulu.\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR = Path(LOG_DIR); LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = globals().get(\"CFG\", {}) if isinstance(globals().get(\"CFG\", {}), dict) else {}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings\n",
    "# ----------------------------\n",
    "CHUNKSIZE   = int(CFG.get(\"PHOT_CHUNKSIZE\", 350_000))\n",
    "ERR_EPS     = float(CFG.get(\"PHOT_ERR_EPS\", 1e-6))\n",
    "\n",
    "SNR_DET_POS = float(CFG.get(\"SNR_DET_POS\", 3.0))  # detection for MAG branch (positive snr only)\n",
    "DET_SIGMA   = float(CFG.get(\"DET_SIGMA\", 3.0))\n",
    "\n",
    "SNR_CLIP    = float(CFG.get(\"SNR_CLIP\", 30.0))    # for stability\n",
    "MIN_FLUX_POS_UJY = float(CFG.get(\"MIN_FLUX_POS_UJY\", 1e-6))\n",
    "\n",
    "MAG_MIN, MAG_MAX   = float(CFG.get(\"MAG_MIN\", -10.0)), float(CFG.get(\"MAG_MAX\", 50.0))\n",
    "MAGERR_FLOOR_DET   = float(CFG.get(\"MAGERR_FLOOR_DET\", 1e-3))\n",
    "MAGERR_FLOOR_ND    = float(CFG.get(\"MAGERR_FLOOR_ND\", 0.75))\n",
    "MAGERR_CAP         = float(CFG.get(\"MAGERR_CAP\", 10.0))\n",
    "\n",
    "WRITE_FORMAT = str(CFG.get(\"PHOT_WRITE_FORMAT\", \"parquet\")).lower()  # \"parquet\" or \"csv.gz\"\n",
    "ONLY_SPLITS  = CFG.get(\"PHOT_ONLY_SPLITS\", None)   # e.g. [\"split_01\"]\n",
    "KEEP_FLUX_DEBUG = bool(CFG.get(\"PHOT_KEEP_FLUX_DEBUG\", False))\n",
    "DROP_BAD_TIME_ROWS = bool(CFG.get(\"PHOT_DROP_BAD_TIME_ROWS\", True))\n",
    "DROP_NAN_FLUX_ROWS  = bool(CFG.get(\"PHOT_DROP_NAN_FLUX_ROWS\", False))  # if True, drop NaN flux instead of zeroing\n",
    "REBUILD_MODE = str(CFG.get(\"PHOT_REBUILD_MODE\", \"wipe_all\")).lower()  # \"wipe_all\" | \"wipe_parts_only\"\n",
    "\n",
    "# v7.1: strict filter or drop invalid\n",
    "STRICT_FILTER = bool(CFG.get(\"PHOT_STRICT_FILTER\", False))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Extinction coefficients (Rubin DustValues().r_x)\n",
    "#    r_x * EBV = A_x  (extinction in mag)\n",
    "# ----------------------------\n",
    "EXT_RLAMBDA = {\n",
    "    \"u\": 4.757217815396922,\n",
    "    \"g\": 3.6605664439892625,\n",
    "    \"r\": 2.70136780871597,\n",
    "    \"i\": 2.0536599130965882,\n",
    "    \"z\": 1.5900964472616756,\n",
    "    \"y\": 1.3077049588254708,\n",
    "}\n",
    "\n",
    "BAND2ID = {\"u\": 0, \"g\": 1, \"r\": 2, \"i\": 3, \"z\": 4, \"y\": 5}\n",
    "ID2BAND = {v: k for k, v in BAND2ID.items()}\n",
    "RLAM_BY_ID = np.array([EXT_RLAMBDA[\"u\"], EXT_RLAMBDA[\"g\"], EXT_RLAMBDA[\"r\"], EXT_RLAMBDA[\"i\"], EXT_RLAMBDA[\"z\"], EXT_RLAMBDA[\"y\"]], dtype=np.float32)\n",
    "\n",
    "# filter aliases\n",
    "FILTER_NUM2STR = {\"0\": \"u\", \"1\": \"g\", \"2\": \"r\", \"3\": \"i\", \"4\": \"z\", \"5\": \"y\"}\n",
    "\n",
    "def _get_ebv_series(df_meta: pd.DataFrame):\n",
    "    # pakai EBV_clip jika ada, fallback EBV, kalau tidak ada -> zeros\n",
    "    if \"EBV_clip\" in df_meta.columns:\n",
    "        s = df_meta[\"EBV_clip\"]\n",
    "    elif \"EBV\" in df_meta.columns:\n",
    "        s = df_meta[\"EBV\"]\n",
    "    else:\n",
    "        s = pd.Series(0.0, index=df_meta.index)\n",
    "    # numeric + fill\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").fillna(0.0)\n",
    "    return s\n",
    "\n",
    "EBV_TRAIN_SER = _get_ebv_series(df_train_meta)\n",
    "EBV_TEST_SER  = _get_ebv_series(df_test_meta)\n",
    "\n",
    "# flux unit assumed uJy -> AB mag zero point:\n",
    "MAG_ZP = float(2.5 * np.log10(3631e6))  # ~23.9 (uJy)\n",
    "K_2P5_LN10 = np.float32(2.5 / np.log(10.0))     # 2.5/ln(10)\n",
    "K_1P0857 = np.float32(1.0857362)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Output root + WIPE (with safety guard)\n",
    "# ----------------------------\n",
    "LC_CLEAN_DIR = ART_DIR / \"lc_clean_mag\"   # keep name for compatibility\n",
    "\n",
    "art_abs = ART_DIR.resolve()\n",
    "lc_abs  = LC_CLEAN_DIR.resolve()\n",
    "\n",
    "try:\n",
    "    ok_rel = lc_abs.is_relative_to(art_abs)\n",
    "except AttributeError:\n",
    "    ok_rel = str(lc_abs).startswith(str(art_abs) + \"/\") or str(lc_abs).startswith(str(art_abs) + \"\\\\\")\n",
    "\n",
    "if not ok_rel:\n",
    "    raise RuntimeError(f\"Safety guard failed: LC_CLEAN_DIR bukan turunan ART_DIR.\\nART_DIR={art_abs}\\nLC_CLEAN_DIR={lc_abs}\")\n",
    "\n",
    "if REBUILD_MODE == \"wipe_all\":\n",
    "    if LC_CLEAN_DIR.exists():\n",
    "        shutil.rmtree(LC_CLEAN_DIR, ignore_errors=True)\n",
    "    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "elif REBUILD_MODE == \"wipe_parts_only\":\n",
    "    LC_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    raise ValueError(\"REBUILD_MODE must be 'wipe_all' or 'wipe_parts_only'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Atomic writer\n",
    "# ----------------------------\n",
    "def _atomic_write_parquet(df: pd.DataFrame, out_path: Path):\n",
    "    tmp = out_path.with_name(out_path.stem + \".tmp\" + out_path.suffix)\n",
    "    try:\n",
    "        df.to_parquet(tmp, index=False)\n",
    "        tmp.replace(out_path)\n",
    "    finally:\n",
    "        if tmp.exists() and (not out_path.exists()):\n",
    "            try: tmp.unlink()\n",
    "            except Exception: pass\n",
    "\n",
    "def _atomic_write_csv_gz(df: pd.DataFrame, out_path: Path):\n",
    "    final_path = out_path.with_suffix(\".csv.gz\")\n",
    "    tmp = final_path.with_name(final_path.stem + \".tmp\" + \"\".join(final_path.suffixes))\n",
    "    try:\n",
    "        df.to_csv(tmp, index=False, compression=\"gzip\")\n",
    "        tmp.replace(final_path)\n",
    "    finally:\n",
    "        if tmp.exists() and (not final_path.exists()):\n",
    "            try: tmp.unlink()\n",
    "            except Exception: pass\n",
    "    return final_path\n",
    "\n",
    "def write_part(df: pd.DataFrame, out_path: Path, fmt: str):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == \"parquet\":\n",
    "        try:\n",
    "            _atomic_write_parquet(df, out_path)\n",
    "            return \"parquet\", out_path\n",
    "        except Exception as e:\n",
    "            alt = _atomic_write_csv_gz(df, out_path)\n",
    "            return f\"csv.gz (fallback from parquet: {type(e).__name__})\", alt\n",
    "    elif fmt == \"csv.gz\":\n",
    "        alt = _atomic_write_csv_gz(df, out_path)\n",
    "        return \"csv.gz\", alt\n",
    "    else:\n",
    "        raise ValueError(\"fmt must be 'parquet' or 'csv.gz'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Core cleaning -> MAG + ASINH_MAG + SNR features\n",
    "# ----------------------------\n",
    "def _normalize_filter_series(filt_ser: pd.Series) -> pd.Series:\n",
    "    f = filt_ser.astype(\"string\").str.strip().str.lower()\n",
    "    # common prefixes\n",
    "    f = f.str.replace(\"lsst_\", \"\", regex=False)\n",
    "    f = f.str.replace(\"rubin_\", \"\", regex=False)\n",
    "    f = f.str.replace(\"band_\", \"\", regex=False)\n",
    "    # numeric 0..5\n",
    "    f = f.replace(FILTER_NUM2STR)\n",
    "    # if still long but endswith ugrizy, take last char\n",
    "    last = f.str[-1]\n",
    "    mask_end = last.isin(list(BAND2ID.keys()))\n",
    "    f = f.where(f.isin(list(BAND2ID.keys())), last.where(mask_end, f))\n",
    "    return f\n",
    "\n",
    "def clean_chunk_to_phot(ch: pd.DataFrame, ebv_ser: pd.Series):\n",
    "    # Required columns\n",
    "    for c in [\"object_id\", \"mjd\", \"flux\", \"flux_err\", \"filter\"]:\n",
    "        if c not in ch.columns:\n",
    "            raise ValueError(f\"iter_lightcurve_chunks chunk missing column: {c}. Found={list(ch.columns)}\")\n",
    "\n",
    "    oid_ser  = ch[\"object_id\"].astype(\"string\").str.strip()\n",
    "    filt_ser = _normalize_filter_series(ch[\"filter\"])\n",
    "\n",
    "    # numeric arrays\n",
    "    mjd  = ch[\"mjd\"].to_numpy(copy=False).astype(np.float32, copy=False)\n",
    "    flux = ch[\"flux\"].to_numpy(copy=False).astype(np.float32, copy=False)\n",
    "    err  = ch[\"flux_err\"].to_numpy(copy=False).astype(np.float32, copy=False)\n",
    "\n",
    "    # sanitize err\n",
    "    err = np.nan_to_num(err, nan=np.float32(ERR_EPS), posinf=np.float32(ERR_EPS), neginf=np.float32(ERR_EPS))\n",
    "    err = np.maximum(err, np.float32(ERR_EPS))\n",
    "\n",
    "    # sanitize flux\n",
    "    flux = flux.astype(np.float32, copy=False)\n",
    "    flux[~np.isfinite(flux)] = np.float32(np.nan)\n",
    "\n",
    "    # band id (robust)\n",
    "    band_id_s = filt_ser.map(BAND2ID)\n",
    "    bad_filter_rows = int(band_id_s.isna().sum())\n",
    "    if bad_filter_rows:\n",
    "        if STRICT_FILTER:\n",
    "            bad = filt_ser[band_id_s.isna()].value_counts().head(10).index.tolist()\n",
    "            raise ValueError(f\"Unknown/invalid filter values encountered (top examples): {bad}\")\n",
    "        keep_f = (~band_id_s.isna()).to_numpy()\n",
    "        oid_ser = oid_ser[keep_f]\n",
    "        filt_ser = filt_ser[keep_f]\n",
    "        mjd = mjd[keep_f]\n",
    "        flux = flux[keep_f]\n",
    "        err = err[keep_f]\n",
    "        band_id_s = band_id_s[keep_f]\n",
    "\n",
    "    band_id = band_id_s.to_numpy(copy=False).astype(np.int8, copy=False)\n",
    "\n",
    "    # EBV map (index=object_id)\n",
    "    ebv = oid_ser.map(ebv_ser).fillna(0.0).to_numpy(copy=False).astype(np.float32, copy=False)\n",
    "    ebv[~np.isfinite(ebv)] = np.float32(0.0)\n",
    "\n",
    "    # A_x\n",
    "    rlam = RLAM_BY_ID[band_id.astype(np.int32)]\n",
    "    A = (rlam * ebv).astype(np.float32, copy=False)\n",
    "\n",
    "    # de-extinction in flux space: f0 = f * 10^(0.4 A)\n",
    "    mul = np.power(np.float32(10.0), (np.float32(0.4) * A)).astype(np.float32, copy=False)\n",
    "    flux_deext = (flux * mul).astype(np.float32, copy=False)\n",
    "    err_deext  = (err  * mul).astype(np.float32, copy=False)\n",
    "\n",
    "    ok_flux = np.isfinite(flux_deext)\n",
    "    nan_flux_rows = int((~ok_flux).sum())\n",
    "\n",
    "    if DROP_NAN_FLUX_ROWS and nan_flux_rows:\n",
    "        keep_flux = ok_flux\n",
    "    else:\n",
    "        keep_flux = None\n",
    "        if nan_flux_rows:\n",
    "            flux_deext[~ok_flux] = np.float32(0.0)\n",
    "\n",
    "    # SNR\n",
    "    denom = np.maximum(err_deext, np.float32(ERR_EPS))\n",
    "    snr = (flux_deext / denom).astype(np.float32, copy=False)\n",
    "    snr = np.clip(snr, -np.float32(SNR_CLIP), np.float32(SNR_CLIP)).astype(np.float32, copy=False)\n",
    "    snr_abs = np.abs(snr).astype(np.float32, copy=False)\n",
    "\n",
    "    detected_pos = (snr > np.float32(SNR_DET_POS)).astype(np.int8, copy=False)\n",
    "\n",
    "    # MAG branch (positive-only; non-detect uses DET_SIGMA*err)\n",
    "    flux_detlim = (np.float32(DET_SIGMA) * err_deext).astype(np.float32, copy=False)\n",
    "    flux_for_mag = np.where(\n",
    "        detected_pos == 1,\n",
    "        np.maximum(flux_deext, np.float32(MIN_FLUX_POS_UJY)),\n",
    "        np.maximum(flux_detlim, np.float32(MIN_FLUX_POS_UJY)),\n",
    "    ).astype(np.float32, copy=False)\n",
    "\n",
    "    mag = (np.float32(MAG_ZP) - np.float32(2.5) * np.log10(flux_for_mag)).astype(np.float32, copy=False)\n",
    "    mag = np.clip(mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32, copy=False)\n",
    "\n",
    "    mag_err = (K_1P0857 * (err_deext / flux_for_mag)).astype(np.float32, copy=False)\n",
    "    mag_err = np.clip(mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32, copy=False)\n",
    "    if MAGERR_FLOOR_ND is not None and float(MAGERR_FLOOR_ND) > 0:\n",
    "        mag_err = np.where(detected_pos == 1, mag_err, np.maximum(mag_err, np.float32(MAGERR_FLOOR_ND))).astype(np.float32, copy=False)\n",
    "\n",
    "    # ASINH magnitude (Luptitude-like; defined for negative flux)\n",
    "    b = np.maximum(np.float32(DET_SIGMA) * err_deext, np.float32(MIN_FLUX_POS_UJY)).astype(np.float32, copy=False)\n",
    "    x = (flux_deext / (np.float32(2.0) * b)).astype(np.float32, copy=False)\n",
    "    asinh_term = np.arcsinh(x).astype(np.float32, copy=False)\n",
    "    ln_b = np.log(b).astype(np.float32, copy=False)\n",
    "\n",
    "    asinh_mag = (np.float32(MAG_ZP) - K_2P5_LN10 * (asinh_term + ln_b)).astype(np.float32, copy=False)\n",
    "    asinh_mag = np.clip(asinh_mag, np.float32(MAG_MIN), np.float32(MAG_MAX)).astype(np.float32, copy=False)\n",
    "\n",
    "    denom_asinh = np.sqrt((flux_deext * flux_deext) + (np.float32(2.0) * b) * (np.float32(2.0) * b)).astype(np.float32, copy=False)\n",
    "    asinh_mag_err = (K_2P5_LN10 * (err_deext / np.maximum(denom_asinh, np.float32(ERR_EPS)))).astype(np.float32, copy=False)\n",
    "    asinh_mag_err = np.clip(asinh_mag_err, np.float32(MAGERR_FLOOR_DET), np.float32(MAGERR_CAP)).astype(np.float32, copy=False)\n",
    "\n",
    "    snr_asinh = np.arcsinh(snr).astype(np.float32, copy=False)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"object_id\": pd.array(oid_ser.to_numpy(copy=False), dtype=\"string\"),\n",
    "        \"mjd\": mjd,\n",
    "        \"band_id\": band_id,\n",
    "        \"mag\": mag,\n",
    "        \"mag_err\": mag_err,\n",
    "        \"asinh_mag\": asinh_mag,\n",
    "        \"asinh_mag_err\": asinh_mag_err,\n",
    "        \"snr\": snr,\n",
    "        \"snr_abs\": snr_abs,\n",
    "        \"snr_asinh\": snr_asinh,\n",
    "        \"detected_pos\": detected_pos,\n",
    "    })\n",
    "\n",
    "    if KEEP_FLUX_DEBUG:\n",
    "        out[\"A_x\"]        = pd.Series(A, dtype=\"float32\")\n",
    "        out[\"flux_deext\"] = pd.Series(flux_deext, dtype=\"float32\")\n",
    "        out[\"err_deext\"]  = pd.Series(err_deext, dtype=\"float32\")\n",
    "        out[\"b_soft\"]     = pd.Series(b, dtype=\"float32\")\n",
    "\n",
    "    dropped_flux = 0\n",
    "    if keep_flux is not None:\n",
    "        keep = keep_flux\n",
    "        dropped_flux = int((~keep).sum())\n",
    "        out = out[keep]\n",
    "\n",
    "    dropped_time = 0\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        t = out[\"mjd\"].to_numpy(copy=False).astype(np.float32, copy=False)\n",
    "        keep_t = np.isfinite(t)\n",
    "        dropped_time = int((~keep_t).sum())\n",
    "        if dropped_time:\n",
    "            out = out[keep_t]\n",
    "\n",
    "    return out, int(dropped_time), int(nan_flux_rows), int(dropped_flux), int(bad_filter_rows)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Process split-wise\n",
    "# ----------------------------\n",
    "splits_to_use = ONLY_SPLITS if (ONLY_SPLITS is not None) else list(SPLIT_LIST)\n",
    "splits_to_use = list(splits_to_use)\n",
    "\n",
    "summary_rows, manifest_rows = [], []\n",
    "\n",
    "def _wipe_parts_dir(out_dir: Path):\n",
    "    if out_dir.exists():\n",
    "        for pat in [\"part_*.parquet\", \"part_*.csv.gz\", \"*.tmp\", \"*.tmp.parquet\", \"*.tmp.csv.gz\"]:\n",
    "            for f in out_dir.glob(pat):\n",
    "                try: f.unlink()\n",
    "                except Exception: pass\n",
    "\n",
    "def process_split(split_name: str, which: str):\n",
    "    ebv_ser = EBV_TRAIN_SER if which == \"train\" else EBV_TEST_SER\n",
    "    out_dir = LC_CLEAN_DIR / split_name / which\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if REBUILD_MODE == \"wipe_parts_only\":\n",
    "        _wipe_parts_dir(out_dir)\n",
    "\n",
    "    t0 = time.time()\n",
    "    part_idx = 0\n",
    "    n_rows_total = 0\n",
    "    n_det_pos = 0\n",
    "    dropped_time_total = 0\n",
    "    nan_flux_total = 0\n",
    "    dropped_flux_total = 0\n",
    "    dropped_bad_filter_total = 0\n",
    "    skipped_empty_parts = 0\n",
    "\n",
    "    mag_min = np.inf\n",
    "    mag_max = -np.inf\n",
    "    asinh_mag_min = np.inf\n",
    "    asinh_mag_max = -np.inf\n",
    "\n",
    "    for ch in iter_lightcurve_chunks(split_name, which, chunksize=CHUNKSIZE):\n",
    "        cleaned, dropped_time, nan_flux, dropped_flux, bad_filter_rows = clean_chunk_to_phot(ch, ebv_ser)\n",
    "\n",
    "        dropped_time_total += int(dropped_time)\n",
    "        nan_flux_total += int(nan_flux)\n",
    "        dropped_flux_total += int(dropped_flux)\n",
    "        dropped_bad_filter_total += int(bad_filter_rows)\n",
    "\n",
    "        if cleaned is None or len(cleaned) == 0:\n",
    "            skipped_empty_parts += 1\n",
    "            continue\n",
    "\n",
    "        n_rows = int(len(cleaned))\n",
    "        n_rows_total += n_rows\n",
    "        n_det_pos += int(cleaned[\"detected_pos\"].to_numpy(copy=False).astype(np.int8).sum())\n",
    "\n",
    "        mag_arr = cleaned[\"mag\"].to_numpy(copy=False).astype(np.float32, copy=False)\n",
    "        fin = np.isfinite(mag_arr)\n",
    "        if fin.any():\n",
    "            mag_min = float(min(mag_min, float(np.min(mag_arr[fin]))))\n",
    "            mag_max = float(max(mag_max, float(np.max(mag_arr[fin]))))\n",
    "\n",
    "        am_arr = cleaned[\"asinh_mag\"].to_numpy(copy=False).astype(np.float32, copy=False)\n",
    "        fin2 = np.isfinite(am_arr)\n",
    "        if fin2.any():\n",
    "            asinh_mag_min = float(min(asinh_mag_min, float(np.min(am_arr[fin2]))))\n",
    "            asinh_mag_max = float(max(asinh_mag_max, float(np.max(am_arr[fin2]))))\n",
    "\n",
    "        out_path = out_dir / f\"part_{part_idx:04d}.parquet\"\n",
    "        used_fmt, final_path = write_part(cleaned, out_path, WRITE_FORMAT)\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"which\": which,\n",
    "            \"part\": int(part_idx),\n",
    "            \"path\": str(final_path),\n",
    "            \"rows\": int(n_rows),\n",
    "            \"format\": str(used_fmt),\n",
    "        })\n",
    "\n",
    "        part_idx += 1\n",
    "        del cleaned, ch\n",
    "        if part_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    summary_rows.append({\n",
    "        \"split\": split_name,\n",
    "        \"which\": which,\n",
    "        \"parts\": int(part_idx),\n",
    "        \"rows\": int(n_rows_total),\n",
    "        \"det_pos_frac\": float(n_det_pos / max(n_rows_total, 1)),\n",
    "        \"mag_min\": (mag_min if np.isfinite(mag_min) else np.nan),\n",
    "        \"mag_max\": (mag_max if np.isfinite(mag_max) else np.nan),\n",
    "        \"asinh_mag_min\": (asinh_mag_min if np.isfinite(asinh_mag_min) else np.nan),\n",
    "        \"asinh_mag_max\": (asinh_mag_max if np.isfinite(asinh_mag_max) else np.nan),\n",
    "        \"dropped_time_rows\": int(dropped_time_total),\n",
    "        \"nan_flux_rows\": int(nan_flux_total),\n",
    "        \"dropped_nan_flux_rows\": int(dropped_flux_total),\n",
    "        \"dropped_bad_filter_rows\": int(dropped_bad_filter_total),\n",
    "        \"skipped_empty_parts\": int(skipped_empty_parts),\n",
    "        \"sec\": float(dt),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"[Stage 4] {split_name}/{which}: parts={part_idx} | rows={n_rows_total:,} | \"\n",
    "        f\"det_pos%={100*(n_det_pos/max(n_rows_total,1)):.2f}% | \"\n",
    "        f\"bad_filter_drop={dropped_bad_filter_total:,} | nan_flux={nan_flux_total:,} | drop_nan_flux={dropped_flux_total:,} | drop_time={dropped_time_total:,} | \"\n",
    "        f\"mag=[{(mag_min if np.isfinite(mag_min) else np.nan):.2f},{(mag_max if np.isfinite(mag_max) else np.nan):.2f}] | \"\n",
    "        f\"asinh_mag=[{(asinh_mag_min if np.isfinite(asinh_mag_min) else np.nan):.2f},{(asinh_mag_max if np.isfinite(asinh_mag_max) else np.nan):.2f}] | \"\n",
    "        f\"time={dt:.1f}s\"\n",
    "    )\n",
    "\n",
    "print(f\"[Stage 4] REBUILD_MODE={REBUILD_MODE} | Writing to: {LC_CLEAN_DIR}\")\n",
    "print(f\"[Stage 4] WRITE_FORMAT={WRITE_FORMAT} | CHUNKSIZE={CHUNKSIZE:,} | DROP_NAN_FLUX_ROWS={DROP_NAN_FLUX_ROWS} | STRICT_FILTER={STRICT_FILTER}\")\n",
    "\n",
    "for s in splits_to_use:\n",
    "    process_split(s, \"train\")\n",
    "    process_split(s, \"test\")\n",
    "\n",
    "df_parts_manifest = pd.DataFrame(manifest_rows)\n",
    "df_summary  = pd.DataFrame(summary_rows)\n",
    "\n",
    "manifest_path = LC_CLEAN_DIR / \"lc_clean_mag_manifest.csv\"\n",
    "summary_path  = LC_CLEAN_DIR / \"lc_clean_mag_summary.csv\"\n",
    "df_parts_manifest.to_csv(manifest_path, index=False)\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "cfg_path = LC_CLEAN_DIR / \"photometric_config_mag.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"STAGE\": \"stage4\",\n",
    "        \"VERSION\": \"v7.1\",\n",
    "        \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "        \"SNR_DET_POS\": float(SNR_DET_POS),\n",
    "        \"DET_SIGMA\": float(DET_SIGMA),\n",
    "        \"ERR_EPS\": float(ERR_EPS),\n",
    "        \"SNR_CLIP\": float(SNR_CLIP),\n",
    "        \"MAG_ZP\": float(MAG_ZP),\n",
    "        \"MAG_MIN\": float(MAG_MIN),\n",
    "        \"MAG_MAX\": float(MAG_MAX),\n",
    "        \"CHUNKSIZE\": int(CHUNKSIZE),\n",
    "        \"WRITE_FORMAT\": str(WRITE_FORMAT),\n",
    "        \"ONLY_SPLITS\": list(splits_to_use),\n",
    "        \"KEEP_FLUX_DEBUG\": bool(KEEP_FLUX_DEBUG),\n",
    "        \"DROP_BAD_TIME_ROWS\": bool(DROP_BAD_TIME_ROWS),\n",
    "        \"DROP_NAN_FLUX_ROWS\": bool(DROP_NAN_FLUX_ROWS),\n",
    "        \"REBUILD_MODE\": str(REBUILD_MODE),\n",
    "        \"STRICT_FILTER\": bool(STRICT_FILTER),\n",
    "        \"SCHEMA\": \"mag + asinh_mag + snr_asinh\",\n",
    "        \"COLUMNS\": [\n",
    "            \"object_id\",\"mjd\",\"band_id\",\n",
    "            \"mag\",\"mag_err\",\n",
    "            \"asinh_mag\",\"asinh_mag_err\",\n",
    "            \"snr\",\"snr_abs\",\"snr_asinh\",\n",
    "            \"detected_pos\",\n",
    "        ] + ([\"A_x\",\"flux_deext\",\"err_deext\",\"b_soft\"] if KEEP_FLUX_DEBUG else []),\n",
    "    }, f, indent=2)\n",
    "\n",
    "# sanity: manifest coverage\n",
    "if len(df_parts_manifest) == 0:\n",
    "    raise RuntimeError(\"Stage 4 produced empty manifest. Check iter_lightcurve_chunks and split routing.\")\n",
    "if (df_parts_manifest[\"rows\"] <= 0).any():\n",
    "    bad = df_parts_manifest.loc[df_parts_manifest[\"rows\"] <= 0].head(5).to_dict(\"records\")\n",
    "    raise RuntimeError(f\"Found empty part files (rows<=0). Examples: {bad}\")\n",
    "\n",
    "print(\"\\n[Stage 4] Done.\")\n",
    "print(f\"- LC_CLEAN_DIR  : {LC_CLEAN_DIR}\")\n",
    "print(f\"- Saved manifest: {manifest_path}\")\n",
    "print(f\"- Saved summary : {summary_path}\")\n",
    "print(f\"- Saved config  : {cfg_path}\")\n",
    "\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    m = df_parts_manifest[(df_parts_manifest[\"split\"] == split_name) & (df_parts_manifest[\"which\"] == which)].sort_values(\"part\")\n",
    "    return m[\"path\"].astype(str).tolist()\n",
    "\n",
    "globals().update({\n",
    "    \"EXT_RLAMBDA\": EXT_RLAMBDA,\n",
    "    \"BAND2ID\": BAND2ID,\n",
    "    \"ID2BAND\": ID2BAND,\n",
    "    \"MAG_ZP\": MAG_ZP,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"lc_clean_mag_manifest\": df_parts_manifest,\n",
    "    \"lc_clean_mag_summary\": df_summary,\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2f50d",
   "metadata": {
    "papermill": {
     "duration": 0.018571,
     "end_time": "2026-01-07T16:45:55.060030",
     "exception": false,
     "start_time": "2026-01-07T16:45:55.041459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Tokenization (Event-based Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5974a5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:45:55.098159Z",
     "iopub.status.busy": "2026-01-07T16:45:55.097768Z",
     "iopub.status.idle": "2026-01-07T16:58:35.018352Z",
     "shell.execute_reply": "2026-01-07T16:58:35.017415Z"
    },
    "papermill": {
     "duration": 759.956303,
     "end_time": "2026-01-07T16:58:35.033829",
     "exception": false,
     "start_time": "2026-01-07T16:45:55.077526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 5 ROUTING SYNC OK\n",
      "- RUN_DIR      : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c\n",
      "- ART_DIR      : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts\n",
      "- LC_CLEAN_DIR : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/lc_clean_mag\n",
      "- manifest_csv : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\n",
      "- meta_sync    : meta already consistent\n",
      "\n",
      "[Stage 5] SETTINGS\n",
      "- TOKEN_MODE_FORCE: asinh\n",
      "- FEATURE_SET     : v2 | dim=13\n",
      "- ADD_META        : True | meta_cols=['EBV_clip', 'log1pZ', 'zerr_rel', 'is_photoz']\n",
      "- L_MAX/TRUNC     : 256 / smart (KEEP_DET_FRAC=0.7)\n",
      "- NUM_BUCKETS     : 256 | SHARD_MAX_OBJECTS=1500\n",
      "- REBUILD_MODE    : wipe_all | COMPRESS_NPZ=False\n",
      "\n",
      "[Stage 5] split_01/train | expected=155\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=26,324 | len_mean 169.8->145.3 | p95 191.7->191.7 | trunc%=3.9% | time=14.78s\n",
      "\n",
      "[Stage 5] split_01/test | expected=364\n",
      "[Stage 5] OK: built=364 (missing_filled=0) | kept_rows=59,235 | len_mean 162.7->148.3 | p95 193.8->193.8 | trunc%=2.2% | time=23.25s\n",
      "\n",
      "[Stage 5] split_02/train | expected=170\n",
      "[Stage 5] OK: built=170 (missing_filled=0) | kept_rows=25,609 | len_mean 150.6->145.9 | p95 195.5->195.5 | trunc%=1.2% | time=15.76s\n",
      "\n",
      "[Stage 5] split_02/test | expected=414\n",
      "[Stage 5] OK: built=414 (missing_filled=0) | kept_rows=71,229 | len_mean 172.1->149.5 | p95 201.0->201.0 | trunc%=3.4% | time=24.96s\n",
      "\n",
      "[Stage 5] split_03/train | expected=138\n",
      "[Stage 5] OK: built=138 (missing_filled=0) | kept_rows=21,676 | len_mean 157.1->145.1 | p95 191.8->191.8 | trunc%=2.2% | time=13.80s\n",
      "\n",
      "[Stage 5] split_03/test | expected=338\n",
      "[Stage 5] OK: built=338 (missing_filled=0) | kept_rows=53,751 | len_mean 159.0->147.0 | p95 194.3->194.3 | trunc%=2.4% | time=22.67s\n",
      "\n",
      "[Stage 5] split_04/train | expected=145\n",
      "[Stage 5] OK: built=145 (missing_filled=0) | kept_rows=22,898 | len_mean 157.9->139.7 | p95 187.0->187.0 | trunc%=2.8% | time=13.05s\n",
      "\n",
      "[Stage 5] split_04/test | expected=332\n",
      "[Stage 5] OK: built=332 (missing_filled=0) | kept_rows=51,408 | len_mean 154.8->140.9 | p95 195.0->195.0 | trunc%=3.0% | time=23.87s\n",
      "\n",
      "[Stage 5] split_05/train | expected=165\n",
      "[Stage 5] OK: built=165 (missing_filled=0) | kept_rows=25,934 | len_mean 157.2->146.9 | p95 191.6->191.6 | trunc%=2.4% | time=15.46s\n",
      "\n",
      "[Stage 5] split_05/test | expected=375\n",
      "[Stage 5] OK: built=375 (missing_filled=0) | kept_rows=61,179 | len_mean 163.1->146.0 | p95 194.0->194.0 | trunc%=2.7% | time=23.23s\n",
      "\n",
      "[Stage 5] split_06/train | expected=155\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,684 | len_mean 165.7->145.8 | p95 198.0->198.0 | trunc%=3.2% | time=13.92s\n",
      "\n",
      "[Stage 5] split_06/test | expected=374\n",
      "[Stage 5] OK: built=374 (missing_filled=0) | kept_rows=57,620 | len_mean 154.1->143.4 | p95 189.3->189.3 | trunc%=1.9% | time=24.61s\n",
      "\n",
      "[Stage 5] split_07/train | expected=165\n",
      "[Stage 5] OK: built=165 (missing_filled=0) | kept_rows=24,473 | len_mean 148.3->146.9 | p95 191.6->191.6 | trunc%=0.6% | time=14.38s\n",
      "\n",
      "[Stage 5] split_07/test | expected=398\n",
      "[Stage 5] OK: built=398 (missing_filled=0) | kept_rows=65,101 | len_mean 163.6->147.9 | p95 194.1->194.1 | trunc%=2.3% | time=25.74s\n",
      "\n",
      "[Stage 5] split_08/train | expected=162\n",
      "[Stage 5] OK: built=162 (missing_filled=0) | kept_rows=25,571 | len_mean 157.8->144.1 | p95 185.9->185.9 | trunc%=1.9% | time=14.11s\n",
      "\n",
      "[Stage 5] split_08/test | expected=387\n",
      "[Stage 5] OK: built=387 (missing_filled=0) | kept_rows=61,498 | len_mean 158.9->148.4 | p95 194.4->194.4 | trunc%=1.6% | time=23.79s\n",
      "\n",
      "[Stage 5] split_09/train | expected=128\n",
      "[Stage 5] OK: built=128 (missing_filled=0) | kept_rows=19,690 | len_mean 153.8->144.8 | p95 192.3->192.3 | trunc%=1.6% | time=12.96s\n",
      "\n",
      "[Stage 5] split_09/test | expected=289\n",
      "[Stage 5] OK: built=289 (missing_filled=0) | kept_rows=47,239 | len_mean 163.5->147.4 | p95 196.0->196.0 | trunc%=3.1% | time=21.78s\n",
      "\n",
      "[Stage 5] split_10/train | expected=144\n",
      "[Stage 5] OK: built=144 (missing_filled=0) | kept_rows=25,151 | len_mean 174.7->150.1 | p95 204.4->204.4 | trunc%=4.2% | time=13.96s\n",
      "\n",
      "[Stage 5] split_10/test | expected=331\n",
      "[Stage 5] OK: built=331 (missing_filled=0) | kept_rows=51,056 | len_mean 154.2->143.7 | p95 188.0->188.0 | trunc%=2.1% | time=23.32s\n",
      "\n",
      "[Stage 5] split_11/train | expected=146\n",
      "[Stage 5] OK: built=146 (missing_filled=0) | kept_rows=22,927 | len_mean 157.0->146.2 | p95 192.0->192.0 | trunc%=1.4% | time=14.02s\n",
      "\n",
      "[Stage 5] split_11/test | expected=325\n",
      "[Stage 5] OK: built=325 (missing_filled=0) | kept_rows=49,723 | len_mean 153.0->144.9 | p95 189.6->189.6 | trunc%=1.2% | time=22.52s\n",
      "\n",
      "[Stage 5] split_12/train | expected=155\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,546 | len_mean 164.8->149.3 | p95 193.9->193.9 | trunc%=3.2% | time=14.82s\n",
      "\n",
      "[Stage 5] split_12/test | expected=353\n",
      "[Stage 5] OK: built=353 (missing_filled=0) | kept_rows=54,499 | len_mean 154.4->145.4 | p95 185.0->185.0 | trunc%=1.1% | time=22.78s\n",
      "\n",
      "[Stage 5] split_13/train | expected=143\n",
      "[Stage 5] OK: built=143 (missing_filled=0) | kept_rows=23,203 | len_mean 162.3->149.3 | p95 190.9->190.9 | trunc%=2.1% | time=13.89s\n",
      "\n",
      "[Stage 5] split_13/test | expected=379\n",
      "[Stage 5] OK: built=379 (missing_filled=0) | kept_rows=63,653 | len_mean 167.9->149.3 | p95 195.1->195.1 | trunc%=2.9% | time=24.74s\n",
      "\n",
      "[Stage 5] split_14/train | expected=154\n",
      "[Stage 5] OK: built=154 (missing_filled=0) | kept_rows=25,706 | len_mean 166.9->151.1 | p95 198.0->198.0 | trunc%=2.6% | time=14.34s\n",
      "\n",
      "[Stage 5] split_14/test | expected=351\n",
      "[Stage 5] OK: built=351 (missing_filled=0) | kept_rows=58,643 | len_mean 167.1->150.2 | p95 193.0->193.0 | trunc%=2.8% | time=23.27s\n",
      "\n",
      "[Stage 5] split_15/train | expected=158\n",
      "[Stage 5] OK: built=158 (missing_filled=0) | kept_rows=23,972 | len_mean 151.7->143.7 | p95 195.0->195.0 | trunc%=2.5% | time=14.50s\n",
      "\n",
      "[Stage 5] split_15/test | expected=342\n",
      "[Stage 5] OK: built=342 (missing_filled=0) | kept_rows=52,943 | len_mean 154.8->143.9 | p95 193.0->193.0 | trunc%=2.3% | time=22.24s\n",
      "\n",
      "[Stage 5] split_16/train | expected=155\n",
      "[Stage 5] OK: built=155 (missing_filled=0) | kept_rows=25,173 | len_mean 162.4->149.0 | p95 199.9->199.9 | trunc%=1.9% | time=14.15s\n",
      "\n",
      "[Stage 5] split_16/test | expected=354\n",
      "[Stage 5] OK: built=354 (missing_filled=0) | kept_rows=58,192 | len_mean 164.4->148.1 | p95 193.3->193.3 | trunc%=2.8% | time=24.18s\n",
      "\n",
      "[Stage 5] split_17/train | expected=153\n",
      "[Stage 5] OK: built=153 (missing_filled=0) | kept_rows=22,705 | len_mean 148.4->145.0 | p95 190.4->190.4 | trunc%=1.3% | time=14.23s\n",
      "\n",
      "[Stage 5] split_17/test | expected=351\n",
      "[Stage 5] OK: built=351 (missing_filled=0) | kept_rows=59,482 | len_mean 169.5->147.3 | p95 198.0->198.0 | trunc%=3.4% | time=22.79s\n",
      "\n",
      "[Stage 5] split_18/train | expected=152\n",
      "[Stage 5] OK: built=152 (missing_filled=0) | kept_rows=21,536 | len_mean 141.7->137.8 | p95 191.2->191.2 | trunc%=1.3% | time=14.51s\n",
      "\n",
      "[Stage 5] split_18/test | expected=345\n",
      "[Stage 5] OK: built=345 (missing_filled=0) | kept_rows=53,887 | len_mean 156.2->141.2 | p95 188.8->188.8 | trunc%=2.3% | time=23.29s\n",
      "\n",
      "[Stage 5] split_19/train | expected=147\n",
      "[Stage 5] OK: built=147 (missing_filled=0) | kept_rows=22,087 | len_mean 150.3->142.0 | p95 188.4->188.4 | trunc%=1.4% | time=13.27s\n",
      "\n",
      "[Stage 5] split_19/test | expected=375\n",
      "[Stage 5] OK: built=375 (missing_filled=0) | kept_rows=56,355 | len_mean 150.3->143.1 | p95 191.0->191.0 | trunc%=1.1% | time=24.34s\n",
      "\n",
      "[Stage 5] split_20/train | expected=153\n",
      "[Stage 5] OK: built=153 (missing_filled=0) | kept_rows=23,519 | len_mean 153.7->143.2 | p95 187.8->187.8 | trunc%=1.3% | time=14.18s\n",
      "\n",
      "[Stage 5] split_20/test | expected=358\n",
      "[Stage 5] OK: built=358 (missing_filled=0) | kept_rows=58,432 | len_mean 163.2->148.5 | p95 190.0->190.0 | trunc%=2.5% | time=23.23s\n",
      "\n",
      "[Stage 5] DONE\n",
      "- token_mode : asinh (forced)\n",
      "- feature_set: v2 | dim=13\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/seq_tokens/seq_manifest_train.csv (rows=3,043)\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/seq_tokens/seq_manifest_test.csv  (rows=7,135)\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/seq_tokens/seq_build_stats.csv\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/seq_tokens/seq_config.json\n",
      "\n",
      "[Stage 5] Smoke test object_id=Dornhoth_anwar_melethron\n",
      "- seq_len=184 | X_shape=(184, 13) | bands_unique=[0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — Sequence Tokenization (Event-based Tokens) (ONE CELL)\n",
    "# REVISI FULL v6.1 (FORCE ASINH + FEATURESET v2 + STAGE4 v7.1 compatible + robust meta + safer reuse)\n",
    "#\n",
    "# Compatible STAGE 4 schemas:\n",
    "# - v7+: columns include: asinh_mag, asinh_mag_err, detected_pos, snr (and/or snr_asinh), band_id\n",
    "# - legacy: columns include: flux_asinh, err_log1p, detected, snr, band_id\n",
    "#\n",
    "# Output:\n",
    "# - ART_DIR/seq_tokens/<split>/<train|test>/shard_*.npz\n",
    "# - ART_DIR/seq_tokens/seq_manifest_train.csv\n",
    "# - ART_DIR/seq_tokens/seq_manifest_test.csv\n",
    "# - ART_DIR/seq_tokens/seq_build_stats.csv\n",
    "# - ART_DIR/seq_tokens/seq_config.json\n",
    "#\n",
    "# NPZ shard arrays:\n",
    "# - object_id: (n_obj,) bytes\n",
    "# - x       : (total_tokens, feature_dim) float32\n",
    "# - band    : (total_tokens,) int8   (band_id per token)\n",
    "# - offsets : (n_obj+1,) int64       (start offsets for each object)\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings, time, shutil, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "for need in [\"ART_DIR\", \"df_train_meta\", \"df_test_meta\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 0 + STAGE 2 dulu (ART_DIR + meta).\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = globals().get(\"CFG\", {})\n",
    "CFG = CFG if isinstance(CFG, dict) else {}\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers\n",
    "# ----------------------------\n",
    "def _safe_string_series(s: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return s.astype(\"string\").str.strip()\n",
    "    except Exception:\n",
    "        return s.astype(str).str.strip()\n",
    "\n",
    "def _find_stage4_manifest(art_dir: Path):\n",
    "    cand = art_dir / \"lc_clean_mag\" / \"lc_clean_mag_manifest.csv\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    if not root.exists():\n",
    "        return None\n",
    "    cands = list(root.glob(\"run_*/artifacts/lc_clean_mag/lc_clean_mag_manifest.csv\"))\n",
    "    if not cands:\n",
    "        cands = list(root.glob(\"run_*/**/lc_clean_mag_manifest.csv\"))\n",
    "    if not cands:\n",
    "        return None\n",
    "    cands = sorted(cands, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def _sync_dirs_from_manifest(manifest_csv: Path):\n",
    "    lc_clean_dir = manifest_csv.parent\n",
    "    art_dir_new  = lc_clean_dir.parent\n",
    "    run_dir_new  = art_dir_new.parent\n",
    "    return run_dir_new, art_dir_new, lc_clean_dir\n",
    "\n",
    "def _read_meta_file(art_dir_synced: Path, which: str) -> pd.DataFrame:\n",
    "    pq = art_dir_synced / f\"{which}_meta.parquet\"\n",
    "    csv = art_dir_synced / f\"{which}_meta.csv\"\n",
    "    if pq.exists():\n",
    "        df = pd.read_parquet(pq)\n",
    "    elif csv.exists():\n",
    "        df = pd.read_csv(csv)\n",
    "    else:\n",
    "        return None\n",
    "    if isinstance(df.index, pd.RangeIndex) and (\"object_id\" in df.columns):\n",
    "        df = df.set_index(\"object_id\", drop=True)\n",
    "    elif (\"object_id\" in df.columns) and (df.index.name != \"object_id\"):\n",
    "        df = df.set_index(\"object_id\", drop=True)\n",
    "    if isinstance(df.index, pd.RangeIndex):\n",
    "        for c in [\"Unnamed: 0\", \"index\"]:\n",
    "            if c in df.columns:\n",
    "                df = df.set_index(c, drop=True)\n",
    "                break\n",
    "    df.index = df.index.astype(\"string\")\n",
    "    return df\n",
    "\n",
    "def _load_meta_if_needed(art_dir_synced: Path):\n",
    "    global df_train_meta, df_test_meta\n",
    "    cand_train = _read_meta_file(art_dir_synced, \"train\")\n",
    "    cand_test  = _read_meta_file(art_dir_synced, \"test\")\n",
    "    if cand_train is None or cand_test is None:\n",
    "        return False, \"meta file not found in synced ART_DIR; keep in-memory\"\n",
    "    try:\n",
    "        if (len(df_train_meta) != len(cand_train)) or (len(df_test_meta) != len(cand_test)):\n",
    "            df_train_meta = cand_train\n",
    "            df_test_meta  = cand_test\n",
    "            return True, \"reloaded meta due to size mismatch\"\n",
    "        sample_ids = df_train_meta.index[:5].astype(str).tolist()\n",
    "        if not all((sid in cand_train.index) for sid in sample_ids):\n",
    "            df_train_meta = cand_train\n",
    "            df_test_meta  = cand_test\n",
    "            return True, \"reloaded meta due to id mismatch\"\n",
    "        return False, \"meta already consistent\"\n",
    "    except Exception as e:\n",
    "        return False, f\"meta reload skipped ({type(e).__name__}: {e})\"\n",
    "\n",
    "def _ensure_meta_features(meta_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure columns needed for per-token meta exist: EBV_clip, log1pZ, zerr_rel, is_photoz.\"\"\"\n",
    "    df = meta_df.copy(deep=False)\n",
    "\n",
    "    # EBV_clip\n",
    "    if \"EBV_clip\" not in df.columns:\n",
    "        if \"EBV\" in df.columns:\n",
    "            df[\"EBV_clip\"] = pd.to_numeric(df[\"EBV\"], errors=\"coerce\")\n",
    "        else:\n",
    "            df[\"EBV_clip\"] = 0.0\n",
    "    df[\"EBV_clip\"] = pd.to_numeric(df[\"EBV_clip\"], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "    # choose Z source\n",
    "    if \"Z\" in df.columns:\n",
    "        z = pd.to_numeric(df[\"Z\"], errors=\"coerce\")\n",
    "    elif \"Z_clip\" in df.columns:\n",
    "        z = pd.to_numeric(df[\"Z_clip\"], errors=\"coerce\")\n",
    "    elif \"photoz\" in df.columns:\n",
    "        z = pd.to_numeric(df[\"photoz\"], errors=\"coerce\")\n",
    "    else:\n",
    "        z = pd.Series(0.0, index=df.index)\n",
    "\n",
    "    z = z.fillna(0.0)\n",
    "    z_pos = np.maximum(z.to_numpy(dtype=np.float32, copy=False), 0.0).astype(np.float32)\n",
    "    df[\"log1pZ\"] = np.log1p(z_pos).astype(np.float32)\n",
    "\n",
    "    # Z_err relative\n",
    "    if \"Z_err\" in df.columns:\n",
    "        zerr = pd.to_numeric(df[\"Z_err\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float32, copy=False)\n",
    "    elif \"Z_err_clip\" in df.columns:\n",
    "        zerr = pd.to_numeric(df[\"Z_err_clip\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=np.float32, copy=False)\n",
    "    else:\n",
    "        zerr = np.zeros((len(df),), dtype=np.float32)\n",
    "\n",
    "    denom = np.maximum(z_pos, np.float32(1e-3))\n",
    "    df[\"zerr_rel\"] = (np.asarray(zerr, dtype=np.float32) / denom).astype(np.float32)\n",
    "\n",
    "    # is_photoz\n",
    "    if \"is_photoz\" in df.columns:\n",
    "        ip = pd.to_numeric(df[\"is_photoz\"], errors=\"coerce\").fillna(0).astype(np.int8)\n",
    "    elif \"photoz\" in df.columns:\n",
    "        ip = pd.to_numeric(df[\"photoz\"], errors=\"coerce\").notna().astype(np.int8)\n",
    "    else:\n",
    "        ip = pd.Series(0, index=df.index, dtype=np.int8)\n",
    "    df[\"is_photoz\"] = ip.astype(np.int8)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Locate STAGE 4 output\n",
    "# ----------------------------\n",
    "manifest_csv = _find_stage4_manifest(ART_DIR)\n",
    "if manifest_csv is None:\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    runs = sorted([p.name for p in root.glob(\"run_*\") if p.is_dir()])[-15:] if root.exists() else []\n",
    "    raise RuntimeError(\n",
    "        \"Output STAGE 4 (lc_clean_mag_manifest.csv) tidak ditemukan.\\n\"\n",
    "        f\"- ART_DIR saat ini: {ART_DIR}\\n\"\n",
    "        f\"- Expected: {ART_DIR/'lc_clean_mag'}\\n\"\n",
    "        f\"- Runs available (last 15): {runs}\\n\"\n",
    "        \"Solusi: pastikan STAGE 4 selesai dan menulis artifacts/lc_clean_mag.\"\n",
    "    )\n",
    "\n",
    "RUN_DIR, ART_DIR, LC_CLEAN_DIR = _sync_dirs_from_manifest(manifest_csv)\n",
    "ART_DIR = Path(ART_DIR); LC_CLEAN_DIR = Path(LC_CLEAN_DIR)\n",
    "\n",
    "print(\"STAGE 5 ROUTING SYNC OK\")\n",
    "print(f\"- RUN_DIR      : {RUN_DIR}\")\n",
    "print(f\"- ART_DIR      : {ART_DIR}\")\n",
    "print(f\"- LC_CLEAN_DIR : {LC_CLEAN_DIR}\")\n",
    "print(f\"- manifest_csv : {manifest_csv}\")\n",
    "\n",
    "reloaded, msg = _load_meta_if_needed(ART_DIR)\n",
    "print(f\"- meta_sync    : {msg}\")\n",
    "\n",
    "# normalize meta index\n",
    "df_train_meta = df_train_meta.copy(deep=False)\n",
    "df_test_meta  = df_test_meta.copy(deep=False)\n",
    "df_train_meta.index = df_train_meta.index.astype(\"string\")\n",
    "df_test_meta.index  = df_test_meta.index.astype(\"string\")\n",
    "\n",
    "# ensure meta derived features exist\n",
    "df_train_meta = _ensure_meta_features(df_train_meta)\n",
    "df_test_meta  = _ensure_meta_features(df_test_meta)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load & validate Stage4 manifest\n",
    "# ----------------------------\n",
    "_df_clean_manifest = pd.read_csv(manifest_csv)\n",
    "_df_clean_manifest.columns = [c.strip() for c in _df_clean_manifest.columns]\n",
    "\n",
    "need_cols = {\"split\", \"which\", \"part\", \"path\"}\n",
    "miss = sorted(list(need_cols - set(_df_clean_manifest.columns)))\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Manifest STAGE 4 missing columns: {miss} | cols={list(_df_clean_manifest.columns)}\")\n",
    "\n",
    "paths = _df_clean_manifest[\"path\"].astype(str).tolist()\n",
    "missing_paths = [p for p in paths if not Path(p).exists()]\n",
    "if missing_paths:\n",
    "    raise RuntimeError(\n",
    "        \"Ada file part STAGE 4 yang hilang.\\n\"\n",
    "        f\"Missing count={len(missing_paths)} | contoh={missing_paths[:10]}\\n\"\n",
    "        \"Solusi: rerun STAGE 4 (wipe_all) untuk regenerasi cache.\"\n",
    "    )\n",
    "\n",
    "def get_clean_parts(split_name: str, which: str):\n",
    "    m = _df_clean_manifest[(_df_clean_manifest[\"split\"] == split_name) & (_df_clean_manifest[\"which\"] == which)]\n",
    "    if m.empty:\n",
    "        return []\n",
    "    return m.sort_values(\"part\")[\"path\"].astype(str).tolist()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Recover SPLIT_LIST + routing ids\n",
    "# ----------------------------\n",
    "if \"split\" not in df_train_meta.columns or \"split\" not in df_test_meta.columns:\n",
    "    raise RuntimeError(\"Kolom `split` tidak ada di meta. Pastikan STAGE 2/3 membuat routing split di meta.\")\n",
    "\n",
    "splits_meta = sorted(set(df_train_meta[\"split\"].astype(str).tolist()) | set(df_test_meta[\"split\"].astype(str).tolist()))\n",
    "splits_in_manifest = sorted(set(_df_clean_manifest[\"split\"].astype(str).tolist()))\n",
    "SPLIT_LIST = sorted(set(splits_in_manifest) | set(splits_meta))\n",
    "SPLITS_TO_CONSIDER = [s for s in SPLIT_LIST if s in splits_in_manifest]\n",
    "\n",
    "train_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "for oid, sp in df_train_meta[\"split\"].astype(str).items():\n",
    "    if sp in train_ids_by_split:\n",
    "        train_ids_by_split[sp].append(str(oid))\n",
    "\n",
    "test_ids_by_split = {s: [] for s in SPLIT_LIST}\n",
    "for oid, sp in df_test_meta[\"split\"].astype(str).items():\n",
    "    if sp in test_ids_by_split:\n",
    "        test_ids_by_split[sp].append(str(oid))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Settings (recommended defaults)\n",
    "# ----------------------------\n",
    "ONLY_SPLITS = CFG.get(\"STAGE5_ONLY_SPLITS\", None)\n",
    "\n",
    "REBUILD_MODE = str(CFG.get(\"STAGE5_REBUILD_MODE\", \"wipe_all\")).lower()   # \"wipe_all\" or \"reuse_if_exists\"\n",
    "if REBUILD_MODE not in (\"wipe_all\", \"reuse_if_exists\"):\n",
    "    REBUILD_MODE = \"wipe_all\"\n",
    "\n",
    "COMPRESS_NPZ = bool(CFG.get(\"STAGE5_COMPRESS_NPZ\", False))\n",
    "SHARD_MAX_OBJECTS = int(CFG.get(\"STAGE5_SHARD_MAX_OBJECTS\", 1500))\n",
    "\n",
    "SNR_TANH_SCALE = float(CFG.get(\"STAGE5_SNR_TANH_SCALE\", 10.0))\n",
    "TIME_CLIP_MAX_DAYS = CFG.get(\"STAGE5_TIME_CLIP_MAX_DAYS\", None)\n",
    "TIME_CLIP_MAX_DAYS = None if TIME_CLIP_MAX_DAYS in [None, \"None\", \"none\", \"\"] else float(TIME_CLIP_MAX_DAYS)\n",
    "DROP_BAD_TIME_ROWS = bool(CFG.get(\"STAGE5_DROP_BAD_TIME_ROWS\", True))\n",
    "\n",
    "L_MAX = int(CFG.get(\"L_MAX\", 256))\n",
    "TRUNC_POLICY = str(CFG.get(\"TRUNC_POLICY\", \"smart\")).lower()      # smart/head/none\n",
    "KEEP_DET_FRAC = float(CFG.get(\"KEEP_DET_FRAC\", 0.70))\n",
    "KEEP_EDGE = bool(CFG.get(\"KEEP_EDGE\", True))\n",
    "USE_RESTFRAME_TIME = bool(CFG.get(\"USE_RESTFRAME_TIME\", True))\n",
    "\n",
    "NUM_BUCKETS = int(CFG.get(\"SEQ_NUM_BUCKETS\", 256))\n",
    "\n",
    "# FORCE ASINH\n",
    "TOKEN_MODE_FORCE = \"asinh\"\n",
    "FEATURE_SET = str(CFG.get(\"SEQ_FEATURE_SET\", \"v2\")).lower()   # v2 recommended\n",
    "ADD_OBJ_META_PER_TOKEN = bool(CFG.get(\"SEQ_ADD_META_PER_TOKEN\", True))\n",
    "\n",
    "OBJ_META_COLS = [\"EBV_clip\", \"log1pZ\", \"zerr_rel\", \"is_photoz\"]\n",
    "\n",
    "SEQ_DIR = Path(ART_DIR) / \"seq_tokens\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if FEATURE_SET == \"v2\":\n",
    "    CORE_FEATURES = [\"t_rel_log\", \"dt_log\", \"dt_band_log\", \"signal\", \"err_log\", \"snr_tanh\", \"detected\", \"band_change\", \"delta_signal\"]\n",
    "else:\n",
    "    CORE_FEATURES = [\"t_rel_log\", \"dt_log\", \"signal\", \"err_log\", \"snr_tanh\", \"detected\"]\n",
    "\n",
    "META_FEATURES = [f\"meta_{c}\" for c in OBJ_META_COLS] if ADD_OBJ_META_PER_TOKEN else []\n",
    "FEATURE_NAMES = CORE_FEATURES + META_FEATURES\n",
    "FEATURE_DIM = len(FEATURE_NAMES)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Reader for cleaned parts (schema auto; force-asinh)\n",
    "# ----------------------------\n",
    "BASE_COLS_MIN = {\"object_id\", \"mjd\", \"band_id\", \"snr\"}\n",
    "\n",
    "def _read_clean_part(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Clean part missing: {p}\")\n",
    "\n",
    "    if p.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    elif p.name.endswith(\".csv.gz\"):\n",
    "        df = pd.read_csv(p, compression=\"gzip\")\n",
    "    else:\n",
    "        df = pd.read_csv(p)\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if not BASE_COLS_MIN.issubset(cols):\n",
    "        raise RuntimeError(f\"Clean part missing base cols {sorted(list(BASE_COLS_MIN - cols))} | file={p}\")\n",
    "\n",
    "    # unify detection column -> detected (int8)\n",
    "    if \"detected\" in cols:\n",
    "        df[\"detected\"] = pd.to_numeric(df[\"detected\"], errors=\"coerce\").fillna(0).astype(np.int8)\n",
    "    elif \"detected_pos\" in cols:\n",
    "        df[\"detected\"] = pd.to_numeric(df[\"detected_pos\"], errors=\"coerce\").fillna(0).astype(np.int8)\n",
    "    else:\n",
    "        df[\"detected\"] = (pd.to_numeric(df[\"snr\"], errors=\"coerce\").fillna(0.0).astype(np.float32) > 0).astype(np.int8)\n",
    "\n",
    "    # FORCE ASINH signal/err\n",
    "    if (\"asinh_mag\" in cols) and (\"asinh_mag_err\" in cols):\n",
    "        df[\"signal\"] = pd.to_numeric(df[\"asinh_mag\"], errors=\"coerce\").astype(np.float32)\n",
    "        err_lin = pd.to_numeric(df[\"asinh_mag_err\"], errors=\"coerce\").astype(np.float32).to_numpy(copy=False)\n",
    "        err_lin = np.nan_to_num(err_lin, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        err_lin = np.maximum(err_lin, 0.0).astype(np.float32)\n",
    "        df[\"err_log\"] = np.log1p(err_lin).astype(np.float32)\n",
    "    elif (\"flux_asinh\" in cols) and (\"err_log1p\" in cols):\n",
    "        df[\"signal\"] = pd.to_numeric(df[\"flux_asinh\"], errors=\"coerce\").astype(np.float32)\n",
    "        df[\"err_log\"] = pd.to_numeric(df[\"err_log1p\"], errors=\"coerce\").astype(np.float32)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"FORCE ASINH gagal: tidak menemukan pasangan kolom asinh_mag/asinh_mag_err \"\n",
    "            \"atau flux_asinh/err_log1p.\\n\"\n",
    "            f\"Found cols sample={list(df.columns)[:40]} | file={p}\"\n",
    "        )\n",
    "\n",
    "    df[\"object_id\"] = _safe_string_series(df[\"object_id\"])\n",
    "    df[\"mjd\"] = pd.to_numeric(df[\"mjd\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"band_id\"] = pd.to_numeric(df[\"band_id\"], errors=\"coerce\").astype(np.int16)\n",
    "    df[\"snr\"] = pd.to_numeric(df[\"snr\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"signal\"] = pd.to_numeric(df[\"signal\"], errors=\"coerce\").astype(np.float32)\n",
    "    df[\"err_log\"] = pd.to_numeric(df[\"err_log\"], errors=\"coerce\").astype(np.float32)\n",
    "\n",
    "    # band sanity -> 0..5\n",
    "    b = df[\"band_id\"].to_numpy(copy=False)\n",
    "    okb = (b >= 0) & (b <= 5)\n",
    "    if not np.all(okb):\n",
    "        df = df[okb]\n",
    "\n",
    "    if DROP_BAD_TIME_ROWS:\n",
    "        df = df[np.isfinite(df[\"mjd\"].to_numpy(copy=False))]\n",
    "\n",
    "    keep = [\"object_id\", \"mjd\", \"band_id\", \"snr\", \"detected\", \"signal\", \"err_log\"]\n",
    "    return df[keep]\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Truncation (smart)\n",
    "# ----------------------------\n",
    "def _smart_truncate(mjd, det, snr, Lmax: int):\n",
    "    n = len(mjd)\n",
    "    if n <= Lmax:\n",
    "        return np.arange(n, dtype=np.int64)\n",
    "\n",
    "    idx_all = np.arange(n, dtype=np.int64)\n",
    "    keep = set()\n",
    "    if KEEP_EDGE and n >= 2:\n",
    "        keep.add(0); keep.add(n - 1)\n",
    "\n",
    "    det_idx = idx_all[det.astype(bool)]\n",
    "    k_det = int(max(0, min(len(det_idx), int(np.floor(Lmax * KEEP_DET_FRAC)))))\n",
    "    if k_det > 0 and len(det_idx) > 0:\n",
    "        score = np.abs(snr[det_idx])\n",
    "        top = det_idx[np.argsort(-score)[:k_det]]\n",
    "        for i in top.tolist():\n",
    "            keep.add(int(i))\n",
    "\n",
    "    if len(keep) < Lmax:\n",
    "        rem = [i for i in idx_all.tolist() if i not in keep]\n",
    "        need = Lmax - len(keep)\n",
    "        if rem and need > 0:\n",
    "            pick = np.linspace(0, len(rem) - 1, num=need, dtype=int)\n",
    "            for p in pick.tolist():\n",
    "                keep.add(int(rem[p]))\n",
    "\n",
    "    out = np.array(sorted(keep), dtype=np.int64)\n",
    "    if len(out) > Lmax:\n",
    "        pos = np.linspace(0, len(out) - 1, num=Lmax, dtype=int)\n",
    "        out = out[pos]\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Meta per-token helper\n",
    "# ----------------------------\n",
    "def _get_obj_meta_vec(meta_df: pd.DataFrame, oid: str) -> np.ndarray:\n",
    "    if (not ADD_OBJ_META_PER_TOKEN) or (oid not in meta_df.index):\n",
    "        return np.zeros((len(OBJ_META_COLS),), dtype=np.float32)\n",
    "    vals = []\n",
    "    for c in OBJ_META_COLS:\n",
    "        if c in meta_df.columns:\n",
    "            v = meta_df.loc[oid, c]\n",
    "            v = float(v) if (v is not None and np.isfinite(v)) else 0.0\n",
    "        else:\n",
    "            v = 0.0\n",
    "        vals.append(v)\n",
    "    return np.asarray(vals, dtype=np.float32)\n",
    "\n",
    "def build_empty_tokens():\n",
    "    X = np.zeros((1, int(FEATURE_DIM)), dtype=np.float32)\n",
    "    B = np.full((1,), -1, dtype=np.int8)\n",
    "    return X, B, 0, 1\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Build tokens per object (v2)\n",
    "# ----------------------------\n",
    "def build_object_tokens(df_obj: pd.DataFrame, meta_df: pd.DataFrame, z_val: float = 0.0):\n",
    "    if df_obj is None or df_obj.empty:\n",
    "        return build_empty_tokens()\n",
    "\n",
    "    mjd = df_obj[\"mjd\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    band = df_obj[\"band_id\"].to_numpy(dtype=np.int16, copy=False)\n",
    "    snr  = df_obj[\"snr\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    det  = df_obj[\"detected\"].to_numpy(dtype=np.int8, copy=False)\n",
    "    sig  = df_obj[\"signal\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    err_log = df_obj[\"err_log\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    order = np.lexsort((band, mjd))\n",
    "    mjd = mjd[order]; band = band[order]; snr = snr[order]; det = det[order]\n",
    "    sig = sig[order]; err_log = err_log[order]\n",
    "\n",
    "    z = float(z_val) if (z_val is not None and np.isfinite(z_val)) else 0.0\n",
    "    z = max(z, 0.0)\n",
    "    denom = (1.0 + z) if USE_RESTFRAME_TIME else 1.0\n",
    "\n",
    "    t0 = float(mjd[0])\n",
    "    t_rel = ((mjd - np.float32(t0)) / np.float32(denom)).astype(np.float32)\n",
    "    if TIME_CLIP_MAX_DAYS is not None:\n",
    "        mx = np.float32(TIME_CLIP_MAX_DAYS)\n",
    "        t_rel = np.clip(t_rel, 0.0, mx)\n",
    "\n",
    "    dt = np.empty_like(t_rel, dtype=np.float32)\n",
    "    dt[0] = np.float32(0.0)\n",
    "    if len(t_rel) > 1:\n",
    "        dt[1:] = np.maximum(t_rel[1:] - t_rel[:-1], 0.0).astype(np.float32)\n",
    "\n",
    "    dt_band = np.zeros_like(dt, dtype=np.float32)\n",
    "    last_mjd = {}\n",
    "    for i in range(len(mjd)):\n",
    "        b = int(band[i])\n",
    "        if b in last_mjd:\n",
    "            dt_band[i] = max(float((mjd[i] - last_mjd[b]) / denom), 0.0)\n",
    "        last_mjd[b] = float(mjd[i])\n",
    "    if TIME_CLIP_MAX_DAYS is not None:\n",
    "        mx = np.float32(TIME_CLIP_MAX_DAYS)\n",
    "        dt = np.clip(dt, 0.0, mx)\n",
    "        dt_band = np.clip(dt_band, 0.0, mx)\n",
    "\n",
    "    t_rel_log = np.log1p(t_rel).astype(np.float32)\n",
    "    dt_log = np.log1p(dt).astype(np.float32)\n",
    "    dt_band_log = np.log1p(dt_band).astype(np.float32)\n",
    "\n",
    "    snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    snr_tanh = np.tanh(snr / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n",
    "    det_f = det.astype(np.float32)\n",
    "\n",
    "    band_change = np.zeros((len(band),), dtype=np.float32)\n",
    "    delta_signal = np.zeros((len(sig),), dtype=np.float32)\n",
    "    if len(band) > 1:\n",
    "        band_change[1:] = (band[1:] != band[:-1]).astype(np.float32)\n",
    "        delta_signal[1:] = (sig[1:] - sig[:-1]).astype(np.float32)\n",
    "\n",
    "    if FEATURE_SET == \"v2\":\n",
    "        X = np.stack(\n",
    "            [t_rel_log, dt_log, dt_band_log, sig, err_log, snr_tanh, det_f, band_change, delta_signal],\n",
    "            axis=1\n",
    "        ).astype(np.float32)\n",
    "    else:\n",
    "        X = np.stack([t_rel_log, dt_log, sig, err_log, snr_tanh, det_f], axis=1).astype(np.float32)\n",
    "\n",
    "    if ADD_OBJ_META_PER_TOKEN:\n",
    "        oid = str(df_obj[\"object_id\"].iloc[0])\n",
    "        mv = _get_obj_meta_vec(meta_df, oid)\n",
    "        mv_rep = np.repeat(mv[None, :], repeats=X.shape[0], axis=0).astype(np.float32)\n",
    "        X = np.concatenate([X, mv_rep], axis=1).astype(np.float32)\n",
    "\n",
    "    L0 = int(X.shape[0])\n",
    "\n",
    "    if L_MAX and int(L_MAX) > 0 and X.shape[0] > int(L_MAX):\n",
    "        if TRUNC_POLICY == \"smart\":\n",
    "            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n",
    "        elif TRUNC_POLICY == \"head\":\n",
    "            keep = np.arange(int(L_MAX), dtype=np.int64)\n",
    "        elif TRUNC_POLICY in (\"none\", \"full\"):\n",
    "            keep = np.arange(X.shape[0], dtype=np.int64)\n",
    "        else:\n",
    "            keep = _smart_truncate(mjd, det, snr, int(L_MAX))\n",
    "\n",
    "        if len(keep) != X.shape[0]:\n",
    "            keep = keep.astype(np.int64)\n",
    "\n",
    "            mjd2 = mjd[keep]\n",
    "            band2 = band[keep].astype(np.int16)\n",
    "            snr2 = snr[keep].astype(np.float32)\n",
    "            det2 = det[keep].astype(np.int8)\n",
    "            sig2 = sig[keep].astype(np.float32)\n",
    "            err_log2 = err_log[keep].astype(np.float32)\n",
    "\n",
    "            t0 = float(mjd2[0])\n",
    "            t_rel2 = ((mjd2 - np.float32(t0)) / np.float32(denom)).astype(np.float32)\n",
    "            if TIME_CLIP_MAX_DAYS is not None:\n",
    "                mx = np.float32(TIME_CLIP_MAX_DAYS)\n",
    "                t_rel2 = np.clip(t_rel2, 0.0, mx)\n",
    "\n",
    "            dt2 = np.empty_like(t_rel2, dtype=np.float32)\n",
    "            dt2[0] = np.float32(0.0)\n",
    "            if len(t_rel2) > 1:\n",
    "                dt2[1:] = np.maximum(t_rel2[1:] - t_rel2[:-1], 0.0).astype(np.float32)\n",
    "\n",
    "            dtb2 = np.zeros_like(dt2, dtype=np.float32)\n",
    "            last_mjd2 = {}\n",
    "            for i in range(len(mjd2)):\n",
    "                b = int(band2[i])\n",
    "                if b in last_mjd2:\n",
    "                    dtb2[i] = max(float((mjd2[i] - last_mjd2[b]) / denom), 0.0)\n",
    "                last_mjd2[b] = float(mjd2[i])\n",
    "\n",
    "            if TIME_CLIP_MAX_DAYS is not None:\n",
    "                mx = np.float32(TIME_CLIP_MAX_DAYS)\n",
    "                dt2 = np.clip(dt2, 0.0, mx)\n",
    "                dtb2 = np.clip(dtb2, 0.0, mx)\n",
    "\n",
    "            t_rel_log2 = np.log1p(t_rel2).astype(np.float32)\n",
    "            dt_log2 = np.log1p(dt2).astype(np.float32)\n",
    "            dtb_log2 = np.log1p(dtb2).astype(np.float32)\n",
    "\n",
    "            snr_tanh2 = np.tanh(snr2 / np.float32(SNR_TANH_SCALE)).astype(np.float32)\n",
    "            det_f2 = det2.astype(np.float32)\n",
    "\n",
    "            band_change2 = np.zeros((len(band2),), dtype=np.float32)\n",
    "            delta_signal2 = np.zeros((len(sig2),), dtype=np.float32)\n",
    "            if len(band2) > 1:\n",
    "                band_change2[1:] = (band2[1:] != band2[:-1]).astype(np.float32)\n",
    "                delta_signal2[1:] = (sig2[1:] - sig2[:-1]).astype(np.float32)\n",
    "\n",
    "            if FEATURE_SET == \"v2\":\n",
    "                Xcore2 = np.stack(\n",
    "                    [t_rel_log2, dt_log2, dtb_log2, sig2, err_log2, snr_tanh2, det_f2, band_change2, delta_signal2],\n",
    "                    axis=1\n",
    "                ).astype(np.float32)\n",
    "            else:\n",
    "                Xcore2 = np.stack([t_rel_log2, dt_log2, sig2, err_log2, snr_tanh2, det_f2], axis=1).astype(np.float32)\n",
    "\n",
    "            if ADD_OBJ_META_PER_TOKEN:\n",
    "                oid = str(df_obj[\"object_id\"].iloc[0])\n",
    "                mv = _get_obj_meta_vec(meta_df, oid)\n",
    "                mv_rep = np.repeat(mv[None, :], repeats=Xcore2.shape[0], axis=0).astype(np.float32)\n",
    "                X2 = np.concatenate([Xcore2, mv_rep], axis=1).astype(np.float32)\n",
    "            else:\n",
    "                X2 = Xcore2\n",
    "\n",
    "            return X2, band2.astype(np.int8), L0, int(X2.shape[0])\n",
    "\n",
    "    return X, band.astype(np.int8), L0, int(X.shape[0])\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Shard writer + reuse manifest\n",
    "# ----------------------------\n",
    "def save_shard(out_path: Path, object_ids, X_concat, B_concat, offsets):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    obj_arr = np.asarray(object_ids, dtype=\"S\")\n",
    "    if COMPRESS_NPZ:\n",
    "        np.savez_compressed(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "    else:\n",
    "        np.savez(out_path, object_id=obj_arr, x=X_concat, band=B_concat, offsets=offsets)\n",
    "\n",
    "def reconstruct_manifest_from_shards(split_name: str, which: str, out_dir: Path):\n",
    "    rows = []\n",
    "    for sp in sorted(out_dir.glob(\"shard_*.npz\")):\n",
    "        data = np.load(sp, allow_pickle=False)\n",
    "        obj = data[\"object_id\"]\n",
    "        offsets = data[\"offsets\"].astype(np.int64)\n",
    "        if offsets.ndim != 1 or offsets.size < 2:\n",
    "            continue\n",
    "        if len(obj) != (offsets.size - 1):\n",
    "            raise RuntimeError(f\"Bad shard (object_id len != offsets-1): {sp}\")\n",
    "        lengths = offsets[1:] - offsets[:-1]\n",
    "        for i in range(len(lengths)):\n",
    "            oid = obj[i]\n",
    "            oid = oid.decode(\"utf-8\", errors=\"ignore\") if isinstance(oid, (bytes, np.bytes_)) else str(oid)\n",
    "            rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(sp),\n",
    "                \"start\": int(offsets[i]),\n",
    "                \"length\": int(lengths[i]),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Bucket builder (IO hemat)\n",
    "# ----------------------------\n",
    "def build_sequences_bucket(split_name: str, which: str, expected_ids: set, out_dir: Path, num_buckets: int = 256):\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"pyarrow tidak tersedia. Di Kaggle biasanya ada.\") from e\n",
    "\n",
    "    parts = get_clean_parts(split_name, which)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"Tidak ada cleaned parts untuk {split_name}/{which}. Cek STAGE 4 output.\")\n",
    "\n",
    "    tmp_dir = Path(ART_DIR) / \"tmp_seq_buckets\" / split_name / which\n",
    "    if tmp_dir.exists():\n",
    "        shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    writers = {}\n",
    "    kept_rows = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    def bucket_idx(series_objid: pd.Series) -> np.ndarray:\n",
    "        h = pd.util.hash_pandas_object(series_objid, index=False).to_numpy(dtype=np.uint64, copy=False)\n",
    "        return (h % np.uint64(num_buckets)).astype(np.int16)\n",
    "\n",
    "    try:\n",
    "        for p in parts:\n",
    "            df = _read_clean_part(p)\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            df = df[df[\"object_id\"].isin(expected_ids)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            kept_rows += int(len(df))\n",
    "            bidx = bucket_idx(df[\"object_id\"])\n",
    "            df[\"_b\"] = bidx\n",
    "\n",
    "            for b, sub in df.groupby(\"_b\", sort=False):\n",
    "                sub = sub.drop(columns=[\"_b\"])\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                fp = tmp_dir / f\"bucket_{int(b):03d}.parquet\"\n",
    "                table = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "                if int(b) not in writers:\n",
    "                    writers[int(b)] = pq.ParquetWriter(fp, table.schema, compression=\"snappy\")\n",
    "                writers[int(b)].write_table(table)\n",
    "\n",
    "            del df\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        for w in list(writers.values()):\n",
    "            try: w.close()\n",
    "            except Exception: pass\n",
    "\n",
    "    meta = df_train_meta if which == \"train\" else df_test_meta\n",
    "\n",
    "    manifest_rows = []\n",
    "    shard_idx = 0\n",
    "    batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n",
    "    built_ids = set()\n",
    "    len_before, len_after = [], []\n",
    "\n",
    "    def flush_shard_local():\n",
    "        nonlocal shard_idx, batch_obj_ids, batch_X, batch_B, batch_len, manifest_rows\n",
    "        if not batch_obj_ids:\n",
    "            return\n",
    "        lengths = np.asarray(batch_len, dtype=np.int64)\n",
    "        offsets = np.zeros(len(lengths) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(lengths)\n",
    "\n",
    "        Xc = np.concatenate(batch_X, axis=0).astype(np.float32)\n",
    "        Bc = np.concatenate(batch_B, axis=0).astype(np.int8)\n",
    "\n",
    "        shard_path = out_dir / f\"shard_{shard_idx:04d}.npz\"\n",
    "        save_shard(shard_path, batch_obj_ids, Xc, Bc, offsets)\n",
    "\n",
    "        for i, oid in enumerate(batch_obj_ids):\n",
    "            manifest_rows.append({\n",
    "                \"object_id\": oid,\n",
    "                \"split\": split_name,\n",
    "                \"which\": which,\n",
    "                \"shard\": str(shard_path),\n",
    "                \"start\": int(offsets[i]),\n",
    "                \"length\": int(lengths[i]),\n",
    "            })\n",
    "\n",
    "        shard_idx += 1\n",
    "        batch_obj_ids, batch_X, batch_B, batch_len = [], [], [], []\n",
    "        gc.collect()\n",
    "\n",
    "    for bf in sorted(tmp_dir.glob(\"bucket_*.parquet\")):\n",
    "        dfb = pd.read_parquet(bf)\n",
    "        if dfb.empty:\n",
    "            continue\n",
    "\n",
    "        for oid, g in dfb.groupby(\"object_id\", sort=False):\n",
    "            oid = str(oid)\n",
    "            if oid in built_ids:\n",
    "                continue\n",
    "\n",
    "            z_val = 0.0\n",
    "            if USE_RESTFRAME_TIME and (oid in meta.index):\n",
    "                if \"Z\" in meta.columns:\n",
    "                    z_val = float(meta.loc[oid, \"Z\"])\n",
    "                elif \"Z_clip\" in meta.columns:\n",
    "                    z_val = float(meta.loc[oid, \"Z_clip\"])\n",
    "                elif \"photoz\" in meta.columns:\n",
    "                    z_val = float(meta.loc[oid, \"photoz\"])\n",
    "\n",
    "            X, B, lb, la = build_object_tokens(g, meta_df=meta, z_val=z_val)\n",
    "            len_before.append(lb); len_after.append(la)\n",
    "\n",
    "            batch_obj_ids.append(oid)\n",
    "            batch_X.append(X)\n",
    "            batch_B.append(B)\n",
    "            batch_len.append(int(X.shape[0]))\n",
    "            built_ids.add(oid)\n",
    "\n",
    "            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                flush_shard_local()\n",
    "\n",
    "        del dfb\n",
    "        gc.collect()\n",
    "\n",
    "    # fill missing objects with empty token\n",
    "    missing_ids = list(expected_ids - built_ids)\n",
    "    if missing_ids:\n",
    "        for oid in missing_ids:\n",
    "            oid = str(oid)\n",
    "            X, B, lb, la = build_empty_tokens()\n",
    "            len_before.append(lb); len_after.append(la)\n",
    "\n",
    "            batch_obj_ids.append(oid)\n",
    "            batch_X.append(X)\n",
    "            batch_B.append(B)\n",
    "            batch_len.append(int(X.shape[0]))\n",
    "            built_ids.add(oid)\n",
    "\n",
    "            if len(batch_obj_ids) >= SHARD_MAX_OBJECTS:\n",
    "                flush_shard_local()\n",
    "\n",
    "    flush_shard_local()\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "    st = {\n",
    "        \"kept_rows\": int(kept_rows),\n",
    "        \"built_objects\": int(len(built_ids)),\n",
    "        \"missing_filled\": int(len(missing_ids)),\n",
    "        \"len_before_mean\": float(np.mean(len_before)) if len_before else 0.0,\n",
    "        \"len_before_p95\": float(np.quantile(len_before, 0.95)) if len_before else 0.0,\n",
    "        \"len_after_mean\": float(np.mean(len_after)) if len_after else 0.0,\n",
    "        \"len_after_p95\": float(np.quantile(len_after, 0.95)) if len_after else 0.0,\n",
    "        \"truncated_frac\": float(np.mean([a < b for a, b in zip(len_after, len_before)])) if len_before else 0.0,\n",
    "        \"time_s\": float(time.time() - t0),\n",
    "    }\n",
    "    return manifest_rows, st\n",
    "\n",
    "# ----------------------------\n",
    "# 12) RUN\n",
    "# ----------------------------\n",
    "splits_to_run = ONLY_SPLITS if (ONLY_SPLITS is not None) else SPLITS_TO_CONSIDER\n",
    "splits_to_run = list(splits_to_run)\n",
    "\n",
    "print(\"\\n[Stage 5] SETTINGS\")\n",
    "print(f\"- TOKEN_MODE_FORCE: {TOKEN_MODE_FORCE}\")\n",
    "print(f\"- FEATURE_SET     : {FEATURE_SET} | dim={FEATURE_DIM}\")\n",
    "print(f\"- ADD_META        : {ADD_OBJ_META_PER_TOKEN} | meta_cols={OBJ_META_COLS}\")\n",
    "print(f\"- L_MAX/TRUNC     : {L_MAX} / {TRUNC_POLICY} (KEEP_DET_FRAC={KEEP_DET_FRAC})\")\n",
    "print(f\"- NUM_BUCKETS     : {NUM_BUCKETS} | SHARD_MAX_OBJECTS={SHARD_MAX_OBJECTS}\")\n",
    "print(f\"- REBUILD_MODE    : {REBUILD_MODE} | COMPRESS_NPZ={COMPRESS_NPZ}\")\n",
    "\n",
    "all_manifest_train, all_manifest_test, split_run_stats = [], [], []\n",
    "\n",
    "def expected_set_for(split_name: str, which: str) -> set:\n",
    "    return set(train_ids_by_split.get(split_name, [])) if which == \"train\" else set(test_ids_by_split.get(split_name, []))\n",
    "\n",
    "for split_name in splits_to_run:\n",
    "    for which in [\"train\", \"test\"]:\n",
    "        out_dir = SEQ_DIR / split_name / which\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        expected_ids = expected_set_for(split_name, which)\n",
    "        if len(expected_ids) == 0:\n",
    "            raise RuntimeError(f\"Expected ids empty for {split_name}/{which}.\")\n",
    "\n",
    "        shard_exists = any(out_dir.glob(\"shard_*.npz\"))\n",
    "        if REBUILD_MODE == \"reuse_if_exists\" and shard_exists:\n",
    "            print(f\"\\n[Stage 5] REUSE (exists): {split_name}/{which}\")\n",
    "            man_rows = reconstruct_manifest_from_shards(split_name, which, out_dir)\n",
    "            if not man_rows:\n",
    "                raise RuntimeError(f\"REUSE mode aktif tapi gagal rekonstruksi manifest: {out_dir}\")\n",
    "\n",
    "            got_ids = set([r[\"object_id\"] for r in man_rows])\n",
    "            miss_ids = expected_ids - got_ids\n",
    "            if miss_ids:\n",
    "                raise RuntimeError(\n",
    "                    f\"REUSE shard tidak cover semua expected ids untuk {split_name}/{which}. \"\n",
    "                    f\"missing={len(miss_ids)} (contoh={list(sorted(miss_ids))[:5]}). \"\n",
    "                    \"Solusi: set STAGE5_REBUILD_MODE='wipe_all' untuk rebuild bersih.\"\n",
    "                )\n",
    "\n",
    "            if which == \"train\":\n",
    "                all_manifest_train.extend(man_rows)\n",
    "            else:\n",
    "                all_manifest_test.extend(man_rows)\n",
    "\n",
    "            split_run_stats.append({\n",
    "                \"split\": split_name, \"which\": which,\n",
    "                \"kept_rows\": 0,\n",
    "                \"built_objects\": len(got_ids),\n",
    "                \"missing_filled\": 0,\n",
    "                \"len_before_mean\": 0.0,\n",
    "                \"len_before_p95\": 0.0,\n",
    "                \"len_after_mean\": 0.0,\n",
    "                \"len_after_p95\": 0.0,\n",
    "                \"truncated_frac\": 0.0,\n",
    "                \"time_s\": 0.0,\n",
    "            })\n",
    "            continue\n",
    "        else:\n",
    "            for f in out_dir.glob(\"shard_*.npz\"):\n",
    "                try: f.unlink()\n",
    "                except Exception: pass\n",
    "\n",
    "        print(f\"\\n[Stage 5] {split_name}/{which} | expected={len(expected_ids):,}\")\n",
    "\n",
    "        manifest_rows, st = build_sequences_bucket(\n",
    "            split_name=split_name,\n",
    "            which=which,\n",
    "            expected_ids=expected_ids,\n",
    "            out_dir=out_dir,\n",
    "            num_buckets=NUM_BUCKETS\n",
    "        )\n",
    "\n",
    "        print(f\"[Stage 5] OK: built={st['built_objects']:,} (missing_filled={st['missing_filled']:,}) | \"\n",
    "              f\"kept_rows={st['kept_rows']:,} | \"\n",
    "              f\"len_mean {st['len_before_mean']:.1f}->{st['len_after_mean']:.1f} | \"\n",
    "              f\"p95 {st['len_before_p95']:.1f}->{st['len_after_p95']:.1f} | \"\n",
    "              f\"trunc%={st['truncated_frac']*100:.1f}% | \"\n",
    "              f\"time={st['time_s']:.2f}s\")\n",
    "\n",
    "        split_run_stats.append({\"split\": split_name, \"which\": which, **st})\n",
    "\n",
    "        if which == \"train\":\n",
    "            all_manifest_train.extend(manifest_rows)\n",
    "        else:\n",
    "            all_manifest_test.extend(manifest_rows)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 13) Save manifests + stats + config\n",
    "# ----------------------------\n",
    "df_m_train = pd.DataFrame(all_manifest_train)\n",
    "df_m_test  = pd.DataFrame(all_manifest_test)\n",
    "\n",
    "if not df_m_train.empty:\n",
    "    df_m_train = df_m_train.sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n",
    "if not df_m_test.empty:\n",
    "    df_m_test = df_m_test.sort_values([\"split\", \"shard\", \"start\"]).reset_index(drop=True)\n",
    "\n",
    "mtrain_path = SEQ_DIR / \"seq_manifest_train.csv\"\n",
    "mtest_path  = SEQ_DIR / \"seq_manifest_test.csv\"\n",
    "df_m_train.to_csv(mtrain_path, index=False)\n",
    "df_m_test.to_csv(mtest_path, index=False)\n",
    "\n",
    "df_stats = pd.DataFrame(split_run_stats)\n",
    "stats_path = SEQ_DIR / \"seq_build_stats.csv\"\n",
    "df_stats.to_csv(stats_path, index=False)\n",
    "\n",
    "cfg_out = {\n",
    "    \"stage\": \"stage5\",\n",
    "    \"version\": \"v6.1\",\n",
    "    \"token_mode\": \"asinh\",\n",
    "    \"token_mode_force\": \"asinh\",\n",
    "    \"feature_set\": FEATURE_SET,\n",
    "    \"feature_names\": FEATURE_NAMES,\n",
    "    \"feature_dim\": int(FEATURE_DIM),\n",
    "    \"obj_meta_cols\": OBJ_META_COLS,\n",
    "    \"add_meta_per_token\": bool(ADD_OBJ_META_PER_TOKEN),\n",
    "    \"snr_tanh_scale\": float(SNR_TANH_SCALE),\n",
    "    \"time_clip_max_days\": None if TIME_CLIP_MAX_DAYS is None else float(TIME_CLIP_MAX_DAYS),\n",
    "    \"compress_npz\": bool(COMPRESS_NPZ),\n",
    "    \"shard_max_objects\": int(SHARD_MAX_OBJECTS),\n",
    "    \"num_buckets\": int(NUM_BUCKETS),\n",
    "    \"L_MAX\": int(L_MAX),\n",
    "    \"TRUNC_POLICY\": str(TRUNC_POLICY),\n",
    "    \"KEEP_DET_FRAC\": float(KEEP_DET_FRAC),\n",
    "    \"KEEP_EDGE\": bool(KEEP_EDGE),\n",
    "    \"USE_RESTFRAME_TIME\": bool(USE_RESTFRAME_TIME),\n",
    "    \"REBUILD_MODE\": str(REBUILD_MODE),\n",
    "    \"RUN_DIR_USED\": str(RUN_DIR),\n",
    "    \"ART_DIR_USED\": str(ART_DIR),\n",
    "    \"LC_CLEAN_DIR_USED\": str(LC_CLEAN_DIR),\n",
    "    \"manifest_csv\": str(manifest_csv),\n",
    "    \"stage4_schema_hint\": \"v7(asinh_mag/asinh_mag_err, detected_pos) or legacy(flux_asinh/err_log1p, detected)\",\n",
    "}\n",
    "cfg_path = SEQ_DIR / \"seq_config.json\"\n",
    "cfg_path.write_text(json.dumps(cfg_out, indent=2))\n",
    "\n",
    "print(\"\\n[Stage 5] DONE\")\n",
    "print(f\"- token_mode : asinh (forced)\")\n",
    "print(f\"- feature_set: {FEATURE_SET} | dim={FEATURE_DIM}\")\n",
    "print(f\"- Saved: {mtrain_path} (rows={len(df_m_train):,})\")\n",
    "print(f\"- Saved: {mtest_path}  (rows={len(df_m_test):,})\")\n",
    "print(f\"- Saved: {stats_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 14) Smoke test\n",
    "# ----------------------------\n",
    "def load_sequence(object_id: str, which: str):\n",
    "    object_id = str(object_id).strip()\n",
    "    m = df_m_train if which == \"train\" else df_m_test\n",
    "    row = m[m[\"object_id\"] == object_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"object_id not found in seq manifest ({which}): {object_id}\")\n",
    "    r = row.iloc[0]\n",
    "    data = np.load(r[\"shard\"], allow_pickle=False)\n",
    "    start = int(r[\"start\"]); length = int(r[\"length\"])\n",
    "    X = data[\"x\"][start:start+length]\n",
    "    B = data[\"band\"][start:start+length]\n",
    "    return X, B\n",
    "\n",
    "_smoke_oid = str(df_train_meta.index[0])\n",
    "X_sm, B_sm = load_sequence(_smoke_oid, \"train\")\n",
    "print(f\"\\n[Stage 5] Smoke test object_id={_smoke_oid}\")\n",
    "print(f\"- seq_len={len(X_sm)} | X_shape={X_sm.shape} | bands_unique={sorted(set(B_sm.tolist()))}\")\n",
    "\n",
    "globals().update({\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"LC_CLEAN_DIR\": LC_CLEAN_DIR,\n",
    "    \"SEQ_DIR\": SEQ_DIR,\n",
    "    \"seq_manifest_train\": df_m_train,\n",
    "    \"seq_manifest_test\": df_m_test,\n",
    "    \"SEQ_FEATURE_NAMES\": FEATURE_NAMES,\n",
    "    \"SEQ_FEATURE_DIM\": int(FEATURE_DIM),\n",
    "    \"SEQ_TOKEN_MODE\": \"asinh\",\n",
    "    \"get_clean_parts\": get_clean_parts,\n",
    "    \"load_sequence\": load_sequence,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59e6a4",
   "metadata": {
    "papermill": {
     "duration": 0.019417,
     "end_time": "2026-01-07T16:58:35.072418",
     "exception": false,
     "start_time": "2026-01-07T16:58:35.053001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Length Policy (Padding, Truncation, Windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942a8f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:58:35.114010Z",
     "iopub.status.busy": "2026-01-07T16:58:35.113637Z",
     "iopub.status.idle": "2026-01-07T16:58:37.325410Z",
     "shell.execute_reply": "2026-01-07T16:58:37.324378Z"
    },
    "papermill": {
     "duration": 2.236017,
     "end_time": "2026-01-07T16:58:37.327450",
     "exception": false,
     "start_time": "2026-01-07T16:58:35.091433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 6] Using SEQ_TOKENS_DIR: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/seq_tokens\n",
      "[Stage 6] Loaded manifests: train_rows=3,043 | test_rows=7,135\n",
      "[Stage 6] token_mode(prefer)=asinh | F=13\n",
      "[Stage 6] token_mode=asinh | score_value_feat=signal | snr_feat=snr_tanh | det_feat=detected\n",
      "\n",
      "TRAIN length stats\n",
      "- n_objects=3,043 | min=17 | p50=150 | p90=183 | p95=194 | p99=256 | max=256\n",
      "\n",
      "TEST length stats\n",
      "- n_objects=7,135 | min=18 | p50=152 | p90=183 | p95=193 | p99=256 | max=256\n",
      "\n",
      "[Stage 6] MAX_LEN=256 (based on p95=194)\n",
      "[Stage 6] Weights: W_SNR=1.0 | W_VAL=0.05 | W_DET=0.05 | SHIFT_BAND_IDS=True | DUMMY_TOKEN_FOR_EMPTY=True\n",
      "\n",
      "[Stage 6] Memmap X sizes approx: train=0.038 GB | test=0.088 GB | dtype=<class 'numpy.float32'>\n",
      "\n",
      "[Stage 6] Building fixed cache (TRAIN) from STAGE 5 manifests...\n",
      "[Stage 6] TRAIN filled=3,043/3,043 | dup=0 | empty=0 | dropped_bad=0 | time=0.36s\n",
      "\n",
      "[Stage 6] Building fixed cache (TEST) from STAGE 5 manifests...\n",
      "[Stage 6] TEST  filled=7,135/7,135 | dup=0 | empty=0 | dropped_bad=0 | time=0.87s\n",
      "\n",
      "[Stage 6] DONE\n",
      "- FIX_DIR: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/fixed_seq\n",
      "- Saved config: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/fixed_seq/length_policy_config.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 — Sequence Length Policy (Padding, Truncation, Windowing)\n",
    "# ONE CELL, Kaggle CPU-SAFE — REVISI FULL v3.1\n",
    "#\n",
    "# Upgrade v3.1:\n",
    "# - Tetap 100% kompatibel STAGE 5 v6+ (signal/err_log) & legacy\n",
    "# - Manifest safety: auto-filter ke expected ids, anti-duplicate object_id\n",
    "# - Target column discovery lebih robust (meta -> fallback train.csv jika PATHS ada)\n",
    "# - REUSE mode: strict file-set + strict coverage\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "for need in [\"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(\"Missing `ART_DIR`. Jalankan STAGE 0 dulu (setup run dir).\")\n",
    "\n",
    "ART_DIR = Path(ART_DIR)\n",
    "CFG = globals().get(\"CFG\", {})\n",
    "CFG = CFG if isinstance(CFG, dict) else {}\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "\n",
    "# ----------------------------\n",
    "# 0a) Locate STAGE 5 outputs (seq_tokens)\n",
    "# ----------------------------\n",
    "def _find_seq_tokens_dir(art_dir: Path) -> Path:\n",
    "    cand = art_dir / \"seq_tokens\"\n",
    "    if cand.exists() and cand.is_dir() and (cand / \"seq_manifest_train.csv\").exists() and (cand / \"seq_manifest_test.csv\").exists():\n",
    "        return cand\n",
    "    for p in art_dir.glob(\"*\"):\n",
    "        if p.is_dir() and (p / \"seq_manifest_train.csv\").exists() and (p / \"seq_manifest_test.csv\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find STAGE 5 seq_tokens directory under ART_DIR.\\n\"\n",
    "        f\"ART_DIR={art_dir}\\n\"\n",
    "        \"Expected: ART_DIR/seq_tokens/seq_manifest_train.csv and seq_manifest_test.csv\"\n",
    "    )\n",
    "\n",
    "SEQ_TOKENS_DIR = Path(globals().get(\"SEQ_TOKENS_DIR\", None) or _find_seq_tokens_dir(ART_DIR))\n",
    "\n",
    "p_mtr = SEQ_TOKENS_DIR / \"seq_manifest_train.csv\"\n",
    "p_mte = SEQ_TOKENS_DIR / \"seq_manifest_test.csv\"\n",
    "p_cfg = SEQ_TOKENS_DIR / \"seq_config.json\"\n",
    "\n",
    "if not p_mtr.exists(): raise FileNotFoundError(f\"Missing: {p_mtr}\")\n",
    "if not p_mte.exists(): raise FileNotFoundError(f\"Missing: {p_mte}\")\n",
    "if not p_cfg.exists(): raise FileNotFoundError(f\"Missing: {p_cfg}\")\n",
    "\n",
    "seq_manifest_train = pd.read_csv(p_mtr)\n",
    "seq_manifest_test  = pd.read_csv(p_mte)\n",
    "\n",
    "with open(p_cfg, \"r\", encoding=\"utf-8\") as f:\n",
    "    seq_cfg = json.load(f) if p_cfg.exists() else {}\n",
    "\n",
    "SEQ_FEATURE_NAMES = (\n",
    "    seq_cfg.get(\"feature_names\", None)\n",
    "    or seq_cfg.get(\"SEQ_FEATURE_NAMES\", None)\n",
    "    or globals().get(\"SEQ_FEATURE_NAMES\", None)\n",
    ")\n",
    "if SEQ_FEATURE_NAMES is None:\n",
    "    raise RuntimeError(\"SEQ_FEATURE_NAMES not found in seq_config.json and not present in globals.\")\n",
    "SEQ_FEATURE_NAMES = list(SEQ_FEATURE_NAMES)\n",
    "feat = {name: i for i, name in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "SEQ_TOKEN_MODE_IN = seq_cfg.get(\"token_mode\", None) or globals().get(\"SEQ_TOKEN_MODE\", None)\n",
    "\n",
    "print(f\"[Stage 6] Using SEQ_TOKENS_DIR: {SEQ_TOKENS_DIR}\")\n",
    "print(f\"[Stage 6] Loaded manifests: train_rows={len(seq_manifest_train):,} | test_rows={len(seq_manifest_test):,}\")\n",
    "print(f\"[Stage 6] token_mode(prefer)={SEQ_TOKEN_MODE_IN} | F={len(SEQ_FEATURE_NAMES)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _resolve_shard_path(x, base_dir: Path) -> str:\n",
    "    p = Path(str(x))\n",
    "    if p.exists():\n",
    "        return str(p)\n",
    "    p2 = base_dir / p\n",
    "    if p2.exists():\n",
    "        return str(p2)\n",
    "    p3 = base_dir / p.name\n",
    "    if p3.exists():\n",
    "        return str(p3)\n",
    "    return str(p)\n",
    "\n",
    "def _pick_first_existing(keys, d):\n",
    "    for k in keys:\n",
    "        if k in d:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def _pick_value_feat(feat_dict, prefer_mode=None):\n",
    "    value_exact = [\n",
    "        \"signal\", \"value\", \"flux\",\n",
    "        \"flux_asinh\", \"asinh_flux\", \"asinh_mag\",\n",
    "        \"flux_asinh_clip\", \"flux_asinh_norm\", \"flux_asinh_scaled\",\n",
    "    ]\n",
    "    mag_exact = [\"mag\", \"mag_norm\", \"mag_clip\", \"mag_scaled\"]\n",
    "\n",
    "    value_feat = _pick_first_existing(value_exact, feat_dict)\n",
    "    mag_feat   = _pick_first_existing(mag_exact, feat_dict)\n",
    "\n",
    "    if value_feat is None:\n",
    "        value_fuzzy = [k for k in feat_dict.keys() if any(t in k for t in [\"signal\", \"flux\", \"value\", \"asinh\"])]\n",
    "        value_feat = sorted(value_fuzzy)[0] if value_fuzzy else None\n",
    "    if mag_feat is None:\n",
    "        mag_fuzzy = [k for k in feat_dict.keys() if \"mag\" in k]\n",
    "        mag_feat = sorted(mag_fuzzy)[0] if mag_fuzzy else None\n",
    "\n",
    "    pm = (str(prefer_mode).lower().strip() if prefer_mode is not None else None)\n",
    "    if pm == \"mag\" and mag_feat is not None:\n",
    "        return \"mag\", mag_feat\n",
    "    if pm == \"asinh\":\n",
    "        if value_feat is not None:\n",
    "            return \"asinh\", value_feat\n",
    "        if mag_feat is not None:\n",
    "            print(\"[WARN] prefer asinh but no value-like feature found; fallback to mag.\")\n",
    "            return \"mag\", mag_feat\n",
    "\n",
    "    if value_feat is not None:\n",
    "        return \"asinh\", value_feat\n",
    "    if mag_feat is not None:\n",
    "        return \"mag\", mag_feat\n",
    "\n",
    "    return None, None\n",
    "\n",
    "# required for downstream model\n",
    "REQ_FOR_MODEL = [\"t_rel_log\", \"dt_log\"]\n",
    "missing_req = [k for k in REQ_FOR_MODEL if k not in feat]\n",
    "if missing_req:\n",
    "    raise ValueError(f\"SEQ_FEATURE_NAMES missing required feats for model: {missing_req}. Head={SEQ_FEATURE_NAMES[:40]}\")\n",
    "\n",
    "SNR_FEAT = _pick_first_existing([\"snr_tanh\", \"snr\"], feat)\n",
    "if SNR_FEAT is None:\n",
    "    raise ValueError(\"SEQ_FEATURE_NAMES missing snr_tanh or snr.\")\n",
    "\n",
    "DET_FEAT = _pick_first_existing([\"detected\", \"detected_pos\"], feat)\n",
    "if DET_FEAT is None:\n",
    "    raise ValueError(\"SEQ_FEATURE_NAMES missing detected or detected_pos.\")\n",
    "\n",
    "SEQ_TOKEN_MODE, SCORE_VALUE_FEAT = _pick_value_feat(feat, prefer_mode=SEQ_TOKEN_MODE_IN)\n",
    "if SEQ_TOKEN_MODE is None or SCORE_VALUE_FEAT is None:\n",
    "    raise ValueError(\n",
    "        \"Cannot infer token mode/value feature from SEQ_FEATURE_NAMES.\\n\"\n",
    "        f\"SEQ_FEATURE_NAMES={SEQ_FEATURE_NAMES}\"\n",
    "    )\n",
    "\n",
    "print(f\"[Stage 6] token_mode={SEQ_TOKEN_MODE} | score_value_feat={SCORE_VALUE_FEAT} | snr_feat={SNR_FEAT} | det_feat={DET_FEAT}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0c) Ensure df_train_meta / df_test_meta exist (rebuild if missing)\n",
    "# ----------------------------\n",
    "if \"df_train_meta\" not in globals() or not isinstance(globals()[\"df_train_meta\"], pd.DataFrame):\n",
    "    if \"df_train_log\" in globals() and isinstance(globals()[\"df_train_log\"], pd.DataFrame):\n",
    "        df_train_meta = globals()[\"df_train_log\"].copy()\n",
    "        df_train_meta[\"object_id\"] = df_train_meta[\"object_id\"].astype(str).apply(_norm_id)\n",
    "        df_train_meta = df_train_meta.set_index(\"object_id\", drop=True)\n",
    "        print(\"[Stage 6] df_train_meta missing -> rebuilt from df_train_log.\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Missing df_train_meta and df_train_log. Jalankan STAGE 0/1 dulu.\")\n",
    "else:\n",
    "    df_train_meta = globals()[\"df_train_meta\"].copy()\n",
    "\n",
    "if \"df_test_meta\" not in globals() or not isinstance(globals()[\"df_test_meta\"], pd.DataFrame):\n",
    "    if \"df_test_log\" in globals() and isinstance(globals()[\"df_test_log\"], pd.DataFrame):\n",
    "        df_test_meta = globals()[\"df_test_log\"].copy()\n",
    "        df_test_meta[\"object_id\"] = df_test_meta[\"object_id\"].astype(str).apply(_norm_id)\n",
    "        df_test_meta = df_test_meta.set_index(\"object_id\", drop=True)\n",
    "        print(\"[Stage 6] df_test_meta missing -> rebuilt from df_test_log.\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Missing df_test_meta and df_test_log. Jalankan STAGE 0/1 dulu.\")\n",
    "else:\n",
    "    df_test_meta = globals()[\"df_test_meta\"].copy()\n",
    "\n",
    "df_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\n",
    "df_test_meta.index  = pd.Index([_norm_id(z) for z in df_test_meta.index],  name=df_test_meta.index.name)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Settings\n",
    "# ----------------------------\n",
    "FORCE_MAX_LEN = CFG.get(\"STAGE6_FORCE_MAX_LEN\", None)\n",
    "MAXLEN_CAPS = (256, 384, 512)\n",
    "\n",
    "if SEQ_TOKEN_MODE == \"asinh\":\n",
    "    W_SNR = float(CFG.get(\"STAGE6_W_SNR\", 1.00))\n",
    "    W_VAL = float(CFG.get(\"STAGE6_W_VAL\", 0.05))\n",
    "    W_DET = float(CFG.get(\"STAGE6_W_DET\", 0.05))\n",
    "else:\n",
    "    W_SNR = float(CFG.get(\"STAGE6_W_SNR\", 1.00))\n",
    "    W_VAL = float(CFG.get(\"STAGE6_W_VAL\", 0.35))\n",
    "    W_DET = float(CFG.get(\"STAGE6_W_DET\", 0.25))\n",
    "\n",
    "PAD_BAND_ID = int(CFG.get(\"STAGE6_PAD_BAND_ID\", 0))\n",
    "DUMMY_TOKEN_FOR_EMPTY = bool(CFG.get(\"STAGE6_DUMMY_TOKEN_FOR_EMPTY\", True))\n",
    "SHIFT_BAND_IDS = bool(CFG.get(\"STAGE6_SHIFT_BAND_IDS\", True))\n",
    "\n",
    "DTYPE_X = np.float32\n",
    "REBUILD_MODE = str(CFG.get(\"STAGE6_REBUILD_MODE\", \"wipe_all\")).lower()  # wipe_all / reuse_if_exists\n",
    "\n",
    "SNR_RAW_TO_TANH_SCALE = float(CFG.get(\"STAGE6_SNR_RAW_TO_TANH_SCALE\", 10.0))  # only if snr(raw)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Length distribution -> choose MAX_LEN\n",
    "# ----------------------------\n",
    "def describe_lengths(m: pd.DataFrame, name: str):\n",
    "    if \"length\" not in m.columns:\n",
    "        raise RuntimeError(f\"Manifest {name} missing 'length' column.\")\n",
    "    L = pd.to_numeric(m[\"length\"], errors=\"coerce\").fillna(0).to_numpy(dtype=np.int32, copy=False)\n",
    "    q = np.percentile(L, [0, 1, 5, 10, 25, 50, 75, 90, 95, 98, 99, 100])\n",
    "    print(f\"\\n{name} length stats\")\n",
    "    print(f\"- n_objects={len(L):,} | min={int(q[0])} | p50={int(q[5])} | p90={int(q[7])} | p95={int(q[8])} | p99={int(q[10])} | max={int(q[-1])}\")\n",
    "    return q\n",
    "\n",
    "q_tr = describe_lengths(seq_manifest_train, \"TRAIN\")\n",
    "q_te = describe_lengths(seq_manifest_test,  \"TEST\")\n",
    "\n",
    "p95 = int(max(q_tr[8], q_te[8]))\n",
    "if FORCE_MAX_LEN is not None and str(FORCE_MAX_LEN) not in [\"None\", \"none\", \"\"]:\n",
    "    MAX_LEN = int(FORCE_MAX_LEN)\n",
    "else:\n",
    "    if p95 <= 256:\n",
    "        MAX_LEN = 256\n",
    "    elif p95 <= 384:\n",
    "        MAX_LEN = 384\n",
    "    else:\n",
    "        MAX_LEN = 512\n",
    "    if MAX_LEN not in MAXLEN_CAPS:\n",
    "        MAX_LEN = int(min(MAXLEN_CAPS, key=lambda x: abs(x - MAX_LEN)))\n",
    "\n",
    "print(f\"\\n[Stage 6] MAX_LEN={MAX_LEN} (based on p95={p95})\")\n",
    "print(f\"[Stage 6] Weights: W_SNR={W_SNR} | W_VAL={W_VAL} | W_DET={W_DET} | SHIFT_BAND_IDS={SHIFT_BAND_IDS} | DUMMY_TOKEN_FOR_EMPTY={DUMMY_TOKEN_FOR_EMPTY}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Window scoring + padding\n",
    "# ----------------------------\n",
    "def _brightness_proxy_from_mag(mag: np.ndarray) -> np.ndarray:\n",
    "    mag = np.nan_to_num(mag, nan=np.float32(0.0), posinf=np.float32(0.0), neginf=np.float32(0.0)).astype(np.float32, copy=False)\n",
    "    if mag.size == 0:\n",
    "        return np.zeros_like(mag, dtype=np.float32)\n",
    "    med = np.float32(np.median(mag))\n",
    "    br = np.maximum(med - mag, np.float32(0.0))\n",
    "    br = np.log1p(br).astype(np.float32, copy=False)\n",
    "    return br\n",
    "\n",
    "def _get_snr_tanh_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    s = X[:, feat[SNR_FEAT]].astype(np.float32, copy=False)\n",
    "    s = np.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "    if SNR_FEAT == \"snr_tanh\":\n",
    "        return np.abs(s).astype(np.float32, copy=False)\n",
    "    return np.abs(np.tanh(s / np.float32(SNR_RAW_TO_TANH_SCALE))).astype(np.float32, copy=False)\n",
    "\n",
    "def _score_tokens(X: np.ndarray) -> np.ndarray:\n",
    "    snr = _get_snr_tanh_from_X(X)\n",
    "    det = X[:, feat[DET_FEAT]].astype(np.float32, copy=False)\n",
    "    det = np.nan_to_num(det, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "\n",
    "    if SEQ_TOKEN_MODE == \"mag\":\n",
    "        mag = X[:, feat[SCORE_VALUE_FEAT]].astype(np.float32, copy=False)\n",
    "        val = _brightness_proxy_from_mag(mag)\n",
    "    else:\n",
    "        val = np.abs(X[:, feat[SCORE_VALUE_FEAT]]).astype(np.float32, copy=False)\n",
    "        val = np.nan_to_num(val, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "\n",
    "    score = (np.float32(W_SNR) * snr) + (np.float32(W_VAL) * val) + (np.float32(W_DET) * det)\n",
    "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "\n",
    "def select_best_window(score: np.ndarray, max_len: int) -> tuple[int, int]:\n",
    "    L = int(score.shape[0])\n",
    "    if L <= max_len:\n",
    "        return 0, L\n",
    "    cs = np.empty(L + 1, dtype=np.float32)\n",
    "    cs[0] = 0.0\n",
    "    np.cumsum(score.astype(np.float32, copy=False), out=cs[1:])\n",
    "    ws = cs[max_len:] - cs[:-max_len]\n",
    "    if ws.size <= 0 or (not np.isfinite(ws).any()):\n",
    "        start = int((L - max_len) // 2)\n",
    "    else:\n",
    "        start = int(np.argmax(ws))\n",
    "    return start, start + max_len\n",
    "\n",
    "def _is_empty_stub(X: np.ndarray, B: np.ndarray) -> bool:\n",
    "    try:\n",
    "        if X is None or B is None:\n",
    "            return True\n",
    "        if int(X.shape[0]) != 1 or int(B.shape[0]) != 1:\n",
    "            return False\n",
    "        if int(B[0]) >= 0:\n",
    "            return False\n",
    "        return bool(np.all(np.abs(X.astype(np.float32, copy=False)) <= np.float32(1e-12)))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def pad_to_fixed(X: np.ndarray, B: np.ndarray, max_len: int):\n",
    "    F = int(X.shape[1]) if (X is not None and getattr(X, \"ndim\", 0) == 2) else int(len(SEQ_FEATURE_NAMES))\n",
    "    Xp = np.zeros((max_len, F), dtype=DTYPE_X)\n",
    "    Bp = np.full((max_len,), PAD_BAND_ID, dtype=np.int8)\n",
    "    Mp = np.zeros((max_len,), dtype=np.int8)\n",
    "\n",
    "    empty_like = (\n",
    "        X is None or B is None\n",
    "        or getattr(X, \"size\", 0) == 0\n",
    "        or getattr(B, \"size\", 0) == 0\n",
    "        or _is_empty_stub(X, B)\n",
    "    )\n",
    "\n",
    "    if empty_like:\n",
    "        if DUMMY_TOKEN_FOR_EMPTY:\n",
    "            Mp[0] = 1\n",
    "            Bp[0] = np.int8(PAD_BAND_ID)\n",
    "            return Xp, Bp, Mp, 0, 0, 1\n",
    "        else:\n",
    "            return Xp, Bp, Mp, 0, 0, 0\n",
    "\n",
    "    L0 = int(X.shape[0])\n",
    "    if L0 <= max_len:\n",
    "        ws, we = 0, L0\n",
    "        Xw, Bw = X, B\n",
    "    else:\n",
    "        sc = _score_tokens(X)\n",
    "        ws, we = select_best_window(sc, max_len=max_len)\n",
    "        Xw, Bw = X[ws:we], B[ws:we]\n",
    "\n",
    "    lw = int(Xw.shape[0])\n",
    "    Xp[:lw] = Xw.astype(DTYPE_X, copy=False)\n",
    "\n",
    "    if SHIFT_BAND_IDS:\n",
    "        Bw16 = Bw.astype(np.int16, copy=False)\n",
    "        Bp[:lw] = np.clip(Bw16 + 1, 0, 127).astype(np.int8, copy=False)\n",
    "    else:\n",
    "        Bp[:lw] = Bw.astype(np.int8, copy=False)\n",
    "\n",
    "    Mp[:lw] = 1\n",
    "    return Xp, Bp, Mp, int(L0), int(ws), int(we)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Fixed cache builder setup\n",
    "# ----------------------------\n",
    "FIX_DIR = ART_DIR / \"fixed_seq\"\n",
    "FIX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_ids = df_train_meta.index.astype(str).tolist()\n",
    "\n",
    "def _try_load_sample_sub_ids():\n",
    "    if \"df_sub\" in globals() and isinstance(globals()[\"df_sub\"], pd.DataFrame) and \"object_id\" in globals()[\"df_sub\"].columns:\n",
    "        return globals()[\"df_sub\"][\"object_id\"].astype(str).str.strip().apply(_norm_id).to_list()\n",
    "    if \"PATHS\" in globals() and isinstance(globals()[\"PATHS\"], dict):\n",
    "        p = globals()[\"PATHS\"].get(\"SAMPLE_SUB\", None)\n",
    "        if p and Path(p).exists():\n",
    "            df = pd.read_csv(p)\n",
    "            if \"object_id\" in df.columns:\n",
    "                return df[\"object_id\"].astype(str).str.strip().apply(_norm_id).to_list()\n",
    "    return None\n",
    "\n",
    "test_ids = _try_load_sample_sub_ids()\n",
    "if test_ids is None:\n",
    "    test_ids = df_test_meta.index.astype(str).tolist()\n",
    "\n",
    "# find y column (meta -> fallback train.csv if exists in PATHS)\n",
    "def _find_target_col(df: pd.DataFrame):\n",
    "    for cand in [\"target\", \"y\", \"label\", \"class\", \"target_id\", \"binary_target\", \"is_tde\", \"is_event\", \"event\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "_y_col = _find_target_col(df_train_meta)\n",
    "if _y_col is None and \"PATHS\" in globals() and isinstance(globals()[\"PATHS\"], dict):\n",
    "    p_train = globals()[\"PATHS\"].get(\"TRAIN_CSV\", None) or globals()[\"PATHS\"].get(\"TRAIN\", None)\n",
    "    if p_train and Path(p_train).exists():\n",
    "        dft = pd.read_csv(p_train)\n",
    "        if \"object_id\" in dft.columns:\n",
    "            dft[\"object_id\"] = dft[\"object_id\"].astype(str).apply(_norm_id)\n",
    "            ycol = _find_target_col(dft)\n",
    "            if ycol is not None:\n",
    "                # join back to meta (safe)\n",
    "                tmp = dft.set_index(\"object_id\")[ycol]\n",
    "                df_train_meta[ycol] = pd.to_numeric(df_train_meta.index.map(tmp), errors=\"coerce\")\n",
    "                _y_col = ycol\n",
    "                print(f\"[Stage 6] target col loaded from TRAIN_CSV: {_y_col}\")\n",
    "\n",
    "if _y_col is None:\n",
    "    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols_head={list(df_train_meta.columns)[:40]}\")\n",
    "\n",
    "y_train = pd.to_numeric(df_train_meta[_y_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n",
    "\n",
    "# dedup checks\n",
    "if len(set(train_ids)) != len(train_ids):\n",
    "    s = pd.Series(train_ids); dup = s[s.duplicated()].head(10).tolist()\n",
    "    raise RuntimeError(f\"train_ids has duplicates. ex={dup}\")\n",
    "if len(set(test_ids)) != len(test_ids):\n",
    "    s = pd.Series(test_ids); dup = s[s.duplicated()].head(10).tolist()\n",
    "    raise RuntimeError(f\"test_ids has duplicates. ex={dup}\")\n",
    "\n",
    "train_row = {oid: i for i, oid in enumerate(train_ids)}\n",
    "test_row  = {oid: i for i, oid in enumerate(test_ids)}\n",
    "\n",
    "NTR, NTE, F = len(train_ids), len(test_ids), len(SEQ_FEATURE_NAMES)\n",
    "\n",
    "def _gb(nbytes): return float(nbytes) / (1024**3)\n",
    "print(f\"\\n[Stage 6] Memmap X sizes approx: train={_gb(NTR*MAX_LEN*F*np.dtype(DTYPE_X).itemsize):.3f} GB | \"\n",
    "      f\"test={_gb(NTE*MAX_LEN*F*np.dtype(DTYPE_X).itemsize):.3f} GB | dtype={DTYPE_X}\")\n",
    "\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "test_X_path  = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path  = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path  = FIX_DIR / \"test_M.dat\"\n",
    "\n",
    "train_len_path = FIX_DIR / \"train_origlen.npy\"\n",
    "train_ws_path  = FIX_DIR / \"train_winstart.npy\"\n",
    "train_we_path  = FIX_DIR / \"train_winend.npy\"\n",
    "test_len_path  = FIX_DIR / \"test_origlen.npy\"\n",
    "test_ws_path   = FIX_DIR / \"test_winstart.npy\"\n",
    "test_we_path   = FIX_DIR / \"test_winend.npy\"\n",
    "\n",
    "def _all_exist(paths):\n",
    "    return all(Path(p).exists() for p in paths)\n",
    "\n",
    "reuse_paths = [\n",
    "    train_X_path, train_B_path, train_M_path,\n",
    "    test_X_path, test_B_path, test_M_path,\n",
    "    FIX_DIR / \"train_ids.npy\", FIX_DIR / \"test_ids.npy\", FIX_DIR / \"train_y.npy\",\n",
    "    train_len_path, train_ws_path, train_we_path,\n",
    "    test_len_path, test_ws_path, test_we_path,\n",
    "    FIX_DIR / \"length_policy_config.json\"\n",
    "]\n",
    "\n",
    "if REBUILD_MODE == \"wipe_all\":\n",
    "    for p in reuse_paths:\n",
    "        try:\n",
    "            Path(p).unlink(missing_ok=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if REBUILD_MODE == \"reuse_if_exists\" and _all_exist(reuse_paths):\n",
    "    print(\"[Stage 6] REUSE: fixed_seq cache already present.\")\n",
    "    globals().update({\n",
    "        \"SEQ_TOKENS_DIR\": SEQ_TOKENS_DIR,\n",
    "        \"FIX_DIR\": FIX_DIR, \"MAX_LEN\": MAX_LEN,\n",
    "        \"FIX_TRAIN_X_PATH\": train_X_path, \"FIX_TRAIN_B_PATH\": train_B_path, \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "        \"FIX_TEST_X_PATH\": test_X_path,  \"FIX_TEST_B_PATH\": test_B_path,  \"FIX_TEST_M_PATH\": test_M_path,\n",
    "        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "        \"FIX_POLICY_CFG_PATH\": FIX_DIR / \"length_policy_config.json\",\n",
    "        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n",
    "        \"SCORE_VALUE_FEAT\": SCORE_VALUE_FEAT,\n",
    "        \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n",
    "        \"PAD_BAND_ID\": PAD_BAND_ID,\n",
    "        \"df_train_meta\": df_train_meta,\n",
    "        \"df_test_meta\": df_test_meta,\n",
    "    })\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # memmaps\n",
    "    Xtr = np.memmap(train_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTR, MAX_LEN, F))\n",
    "    Btr = np.memmap(train_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "    Mtr = np.memmap(train_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTR, MAX_LEN))\n",
    "\n",
    "    Xte = np.memmap(test_X_path, dtype=DTYPE_X, mode=\"w+\", shape=(NTE, MAX_LEN, F))\n",
    "    Bte = np.memmap(test_B_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "    Mte = np.memmap(test_M_path, dtype=np.int8,  mode=\"w+\", shape=(NTE, MAX_LEN))\n",
    "\n",
    "    origlen_tr  = np.zeros((NTR,), dtype=np.int32)\n",
    "    winstart_tr = np.zeros((NTR,), dtype=np.int32)\n",
    "    winend_tr   = np.zeros((NTR,), dtype=np.int32)\n",
    "\n",
    "    origlen_te  = np.zeros((NTE,), dtype=np.int32)\n",
    "    winstart_te = np.zeros((NTE,), dtype=np.int32)\n",
    "    winend_te   = np.zeros((NTE,), dtype=np.int32)\n",
    "\n",
    "    filled_tr = np.zeros((NTR,), dtype=np.uint8)\n",
    "    filled_te = np.zeros((NTE,), dtype=np.uint8)\n",
    "\n",
    "    def _prep_manifest(m: pd.DataFrame, expected_set: set, which: str) -> pd.DataFrame:\n",
    "        m2 = m.copy()\n",
    "        for c in [\"object_id\", \"shard\", \"start\", \"length\"]:\n",
    "            if c not in m2.columns:\n",
    "                raise RuntimeError(f\"Manifest missing '{c}'. cols={list(m2.columns)}\")\n",
    "\n",
    "        m2[\"object_id\"] = m2[\"object_id\"].astype(str).apply(_norm_id)\n",
    "        m2 = m2[m2[\"object_id\"].isin(expected_set)].copy()\n",
    "\n",
    "        # drop duplicate objects (keep first)\n",
    "        if m2[\"object_id\"].duplicated().any():\n",
    "            dups = m2.loc[m2[\"object_id\"].duplicated(), \"object_id\"].head(10).tolist()\n",
    "            print(f\"[WARN] {which} manifest has duplicate object_id; dropping duplicates. ex={dups}\")\n",
    "            m2 = m2.drop_duplicates(\"object_id\", keep=\"first\")\n",
    "\n",
    "        m2[\"shard\"] = m2[\"shard\"].astype(str).apply(lambda s: _resolve_shard_path(s, SEQ_TOKENS_DIR))\n",
    "        m2[\"start\"] = pd.to_numeric(m2[\"start\"], errors=\"coerce\").fillna(-1).astype(np.int64)\n",
    "        m2[\"length\"] = pd.to_numeric(m2[\"length\"], errors=\"coerce\").fillna(0).astype(np.int64)\n",
    "        m2 = m2[(m2[\"start\"] >= 0) & (m2[\"length\"] >= 0)].copy()\n",
    "\n",
    "        return m2\n",
    "\n",
    "    exp_train = set(train_ids)\n",
    "    exp_test  = set(test_ids)\n",
    "    m_train2 = _prep_manifest(seq_manifest_train, exp_train, \"train\")\n",
    "    m_test2  = _prep_manifest(seq_manifest_test,  exp_test,  \"test\")\n",
    "\n",
    "    def process_manifest_into_memmap(m2: pd.DataFrame, which: str):\n",
    "        if which == \"train\":\n",
    "            row_map = train_row\n",
    "            Xmm, Bmm, Mmm = Xtr, Btr, Mtr\n",
    "            origlen, ws_arr, we_arr = origlen_tr, winstart_tr, winend_tr\n",
    "            filled_mask = filled_tr\n",
    "            expected_n = NTR\n",
    "        else:\n",
    "            row_map = test_row\n",
    "            Xmm, Bmm, Mmm = Xte, Bte, Mte\n",
    "            origlen, ws_arr, we_arr = origlen_te, winstart_te, winend_te\n",
    "            filled_mask = filled_te\n",
    "            expected_n = NTE\n",
    "\n",
    "        shard_paths = m2[\"shard\"].unique().tolist()\n",
    "        miss_sh = [p for p in shard_paths if not Path(p).exists()]\n",
    "        if miss_sh:\n",
    "            raise RuntimeError(f\"Missing shard files ({which}): count={len(miss_sh)} | ex={miss_sh[:5]}\")\n",
    "\n",
    "        filled = dup = empty = dropped_bad = 0\n",
    "        t0 = time.time()\n",
    "        get = row_map.get\n",
    "\n",
    "        for si, (shard_path, g) in enumerate(m2.groupby(\"shard\", sort=True), start=1):\n",
    "            data = np.load(shard_path, allow_pickle=False)\n",
    "            if \"x\" not in data or \"band\" not in data:\n",
    "                raise RuntimeError(f\"Shard missing keys ['x','band']. Got={list(data.keys())} | shard={shard_path}\")\n",
    "\n",
    "            x_all = data[\"x\"]\n",
    "            b_all = data[\"band\"]\n",
    "\n",
    "            oids = g[\"object_id\"].to_numpy(copy=False)\n",
    "            starts = g[\"start\"].to_numpy(dtype=np.int64, copy=False)\n",
    "            lens   = g[\"length\"].to_numpy(dtype=np.int64, copy=False)\n",
    "\n",
    "            for oid, st, ln in zip(oids, starts, lens):\n",
    "                idx = get(str(oid), -1)\n",
    "                if idx < 0:\n",
    "                    continue\n",
    "                if ln <= 0:\n",
    "                    empty += 1\n",
    "                    continue\n",
    "                if filled_mask[idx]:\n",
    "                    dup += 1\n",
    "                    continue\n",
    "\n",
    "                end = int(st + ln)\n",
    "                if st < 0 or end > x_all.shape[0] or end > b_all.shape[0]:\n",
    "                    dropped_bad += 1\n",
    "                    continue\n",
    "\n",
    "                X = x_all[st:end]\n",
    "                B = b_all[st:end]\n",
    "\n",
    "                Xp, Bp, Mp, L0, ws, we = pad_to_fixed(X, B, max_len=MAX_LEN)\n",
    "\n",
    "                Xmm[idx, :, :] = Xp\n",
    "                Bmm[idx, :] = Bp\n",
    "                Mmm[idx, :] = Mp\n",
    "                origlen[idx] = int(L0)\n",
    "                ws_arr[idx] = int(ws)\n",
    "                we_arr[idx] = int(we)\n",
    "                filled_mask[idx] = 1\n",
    "                filled += 1\n",
    "\n",
    "                if filled % 2000 == 0:\n",
    "                    gc.collect()\n",
    "\n",
    "            if si % 25 == 0:\n",
    "                print(f\"[Stage 6][{which}] shards_processed={si:,}/{len(shard_paths):,} | filled={filled:,}\")\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        return {\n",
    "            \"filled\": int(filled),\n",
    "            \"dup_skipped\": int(dup),\n",
    "            \"empty_len\": int(empty),\n",
    "            \"dropped_bad_slices\": int(dropped_bad),\n",
    "            \"time_s\": float(elapsed),\n",
    "            \"expected\": int(expected_n)\n",
    "        }\n",
    "\n",
    "    print(\"\\n[Stage 6] Building fixed cache (TRAIN) from STAGE 5 manifests...\")\n",
    "    st_tr = process_manifest_into_memmap(m_train2, \"train\")\n",
    "    print(f\"[Stage 6] TRAIN filled={st_tr['filled']:,}/{st_tr['expected']:,} | dup={st_tr['dup_skipped']:,} | empty={st_tr['empty_len']:,} | dropped_bad={st_tr['dropped_bad_slices']:,} | time={st_tr['time_s']:.2f}s\")\n",
    "\n",
    "    print(\"\\n[Stage 6] Building fixed cache (TEST) from STAGE 5 manifests...\")\n",
    "    st_te = process_manifest_into_memmap(m_test2, \"test\")\n",
    "    print(f\"[Stage 6] TEST  filled={st_te['filled']:,}/{st_te['expected']:,} | dup={st_te['dup_skipped']:,} | empty={st_te['empty_len']:,} | dropped_bad={st_te['dropped_bad_slices']:,} | time={st_te['time_s']:.2f}s\")\n",
    "\n",
    "    Xtr.flush(); Btr.flush(); Mtr.flush()\n",
    "    Xte.flush(); Bte.flush(); Mte.flush()\n",
    "\n",
    "    miss_tr = np.where(filled_tr == 0)[0]\n",
    "    miss_te = np.where(filled_te == 0)[0]\n",
    "    if len(miss_tr) > 0:\n",
    "        ex = [train_ids[i] for i in miss_tr[:10]]\n",
    "        raise RuntimeError(f\"[Stage 6] TRAIN missing filled rows: {len(miss_tr):,}/{NTR:,} | ex={ex}\")\n",
    "    if len(miss_te) > 0:\n",
    "        ex = [test_ids[i] for i in miss_te[:10]]\n",
    "        raise RuntimeError(f\"[Stage 6] TEST missing filled rows: {len(miss_te):,}/{NTE:,} | ex={ex}\")\n",
    "\n",
    "    np.save(FIX_DIR / \"train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n",
    "    np.save(FIX_DIR / \"test_ids.npy\",  np.asarray(test_ids,  dtype=\"S\"))\n",
    "    np.save(FIX_DIR / \"train_y.npy\",   y_train)\n",
    "\n",
    "    np.save(train_len_path, origlen_tr)\n",
    "    np.save(train_ws_path,  winstart_tr)\n",
    "    np.save(train_we_path,  winend_tr)\n",
    "\n",
    "    np.save(test_len_path, origlen_te)\n",
    "    np.save(test_ws_path,  winstart_te)\n",
    "    np.save(test_we_path,  winend_te)\n",
    "\n",
    "    policy_cfg = {\n",
    "        \"stage5_inputs\": {\n",
    "            \"SEQ_TOKENS_DIR\": str(SEQ_TOKENS_DIR),\n",
    "            \"seq_manifest_train\": str(p_mtr),\n",
    "            \"seq_manifest_test\": str(p_mte),\n",
    "            \"seq_config\": str(p_cfg),\n",
    "        },\n",
    "        \"token_mode\": SEQ_TOKEN_MODE,\n",
    "        \"score_value_feat\": SCORE_VALUE_FEAT,\n",
    "        \"snr_feat\": SNR_FEAT,\n",
    "        \"det_feat\": DET_FEAT,\n",
    "        \"max_len\": int(MAX_LEN),\n",
    "        \"feature_names\": list(SEQ_FEATURE_NAMES),\n",
    "        \"score_weights\": {\"W_SNR\": float(W_SNR), \"W_VAL\": float(W_VAL), \"W_DET\": float(W_DET)},\n",
    "        \"window_policy\": \"best_contiguous_window_by_max_sum(score)\",\n",
    "        \"padding\": {\n",
    "            \"PAD_BAND_ID\": int(PAD_BAND_ID),\n",
    "            \"SHIFT_BAND_IDS\": bool(SHIFT_BAND_IDS),\n",
    "            \"DUMMY_TOKEN_FOR_EMPTY\": bool(DUMMY_TOKEN_FOR_EMPTY),\n",
    "        },\n",
    "        \"dtype_X\": \"float32\",\n",
    "        \"order\": {\"train\": \"df_train_meta.index\", \"test\": \"df_sub.object_id if exists else df_test_meta.index\", \"y_col\": str(_y_col)},\n",
    "        \"stats\": {\"train\": st_tr, \"test\": st_te},\n",
    "    }\n",
    "\n",
    "    cfg_path = FIX_DIR / \"length_policy_config.json\"\n",
    "    with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(policy_cfg, f, indent=2)\n",
    "\n",
    "    print(\"\\n[Stage 6] DONE\")\n",
    "    print(f\"- FIX_DIR: {FIX_DIR}\")\n",
    "    print(f\"- Saved config: {cfg_path}\")\n",
    "\n",
    "    globals().update({\n",
    "        \"SEQ_TOKENS_DIR\": SEQ_TOKENS_DIR,\n",
    "        \"SEQ_FEATURE_NAMES\": SEQ_FEATURE_NAMES,\n",
    "        \"FIX_DIR\": FIX_DIR,\n",
    "        \"MAX_LEN\": MAX_LEN,\n",
    "        \"FIX_TRAIN_X_PATH\": train_X_path,\n",
    "        \"FIX_TRAIN_B_PATH\": train_B_path,\n",
    "        \"FIX_TRAIN_M_PATH\": train_M_path,\n",
    "        \"FIX_TEST_X_PATH\": test_X_path,\n",
    "        \"FIX_TEST_B_PATH\": test_B_path,\n",
    "        \"FIX_TEST_M_PATH\": test_M_path,\n",
    "        \"FIX_TRAIN_Y_PATH\": FIX_DIR / \"train_y.npy\",\n",
    "        \"FIX_TRAIN_IDS_PATH\": FIX_DIR / \"train_ids.npy\",\n",
    "        \"FIX_TEST_IDS_PATH\": FIX_DIR / \"test_ids.npy\",\n",
    "        \"FIX_POLICY_CFG_PATH\": cfg_path,\n",
    "        \"SEQ_TOKEN_MODE\": SEQ_TOKEN_MODE,\n",
    "        \"SCORE_VALUE_FEAT\": SCORE_VALUE_FEAT,\n",
    "        \"SNR_FEAT\": SNR_FEAT,\n",
    "        \"DET_FEAT\": DET_FEAT,\n",
    "        \"SHIFT_BAND_IDS\": SHIFT_BAND_IDS,\n",
    "        \"PAD_BAND_ID\": PAD_BAND_ID,\n",
    "        \"df_train_meta\": df_train_meta,\n",
    "        \"df_test_meta\": df_test_meta,\n",
    "    })\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75007881",
   "metadata": {
    "papermill": {
     "duration": 0.019858,
     "end_time": "2026-01-07T16:58:37.367701",
     "exception": false,
     "start_time": "2026-01-07T16:58:37.347843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Split (Object-Level, Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "968157af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:58:37.410312Z",
     "iopub.status.busy": "2026-01-07T16:58:37.409912Z",
     "iopub.status.idle": "2026-01-07T16:58:39.657313Z",
     "shell.execute_reply": "2026-01-07T16:58:39.656370Z"
    },
    "papermill": {
     "duration": 2.272516,
     "end_time": "2026-01-07T16:58:39.659433",
     "exception": false,
     "start_time": "2026-01-07T16:58:37.386917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 7] seed=2025 | default_splits=5 | MIN_POS_PER_FOLD=3 enforce_pos=True | MIN_NEG_PER_FOLD=1 enforce_neg=True | group_by_split=False fallback_group=True | holdout_fallback=True holdout_frac=0.2\n",
      "[Stage 7] N=3,043 pos=148 neg=2,895 pos%=4.863621% | pos_weight~19.5608 | order_source=/kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/fixed_seq/train_ids.npy\n",
      "[Stage 7] Candidate n_splits=5 | enforce_pos=True enforce_neg=True\n",
      "[Stage 7] FINAL: n_splits=5 | cv_type=StratifiedKFold | min_pos_in_fold=29 | min_neg_in_fold=579\n",
      "\n",
      "[Stage 7] CV split OK\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/cv/cv_folds.csv\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/cv/cv_folds.npz\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/cv/cv_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/cv/fold_stats.csv\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/cv/cv_config.json\n",
      "CV=StratifiedKFold n_splits=5 seed=2025\n",
      "Order source: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/fixed_seq/train_ids.npy\n",
      "Target column: target\n",
      "Total: N=3043 | pos=148 | neg=2895 | pos%=4.863621% | pos_weight~19.560811\n",
      "Per-fold distribution (val):\n",
      "- fold 0: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 1: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 2: n=   609 | pos=   30 | neg=   579 | pos%= 4.926108%\n",
      "- fold 3: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n",
      "- fold 4: n=   608 | pos=   29 | neg=   579 | pos%= 4.769737%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 — CV Split (Object-Level, Stratified) (ONE CELL)\n",
    "# REVISI FULL v2.7 (UPGRADE: stronger auto-k, per-k group fallback, fold_stats csv, cv_train_ids.npy)\n",
    "#\n",
    "# Output:\n",
    "# - artifacts/cv/cv_folds.csv\n",
    "# - artifacts/cv/cv_folds.npz   (train_idx_f + val_idx_f + optional holdout_val_mask)\n",
    "# - artifacts/cv/cv_report.txt\n",
    "# - artifacts/cv/cv_config.json\n",
    "# - artifacts/cv/fold_stats.csv\n",
    "# - artifacts/cv/cv_train_ids.npy\n",
    "# - (optional) artifacts/cv/cv_holdout_val_mask.npy\n",
    "# - globals: fold_assign, folds, n_splits, CV_DIR\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "for need in [\"df_train_meta\", \"ART_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan minimal STAGE 2 dulu (df_train_meta & ART_DIR).\")\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "ART_DIR = Path(ART_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CV Settings\n",
    "# ----------------------------\n",
    "DEFAULT_SPLITS = 5\n",
    "FORCE_N_SPLITS = None              # set int kalau mau paksa (mis. 3), else None\n",
    "\n",
    "MIN_POS_PER_FOLD = 3               # umum 2–10 (imbalance -> kecilkan)\n",
    "ENFORCE_MIN_POS_PER_FOLD = True    # kalau True: n_splits turun otomatis sampai min_pos>=MIN_POS_PER_FOLD (atau fallback holdout)\n",
    "\n",
    "MIN_NEG_PER_FOLD = 1               # biasanya aman 1 (neg biasanya banyak). Boleh dinaikkan kalau perlu.\n",
    "ENFORCE_MIN_NEG_PER_FOLD = True\n",
    "\n",
    "USE_GROUP_BY_SPLIT = False         # True => prefer StratifiedGroupKFold (groups=df_train_meta[\"split\"])\n",
    "AUTO_FALLBACK_GROUP = True         # True => kalau group-cv gagal, fallback ke StratifiedKFold\n",
    "\n",
    "HOLDOUT_FALLBACK = True            # True => kalau CV tidak mungkin, pakai holdout\n",
    "HOLDOUT_FRAC = 0.20                # target val fraction untuk holdout (stratified)\n",
    "\n",
    "SAVE_PARQUET_FOLDS = False         # opsional: save cv_folds.parquet jika pyarrow tersedia\n",
    "\n",
    "print(\n",
    "    f\"[Stage 7] seed={SEED} | default_splits={DEFAULT_SPLITS} | \"\n",
    "    f\"MIN_POS_PER_FOLD={MIN_POS_PER_FOLD} enforce_pos={ENFORCE_MIN_POS_PER_FOLD} | \"\n",
    "    f\"MIN_NEG_PER_FOLD={MIN_NEG_PER_FOLD} enforce_neg={ENFORCE_MIN_NEG_PER_FOLD} | \"\n",
    "    f\"group_by_split={USE_GROUP_BY_SPLIT} fallback_group={AUTO_FALLBACK_GROUP} | \"\n",
    "    f\"holdout_fallback={HOLDOUT_FALLBACK} holdout_frac={HOLDOUT_FRAC}\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Helpers\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, bytearray, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _decode_ids(arr) -> list:\n",
    "    return [_norm_id(x) for x in arr.tolist()]\n",
    "\n",
    "def _safe_str_list(idx) -> list:\n",
    "    return pd.Index(idx).astype(\"string\").str.strip().astype(str).tolist()\n",
    "\n",
    "def _find_train_ids_npy(art_dir: Path):\n",
    "    # priority 1: FIX_DIR (Stage 6)\n",
    "    if \"FIX_DIR\" in globals():\n",
    "        p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # priority 2: ART_DIR/fixed_seq\n",
    "    p = art_dir / \"fixed_seq\" / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "    # priority 3: scan mallorn_run runs (latest mtime)\n",
    "    root = Path(\"/kaggle/working/mallorn_run\")\n",
    "    if root.exists():\n",
    "        cands = list(root.glob(\"run_*/artifacts/fixed_seq/train_ids.npy\"))\n",
    "        if cands:\n",
    "            cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "            return cands[0]\n",
    "    return None\n",
    "\n",
    "def _pick_target_col(df: pd.DataFrame):\n",
    "    for cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Determine train_ids ordering (prefer fixed cache from STAGE 6)\n",
    "# ----------------------------\n",
    "p_ids = _find_train_ids_npy(ART_DIR)\n",
    "if p_ids is not None:\n",
    "    raw = np.load(p_ids, allow_pickle=False)\n",
    "    train_ids = _decode_ids(raw)\n",
    "    order_source = str(p_ids)\n",
    "else:\n",
    "    train_ids = _safe_str_list(df_train_meta.index)\n",
    "    order_source = \"df_train_meta.index\"\n",
    "\n",
    "if len(train_ids) != len(set(train_ids)):\n",
    "    s = pd.Series(train_ids)\n",
    "    dup = s[s.duplicated()].iloc[:10].tolist()\n",
    "    raise RuntimeError(f\"[Stage 7] train_ids has duplicates (examples): {dup}\")\n",
    "\n",
    "N = int(len(train_ids))\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Normalize meta index (string+strip) + fast mapping\n",
    "# ----------------------------\n",
    "meta = df_train_meta.copy()\n",
    "meta_ids = _safe_str_list(meta.index)\n",
    "\n",
    "if len(meta_ids) != len(set(meta_ids)):\n",
    "    vc = pd.Series(meta_ids).value_counts()\n",
    "    dup = vc[vc > 1].index.tolist()[:10]\n",
    "    raise RuntimeError(f\"[Stage 7] df_train_meta index has duplicates after str/strip (examples): {dup}\")\n",
    "\n",
    "meta.index = pd.Index(meta_ids, name=\"object_id\")\n",
    "\n",
    "pos_idx = meta.index.get_indexer(train_ids)\n",
    "missing_mask = (pos_idx < 0)\n",
    "if missing_mask.any():\n",
    "    ex = [train_ids[i] for i in np.where(missing_mask)[0][:10]]\n",
    "    raise RuntimeError(\n",
    "        \"[Stage 7] Some train_ids not found in df_train_meta (after str/strip index).\\n\"\n",
    "        f\"Missing count={int(missing_mask.sum())} | ex={ex}\\n\"\n",
    "        \"Solusi: pastikan df_train_meta index adalah object_id dan konsisten dengan fixed_seq train_ids (kalau pakai Stage 6).\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Robust target -> y (ordered by train_ids)\n",
    "# ----------------------------\n",
    "target_col = _pick_target_col(meta)\n",
    "if target_col is None:\n",
    "    raise RuntimeError(f\"[Stage 7] Cannot find target column in df_train_meta. cols(sample)={list(meta.columns)[:40]}\")\n",
    "\n",
    "y_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).to_numpy(copy=False)\n",
    "y = y_all[pos_idx]\n",
    "y = (y > 0).astype(np.int8)\n",
    "\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "if pos == 0 or neg == 0:\n",
    "    raise RuntimeError(f\"[Stage 7] Invalid class distribution: pos={pos}, neg={neg}. Cannot do stratified split.\")\n",
    "\n",
    "pos_rate = pos / max(N, 1)\n",
    "pos_weight = float(neg / max(pos, 1))\n",
    "print(f\"[Stage 7] N={N:,} pos={pos:,} neg={neg:,} pos%={pos_rate*100:.6f}% | pos_weight~{pos_weight:.4f} | order_source={order_source}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Optional groups (by split)\n",
    "# ----------------------------\n",
    "groups = None\n",
    "group_col = None\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    for cand in [\"split\", \"split_id\", \"split_name\", \"split_idx\"]:\n",
    "        if cand in meta.columns:\n",
    "            group_col = cand\n",
    "            break\n",
    "    if group_col is None:\n",
    "        if not AUTO_FALLBACK_GROUP:\n",
    "            raise RuntimeError(\"[Stage 7] USE_GROUP_BY_SPLIT=True but no split column found in df_train_meta.\")\n",
    "        print(\"[Stage 7] WARN: split column not found; fallback to StratifiedKFold.\")\n",
    "        USE_GROUP_BY_SPLIT = False\n",
    "    else:\n",
    "        g_all = meta[group_col].astype(\"string\").str.strip().astype(str).to_numpy(copy=False)\n",
    "        groups = g_all[pos_idx]\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Choose n_splits safely + auto-adjust\n",
    "# ----------------------------\n",
    "max_splits_by_pos = pos\n",
    "max_splits_by_neg = neg\n",
    "max_splits_by_minpos = max(1, pos // max(int(MIN_POS_PER_FOLD), 1))\n",
    "max_splits_by_minneg = max(1, neg // max(int(MIN_NEG_PER_FOLD), 1))\n",
    "\n",
    "n0 = min(DEFAULT_SPLITS, max_splits_by_pos, max_splits_by_neg, max_splits_by_minpos, max_splits_by_minneg)\n",
    "if FORCE_N_SPLITS is not None:\n",
    "    n0 = int(FORCE_N_SPLITS)\n",
    "\n",
    "print(f\"[Stage 7] Candidate n_splits={n0} | enforce_pos={ENFORCE_MIN_POS_PER_FOLD} enforce_neg={ENFORCE_MIN_NEG_PER_FOLD}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Build folds with robust fallback (group->non-group per-k)\n",
    "# ----------------------------\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedGroupKFold\n",
    "    except Exception:\n",
    "        StratifiedGroupKFold = None\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn is not available in this environment.\") from e\n",
    "\n",
    "def _try_split_kfold(k: int, use_group: bool):\n",
    "    fold_assign = np.full(N, -1, dtype=np.int16)\n",
    "    folds = []\n",
    "    per = []\n",
    "\n",
    "    if use_group:\n",
    "        if StratifiedGroupKFold is None:\n",
    "            return (False, \"StratifiedGroupKFold(unavailable)\", None, None, None)\n",
    "        if groups is None:\n",
    "            return (False, \"StratifiedGroupKFold(groups=None)\", None, None, None)\n",
    "        splitter = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "        split_iter = splitter.split(np.zeros(N), y, groups=groups)\n",
    "        cv_type = f\"StratifiedGroupKFold({group_col})\"\n",
    "    else:\n",
    "        splitter = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "        split_iter = splitter.split(np.zeros(N), y)\n",
    "        cv_type = \"StratifiedKFold\"\n",
    "\n",
    "    try:\n",
    "        for fold, (tr_idx, val_idx) in enumerate(split_iter):\n",
    "            fold_assign[val_idx] = fold\n",
    "            yf = y[val_idx]\n",
    "            pf = int((yf == 1).sum())\n",
    "            nf = int((yf == 0).sum())\n",
    "            per.append((len(val_idx), pf, nf))\n",
    "            folds.append({\n",
    "                \"fold\": int(fold),\n",
    "                \"train_idx\": tr_idx.astype(np.int32, copy=False),\n",
    "                \"val_idx\": val_idx.astype(np.int32, copy=False),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        return (False, f\"{cv_type} (error: {type(e).__name__}: {e})\", None, None, None)\n",
    "\n",
    "    if (fold_assign < 0).any():\n",
    "        return (False, f\"{cv_type} (unassigned)\", None, None, None)\n",
    "\n",
    "    # strict: no fold with empty class\n",
    "    for (_, pf, nf) in per:\n",
    "        if pf == 0 or nf == 0:\n",
    "            return (False, f\"{cv_type} (empty class in fold)\", None, None, None)\n",
    "\n",
    "    return (True, cv_type, fold_assign, folds, per)\n",
    "\n",
    "def _passes_min_constraints(per):\n",
    "    if not per:\n",
    "        return False, 0, 0\n",
    "    min_pos_seen = min(pf for (_, pf, _) in per)\n",
    "    min_neg_seen = min(nf for (_, _, nf) in per)\n",
    "    if ENFORCE_MIN_POS_PER_FOLD and (min_pos_seen < int(MIN_POS_PER_FOLD)) and (FORCE_N_SPLITS is None):\n",
    "        return False, min_pos_seen, min_neg_seen\n",
    "    if ENFORCE_MIN_NEG_PER_FOLD and (min_neg_seen < int(MIN_NEG_PER_FOLD)) and (FORCE_N_SPLITS is None):\n",
    "        return False, min_pos_seen, min_neg_seen\n",
    "    return True, min_pos_seen, min_neg_seen\n",
    "\n",
    "def _make_holdout():\n",
    "    if pos < 2 or neg < 2:\n",
    "        raise RuntimeError(f\"[Stage 7] Cannot build holdout safely. Need pos>=2 and neg>=2. Got pos={pos}, neg={neg}.\")\n",
    "\n",
    "    val_n = int(round(N * float(HOLDOUT_FRAC)))\n",
    "    val_n = max(val_n, 2)\n",
    "    val_n = min(val_n, N - 2)\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=(val_n / N), random_state=SEED)\n",
    "    tr_idx, val_idx = next(splitter.split(np.zeros(N), y))\n",
    "\n",
    "    # ensure val has both classes\n",
    "    pf = int((y[val_idx] == 1).sum())\n",
    "    nf = int((y[val_idx] == 0).sum())\n",
    "    if pf == 0 or nf == 0:\n",
    "        raise RuntimeError(f\"[Stage 7] Holdout produced empty class in val. pos_val={pf} neg_val={nf}. Try different seed/frac.\")\n",
    "\n",
    "    fold_assign = np.zeros(N, dtype=np.int16)  # no -1 (downstream safe)\n",
    "    val_mask = np.zeros(N, dtype=np.uint8)\n",
    "    val_mask[val_idx] = 1\n",
    "\n",
    "    folds = [{\n",
    "        \"fold\": 0,\n",
    "        \"train_idx\": tr_idx.astype(np.int32, copy=False),\n",
    "        \"val_idx\": val_idx.astype(np.int32, copy=False),\n",
    "    }]\n",
    "    per = [(len(val_idx), pf, nf)]\n",
    "    return 1, \"Holdout(StratifiedShuffleSplit)\", fold_assign, folds, per, val_mask\n",
    "\n",
    "best = None\n",
    "val_mask_holdout = None\n",
    "\n",
    "if n0 >= 2:\n",
    "    for k in range(n0, 1, -1):\n",
    "        # try group first (if requested), else plain\n",
    "        tried = []\n",
    "        if USE_GROUP_BY_SPLIT:\n",
    "            tried.append(True)\n",
    "        tried.append(False)  # always allow non-group attempt\n",
    "\n",
    "        for use_group in tried:\n",
    "            ok, cv_type, fa, folds_k, per = _try_split_kfold(k, use_group=use_group)\n",
    "            if not ok:\n",
    "                continue\n",
    "\n",
    "            passed, min_pos_seen, min_neg_seen = _passes_min_constraints(per)\n",
    "            if not passed:\n",
    "                continue\n",
    "\n",
    "            best = (k, cv_type, fa, folds_k, per, min_pos_seen, min_neg_seen)\n",
    "            break\n",
    "\n",
    "        if best is not None:\n",
    "            break\n",
    "\n",
    "# if strict constraints failed, pick first valid anyway (still must have both classes per fold)\n",
    "if best is None and n0 >= 2:\n",
    "    for k in range(n0, 1, -1):\n",
    "        ok, cv_type, fa, folds_k, per = _try_split_kfold(k, use_group=False)\n",
    "        if ok:\n",
    "            min_pos_seen = min(pf for (_, pf, _) in per) if per else 0\n",
    "            min_neg_seen = min(nf for (_, _, nf) in per) if per else 0\n",
    "            best = (k, cv_type, fa, folds_k, per, min_pos_seen, min_neg_seen)\n",
    "            print(f\"[Stage 7] NOTE: Could not satisfy min constraints. Using k={k} with min_pos={min_pos_seen}, min_neg={min_neg_seen}.\")\n",
    "            break\n",
    "\n",
    "# fallback to holdout\n",
    "if best is None:\n",
    "    if HOLDOUT_FALLBACK:\n",
    "        n_splits, cv_type, fold_assign, folds, per, val_mask_holdout = _make_holdout()\n",
    "        min_pos_seen = per[0][1] if per else 0\n",
    "        min_neg_seen = per[0][2] if per else 0\n",
    "        best = (n_splits, cv_type, fold_assign, folds, per, min_pos_seen, min_neg_seen)\n",
    "        print(f\"[Stage 7] FALLBACK -> {cv_type} | val_pos={min_pos_seen} val_neg={min_neg_seen}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"[Stage 7] Failed to build a valid CV split. Try smaller DEFAULT_SPLITS / FORCE_N_SPLITS, or enable HOLDOUT_FALLBACK.\")\n",
    "\n",
    "# unpack\n",
    "n_splits, cv_type, fold_assign, folds, per, min_pos_seen, min_neg_seen = best\n",
    "print(f\"[Stage 7] FINAL: n_splits={n_splits} | cv_type={cv_type} | min_pos_in_fold={min_pos_seen} | min_neg_in_fold={min_neg_seen}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Report + fold stats\n",
    "# ----------------------------\n",
    "lines = []\n",
    "lines.append(f\"CV={cv_type} n_splits={n_splits} seed={SEED}\")\n",
    "lines.append(f\"Order source: {order_source}\")\n",
    "lines.append(f\"Target column: {target_col}\")\n",
    "lines.append(f\"Total: N={N} | pos={pos} | neg={neg} | pos%={pos_rate*100:.6f}% | pos_weight~{pos_weight:.6f}\")\n",
    "if USE_GROUP_BY_SPLIT:\n",
    "    lines.append(f\"Group requested: {group_col} | used_group={('Group' in cv_type)}\")\n",
    "\n",
    "lines.append(\"Per-fold distribution (val):\")\n",
    "fold_rows = []\n",
    "if n_splits >= 2:\n",
    "    for f in range(n_splits):\n",
    "        idx = np.where(fold_assign == f)[0]\n",
    "        yf = y[idx]\n",
    "        pf = int((yf == 1).sum())\n",
    "        nf = int((yf == 0).sum())\n",
    "        lines.append(f\"- fold {f}: n={len(idx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(idx),1))*100:9.6f}%\")\n",
    "        fold_rows.append({\"fold\": f, \"n_val\": len(idx), \"pos_val\": pf, \"neg_val\": nf, \"pos_frac_val\": pf/max(len(idx),1)})\n",
    "else:\n",
    "    vidx = folds[0][\"val_idx\"]\n",
    "    yf = y[vidx]\n",
    "    pf = int((yf == 1).sum())\n",
    "    nf = int((yf == 0).sum())\n",
    "    lines.append(f\"- holdout val: n={len(vidx):6d} | pos={pf:5d} | neg={nf:6d} | pos%={(pf/max(len(vidx),1))*100:9.6f}%\")\n",
    "    lines.append(\"NOTE: holdout mode uses folds[0].train_idx / folds[0].val_idx; fold_assign is all zeros (no -1).\")\n",
    "    fold_rows.append({\"fold\": 0, \"n_val\": len(vidx), \"pos_val\": pf, \"neg_val\": nf, \"pos_frac_val\": pf/max(len(vidx),1)})\n",
    "\n",
    "df_fold_stats = pd.DataFrame(fold_rows)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Save artifacts\n",
    "# ----------------------------\n",
    "CV_DIR = ART_DIR / \"cv\"\n",
    "CV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_folds = pd.DataFrame({\"object_id\": train_ids, \"fold\": fold_assign.astype(int)})\n",
    "\n",
    "folds_csv = CV_DIR / \"cv_folds.csv\"\n",
    "df_folds.to_csv(folds_csv, index=False)\n",
    "\n",
    "if SAVE_PARQUET_FOLDS:\n",
    "    try:\n",
    "        df_folds.to_parquet(CV_DIR / \"cv_folds.parquet\", index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[Stage 7] WARN: parquet save skipped ({type(e).__name__}: {e})\")\n",
    "\n",
    "# save ids for downstream consistency (even without Stage 6)\n",
    "np.save(CV_DIR / \"cv_train_ids.npy\", np.asarray(train_ids, dtype=\"S\"))\n",
    "\n",
    "npz_path = CV_DIR / \"cv_folds.npz\"\n",
    "npz_kwargs = {}\n",
    "for fd in folds:\n",
    "    f = int(fd[\"fold\"])\n",
    "    npz_kwargs[f\"train_idx_{f}\"] = fd[\"train_idx\"].astype(np.int32, copy=False)\n",
    "    npz_kwargs[f\"val_idx_{f}\"]   = fd[\"val_idx\"].astype(np.int32, copy=False)\n",
    "\n",
    "if val_mask_holdout is not None:\n",
    "    npz_kwargs[\"holdout_val_mask\"] = val_mask_holdout.astype(np.uint8, copy=False)\n",
    "    np.save(CV_DIR / \"cv_holdout_val_mask.npy\", val_mask_holdout.astype(np.uint8, copy=False))\n",
    "\n",
    "np.savez(npz_path, **npz_kwargs)\n",
    "\n",
    "report_path = CV_DIR / \"cv_report.txt\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "fold_stats_path = CV_DIR / \"fold_stats.csv\"\n",
    "df_fold_stats.to_csv(fold_stats_path, index=False)\n",
    "\n",
    "cfg_path = CV_DIR / \"cv_config.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"seed\": SEED,\n",
    "            \"n_splits\": int(n_splits),\n",
    "            \"cv_type\": cv_type,\n",
    "            \"min_pos_per_fold\": int(MIN_POS_PER_FOLD),\n",
    "            \"min_neg_per_fold\": int(MIN_NEG_PER_FOLD),\n",
    "            \"enforce_min_pos_per_fold\": bool(ENFORCE_MIN_POS_PER_FOLD),\n",
    "            \"enforce_min_neg_per_fold\": bool(ENFORCE_MIN_NEG_PER_FOLD),\n",
    "            \"use_group_by_split_requested\": bool(USE_GROUP_BY_SPLIT),\n",
    "            \"auto_fallback_group\": bool(AUTO_FALLBACK_GROUP),\n",
    "            \"holdout_fallback\": bool(HOLDOUT_FALLBACK),\n",
    "            \"holdout_frac\": float(HOLDOUT_FRAC),\n",
    "            \"order_source\": order_source,\n",
    "            \"target_col\": target_col,\n",
    "            \"group_col\": group_col,\n",
    "            \"pos_weight_hint\": float(pos_weight),\n",
    "            \"summary\": {\n",
    "                \"N\": int(N), \"pos\": int(pos), \"neg\": int(neg), \"pos_rate\": float(pos_rate),\n",
    "                \"min_pos_in_fold\": int(min_pos_seen), \"min_neg_in_fold\": int(min_neg_seen),\n",
    "            },\n",
    "            \"artifacts\": {\n",
    "                \"folds_csv\": str(folds_csv),\n",
    "                \"folds_npz\": str(npz_path),\n",
    "                \"report_txt\": str(report_path),\n",
    "                \"fold_stats_csv\": str(fold_stats_path),\n",
    "                \"cv_train_ids_npy\": str(CV_DIR / \"cv_train_ids.npy\"),\n",
    "                \"holdout_val_mask_npy\": (str(CV_DIR / \"cv_holdout_val_mask.npy\") if val_mask_holdout is not None else None),\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n[Stage 7] CV split OK\")\n",
    "print(f\"- Saved: {folds_csv}\")\n",
    "print(f\"- Saved: {npz_path}\")\n",
    "print(f\"- Saved: {report_path}\")\n",
    "print(f\"- Saved: {fold_stats_path}\")\n",
    "print(f\"- Saved: {cfg_path}\")\n",
    "\n",
    "tail_n = min(len(lines), 12)\n",
    "print(\"\\n\".join(lines[-tail_n:]))\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Export globals for next stage\n",
    "# ----------------------------\n",
    "globals().update({\n",
    "    \"CV_DIR\": CV_DIR,\n",
    "    \"n_splits\": int(n_splits),\n",
    "    \"train_ids_ordered\": train_ids,\n",
    "    \"y_ordered\": y,\n",
    "    \"fold_assign\": fold_assign,\n",
    "    \"folds\": folds,\n",
    "    \"CV_FOLDS_CSV\": folds_csv,\n",
    "    \"CV_FOLDS_NPZ\": npz_path,\n",
    "    \"CV_CFG_PATH\": cfg_path,\n",
    "    \"CV_TYPE\": cv_type,\n",
    "    \"CV_ORDER_SOURCE\": order_source,\n",
    "    \"POS_WEIGHT_HINT\": float(pos_weight),\n",
    "    \"HOLDOUT_VAL_MASK\": (val_mask_holdout if val_mask_holdout is not None else None),\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b604f",
   "metadata": {
    "papermill": {
     "duration": 0.021045,
     "end_time": "2026-01-07T16:58:39.701018",
     "exception": false,
     "start_time": "2026-01-07T16:58:39.679973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b06a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:58:39.743157Z",
     "iopub.status.busy": "2026-01-07T16:58:39.742727Z",
     "iopub.status.idle": "2026-01-07T19:45:33.272712Z",
     "shell.execute_reply": "2026-01-07T19:45:33.271741Z"
    },
    "papermill": {
     "duration": 10013.554005,
     "end_time": "2026-01-07T19:45:33.274782",
     "exception": false,
     "start_time": "2026-01-07T16:58:39.720777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8] token_mode=asinh | VAL_FEAT=signal | VAL_IS_MAG=False\n",
      "[Stage 8] SHIFT_BAND_IDS(from stage6)=True | PAD_BAND_ID=0 | X_dtype=<class 'numpy.float32'>\n",
      "[Stage 8] Building AGG sequence features (one-time, cached)... chunk=1024\n",
      "[Stage 8] AGG built: shape=(3043, 31) | time=0.2s\n",
      "[Stage 8] TRAIN CONFIG (CPU)\n",
      "- N=3,043 | pos=148 | neg=2,895 | pos%=4.863621%\n",
      "- token_mode=asinh | VAL_FEAT=signal | val_is_mag=False | g_dim=38 | use_agg_seq=True\n",
      "- Model: d_model=160 heads=4 layers=3 dropout=0.14\n",
      "- Batch=16 grad_accum=2 epochs=18 lr=0.0005\n",
      "- balance_mode=pos_weight | focal_gamma=0.0 | primary_metric=ap\n",
      "- ema=True(0.999)\n",
      "- CKPT_DIR=/kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/checkpoints\n",
      "- OOF_DIR =/kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof\n",
      "- LOG_DIR =/kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/logs\n",
      "\n",
      "[Stage 8] FOLD 0 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=pos_weight | primary=ap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17/3557107592.py:438: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  Xt = torch.from_numpy(X)   # dtype as memmap; model casts to float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=3.17e-04 | opt_steps=  77 | train_loss=1.43055 | val_loss=0.73953 | auc=0.44738 | ap=0.04261 | f1@0.5=0.0942 | best_ep=1 | pat=6\n",
      "  epoch 02 | lr=5.00e-04 | opt_steps=  77 | train_loss=1.37951 | val_loss=0.73609 | auc=0.58515 | ap=0.05921 | f1@0.5=0.0949 | best_ep=2 | pat=6\n",
      "  epoch 03 | lr=4.93e-04 | opt_steps=  77 | train_loss=1.29732 | val_loss=0.73399 | auc=0.65682 | ap=0.07847 | f1@0.5=0.1034 | best_ep=3 | pat=6\n",
      "  epoch 04 | lr=4.77e-04 | opt_steps=  77 | train_loss=1.23875 | val_loss=0.72628 | auc=0.68682 | ap=0.09595 | f1@0.5=0.1207 | best_ep=4 | pat=6\n",
      "  epoch 05 | lr=4.53e-04 | opt_steps=  77 | train_loss=1.20333 | val_loss=0.71861 | auc=0.70535 | ap=0.10823 | f1@0.5=0.1379 | best_ep=5 | pat=6\n",
      "  epoch 06 | lr=4.21e-04 | opt_steps=  77 | train_loss=1.14084 | val_loss=0.70900 | auc=0.71980 | ap=0.12962 | f1@0.5=0.1489 | best_ep=6 | pat=6\n",
      "  epoch 07 | lr=3.83e-04 | opt_steps=  77 | train_loss=1.17126 | val_loss=0.69440 | auc=0.72925 | ap=0.13177 | f1@0.5=0.1551 | best_ep=7 | pat=6\n",
      "  epoch 08 | lr=3.40e-04 | opt_steps=  77 | train_loss=1.08968 | val_loss=0.68116 | auc=0.74059 | ap=0.13788 | f1@0.5=0.1629 | best_ep=8 | pat=6\n",
      "  epoch 09 | lr=2.93e-04 | opt_steps=  77 | train_loss=1.08953 | val_loss=0.66789 | auc=0.75020 | ap=0.14154 | f1@0.5=0.1701 | best_ep=9 | pat=6\n",
      "  epoch 10 | lr=2.45e-04 | opt_steps=  77 | train_loss=1.05262 | val_loss=0.65734 | auc=0.75907 | ap=0.14418 | f1@0.5=0.1721 | best_ep=10 | pat=6\n",
      "  epoch 11 | lr=1.97e-04 | opt_steps=  77 | train_loss=1.05435 | val_loss=0.64363 | auc=0.76828 | ap=0.15367 | f1@0.5=0.1745 | best_ep=11 | pat=6\n",
      "  epoch 12 | lr=1.51e-04 | opt_steps=  77 | train_loss=1.04074 | val_loss=0.63185 | auc=0.77594 | ap=0.15438 | f1@0.5=0.1667 | best_ep=12 | pat=6\n",
      "  epoch 13 | lr=1.09e-04 | opt_steps=  77 | train_loss=1.03191 | val_loss=0.62222 | auc=0.78365 | ap=0.16238 | f1@0.5=0.1716 | best_ep=13 | pat=6\n",
      "  epoch 14 | lr=7.19e-05 | opt_steps=  77 | train_loss=1.03311 | val_loss=0.61248 | auc=0.79108 | ap=0.16859 | f1@0.5=0.1763 | best_ep=14 | pat=6\n",
      "  epoch 15 | lr=4.17e-05 | opt_steps=  77 | train_loss=1.06096 | val_loss=0.60113 | auc=0.79781 | ap=0.17289 | f1@0.5=0.1799 | best_ep=15 | pat=6\n",
      "  epoch 16 | lr=1.93e-05 | opt_steps=  77 | train_loss=1.03801 | val_loss=0.59021 | auc=0.80311 | ap=0.18794 | f1@0.5=0.1871 | best_ep=16 | pat=6\n",
      "  epoch 17 | lr=5.56e-06 | opt_steps=  77 | train_loss=1.04235 | val_loss=0.57892 | auc=0.80823 | ap=0.19260 | f1@0.5=0.1880 | best_ep=17 | pat=6\n",
      "  epoch 18 | lr=1.00e-06 | opt_steps=  77 | train_loss=1.01434 | val_loss=0.56850 | auc=0.81232 | ap=0.19795 | f1@0.5=0.1931 | best_ep=18 | pat=6\n",
      "\n",
      "[Stage 8] FOLD 1 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=pos_weight | primary=ap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=3.17e-04 | opt_steps=  77 | train_loss=1.42062 | val_loss=0.76466 | auc=0.58319 | ap=0.06695 | f1@0.5=0.0943 | best_ep=1 | pat=6\n",
      "  epoch 02 | lr=5.00e-04 | opt_steps=  77 | train_loss=1.36111 | val_loss=0.75293 | auc=0.64208 | ap=0.07950 | f1@0.5=0.0974 | best_ep=2 | pat=6\n",
      "  epoch 03 | lr=4.93e-04 | opt_steps=  77 | train_loss=1.33772 | val_loss=0.73489 | auc=0.69275 | ap=0.08748 | f1@0.5=0.1056 | best_ep=3 | pat=6\n",
      "  epoch 04 | lr=4.77e-04 | opt_steps=  77 | train_loss=1.29121 | val_loss=0.71734 | auc=0.71923 | ap=0.09003 | f1@0.5=0.1210 | best_ep=4 | pat=6\n",
      "  epoch 05 | lr=4.53e-04 | opt_steps=  77 | train_loss=1.24577 | val_loss=0.70064 | auc=0.73828 | ap=0.09441 | f1@0.5=0.1276 | best_ep=5 | pat=6\n",
      "  epoch 06 | lr=4.21e-04 | opt_steps=  77 | train_loss=1.17780 | val_loss=0.68499 | auc=0.75222 | ap=0.10197 | f1@0.5=0.1447 | best_ep=6 | pat=6\n",
      "  epoch 07 | lr=3.83e-04 | opt_steps=  77 | train_loss=1.13872 | val_loss=0.66871 | auc=0.76160 | ap=0.11322 | f1@0.5=0.1642 | best_ep=7 | pat=6\n",
      "  epoch 08 | lr=3.40e-04 | opt_steps=  77 | train_loss=1.11709 | val_loss=0.65556 | auc=0.77225 | ap=0.14871 | f1@0.5=0.1728 | best_ep=8 | pat=6\n",
      "  epoch 09 | lr=2.93e-04 | opt_steps=  77 | train_loss=1.08710 | val_loss=0.64249 | auc=0.78181 | ap=0.15674 | f1@0.5=0.1789 | best_ep=9 | pat=6\n",
      "  epoch 10 | lr=2.45e-04 | opt_steps=  77 | train_loss=1.07501 | val_loss=0.62981 | auc=0.79016 | ap=0.15962 | f1@0.5=0.1751 | best_ep=10 | pat=6\n",
      "  epoch 11 | lr=1.97e-04 | opt_steps=  77 | train_loss=1.04967 | val_loss=0.61899 | auc=0.79822 | ap=0.16746 | f1@0.5=0.1730 | best_ep=11 | pat=6\n",
      "  epoch 12 | lr=1.51e-04 | opt_steps=  77 | train_loss=1.05090 | val_loss=0.60502 | auc=0.80518 | ap=0.17446 | f1@0.5=0.1758 | best_ep=12 | pat=6\n",
      "  epoch 13 | lr=1.09e-04 | opt_steps=  77 | train_loss=1.02809 | val_loss=0.59447 | auc=0.81272 | ap=0.18446 | f1@0.5=0.1818 | best_ep=13 | pat=6\n",
      "  epoch 14 | lr=7.19e-05 | opt_steps=  77 | train_loss=1.03647 | val_loss=0.58490 | auc=0.81854 | ap=0.19092 | f1@0.5=0.1783 | best_ep=14 | pat=6\n",
      "  epoch 15 | lr=4.17e-05 | opt_steps=  77 | train_loss=1.02431 | val_loss=0.57440 | auc=0.82320 | ap=0.19735 | f1@0.5=0.1847 | best_ep=15 | pat=6\n",
      "  epoch 16 | lr=1.93e-05 | opt_steps=  77 | train_loss=1.03080 | val_loss=0.56494 | auc=0.82666 | ap=0.20469 | f1@0.5=0.1901 | best_ep=16 | pat=6\n",
      "  epoch 17 | lr=5.56e-06 | opt_steps=  77 | train_loss=1.04002 | val_loss=0.55768 | auc=0.82890 | ap=0.20654 | f1@0.5=0.1917 | best_ep=17 | pat=6\n",
      "  epoch 18 | lr=1.00e-06 | opt_steps=  77 | train_loss=1.01186 | val_loss=0.55102 | auc=0.83195 | ap=0.21444 | f1@0.5=0.1983 | best_ep=18 | pat=6\n",
      "\n",
      "[Stage 8] FOLD 2 | train=2,434 val=609 | pos=118 neg=2,316 | pos_weight=19.6271 | balance_mode=pos_weight | primary=ap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=3.17e-04 | opt_steps=  77 | train_loss=1.44554 | val_loss=0.74480 | auc=0.40518 | ap=0.03906 | f1@0.5=0.0940 | best_ep=1 | pat=6\n",
      "  epoch 02 | lr=5.00e-04 | opt_steps=  77 | train_loss=1.37330 | val_loss=0.73448 | auc=0.54093 | ap=0.05156 | f1@0.5=0.0951 | best_ep=2 | pat=6\n",
      "  epoch 03 | lr=4.93e-04 | opt_steps=  77 | train_loss=1.29543 | val_loss=0.71991 | auc=0.61600 | ap=0.06143 | f1@0.5=0.1086 | best_ep=3 | pat=6\n",
      "  epoch 04 | lr=4.77e-04 | opt_steps=  77 | train_loss=1.24836 | val_loss=0.70512 | auc=0.64692 | ap=0.06690 | f1@0.5=0.1268 | best_ep=4 | pat=6\n",
      "  epoch 05 | lr=4.53e-04 | opt_steps=  77 | train_loss=1.23403 | val_loss=0.69092 | auc=0.66799 | ap=0.07235 | f1@0.5=0.1448 | best_ep=5 | pat=6\n",
      "  epoch 06 | lr=4.21e-04 | opt_steps=  77 | train_loss=1.16847 | val_loss=0.67976 | auc=0.68653 | ap=0.08000 | f1@0.5=0.1561 | best_ep=6 | pat=6\n",
      "  epoch 07 | lr=3.83e-04 | opt_steps=  77 | train_loss=1.18996 | val_loss=0.66577 | auc=0.70484 | ap=0.08899 | f1@0.5=0.1620 | best_ep=7 | pat=6\n",
      "  epoch 08 | lr=3.40e-04 | opt_steps=  77 | train_loss=1.11582 | val_loss=0.65689 | auc=0.72257 | ap=0.09959 | f1@0.5=0.1703 | best_ep=8 | pat=6\n",
      "  epoch 09 | lr=2.93e-04 | opt_steps=  77 | train_loss=1.07780 | val_loss=0.64772 | auc=0.73898 | ap=0.10526 | f1@0.5=0.1725 | best_ep=9 | pat=6\n",
      "  epoch 10 | lr=2.45e-04 | opt_steps=  77 | train_loss=1.10176 | val_loss=0.63838 | auc=0.75642 | ap=0.11286 | f1@0.5=0.1711 | best_ep=10 | pat=6\n",
      "  epoch 11 | lr=1.97e-04 | opt_steps=  77 | train_loss=1.09694 | val_loss=0.62854 | auc=0.77110 | ap=0.13905 | f1@0.5=0.1728 | best_ep=11 | pat=6\n",
      "  epoch 12 | lr=1.51e-04 | opt_steps=  77 | train_loss=1.06063 | val_loss=0.62006 | auc=0.78359 | ap=0.14917 | f1@0.5=0.1739 | best_ep=12 | pat=6\n",
      "  epoch 13 | lr=1.09e-04 | opt_steps=  77 | train_loss=1.05689 | val_loss=0.61046 | auc=0.79545 | ap=0.16127 | f1@0.5=0.1781 | best_ep=13 | pat=6\n",
      "  epoch 14 | lr=7.19e-05 | opt_steps=  77 | train_loss=1.04916 | val_loss=0.60445 | auc=0.80645 | ap=0.17565 | f1@0.5=0.1775 | best_ep=14 | pat=6\n",
      "  epoch 15 | lr=4.17e-05 | opt_steps=  77 | train_loss=1.06158 | val_loss=0.59437 | auc=0.81739 | ap=0.17805 | f1@0.5=0.1825 | best_ep=15 | pat=6\n",
      "  epoch 16 | lr=1.93e-05 | opt_steps=  77 | train_loss=1.03459 | val_loss=0.58399 | auc=0.82677 | ap=0.20036 | f1@0.5=0.1884 | best_ep=16 | pat=6\n",
      "  epoch 17 | lr=5.56e-06 | opt_steps=  77 | train_loss=1.04823 | val_loss=0.57514 | auc=0.83443 | ap=0.22222 | f1@0.5=0.1926 | best_ep=17 | pat=6\n",
      "  epoch 18 | lr=1.00e-06 | opt_steps=  77 | train_loss=1.03364 | val_loss=0.56722 | auc=0.84093 | ap=0.22874 | f1@0.5=0.1948 | best_ep=18 | pat=6\n",
      "\n",
      "[Stage 8] FOLD 3 | train=2,435 val=608 | pos=119 neg=2,316 | pos_weight=19.4622 | balance_mode=pos_weight | primary=ap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=3.17e-04 | opt_steps=  77 | train_loss=1.44714 | val_loss=0.67046 | auc=0.63844 | ap=0.06750 | f1@0.5=0.0351 | best_ep=1 | pat=6\n",
      "  epoch 02 | lr=5.00e-04 | opt_steps=  77 | train_loss=1.34890 | val_loss=0.67763 | auc=0.70169 | ap=0.07758 | f1@0.5=0.1567 | best_ep=2 | pat=6\n",
      "  epoch 03 | lr=4.93e-04 | opt_steps=  77 | train_loss=1.28500 | val_loss=0.68336 | auc=0.73391 | ap=0.08532 | f1@0.5=0.1388 | best_ep=3 | pat=6\n",
      "  epoch 04 | lr=4.77e-04 | opt_steps=  77 | train_loss=1.24150 | val_loss=0.68043 | auc=0.75630 | ap=0.09400 | f1@0.5=0.1417 | best_ep=4 | pat=6\n",
      "  epoch 05 | lr=4.53e-04 | opt_steps=  77 | train_loss=1.21423 | val_loss=0.67447 | auc=0.77649 | ap=0.10419 | f1@0.5=0.1485 | best_ep=5 | pat=6\n",
      "  epoch 06 | lr=4.21e-04 | opt_steps=  77 | train_loss=1.18304 | val_loss=0.66608 | auc=0.79507 | ap=0.11771 | f1@0.5=0.1513 | best_ep=6 | pat=6\n",
      "  epoch 07 | lr=3.83e-04 | opt_steps=  77 | train_loss=1.14684 | val_loss=0.65846 | auc=0.81234 | ap=0.13530 | f1@0.5=0.1534 | best_ep=7 | pat=6\n",
      "  epoch 08 | lr=3.40e-04 | opt_steps=  77 | train_loss=1.11738 | val_loss=0.65109 | auc=0.82532 | ap=0.15910 | f1@0.5=0.1584 | best_ep=8 | pat=6\n",
      "  epoch 09 | lr=2.93e-04 | opt_steps=  77 | train_loss=1.09284 | val_loss=0.64146 | auc=0.83837 | ap=0.18246 | f1@0.5=0.1622 | best_ep=9 | pat=6\n",
      "  epoch 10 | lr=2.45e-04 | opt_steps=  77 | train_loss=1.07286 | val_loss=0.63249 | auc=0.84897 | ap=0.20943 | f1@0.5=0.1672 | best_ep=10 | pat=6\n",
      "  epoch 11 | lr=1.97e-04 | opt_steps=  77 | train_loss=1.07273 | val_loss=0.62365 | auc=0.85546 | ap=0.21756 | f1@0.5=0.1731 | best_ep=11 | pat=6\n",
      "  epoch 12 | lr=1.51e-04 | opt_steps=  77 | train_loss=1.05742 | val_loss=0.61326 | auc=0.86284 | ap=0.23445 | f1@0.5=0.1782 | best_ep=12 | pat=6\n",
      "  epoch 13 | lr=1.09e-04 | opt_steps=  77 | train_loss=1.08208 | val_loss=0.60050 | auc=0.86868 | ap=0.26577 | f1@0.5=0.1837 | best_ep=13 | pat=6\n",
      "  epoch 14 | lr=7.19e-05 | opt_steps=  77 | train_loss=1.04817 | val_loss=0.59196 | auc=0.87332 | ap=0.27623 | f1@0.5=0.1806 | best_ep=14 | pat=6\n",
      "  epoch 15 | lr=4.17e-05 | opt_steps=  77 | train_loss=1.05392 | val_loss=0.58202 | auc=0.87732 | ap=0.28954 | f1@0.5=0.1877 | best_ep=15 | pat=6\n",
      "  epoch 16 | lr=1.93e-05 | opt_steps=  77 | train_loss=1.04065 | val_loss=0.57404 | auc=0.88011 | ap=0.30035 | f1@0.5=0.1919 | best_ep=16 | pat=6\n",
      "  epoch 17 | lr=5.56e-06 | opt_steps=  77 | train_loss=1.03386 | val_loss=0.56616 | auc=0.88250 | ap=0.30886 | f1@0.5=0.2008 | best_ep=17 | pat=6\n",
      "  epoch 18 | lr=1.00e-06 | opt_steps=  77 | train_loss=1.02092 | val_loss=0.55842 | auc=0.88434 | ap=0.31709 | f1@0.5=0.1976 | best_ep=18 | pat=6\n",
      "\n",
      "[Stage 8] FOLD 4 | train=2,435 val=608 | pos=119 neg=2,316 | pos_weight=19.4622 | balance_mode=pos_weight | primary=ap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 01 | lr=3.17e-04 | opt_steps=  77 | train_loss=1.41294 | val_loss=0.69891 | auc=0.36332 | ap=0.03558 | f1@0.5=0.0652 | best_ep=1 | pat=6\n",
      "  epoch 02 | lr=5.00e-04 | opt_steps=  77 | train_loss=1.40370 | val_loss=0.69652 | auc=0.56852 | ap=0.05298 | f1@0.5=0.1143 | best_ep=2 | pat=6\n",
      "  epoch 03 | lr=4.93e-04 | opt_steps=  77 | train_loss=1.31046 | val_loss=0.69383 | auc=0.65648 | ap=0.06693 | f1@0.5=0.1232 | best_ep=3 | pat=6\n",
      "  epoch 04 | lr=4.77e-04 | opt_steps=  77 | train_loss=1.31778 | val_loss=0.68830 | auc=0.69263 | ap=0.07572 | f1@0.5=0.1357 | best_ep=4 | pat=6\n",
      "  epoch 05 | lr=4.53e-04 | opt_steps=  77 | train_loss=1.25135 | val_loss=0.68092 | auc=0.72086 | ap=0.08710 | f1@0.5=0.1488 | best_ep=5 | pat=6\n",
      "  epoch 06 | lr=4.21e-04 | opt_steps=  77 | train_loss=1.21256 | val_loss=0.67088 | auc=0.74123 | ap=0.09985 | f1@0.5=0.1574 | best_ep=6 | pat=6\n",
      "  epoch 07 | lr=3.83e-04 | opt_steps=  77 | train_loss=1.16973 | val_loss=0.66086 | auc=0.75797 | ap=0.11341 | f1@0.5=0.1682 | best_ep=7 | pat=6\n",
      "  epoch 08 | lr=3.40e-04 | opt_steps=  77 | train_loss=1.13679 | val_loss=0.65167 | auc=0.76994 | ap=0.12641 | f1@0.5=0.1661 | best_ep=8 | pat=6\n",
      "  epoch 09 | lr=2.93e-04 | opt_steps=  77 | train_loss=1.12921 | val_loss=0.64264 | auc=0.78209 | ap=0.14535 | f1@0.5=0.1683 | best_ep=9 | pat=6\n",
      "  epoch 10 | lr=2.45e-04 | opt_steps=  77 | train_loss=1.09120 | val_loss=0.63353 | auc=0.79281 | ap=0.17293 | f1@0.5=0.1688 | best_ep=10 | pat=6\n",
      "  epoch 11 | lr=1.97e-04 | opt_steps=  77 | train_loss=1.08057 | val_loss=0.62395 | auc=0.80228 | ap=0.19212 | f1@0.5=0.1705 | best_ep=11 | pat=6\n",
      "  epoch 12 | lr=1.51e-04 | opt_steps=  77 | train_loss=1.09577 | val_loss=0.61216 | auc=0.81103 | ap=0.21989 | f1@0.5=0.1757 | best_ep=12 | pat=6\n",
      "  epoch 13 | lr=1.09e-04 | opt_steps=  77 | train_loss=1.07538 | val_loss=0.60345 | auc=0.81859 | ap=0.23538 | f1@0.5=0.1793 | best_ep=13 | pat=6\n",
      "  epoch 14 | lr=7.19e-05 | opt_steps=  77 | train_loss=1.05267 | val_loss=0.59487 | auc=0.82550 | ap=0.26157 | f1@0.5=0.1773 | best_ep=14 | pat=6\n",
      "  epoch 15 | lr=4.17e-05 | opt_steps=  77 | train_loss=1.04287 | val_loss=0.58503 | auc=0.83170 | ap=0.27615 | f1@0.5=0.1838 | best_ep=15 | pat=6\n",
      "  epoch 16 | lr=1.93e-05 | opt_steps=  77 | train_loss=1.03378 | val_loss=0.57608 | auc=0.83563 | ap=0.30020 | f1@0.5=0.1940 | best_ep=16 | pat=6\n",
      "  epoch 17 | lr=5.56e-06 | opt_steps=  77 | train_loss=1.04830 | val_loss=0.56793 | auc=0.83944 | ap=0.30420 | f1@0.5=0.1977 | best_ep=17 | pat=6\n",
      "  epoch 18 | lr=1.00e-06 | opt_steps=  77 | train_loss=1.05738 | val_loss=0.56034 | auc=0.84194 | ap=0.30800 | f1@0.5=0.2039 | best_ep=18 | pat=6\n",
      "\n",
      "[Stage 8] TRAIN DONE\n",
      "- elapsed: 166.88 min\n",
      "- OOF saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/oof_prob.npy\n",
      "- OOF saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/oof_prob.csv\n",
      "- fold metrics: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/fold_metrics.json\n",
      "- OOF rows valid: 3,043/3,043\n",
      "- OOF AUC (valid-only): 0.84202\n",
      "- OOF AP  (valid-only): 0.22406\n",
      "- OOF F1@0.5 (valid-only): 0.1975\n",
      "- OOF best F1=0.3060 @ thr=0.6426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 8 — Train Multiband Event Transformer\n",
    "# REVISI FULL v4.5 (IMPROVED: PR-AUC primary metric + optional focal + safer agg chunk + better logging)\n",
    "#\n",
    "# Key upgrades v4.5:\n",
    "# - Primary metric default = PR-AUC (AveragePrecision) (lebih cocok imbalance) + tie-break val_loss\n",
    "# - Optional focal loss (CFG[\"focal_gamma\"] > 0) for extreme imbalance\n",
    "# - Auto chunk size for AGG features to avoid RAM spikes\n",
    "# - Per-fold training history saved to LOG_DIR/fold_{fold}_history.csv\n",
    "# - Stabilize encoder input with LayerNorm\n",
    "# - Keeps: global_features_raw.npy caching + EMA + pos_weight default\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, math, time, warnings, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal previous stages\n",
    "# ----------------------------\n",
    "need_min = [\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_train_meta\",\"n_splits\",\"folds\"]\n",
    "for k in need_min:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..7 dulu dengan urutan benar.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0a) Resolve train_ids ordering + labels (FAST + robust)\n",
    "# ----------------------------\n",
    "def _decode_ids(arr):\n",
    "    out = []\n",
    "    for x in arr.tolist():\n",
    "        if isinstance(x, (bytes, bytearray, np.bytes_)):\n",
    "            s = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            s = str(x)\n",
    "        out.append(s.strip())\n",
    "    return out\n",
    "\n",
    "if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n",
    "    train_ids = [str(x).strip() for x in list(globals()[\"train_ids_ordered\"])]\n",
    "else:\n",
    "    p = Path(globals()[\"FIX_DIR\"]) / \"train_ids.npy\"\n",
    "    if p.exists():\n",
    "        raw = np.load(p, allow_pickle=False)\n",
    "        train_ids = _decode_ids(raw)\n",
    "    else:\n",
    "        train_ids = pd.Index(globals()[\"df_train_meta\"].index).astype(\"string\").str.strip().astype(str).tolist()\n",
    "\n",
    "df_train_meta = globals()[\"df_train_meta\"]\n",
    "target_col = None\n",
    "for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n",
    "    if cand in df_train_meta.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise RuntimeError(f\"Cannot find target column in df_train_meta. cols(sample)={list(df_train_meta.columns)[:40]}\")\n",
    "\n",
    "meta = df_train_meta.copy()\n",
    "meta.index = pd.Index(pd.Index(meta.index).astype(\"string\").str.strip().astype(str), name=\"object_id\")\n",
    "\n",
    "pos_idx = meta.index.get_indexer(train_ids)\n",
    "if (pos_idx < 0).any():\n",
    "    miss = [train_ids[i] for i in np.where(pos_idx < 0)[0][:10]]\n",
    "    raise RuntimeError(f\"Some train_ids not found in df_train_meta.index (after str/strip). ex={miss}\")\n",
    "\n",
    "y_all = pd.to_numeric(meta[target_col], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n",
    "y = y_all[pos_idx]\n",
    "y = (y > 0).astype(np.int8)\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Ensure output dirs exist\n",
    "# ----------------------------\n",
    "if \"RUN_DIR\" in globals() and globals()[\"RUN_DIR\"] is not None:\n",
    "    RUN_DIR = Path(globals()[\"RUN_DIR\"])\n",
    "else:\n",
    "    if \"ART_DIR\" in globals() and globals()[\"ART_DIR\"] is not None:\n",
    "        RUN_DIR = Path(globals()[\"ART_DIR\"]).parent\n",
    "    else:\n",
    "        RUN_DIR = Path(\"/kaggle/working/mallorn_run\")\n",
    "\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", RUN_DIR / \"artifacts\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CKPT_DIR = Path(globals().get(\"CKPT_DIR\", RUN_DIR / \"checkpoints\"))\n",
    "OOF_DIR  = Path(globals().get(\"OOF_DIR\",  RUN_DIR / \"oof\"))\n",
    "LOG_DIR  = Path(globals().get(\"LOG_DIR\",  RUN_DIR / \"logs\"))\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "globals().update({\"RUN_DIR\": RUN_DIR, \"ART_DIR\": ART_DIR, \"CKPT_DIR\": CKPT_DIR, \"OOF_DIR\": OOF_DIR, \"LOG_DIR\": LOG_DIR})\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Torch imports + CPU safety\n",
    "# ----------------------------\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# threads (safe)\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# metrics\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    _HAS_AP = True\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"scikit-learn metrics tidak tersedia (roc_auc_score/average_precision_score).\") from e\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open memmaps (fixed seq)\n",
    "# ----------------------------\n",
    "FIX_DIR = Path(globals()[\"FIX_DIR\"])\n",
    "N = len(train_ids)\n",
    "L = int(globals()[\"MAX_LEN\"])\n",
    "SEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "feat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n",
    "\n",
    "train_X_path = FIX_DIR / \"train_X.dat\"\n",
    "train_B_path = FIX_DIR / \"train_B.dat\"\n",
    "train_M_path = FIX_DIR / \"train_M.dat\"\n",
    "\n",
    "for p in [train_X_path, train_B_path, train_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2b) Read Stage6 policy (SHIFT_BAND_IDS + dtype_X + score_value_feat + token_mode)\n",
    "# ----------------------------\n",
    "SHIFT_BAND_IDS = False\n",
    "PAD_BAND_ID = 0\n",
    "DTYPE_X_MEMMAP = np.float32\n",
    "SEQ_TOKEN_MODE = None\n",
    "SCORE_VALUE_FEAT = globals().get(\"SCORE_VALUE_FEAT\", None)\n",
    "\n",
    "policy_path = FIX_DIR / \"length_policy_config.json\"\n",
    "if policy_path.exists():\n",
    "    try:\n",
    "        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pol = json.load(f)\n",
    "        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n",
    "        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n",
    "        dt = str(pol.get(\"dtype_X\", \"float32\")).lower()\n",
    "        DTYPE_X_MEMMAP = np.float16 if ((\"float16\" in dt) or (\"fp16\" in dt)) else np.float32\n",
    "        if SCORE_VALUE_FEAT is None:\n",
    "            SCORE_VALUE_FEAT = pol.get(\"score_value_feat\", None)\n",
    "        if SEQ_TOKEN_MODE is None:\n",
    "            SEQ_TOKEN_MODE = pol.get(\"token_mode\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "X_mm = np.memmap(train_X_path, dtype=DTYPE_X_MEMMAP, mode=\"r\", shape=(N, L, Fdim))\n",
    "B_mm = np.memmap(train_B_path, dtype=np.int8,        mode=\"r\", shape=(N, L))\n",
    "M_mm = np.memmap(train_M_path, dtype=np.int8,        mode=\"r\", shape=(N, L))\n",
    "\n",
    "# ----------------------------\n",
    "# 2c) Robust value feature resolution\n",
    "# ----------------------------\n",
    "def _pick_val_feat(feat_map, prefer=None):\n",
    "    cand = []\n",
    "    if prefer is not None:\n",
    "        cand.append(str(prefer))\n",
    "    cand += [\n",
    "        \"signal\", \"value\", \"flux\",\n",
    "        \"flux_asinh\", \"flux_asinh_clip\", \"flux_asinh_norm\", \"flux_asinh_scaled\",\n",
    "        \"mag\", \"mag_norm\", \"mag_clip\", \"mag_scaled\",\n",
    "        \"delta_signal\", \"delta_flux\", \"signal_clip\", \"signal_norm\",\n",
    "    ]\n",
    "    for c in cand:\n",
    "        if c and (c in feat_map):\n",
    "            return c\n",
    "    keys = list(feat_map.keys())\n",
    "    fuzzy = [k for k in keys if any(t in k for t in [\"signal\", \"flux\", \"value\", \"asinh\", \"mag\"])]\n",
    "    if fuzzy:\n",
    "        return sorted(fuzzy)[0]\n",
    "    return None\n",
    "\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    SEQ_TOKEN_MODE = globals().get(\"SEQ_TOKEN_MODE\", None)\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    SEQ_TOKEN_MODE = \"mag\" if any((k == \"mag\") or k.startswith(\"mag\") for k in feat.keys()) else \"asinh\"\n",
    "SEQ_TOKEN_MODE = str(SEQ_TOKEN_MODE).lower().strip()\n",
    "\n",
    "VAL_FEAT = _pick_val_feat(feat, prefer=SCORE_VALUE_FEAT)\n",
    "if VAL_FEAT is None:\n",
    "    raise RuntimeError(\"Cannot resolve VAL_FEAT from SEQ_FEATURE_NAMES.\")\n",
    "\n",
    "for k in [\"snr_tanh\",\"detected\"]:\n",
    "    if k not in feat:\n",
    "        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n",
    "\n",
    "VAL_IS_MAG = (SEQ_TOKEN_MODE == \"mag\") and (\"mag\" in VAL_FEAT)\n",
    "\n",
    "print(f\"[Stage 8] token_mode={SEQ_TOKEN_MODE} | VAL_FEAT={VAL_FEAT} | VAL_IS_MAG={VAL_IS_MAG}\")\n",
    "print(f\"[Stage 8] SHIFT_BAND_IDS(from stage6)={SHIFT_BAND_IDS} | PAD_BAND_ID={PAD_BAND_ID} | X_dtype={DTYPE_X_MEMMAP}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build RAW meta global features (no leak)\n",
    "# ----------------------------\n",
    "if (\"EBV_clip\" in meta.columns):\n",
    "    EBV_used = pd.to_numeric(meta[\"EBV_clip\"], errors=\"coerce\")\n",
    "elif (\"EBV\" in meta.columns):\n",
    "    EBV_used = pd.to_numeric(meta[\"EBV\"], errors=\"coerce\")\n",
    "else:\n",
    "    EBV_used = pd.Series(np.zeros((len(meta),), dtype=np.float32), index=meta.index)\n",
    "\n",
    "BASE_G_COLS = [\"Z\",\"Z_err\",\"EBV_used\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\n",
    "\n",
    "tmp_meta = meta.copy()\n",
    "tmp_meta[\"EBV_used\"] = EBV_used\n",
    "for c in BASE_G_COLS:\n",
    "    if c not in tmp_meta.columns:\n",
    "        tmp_meta[c] = 0.0\n",
    "\n",
    "G_meta = tmp_meta.iloc[pos_idx][BASE_G_COLS].copy()\n",
    "for c in BASE_G_COLS:\n",
    "    G_meta[c] = pd.to_numeric(G_meta[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "G_meta_np = G_meta.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "with open(Path(LOG_DIR)/\"global_meta_cols.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"cols\": BASE_G_COLS}, f, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 3b) Sequence aggregate features (global + per-band) + CACHE\n",
    "# ----------------------------\n",
    "USE_AGG_SEQ_FEATURES = True\n",
    "N_BANDS = 6\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1.0)\n",
    "\n",
    "def _auto_chunk(L, F, target_mb=220, min_chunk=64, max_chunk=2048):\n",
    "    # approx bytes = chunk * L * F * 4\n",
    "    target_bytes = float(target_mb) * (1024**2)\n",
    "    denom = float(L) * float(F) * 4.0\n",
    "    if denom <= 0:\n",
    "        return min_chunk\n",
    "    c = int(target_bytes // denom)\n",
    "    c = max(min_chunk, min(max_chunk, c))\n",
    "    return int(c)\n",
    "\n",
    "def build_agg_seq_features(X_mm, B_mm, M_mm, chunk=None):\n",
    "    snr_i = feat[\"snr_tanh\"]\n",
    "    det_i = feat[\"detected\"]\n",
    "    val_i = feat[VAL_FEAT]\n",
    "\n",
    "    if chunk is None:\n",
    "        chunk = _auto_chunk(L, Fdim, target_mb=220, min_chunk=64, max_chunk=1024)\n",
    "\n",
    "    out_chunks = []\n",
    "    for s in range(0, N, chunk):\n",
    "        e = min(N, s + chunk)\n",
    "\n",
    "        # NOTE: slicing memmap will read chunk; keep chunk modest to avoid RAM spikes\n",
    "        Xc = np.asarray(X_mm[s:e])  # (B,L,F)\n",
    "        Bc = np.asarray(B_mm[s:e])  # (B,L)\n",
    "        Mc = np.asarray(M_mm[s:e])  # (B,L)\n",
    "\n",
    "        real = (Mc == 1)\n",
    "        tok_count = real.sum(axis=1).astype(np.float32)\n",
    "\n",
    "        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32, copy=False)\n",
    "        det = (Xc[:, :, det_i] > 0.5).astype(np.float32, copy=False)\n",
    "        val = Xc[:, :, val_i].astype(np.float32, copy=False)\n",
    "\n",
    "        snr_r = snr * real\n",
    "        det_r = det * real\n",
    "\n",
    "        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n",
    "        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n",
    "        max_abs_snr = np.where(tok_count > 0, snr_r.max(axis=1), 0.0).astype(np.float32)\n",
    "\n",
    "        if VAL_IS_MAG:\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            mean_val = np.nan_to_num(np.nanmean(val_r, axis=1).astype(np.float32), nan=0.0)\n",
    "            std_val  = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32),  nan=0.0)\n",
    "            min_val  = np.nan_to_num(np.nanmin(val_r, axis=1).astype(np.float32),  nan=0.0)\n",
    "            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n",
    "        else:\n",
    "            aval = np.abs(val).astype(np.float32, copy=False)\n",
    "            aval_r = aval * real\n",
    "            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            std_val = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32), nan=0.0)\n",
    "            max_aval = np.where(tok_count > 0, aval_r.max(axis=1), 0.0).astype(np.float32)\n",
    "            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n",
    "\n",
    "        # band ids to [0..N_BANDS-1]\n",
    "        if SHIFT_BAND_IDS:\n",
    "            Badj = Bc.astype(np.int16, copy=False)\n",
    "            Badj = np.where(real, np.clip(Badj - 1, 0, N_BANDS - 1), 0).astype(np.int16, copy=False)\n",
    "        else:\n",
    "            Badj = Bc.astype(np.int16, copy=False)\n",
    "\n",
    "        per_band = []\n",
    "        for b in range(N_BANDS):\n",
    "            bm = (Badj == b) & real\n",
    "            cnt = bm.sum(axis=1).astype(np.float32)\n",
    "\n",
    "            detb = (det * bm).sum(axis=1).astype(np.float32)\n",
    "            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n",
    "\n",
    "            det_frac_b = _safe_div(detb, cnt)\n",
    "            mean_abs_snr_b = _safe_div(snrb, cnt)\n",
    "\n",
    "            if VAL_IS_MAG:\n",
    "                vb = np.where(bm, val, np.nan)\n",
    "                mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n",
    "            else:\n",
    "                ab = (np.abs(val).astype(np.float32) * bm).sum(axis=1).astype(np.float32)\n",
    "                mean_val_b = _safe_div(ab, cnt).astype(np.float32)\n",
    "\n",
    "            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n",
    "\n",
    "        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n",
    "\n",
    "        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n",
    "        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n",
    "\n",
    "        out_chunks.append(agg)\n",
    "\n",
    "        del Xc, Bc, Mc, Badj\n",
    "        if ((s // chunk) % 4) == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    return np.concatenate(out_chunks, axis=0).astype(np.float32)\n",
    "\n",
    "# ---- cache key for G_seq ----\n",
    "def _hash_cfg(d: dict) -> str:\n",
    "    s = json.dumps(d, sort_keys=True, ensure_ascii=True)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "G_CACHE_DIR = FIX_DIR\n",
    "G_RAW_CACHE = G_CACHE_DIR / \"global_features_raw.npy\"\n",
    "G_RAW_META  = G_CACHE_DIR / \"global_features_raw_meta.json\"\n",
    "\n",
    "agg_spec = {\n",
    "    \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n",
    "    \"N\": int(N),\n",
    "    \"L\": int(L),\n",
    "    \"Fdim\": int(Fdim),\n",
    "    \"n_bands\": int(N_BANDS),\n",
    "    \"seq_token_mode\": str(SEQ_TOKEN_MODE),\n",
    "    \"val_feat\": str(VAL_FEAT),\n",
    "    \"val_is_mag\": bool(VAL_IS_MAG),\n",
    "    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n",
    "    \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n",
    "    \"meta_cols\": list(BASE_G_COLS),\n",
    "}\n",
    "agg_hash = _hash_cfg(agg_spec)\n",
    "\n",
    "G_raw_np = None\n",
    "if G_RAW_CACHE.exists() and G_RAW_META.exists():\n",
    "    try:\n",
    "        old = json.loads(G_RAW_META.read_text())\n",
    "        if old.get(\"agg_hash\", None) == agg_hash and int(old.get(\"N\", -1)) == int(N):\n",
    "            G_raw_np = np.load(G_RAW_CACHE, allow_pickle=False).astype(np.float32, copy=False)\n",
    "            if G_raw_np.shape[0] != N:\n",
    "                G_raw_np = None\n",
    "    except Exception:\n",
    "        G_raw_np = None\n",
    "\n",
    "if G_raw_np is None:\n",
    "    if USE_AGG_SEQ_FEATURES:\n",
    "        chunk0 = _auto_chunk(L, Fdim, target_mb=220, min_chunk=64, max_chunk=1024)\n",
    "        print(f\"[Stage 8] Building AGG sequence features (one-time, cached)... chunk={chunk0}\")\n",
    "        t0 = time.time()\n",
    "        G_seq_np = build_agg_seq_features(X_mm, B_mm, M_mm, chunk=chunk0)\n",
    "        print(f\"[Stage 8] AGG built: shape={G_seq_np.shape} | time={time.time()-t0:.1f}s\")\n",
    "    else:\n",
    "        G_seq_np = np.zeros((N,0), dtype=np.float32)\n",
    "\n",
    "    G_raw_np = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\n",
    "    np.save(G_RAW_CACHE, G_raw_np.astype(np.float32, copy=False))\n",
    "    G_RAW_META.write_text(json.dumps({\"agg_hash\": agg_hash, \"N\": int(N), \"spec\": agg_spec}, indent=2))\n",
    "\n",
    "g_dim = int(G_raw_np.shape[1])\n",
    "\n",
    "with open(Path(LOG_DIR)/\"global_feature_spec.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"agg_hash\": agg_hash,\n",
    "            \"meta_cols\": BASE_G_COLS,\n",
    "            \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n",
    "            \"token_mode\": SEQ_TOKEN_MODE,\n",
    "            \"val_feat\": VAL_FEAT,\n",
    "            \"val_is_mag\": bool(VAL_IS_MAG),\n",
    "            \"total_g_dim\": int(g_dim),\n",
    "            \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "            \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n",
    "            \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n",
    "            \"score_value_feat_from_stage6\": (None if SCORE_VALUE_FEAT is None else str(SCORE_VALUE_FEAT)),\n",
    "            \"cache\": {\"path\": str(G_RAW_CACHE), \"meta\": str(G_RAW_META)},\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dataset / Loader\n",
    "# ----------------------------\n",
    "class MemmapSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, idx, X_mm, B_mm, M_mm, G_raw_np, y=None):\n",
    "        self.idx = np.asarray(idx, dtype=np.int32)\n",
    "        self.X_mm = X_mm\n",
    "        self.B_mm = B_mm\n",
    "        self.M_mm = M_mm\n",
    "        self.G_raw = G_raw_np\n",
    "        self.y = None if y is None else np.asarray(y, dtype=np.int8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "        X = np.asarray(self.X_mm[j])                   # (L,F)\n",
    "        B = np.asarray(self.B_mm[j]).astype(np.int64)  # (L,)\n",
    "        M = np.asarray(self.M_mm[j]).astype(np.int64)  # (L,)\n",
    "        G0 = np.asarray(self.G_raw[j], dtype=np.float32)\n",
    "\n",
    "        Xt = torch.from_numpy(X)   # dtype as memmap; model casts to float32\n",
    "        Bt = torch.from_numpy(B)\n",
    "        Mt = torch.from_numpy(M)\n",
    "        Gt = torch.from_numpy(G0)\n",
    "\n",
    "        if self.y is None:\n",
    "            return Xt, Bt, Mt, Gt\n",
    "\n",
    "        yy = float(self.y[j])\n",
    "        return Xt, Bt, Mt, Gt, torch.tensor(yy, dtype=torch.float32)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, sampler=None):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(sampler is None and shuffle),\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# 5) EMA helper\n",
    "# ----------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = float(decay)\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        d = self.decay\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            self.shadow[name].mul_(d).add_(p.detach(), alpha=(1.0 - d))\n",
    "\n",
    "    def store(self, model):\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.backup[name] = p.detach().clone()\n",
    "\n",
    "    def copy_to(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    def restore(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.data.copy_(self.backup[name].data)\n",
    "        self.backup = {}\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Model\n",
    "# ----------------------------\n",
    "class MultibandEventTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim, max_len, n_bands=6,\n",
    "        d_model=160, n_heads=4, n_layers=3, ff_mult=2,\n",
    "        dropout=0.14, g_dim=0,\n",
    "        shift_band_ids=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_bands = int(n_bands)\n",
    "        self.d_model = int(d_model)\n",
    "        self.max_len = int(max_len)\n",
    "        self.shift_band_ids = bool(shift_band_ids)\n",
    "\n",
    "        self.x_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.x_ln = nn.LayerNorm(d_model)  # stabilizer\n",
    "\n",
    "        self.band_emb = nn.Embedding(self.n_bands, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * ff_mult),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.attn = nn.Linear(d_model, 1)\n",
    "        self.pool_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        g_out = max(32, d_model // 2)\n",
    "        self.g_proj = nn.Sequential(\n",
    "            nn.Linear(g_dim, g_out),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model + g_out, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def set_global_scaler(self, mean_np, std_np):\n",
    "        mean_t = torch.tensor(mean_np, dtype=torch.float32)\n",
    "        std_t  = torch.tensor(std_np, dtype=torch.float32)\n",
    "        self.register_buffer(\"g_mean_buf\", mean_t, persistent=False)\n",
    "        self.register_buffer(\"g_std_buf\", std_t, persistent=False)\n",
    "\n",
    "    def forward(self, X, band_id, mask, G_raw):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        if self.shift_band_ids:\n",
    "            real = (mask == 1)\n",
    "            if real.any():\n",
    "                band2 = band_id.clone()\n",
    "                band2[real] = (band2[real] - 1).clamp(0, self.n_bands - 1)\n",
    "                band2[~real] = 0\n",
    "                band_id = band2\n",
    "            else:\n",
    "                band_id = torch.zeros_like(band_id)\n",
    "\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1)\n",
    "\n",
    "        pad_mask = (mask == 0)\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        h = self.x_proj(X)\n",
    "        h = self.x_ln(h)\n",
    "        h = h + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        # attention pooling\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        # mean pooling\n",
    "        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n",
    "        denom = valid.sum(dim=1).clamp_min(1.0)\n",
    "        pooled_mean = (h * valid).sum(dim=1) / denom\n",
    "\n",
    "        # max pooling\n",
    "        h_masked = h.masked_fill(pad_mask.unsqueeze(-1), -1e9)\n",
    "        pooled_max = torch.max(h_masked, dim=1).values\n",
    "        pooled_max = torch.where(torch.isfinite(pooled_max), pooled_max, torch.zeros_like(pooled_max))\n",
    "\n",
    "        pooled = (0.50 * pooled_attn) + (0.30 * pooled_mean) + (0.20 * pooled_max)\n",
    "        pooled = self.pool_ln(pooled)\n",
    "\n",
    "        # global\n",
    "        G = G_raw.to(torch.float32)\n",
    "        G = (G - self.g_mean_buf) / self.g_std_buf\n",
    "        g = self.g_proj(G)\n",
    "\n",
    "        z = torch.cat([pooled, g], dim=1)\n",
    "        return self.head(z).squeeze(-1)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Training config (improved defaults)\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    \"d_model\": 160,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 3,\n",
    "    \"ff_mult\": 2,\n",
    "    \"dropout\": 0.14,\n",
    "\n",
    "    \"batch_size\": 16,\n",
    "    \"grad_accum\": 2,\n",
    "\n",
    "    \"epochs\": 18,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.02,\n",
    "\n",
    "    \"patience\": 6,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "\n",
    "    # imbalance:\n",
    "    # \"pos_weight\" recommended default; PR-AUC becomes primary metric\n",
    "    \"balance_mode\": \"pos_weight\",   # pos_weight / sampler / both\n",
    "    \"label_smoothing\": 0.02,\n",
    "\n",
    "    # optional focal (set >0 if imbalance ekstrem; start 1.0~2.0)\n",
    "    \"focal_gamma\": 0.0,             # 0.0 = OFF\n",
    "\n",
    "    \"primary_metric\": \"ap\",         # \"ap\" (PR-AUC) or \"auc\"\n",
    "    \"scheduler\": \"onecycle\",\n",
    "\n",
    "    \"use_ema\": True,\n",
    "    \"ema_decay\": 0.999,\n",
    "\n",
    "    # augmentation (keep moderate on CPU)\n",
    "    \"aug_tokendrop_p\": 0.05,\n",
    "    \"aug_value_noise\": 0.010,\n",
    "    \"aug_featdrop_p\": 0.00,\n",
    "}\n",
    "\n",
    "# adapt for long sequences\n",
    "if L >= 512:\n",
    "    CFG[\"d_model\"] = 128\n",
    "    CFG[\"n_heads\"] = 4\n",
    "    CFG[\"n_layers\"] = 2\n",
    "    CFG[\"batch_size\"] = 12\n",
    "    CFG[\"grad_accum\"] = 2\n",
    "    CFG[\"lr\"] = 4e-4\n",
    "    CFG[\"dropout\"] = 0.16\n",
    "\n",
    "cfg_path = Path(LOG_DIR) / \"train_cfg_stage8.json\"\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "\n",
    "pos_all = int((y == 1).sum())\n",
    "neg_all = int((y == 0).sum())\n",
    "print(\"[Stage 8] TRAIN CONFIG (CPU)\")\n",
    "print(f\"- N={N:,} | pos={pos_all:,} | neg={neg_all:,} | pos%={pos_all/max(N,1)*100:.6f}%\")\n",
    "print(f\"- token_mode={SEQ_TOKEN_MODE} | VAL_FEAT={VAL_FEAT} | val_is_mag={VAL_IS_MAG} | g_dim={g_dim} | use_agg_seq={USE_AGG_SEQ_FEATURES}\")\n",
    "print(f\"- Model: d_model={CFG['d_model']} heads={CFG['n_heads']} layers={CFG['n_layers']} dropout={CFG['dropout']}\")\n",
    "print(f\"- Batch={CFG['batch_size']} grad_accum={CFG['grad_accum']} epochs={CFG['epochs']} lr={CFG['lr']}\")\n",
    "print(f\"- balance_mode={CFG['balance_mode']} | focal_gamma={CFG['focal_gamma']} | primary_metric={CFG['primary_metric']}\")\n",
    "print(f\"- ema={CFG['use_ema']}({CFG['ema_decay']})\")\n",
    "print(f\"- CKPT_DIR={CKPT_DIR}\")\n",
    "print(f\"- OOF_DIR ={OOF_DIR}\")\n",
    "print(f\"- LOG_DIR ={LOG_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Helpers\n",
    "# ----------------------------\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def f1_binary(y_true, y_pred01):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred01 = y_pred01.astype(np.int32)\n",
    "    tp = int(((y_true == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred01 == 0)).sum())\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return float(2 * prec * rec / (prec + rec))\n",
    "\n",
    "def best_f1_threshold(y_true, prob, grid=240):\n",
    "    y_true = y_true.astype(np.int8)\n",
    "    prob = prob.astype(np.float32)\n",
    "    best_thr, best_f1 = 0.5, -1.0\n",
    "    for thr in np.linspace(0.01, 0.99, grid, dtype=np.float32):\n",
    "        f1 = f1_binary(y_true, (prob >= thr).astype(np.int8))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = float(f1), float(thr)\n",
    "    return best_thr, best_f1\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_model(model, loader, criterion_eval, use_ema=False, ema=None):\n",
    "    model.eval()\n",
    "    if use_ema and (ema is not None):\n",
    "        ema.store(model)\n",
    "        ema.copy_to(model)\n",
    "\n",
    "    losses, logits_all, y_all = [], [], []\n",
    "    for batch in loader:\n",
    "        Xb, Bb, Mb, Gb, yb = batch\n",
    "        Xb = Xb.to(device); Bb = Bb.to(device); Mb = Mb.to(device); Gb = Gb.to(device); yb = yb.to(device)\n",
    "        logit = model(Xb, Bb, Mb, Gb)\n",
    "        loss = criterion_eval(logit, yb)\n",
    "        losses.append(float(loss.item()))\n",
    "        logits_all.append(logit.detach().cpu().numpy())\n",
    "        y_all.append(yb.detach().cpu().numpy())\n",
    "\n",
    "    if use_ema and (ema is not None):\n",
    "        ema.restore(model)\n",
    "\n",
    "    logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,), dtype=np.float32)\n",
    "    y_all = np.concatenate(y_all, axis=0).astype(np.int8) if y_all else np.zeros((0,), dtype=np.int8)\n",
    "\n",
    "    probs = sigmoid_np(logits_all)\n",
    "    pred01 = (probs >= 0.5).astype(np.int8)\n",
    "\n",
    "    f1 = f1_binary(y_all, pred01)\n",
    "    if len(np.unique(y_all)) == 2:\n",
    "        auc = float(roc_auc_score(y_all, probs))\n",
    "        ap  = float(average_precision_score(y_all, probs))\n",
    "    else:\n",
    "        auc, ap = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    return float(np.mean(losses) if losses else np.nan), probs, y_all, f1, auc, ap\n",
    "\n",
    "def fit_scaler_fold(G_raw_np, tr_idx):\n",
    "    X = G_raw_np[tr_idx]\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    std  = X.std(axis=0).astype(np.float32)\n",
    "    std  = np.where(std < 1e-6, 1.0, std).astype(np.float32)\n",
    "    return mean, std\n",
    "\n",
    "def make_adamw_param_groups(model, weight_decay):\n",
    "    decay, no_decay = [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        ln = name.lower()\n",
    "        if name.endswith(\".bias\") or (\"norm\" in ln) or (\"ln\" in ln):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    return [\n",
    "        {\"params\": decay, \"weight_decay\": float(weight_decay)},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "# focal with logits (optional) + optional pos_weight already inside BCE\n",
    "def make_train_loss(pos_weight_t=None, focal_gamma=0.0):\n",
    "    focal_gamma = float(focal_gamma)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t, reduction=\"none\") if pos_weight_t is not None else nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    def _loss(logit, target):\n",
    "        # target in [0,1]\n",
    "        target = target.to(logit.dtype)\n",
    "        base = bce(logit, target)  # (B,)\n",
    "        if focal_gamma <= 0:\n",
    "            return base.mean()\n",
    "        p = torch.sigmoid(logit)\n",
    "        pt = torch.where(target > 0.5, p, 1.0 - p)\n",
    "        w = (1.0 - pt).clamp_min(0.0).pow(focal_gamma)\n",
    "        return (w * base).mean()\n",
    "    return _loss\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Batch augment (safe)\n",
    "# ----------------------------\n",
    "def apply_batch_aug(Xb, Bb, Mb, cfg, feat_map, val_feat_name):\n",
    "    p_drop  = float(cfg.get(\"aug_tokendrop_p\", 0.0))\n",
    "    noise   = float(cfg.get(\"aug_value_noise\", 0.0))\n",
    "    p_fdrop = float(cfg.get(\"aug_featdrop_p\", 0.0))\n",
    "\n",
    "    # token drop\n",
    "    if p_drop and p_drop > 0:\n",
    "        real = (Mb == 1)\n",
    "        if real.any():\n",
    "            rnd = torch.rand_like(Mb.float())\n",
    "            drop = (rnd < p_drop) & real\n",
    "\n",
    "            # ensure at least 1 token remains for samples that had any\n",
    "            nreal = real.sum(dim=1)\n",
    "            ndrop = drop.sum(dim=1)\n",
    "            bad = (nreal > 0) & (ndrop >= nreal)\n",
    "            if bad.any():\n",
    "                bad_idx = torch.where(bad)[0].tolist()\n",
    "                for bi in bad_idx:\n",
    "                    pos = torch.where(real[bi])[0]\n",
    "                    if pos.numel() > 0:\n",
    "                        keep_one = pos[int(torch.randint(0, pos.numel(), (1,)).item())]\n",
    "                        drop[bi, keep_one] = False\n",
    "\n",
    "            Mb = Mb.clone()\n",
    "            Mb[drop] = 0\n",
    "\n",
    "    # value noise\n",
    "    if noise and noise > 0 and (val_feat_name in feat_map):\n",
    "        vi = int(feat_map[val_feat_name])\n",
    "        real = (Mb == 1)\n",
    "        n = int(real.sum().item())\n",
    "        if n > 0:\n",
    "            eps = torch.randn((n,), device=Xb.device, dtype=Xb.dtype) * float(noise)\n",
    "            Xb = Xb.clone()\n",
    "            col = Xb[:, :, vi].clone()\n",
    "            col[real] = col[real] + eps\n",
    "            Xb[:, :, vi] = col\n",
    "\n",
    "    # feature dropout (optional): only on key features\n",
    "    if p_fdrop and p_fdrop > 0:\n",
    "        real = (Mb == 1)\n",
    "        if real.any():\n",
    "            cand_feats = []\n",
    "            for nm in [val_feat_name, \"snr_tanh\"]:\n",
    "                if nm in feat_map:\n",
    "                    cand_feats.append(int(feat_map[nm]))\n",
    "            if cand_feats:\n",
    "                Xb = Xb.clone()\n",
    "                rnd = torch.rand_like(Mb.float())\n",
    "                for fi in cand_feats:\n",
    "                    mask_drop = (rnd < p_fdrop) & real\n",
    "                    col = Xb[:, :, fi].clone()\n",
    "                    col[mask_drop] = 0.0\n",
    "                    Xb[:, :, fi] = col\n",
    "\n",
    "    return Xb, Bb, Mb\n",
    "\n",
    "# ----------------------------\n",
    "# 10) CV Train\n",
    "# ----------------------------\n",
    "oof_prob = np.full((N,), np.nan, dtype=np.float32)\n",
    "fold_metrics = []\n",
    "\n",
    "start_time = time.time()\n",
    "n_splits = int(globals()[\"n_splits\"])\n",
    "cv_type = str(globals().get(\"CV_TYPE\", \"\"))\n",
    "\n",
    "primary_metric = str(CFG.get(\"primary_metric\", \"ap\")).lower().strip()\n",
    "if primary_metric not in (\"ap\", \"auc\"):\n",
    "    primary_metric = \"ap\"\n",
    "\n",
    "for fold_info in globals()[\"folds\"]:\n",
    "    fold = int(fold_info.get(\"fold\", 0))\n",
    "    tr_idx = np.asarray(fold_info[\"train_idx\"], dtype=np.int32)\n",
    "    val_idx = np.asarray(fold_info[\"val_idx\"], dtype=np.int32)\n",
    "\n",
    "    y_tr = y[tr_idx]\n",
    "    pos_f = int((y_tr == 1).sum())\n",
    "    neg_f = int((y_tr == 0).sum())\n",
    "    if pos_f == 0:\n",
    "        raise RuntimeError(f\"[Stage 8] Fold {fold}: no positives in training split.\")\n",
    "\n",
    "    balance_mode = str(CFG.get(\"balance_mode\", \"pos_weight\")).lower().strip()\n",
    "    use_sampler = balance_mode in (\"sampler\", \"both\")\n",
    "    use_posw    = balance_mode in (\"pos_weight\", \"both\")\n",
    "\n",
    "    pos_weight = float(neg_f / max(pos_f, 1))\n",
    "    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device) if use_posw else None\n",
    "\n",
    "    ls = float(CFG.get(\"label_smoothing\", 0.0))\n",
    "    def smooth(yb):\n",
    "        if ls <= 0:\n",
    "            return yb\n",
    "        return yb * (1.0 - ls) + 0.5 * ls\n",
    "\n",
    "    # train loss (weighted + optional focal)\n",
    "    focal_gamma = float(CFG.get(\"focal_gamma\", 0.0))\n",
    "    loss_train_fn = make_train_loss(pos_weight_t=pos_weight_t, focal_gamma=focal_gamma)\n",
    "    # eval loss (unweighted BCE for comparability)\n",
    "    criterion_eval  = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print(f\"\\n[Stage 8] FOLD {fold} | train={len(tr_idx):,} val={len(val_idx):,} \"\n",
    "          f\"| pos={pos_f:,} neg={neg_f:,} | pos_weight={pos_weight:.4f} | balance_mode={balance_mode} | primary={primary_metric}\")\n",
    "\n",
    "    g_mean, g_std = fit_scaler_fold(G_raw_np, tr_idx)\n",
    "\n",
    "    ds_tr = MemmapSeqDataset(tr_idx, X_mm, B_mm, M_mm, G_raw_np, y=y)\n",
    "    ds_va = MemmapSeqDataset(val_idx, X_mm, B_mm, M_mm, G_raw_np, y=y)\n",
    "\n",
    "    sampler = None\n",
    "    if use_sampler:\n",
    "        ytr_local = y[tr_idx]\n",
    "        w = np.ones((len(tr_idx),), dtype=np.float32)\n",
    "        w[ytr_local == 1] = float(neg_f / max(pos_f, 1))\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=torch.from_numpy(w),\n",
    "            num_samples=len(tr_idx),\n",
    "            replacement=True\n",
    "        )\n",
    "\n",
    "    dl_tr = make_loader(ds_tr, batch_size=int(CFG[\"batch_size\"]), shuffle=True, sampler=sampler)\n",
    "    dl_va = make_loader(ds_va, batch_size=int(CFG[\"batch_size\"]), shuffle=False)\n",
    "\n",
    "    model = MultibandEventTransformer(\n",
    "        feat_dim=Fdim,\n",
    "        max_len=L,\n",
    "        n_bands=6,\n",
    "        d_model=int(CFG[\"d_model\"]),\n",
    "        n_heads=int(CFG[\"n_heads\"]),\n",
    "        n_layers=int(CFG[\"n_layers\"]),\n",
    "        ff_mult=int(CFG[\"ff_mult\"]),\n",
    "        dropout=float(CFG[\"dropout\"]),\n",
    "        g_dim=g_dim,\n",
    "        shift_band_ids=bool(SHIFT_BAND_IDS),\n",
    "    ).to(device)\n",
    "    model.set_global_scaler(g_mean, g_std)\n",
    "\n",
    "    param_groups = make_adamw_param_groups(model, weight_decay=float(CFG[\"weight_decay\"]))\n",
    "    opt = torch.optim.AdamW(param_groups, lr=float(CFG[\"lr\"]))\n",
    "\n",
    "    scheduler = None\n",
    "    grad_accum = int(CFG[\"grad_accum\"])\n",
    "    if str(CFG.get(\"scheduler\",\"\")).lower() == \"onecycle\":\n",
    "        steps_per_epoch_opt = int(math.ceil(len(dl_tr) / max(grad_accum, 1)))\n",
    "        steps_per_epoch_opt = max(steps_per_epoch_opt, 1)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt,\n",
    "            max_lr=float(CFG[\"lr\"]),\n",
    "            epochs=int(CFG[\"epochs\"]),\n",
    "            steps_per_epoch=steps_per_epoch_opt,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"cos\",\n",
    "            div_factor=10.0,\n",
    "            final_div_factor=50.0,\n",
    "        )\n",
    "\n",
    "    use_ema = bool(CFG.get(\"use_ema\", True))\n",
    "    ema = EMA(model, decay=float(CFG.get(\"ema_decay\", 0.999))) if use_ema else None\n",
    "\n",
    "    # best tracking\n",
    "    best_score = -1e18\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_probs = None\n",
    "    best_thr = 0.5\n",
    "    best_f1_at_bestthr = -1.0\n",
    "    patience_left = int(CFG[\"patience\"])\n",
    "\n",
    "    # history log\n",
    "    hist_rows = []\n",
    "\n",
    "    for epoch in range(1, int(CFG[\"epochs\"]) + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        accum = 0\n",
    "        opt_steps = 0\n",
    "\n",
    "        for batch in dl_tr:\n",
    "            Xb, Bb, Mb, Gb, yb = batch\n",
    "            Xb = Xb.to(device).to(torch.float32)\n",
    "            Bb = Bb.to(device)\n",
    "            Mb = Mb.to(device)\n",
    "            Gb = Gb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            # aug\n",
    "            Xb, Bb, Mb = apply_batch_aug(Xb, Bb, Mb, CFG, feat, VAL_FEAT)\n",
    "\n",
    "            yb_s = smooth(yb)\n",
    "            logit = model(Xb, Bb, Mb, Gb)\n",
    "            loss = loss_train_fn(logit, yb_s)\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "            (loss / float(grad_accum)).backward()\n",
    "            accum += 1\n",
    "\n",
    "            if accum == grad_accum:\n",
    "                if CFG[\"max_grad_norm\"] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                opt_steps += 1\n",
    "                accum = 0\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        if accum > 0:\n",
    "            if CFG[\"max_grad_norm\"] is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CFG[\"max_grad_norm\"]))\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            opt_steps += 1\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        train_loss = total_loss / max(n_batches, 1)\n",
    "\n",
    "        val_loss, probs, y_val, f1_05, val_auc, val_ap = eval_model(model, dl_va, criterion_eval, use_ema=use_ema, ema=ema)\n",
    "        score = float(val_ap) if primary_metric == \"ap\" else float(val_auc)\n",
    "\n",
    "        # improve rule: primary metric first, tie-break val_loss\n",
    "        improved = (score > best_score + 1e-7) or (math.isnan(best_score) and not math.isnan(score))\n",
    "        if (not improved) and (abs(score - best_score) <= 1e-7) and (val_loss < best_val_loss - 1e-6):\n",
    "            improved = True\n",
    "\n",
    "        if improved:\n",
    "            best_score = float(score)\n",
    "            best_val_loss = float(val_loss)\n",
    "            best_epoch = int(epoch)\n",
    "            best_probs = probs.copy()\n",
    "            best_thr, best_f1_at_bestthr = best_f1_threshold(y_val, best_probs, grid=240)\n",
    "\n",
    "            ckpt_path = CKPT_DIR / f\"fold_{fold}.pt\"\n",
    "            payload = {\n",
    "                \"fold\": fold,\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"cfg\": CFG,\n",
    "                \"seq_feature_names\": SEQ_FEATURE_NAMES,\n",
    "                \"max_len\": L,\n",
    "                \"token_mode\": SEQ_TOKEN_MODE,\n",
    "                \"val_feat\": VAL_FEAT,\n",
    "                \"val_is_mag\": bool(VAL_IS_MAG),\n",
    "                \"global_meta_cols\": BASE_G_COLS,\n",
    "                \"use_agg_seq_features\": bool(USE_AGG_SEQ_FEATURES),\n",
    "                \"global_feature_cache\": {\"path\": str(G_RAW_CACHE), \"meta\": str(G_RAW_META), \"agg_hash\": agg_hash},\n",
    "                \"global_scaler\": {\"mean\": g_mean.astype(np.float32), \"std\": g_std.astype(np.float32)},\n",
    "                \"pos_weight_train\": float(pos_weight),\n",
    "                \"balance_mode\": balance_mode,\n",
    "                \"focal_gamma\": float(focal_gamma),\n",
    "                \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "                \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n",
    "                \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n",
    "                \"cv_type\": str(cv_type),\n",
    "                \"score_value_feat_from_stage6\": (None if SCORE_VALUE_FEAT is None else str(SCORE_VALUE_FEAT)),\n",
    "                \"primary_metric\": primary_metric,\n",
    "                \"best_thr_val_f1\": float(best_thr),\n",
    "                \"best_f1_val\": float(best_f1_at_bestthr),\n",
    "                \"best_val_auc\": float(val_auc),\n",
    "                \"best_val_ap\": float(val_ap),\n",
    "                \"best_val_loss\": float(best_val_loss),\n",
    "            }\n",
    "            if ema is not None:\n",
    "                payload[\"ema_shadow\"] = {k: v.detach().cpu() for k, v in ema.shadow.items()}\n",
    "                payload[\"ema_decay\"] = float(ema.decay)\n",
    "\n",
    "            torch.save(payload, ckpt_path)\n",
    "            patience_left = int(CFG[\"patience\"])\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        hist_rows.append({\n",
    "            \"epoch\": int(epoch),\n",
    "            \"lr\": float(lr_now),\n",
    "            \"opt_steps\": int(opt_steps),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_auc\": float(val_auc),\n",
    "            \"val_ap\": float(val_ap),\n",
    "            \"f1_at_0p5\": float(f1_05),\n",
    "            \"score_primary\": float(score),\n",
    "            \"best_epoch\": int(best_epoch),\n",
    "            \"patience_left\": int(patience_left),\n",
    "        })\n",
    "\n",
    "        print(f\"  epoch {epoch:02d} | lr={lr_now:.2e} | opt_steps={opt_steps:4d} | \"\n",
    "              f\"train_loss={train_loss:.5f} | val_loss={val_loss:.5f} | auc={val_auc:.5f} | ap={val_ap:.5f} | \"\n",
    "              f\"f1@0.5={f1_05:.4f} | best_ep={best_epoch} | pat={patience_left}\")\n",
    "\n",
    "        if patience_left <= 0:\n",
    "            break\n",
    "\n",
    "    # save history\n",
    "    try:\n",
    "        pd.DataFrame(hist_rows).to_csv(Path(LOG_DIR)/f\"fold_{fold}_history.csv\", index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if best_probs is None:\n",
    "        raise RuntimeError(f\"Fold {fold}: best_probs is None (unexpected).\")\n",
    "\n",
    "    oof_prob[val_idx] = best_probs.astype(np.float32)\n",
    "\n",
    "    pred01 = (best_probs >= 0.5).astype(np.int8)\n",
    "    best_f1_05 = f1_binary(y[val_idx], pred01)\n",
    "\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold,\n",
    "        \"val_size\": int(len(val_idx)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"primary_metric\": primary_metric,\n",
    "        \"best_primary_score\": float(best_score),\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"val_auc_at_best\": float(val_auc),\n",
    "        \"val_ap_at_best\": float(val_ap),\n",
    "        \"f1_at_0p5\": float(best_f1_05),\n",
    "        \"best_thr_val_f1\": float(best_thr),\n",
    "        \"best_f1_val\": float(best_f1_at_bestthr),\n",
    "        \"pos_weight_train\": float(pos_weight),\n",
    "        \"focal_gamma\": float(focal_gamma),\n",
    "        \"g_dim\": int(g_dim),\n",
    "        \"use_agg_seq\": bool(USE_AGG_SEQ_FEATURES),\n",
    "        \"balance_mode\": balance_mode,\n",
    "        \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "        \"ema_used\": bool(use_ema),\n",
    "        \"val_feat\": str(VAL_FEAT),\n",
    "        \"val_is_mag\": bool(VAL_IS_MAG),\n",
    "    })\n",
    "\n",
    "    del model, opt, ds_tr, ds_va, dl_tr, dl_va, ema\n",
    "    gc.collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Save OOF artifacts + summary\n",
    "# ----------------------------\n",
    "oof_path_npy = OOF_DIR / \"oof_prob.npy\"\n",
    "np.save(oof_path_npy, oof_prob)\n",
    "\n",
    "df_oof = pd.DataFrame({\"object_id\": train_ids, \"target\": y.astype(int), \"oof_prob\": oof_prob.astype(np.float32)})\n",
    "oof_path_csv = OOF_DIR / \"oof_prob.csv\"\n",
    "df_oof.to_csv(oof_path_csv, index=False)\n",
    "\n",
    "metrics_path = OOF_DIR / \"fold_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"fold_metrics\": fold_metrics, \"elapsed_sec\": float(elapsed), \"cv_type\": str(cv_type)}, f, indent=2)\n",
    "\n",
    "valid = np.isfinite(oof_prob)\n",
    "if valid.any() and len(np.unique(y[valid])) == 2:\n",
    "    oof_auc = float(roc_auc_score(y[valid], oof_prob[valid]))\n",
    "    oof_ap  = float(average_precision_score(y[valid], oof_prob[valid]))\n",
    "else:\n",
    "    oof_auc = float(\"nan\")\n",
    "    oof_ap  = float(\"nan\")\n",
    "\n",
    "if valid.any():\n",
    "    oof_pred01 = (oof_prob[valid] >= 0.5).astype(np.int8)\n",
    "    oof_f1_05 = f1_binary(y[valid], oof_pred01)\n",
    "    oof_thr, oof_bestf1 = best_f1_threshold(y[valid], oof_prob[valid], grid=300)\n",
    "else:\n",
    "    oof_f1_05 = float(\"nan\")\n",
    "    oof_thr, oof_bestf1 = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "print(\"\\n[Stage 8] TRAIN DONE\")\n",
    "print(f\"- elapsed: {elapsed/60:.2f} min\")\n",
    "print(f\"- OOF saved: {oof_path_npy}\")\n",
    "print(f\"- OOF saved: {oof_path_csv}\")\n",
    "print(f\"- fold metrics: {metrics_path}\")\n",
    "print(f\"- OOF rows valid: {int(valid.sum()):,}/{N:,}\")\n",
    "print(f\"- OOF AUC (valid-only): {oof_auc:.5f}\")\n",
    "print(f\"- OOF AP  (valid-only): {oof_ap:.5f}\")\n",
    "print(f\"- OOF F1@0.5 (valid-only): {oof_f1_05:.4f}\")\n",
    "print(f\"- OOF best F1={oof_bestf1:.4f} @ thr={oof_thr:.4f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"oof_prob\": oof_prob,\n",
    "    \"OOF_PROB_PATH\": oof_path_npy,\n",
    "    \"OOF_CSV_PATH\": oof_path_csv,\n",
    "    \"FOLD_METRICS_PATH\": metrics_path,\n",
    "    \"TRAIN_CFG_PATH\": cfg_path,\n",
    "    \"VAL_FEAT\": VAL_FEAT,\n",
    "    \"VAL_IS_MAG\": VAL_IS_MAG,\n",
    "    \"OOF_BEST_THR_F1\": float(oof_thr) if np.isfinite(oof_thr) else None,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fc4c1",
   "metadata": {
    "papermill": {
     "duration": 0.023674,
     "end_time": "2026-01-07T19:45:33.322484",
     "exception": false,
     "start_time": "2026-01-07T19:45:33.298810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OOF Prediction + Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e4ba43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T19:45:33.374048Z",
     "iopub.status.busy": "2026-01-07T19:45:33.373660Z",
     "iopub.status.idle": "2026-01-07T19:45:33.754914Z",
     "shell.execute_reply": "2026-01-07T19:45:33.753934Z"
    },
    "papermill": {
     "duration": 0.409863,
     "end_time": "2026-01-07T19:45:33.757040",
     "exception": false,
     "start_time": "2026-01-07T19:45:33.347177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9] Loaded OOF from: csv(oof_prob.csv)\n",
      "[Stage 9] Valid rows: 3,043/3,043 (holdout mode => valid << total)\n",
      "[Stage 9] pos=148 | neg=2,895 | pos%=4.863621% | target_col=target\n",
      "[Stage 9] DONE\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/threshold_tuning.json\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/threshold_report.txt\n",
      "- Saved: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/threshold_table_top1000.csv\n",
      "- OOF AUC (valid-only): 0.842018 | AP: 0.224057\n",
      "- DEFAULT BEST_THR (ge/F1) = 0.647303 | F1=0.310905 (P=0.236749 R=0.452703)\n",
      "- BEST_THR_GE_MCC          = 0.643748 | MCC=0.280166\n",
      "- BEST_THR_GE_BACC         = 0.549385 | BACC=0.763373\n",
      "- BEST_THR_GT_F1           = 0.647290 (gt) | ge_equiv=0.647290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/3056600575.py:196: RuntimeWarning: invalid value encountered in divide\n",
      "  mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 9 — OOF Prediction + Threshold Tuning (ONE CELL)\n",
    "# REVISI FULL v4.4 (EXACT unique-level sweep + add F-beta + constraints + better tie-break)\n",
    "#\n",
    "# Upgrades v4.4:\n",
    "# - Exact sweep on UNIQUE prob levels (more accurate than mixed candidate grids)\n",
    "# - Still supports rule=\"ge\" (>=) and rule=\"gt\" (>)\n",
    "# - Provides ge_equiv thresholds for gt (via nextafter) for downstream that uses >=\n",
    "# - Adds F0.5 and F2 (precision/recall emphasis)\n",
    "# - Optional constraints: best thr with min_precision / min_recall\n",
    "# - Improved tie-break: after metric ties, prefer smaller |pos_pred - pos| gap\n",
    "#\n",
    "# Output:\n",
    "# - OOF_DIR/threshold_tuning.json\n",
    "# - OOF_DIR/threshold_report.txt\n",
    "# - OOF_DIR/threshold_table_top1000.csv\n",
    "# - globals: BEST_THR, BEST_THR_GE_F1, BEST_THR_GE_MCC, ... + tables\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, math, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"OOF_DIR\", \"df_train_meta\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..8 dulu.\")\n",
    "\n",
    "OOF_DIR = Path(OOF_DIR)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: robust stringify id\n",
    "# ----------------------------\n",
    "def _to_str_list(ids):\n",
    "    out = []\n",
    "    for x in ids:\n",
    "        if isinstance(x, (bytes, np.bytes_, bytearray)):\n",
    "            out.append(x.decode(\"utf-8\", errors=\"ignore\").strip())\n",
    "        else:\n",
    "            out.append(str(x).strip())\n",
    "    return out\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a.reshape(1)\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Detect target column in df_train_meta\n",
    "# ----------------------------\n",
    "def _detect_target_col(df):\n",
    "    for cand in [\"target\",\"y\",\"label\",\"class\",\"is_tde\",\"binary_target\",\"target_id\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "TARGET_COL = _detect_target_col(df_train_meta)\n",
    "if TARGET_COL is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot detect target column in df_train_meta. \"\n",
    "        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n",
    "    )\n",
    "\n",
    "# normalize meta index to string+strip once\n",
    "meta = df_train_meta.copy()\n",
    "meta.index = pd.Index(pd.Index(meta.index).astype(\"string\").str.strip().astype(str), name=\"object_id\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load OOF (prefer CSV)\n",
    "# ----------------------------\n",
    "def _load_oof():\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n",
    "            ids = df[\"object_id\"].astype(str).str.strip().tolist()\n",
    "            prob = _as_1d_float32(df[\"oof_prob\"].to_numpy())\n",
    "            return ids, prob, \"csv(oof_prob.csv)\"\n",
    "\n",
    "    if \"oof_prob\" in globals():\n",
    "        prob = _as_1d_float32(globals()[\"oof_prob\"])\n",
    "        if isinstance(prob, np.ndarray) and prob.ndim == 1 and len(prob) > 0:\n",
    "            if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n",
    "                ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n",
    "                return ids, prob, \"globals(oof_prob + train_ids_ordered)\"\n",
    "            if len(prob) == len(meta):\n",
    "                ids = _to_str_list(meta.index.tolist())\n",
    "                return ids, prob, \"globals(oof_prob + df_train_meta.index)\"\n",
    "\n",
    "    pnpy = OOF_DIR / \"oof_prob.npy\"\n",
    "    if pnpy.exists():\n",
    "        prob = _as_1d_float32(np.load(pnpy, allow_pickle=False))\n",
    "        if \"train_ids_ordered\" in globals() and globals()[\"train_ids_ordered\"] is not None:\n",
    "            ids = _to_str_list(list(globals()[\"train_ids_ordered\"]))\n",
    "            return ids, prob, \"npy(oof_prob.npy + train_ids_ordered)\"\n",
    "        if len(prob) == len(meta):\n",
    "            ids = _to_str_list(meta.index.tolist())\n",
    "            return ids, prob, \"npy(oof_prob.npy + df_train_meta.index)\"\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob not found (csv/globals/npy). Jalankan STAGE 8 dulu.\")\n",
    "\n",
    "train_ids, oof_prob, src = _load_oof()\n",
    "oof_prob = _as_1d_float32(oof_prob).astype(np.float32)\n",
    "\n",
    "if len(train_ids) != len(oof_prob):\n",
    "    raise RuntimeError(f\"OOF length mismatch: len(train_ids)={len(train_ids)} vs len(oof_prob)={len(oof_prob)}\")\n",
    "\n",
    "# IMPORTANT: keep NaN for holdout-safe (do NOT nan_to_num->0)\n",
    "valid = np.isfinite(oof_prob)\n",
    "if not valid.any():\n",
    "    raise RuntimeError(\"All oof_prob are non-finite (NaN/inf). Check STAGE 8 output.\")\n",
    "\n",
    "# align y by train_ids via fast indexer\n",
    "idx = meta.index.get_indexer(train_ids)\n",
    "if (idx < 0).any():\n",
    "    bad = [train_ids[i] for i in np.where(idx < 0)[0][:10]]\n",
    "    raise KeyError(\n",
    "        f\"OOF ids not found in df_train_meta.index (string-normalized). ex={bad} | missing_n={int((idx<0).sum())}\"\n",
    "    )\n",
    "\n",
    "y_raw = pd.to_numeric(meta[TARGET_COL], errors=\"coerce\").fillna(0).astype(np.int16).to_numpy(copy=False)\n",
    "y_all = (y_raw[idx] > 0).astype(np.int8)\n",
    "\n",
    "# filter valid rows\n",
    "train_ids_v = [train_ids[i] for i in np.where(valid)[0]]\n",
    "p_v = np.clip(oof_prob[valid].astype(np.float32), 0.0, 1.0)\n",
    "y_v = y_all[valid].astype(np.int8)\n",
    "\n",
    "N_all = int(len(y_all))\n",
    "N = int(len(y_v))\n",
    "pos = int((y_v == 1).sum())\n",
    "neg = int((y_v == 0).sum())\n",
    "\n",
    "print(f\"[Stage 9] Loaded OOF from: {src}\")\n",
    "print(f\"[Stage 9] Valid rows: {N:,}/{N_all:,} (holdout mode => valid << total)\")\n",
    "print(f\"[Stage 9] pos={pos:,} | neg={neg:,} | pos%={pos/max(N,1)*100:.6f}% | target_col={TARGET_COL}\")\n",
    "\n",
    "uy = set(np.unique(y_v).tolist())\n",
    "if not uy.issubset({0, 1}):\n",
    "    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n",
    "\n",
    "# threshold-free sanity\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    auc_oof = float(roc_auc_score(y_v, p_v)) if (len(uy) == 2 and N > 1) else float(\"nan\")\n",
    "    ap_oof  = float(average_precision_score(y_v, p_v)) if (len(uy) == 2 and N > 1) else float(\"nan\")\n",
    "except Exception:\n",
    "    auc_oof = float(\"nan\")\n",
    "    ap_oof  = float(\"nan\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Metric helpers (vectorized)\n",
    "# ----------------------------\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1e-12)\n",
    "\n",
    "def _fbeta(prec, rec, beta):\n",
    "    beta2 = float(beta) ** 2\n",
    "    return _safe_div((1.0 + beta2) * prec * rec, beta2 * prec + rec)\n",
    "\n",
    "def _metrics_from_counts(tp, fp, fn, tn):\n",
    "    tp = tp.astype(np.float64); fp = fp.astype(np.float64)\n",
    "    fn = fn.astype(np.float64); tn = tn.astype(np.float64)\n",
    "\n",
    "    prec = _safe_div(tp, tp + fp)\n",
    "    rec  = _safe_div(tp, tp + fn)\n",
    "\n",
    "    f1   = _safe_div(2 * prec * rec, prec + rec)\n",
    "    f05  = _fbeta(prec, rec, beta=0.5)\n",
    "    f2   = _fbeta(prec, rec, beta=2.0)\n",
    "\n",
    "    acc  = _safe_div(tp + tn, tp + fp + fn + tn)\n",
    "\n",
    "    tpr  = _safe_div(tp, tp + fn)\n",
    "    tnr  = _safe_div(tn, tn + fp)\n",
    "    bacc = 0.5 * (tpr + tnr)\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n",
    "\n",
    "    return f1, f05, f2, prec, rec, acc, bacc, mcc\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Exact sweep on UNIQUE probability levels (fast + exact)\n",
    "# ----------------------------\n",
    "p = p_v.astype(np.float32)\n",
    "y = y_v.astype(np.int8)\n",
    "\n",
    "ord_desc = np.argsort(-p, kind=\"mergesort\")  # stable\n",
    "p_sorted = p[ord_desc]\n",
    "y_sorted = y[ord_desc]\n",
    "\n",
    "pos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\n",
    "neg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\n",
    "\n",
    "pos_total = int(pos_prefix[-1]) if N > 0 else 0\n",
    "neg_total = int(neg_prefix[-1]) if N > 0 else 0\n",
    "\n",
    "# unique groups in descending sorted probs\n",
    "# group boundaries where p changes\n",
    "if N == 0:\n",
    "    raise RuntimeError(\"No valid rows to tune thresholds.\")\n",
    "\n",
    "chg = np.ones((N,), dtype=bool)\n",
    "chg[1:] = (p_sorted[1:] != p_sorted[:-1])\n",
    "\n",
    "grp_starts = np.where(chg)[0].astype(np.int64)\n",
    "grp_vals = p_sorted[grp_starts].astype(np.float32)\n",
    "\n",
    "# ends are starts next -1, last ends at N-1\n",
    "grp_ends = np.empty_like(grp_starts)\n",
    "grp_ends[:-1] = grp_starts[1:] - 1\n",
    "grp_ends[-1] = N - 1\n",
    "\n",
    "# for ge: pred positive includes current group => k = end+1\n",
    "k_ge = (grp_ends + 1).astype(np.int64)\n",
    "# for gt: pred positive excludes equals => k = start (count of > thr)\n",
    "k_gt = grp_starts.astype(np.int64)\n",
    "\n",
    "# also add edge thresholds:\n",
    "# - for ge: thr just above max => k=0\n",
    "# - for gt: thr below min => k=N\n",
    "thr_ge_edge = np.nextafter(np.float32(grp_vals[0]), np.float32(1.0))  # >max (if max<1)\n",
    "thr_gt_edge = np.float32(0.0)  # <=0 means almost all > thr depending (gt uses >)\n",
    "\n",
    "# build table function from k and thr\n",
    "def _table_from_k(thr_vals, k_vals, rule):\n",
    "    k = np.clip(k_vals, 0, N).astype(np.int64)\n",
    "    tp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\n",
    "    fp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\n",
    "    fn = (pos_total - tp).astype(np.int64)\n",
    "    tn = (neg_total - fp).astype(np.int64)\n",
    "\n",
    "    f1, f05, f2, prec, rec, acc, bacc, mcc = _metrics_from_counts(tp, fp, fn, tn)\n",
    "    pos_pred = k.astype(np.int64)\n",
    "    gap = np.abs(pos_pred - pos_total).astype(np.int64)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"thr\": thr_vals.astype(np.float32),\n",
    "        \"rule\": rule,\n",
    "        \"f1\": f1.astype(np.float32),\n",
    "        \"f05\": f05.astype(np.float32),\n",
    "        \"f2\": f2.astype(np.float32),\n",
    "        \"precision\": prec.astype(np.float32),\n",
    "        \"recall\": rec.astype(np.float32),\n",
    "        \"accuracy\": acc.astype(np.float32),\n",
    "        \"balanced_accuracy\": bacc.astype(np.float32),\n",
    "        \"mcc\": mcc.astype(np.float32),\n",
    "        \"tp\": tp.astype(np.int64),\n",
    "        \"fp\": fp.astype(np.int64),\n",
    "        \"fn\": fn.astype(np.int64),\n",
    "        \"tn\": tn.astype(np.int64),\n",
    "        \"pos_pred\": pos_pred.astype(np.int64),\n",
    "        \"pos_pred_gap\": gap.astype(np.int64),\n",
    "    })\n",
    "\n",
    "# base tables (unique)\n",
    "thr_table_ge = _table_from_k(grp_vals, k_ge, \"ge\")\n",
    "thr_table_gt = _table_from_k(grp_vals, k_gt, \"gt\")\n",
    "\n",
    "# add edge rows\n",
    "# ge edge: thr > max => k=0 (if thr_ge_edge==max when max==1, still ok)\n",
    "edge_ge = _table_from_k(np.array([thr_ge_edge], np.float32), np.array([0], np.int64), \"ge\")\n",
    "thr_table_ge = pd.concat([edge_ge, thr_table_ge], axis=0, ignore_index=True)\n",
    "\n",
    "# gt edge: thr < min => k=N (use thr=-eps as 0 with >0 rule; safest use thr=-1e-7 then clip? keep 0 with separate row)\n",
    "# better: use thr = np.nextafter(0, -1) to include probs==0 in >thr; but keep in [0,1] by storing 0 and k computed separately.\n",
    "edge_gt = _table_from_k(np.array([np.float32(0.0)], np.float32), np.array([N], np.int64), \"gt\")\n",
    "thr_table_gt = pd.concat([thr_table_gt, edge_gt], axis=0, ignore_index=True)\n",
    "\n",
    "# de-dup thresholds per rule (keep best by pos_pred count uniqueness)\n",
    "thr_table_ge = thr_table_ge.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\n",
    "thr_table_gt = thr_table_gt.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Add extra candidate thresholds (prevalence & Stage8 best)\n",
    "# ----------------------------\n",
    "# prevalence-match (roughly pos_pred ~ pos for ge)\n",
    "if pos_total > 0:\n",
    "    thr_prev = float(p_sorted[min(pos_total - 1, N - 1)])\n",
    "else:\n",
    "    thr_prev = 1.0\n",
    "\n",
    "extra_thr = [0.5, thr_prev, 0.0, 1.0]\n",
    "if \"OOF_BEST_THR_F1\" in globals() and globals()[\"OOF_BEST_THR_F1\"] is not None:\n",
    "    try:\n",
    "        extra_thr.append(float(globals()[\"OOF_BEST_THR_F1\"]))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _eval_specific(thr, rule):\n",
    "    thr = float(thr)\n",
    "    # use sorted probs and rule to find k\n",
    "    if rule == \"ge\":\n",
    "        # count(prob >= thr)\n",
    "        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"right\"))\n",
    "    else:\n",
    "        # count(prob > thr)\n",
    "        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"left\"))\n",
    "    k0 = max(0, min(k0, N))\n",
    "\n",
    "    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n",
    "    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n",
    "    fn0 = int(pos_total - tp0)\n",
    "    tn0 = int(neg_total - fp0)\n",
    "\n",
    "    prec0 = tp0 / max(tp0 + fp0, 1)\n",
    "    rec0  = tp0 / max(tp0 + fn0, 1)\n",
    "    f10 = 0.0 if (tp0 == 0 or (prec0 + rec0) == 0) else (2 * prec0 * rec0 / (prec0 + rec0))\n",
    "    f05 = 0.0 if (prec0 == 0 and rec0 == 0) else ((1.25 * prec0 * rec0) / max(0.25 * prec0 + rec0, 1e-12))\n",
    "    f2  = 0.0 if (prec0 == 0 and rec0 == 0) else ((5.00 * prec0 * rec0) / max(4.00 * prec0 + rec0, 1e-12))\n",
    "    acc0 = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n",
    "    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n",
    "    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n",
    "    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n",
    "    gap0 = abs(k0 - pos_total)\n",
    "\n",
    "    return {\n",
    "        \"thr\": thr, \"rule\": rule,\n",
    "        \"f1\": float(f10), \"f05\": float(f05), \"f2\": float(f2),\n",
    "        \"precision\": float(prec0), \"recall\": float(rec0),\n",
    "        \"accuracy\": float(acc0), \"balanced_accuracy\": float(bacc0), \"mcc\": float(mcc0),\n",
    "        \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0,\n",
    "        \"pos_pred\": int(k0), \"pos_pred_gap\": int(gap0),\n",
    "    }\n",
    "\n",
    "extra_rows = []\n",
    "for t in extra_thr:\n",
    "    extra_rows.append(_eval_specific(t, \"ge\"))\n",
    "    extra_rows.append(_eval_specific(t, \"gt\"))\n",
    "\n",
    "extra_df = pd.DataFrame(extra_rows)\n",
    "# merge (prefer keeping best metric rows if duplicates)\n",
    "thr_table_ge = pd.concat([thr_table_ge, extra_df[extra_df[\"rule\"]==\"ge\"]], ignore_index=True)\n",
    "thr_table_gt = pd.concat([thr_table_gt, extra_df[extra_df[\"rule\"]==\"gt\"]], ignore_index=True)\n",
    "thr_table_ge = thr_table_ge.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\n",
    "thr_table_gt = thr_table_gt.drop_duplicates(subset=[\"thr\",\"rule\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Best pickers + constraints\n",
    "# ----------------------------\n",
    "def _pick_best(df, primary, tie_cols=None):\n",
    "    # primary desc, tie cols desc, last: pos_pred_gap asc (prefer prevalence match)\n",
    "    tie_cols = tie_cols or []\n",
    "    sort_cols = [primary] + tie_cols + [\"pos_pred_gap\"]\n",
    "    asc = [False] * (1 + len(tie_cols)) + [True]\n",
    "    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n",
    "\n",
    "def _pick_best_constrained(df, primary, min_precision=None, min_recall=None):\n",
    "    dd = df.copy()\n",
    "    if min_precision is not None:\n",
    "        dd = dd[dd[\"precision\"] >= float(min_precision)]\n",
    "    if min_recall is not None:\n",
    "        dd = dd[dd[\"recall\"] >= float(min_recall)]\n",
    "    if len(dd) == 0:\n",
    "        return None\n",
    "    return _pick_best(dd, primary, tie_cols=[\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\n",
    "\n",
    "# baseline\n",
    "base05_ge = _eval_specific(0.5, \"ge\")\n",
    "base05_gt = _eval_specific(0.5, \"gt\")\n",
    "\n",
    "# best per rule\n",
    "best_ge_f1   = _pick_best(thr_table_ge, \"f1\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\n",
    "best_ge_f05  = _pick_best(thr_table_ge, \"f05\", [\"mcc\",\"balanced_accuracy\",\"precision\",\"recall\"])\n",
    "best_ge_f2   = _pick_best(thr_table_ge, \"f2\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\n",
    "best_ge_mcc  = _pick_best(thr_table_ge, \"mcc\", [\"f1\",\"balanced_accuracy\",\"accuracy\"])\n",
    "best_ge_bacc = _pick_best(thr_table_ge, \"balanced_accuracy\", [\"mcc\",\"accuracy\",\"f1\"])\n",
    "\n",
    "best_gt_f1   = _pick_best(thr_table_gt, \"f1\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\n",
    "best_gt_f05  = _pick_best(thr_table_gt, \"f05\", [\"mcc\",\"balanced_accuracy\",\"precision\",\"recall\"])\n",
    "best_gt_f2   = _pick_best(thr_table_gt, \"f2\",  [\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\"])\n",
    "best_gt_mcc  = _pick_best(thr_table_gt, \"mcc\", [\"f1\",\"balanced_accuracy\",\"accuracy\"])\n",
    "best_gt_bacc = _pick_best(thr_table_gt, \"balanced_accuracy\", [\"mcc\",\"accuracy\",\"f1\"])\n",
    "\n",
    "# optional constraint examples (edit if you want)\n",
    "MIN_PREC = None   # e.g. 0.80\n",
    "MIN_REC  = None   # e.g. 0.30\n",
    "best_ge_f1_con = _pick_best_constrained(thr_table_ge, \"f1\", min_precision=MIN_PREC, min_recall=MIN_REC)\n",
    "\n",
    "# gt -> ge equivalent threshold (so downstream can still do prob >= thr)\n",
    "def _gt_to_ge_equiv(thr_gt):\n",
    "    thr_gt = float(thr_gt)\n",
    "    thr_ge = float(np.nextafter(np.float32(thr_gt), np.float32(1.0)))\n",
    "    return float(min(max(thr_ge, 0.0), 1.0))\n",
    "\n",
    "BEST_THR_GE_F1   = float(best_ge_f1[\"thr\"])\n",
    "BEST_THR_GE_F05  = float(best_ge_f05[\"thr\"])\n",
    "BEST_THR_GE_F2   = float(best_ge_f2[\"thr\"])\n",
    "BEST_THR_GE_MCC  = float(best_ge_mcc[\"thr\"])\n",
    "BEST_THR_GE_BACC = float(best_ge_bacc[\"thr\"])\n",
    "\n",
    "BEST_THR_GT_F1   = float(best_gt_f1[\"thr\"])\n",
    "BEST_THR_GT_F05  = float(best_gt_f05[\"thr\"])\n",
    "BEST_THR_GT_F2   = float(best_gt_f2[\"thr\"])\n",
    "BEST_THR_GT_MCC  = float(best_gt_mcc[\"thr\"])\n",
    "BEST_THR_GT_BACC = float(best_gt_bacc[\"thr\"])\n",
    "\n",
    "BEST_THR_GT_F1_GE_EQUIV   = _gt_to_ge_equiv(BEST_THR_GT_F1)\n",
    "BEST_THR_GT_MCC_GE_EQUIV  = _gt_to_ge_equiv(BEST_THR_GT_MCC)\n",
    "BEST_THR_GT_BACC_GE_EQUIV = _gt_to_ge_equiv(BEST_THR_GT_BACC)\n",
    "\n",
    "# default threshold choice (keep F1 + GE)\n",
    "BEST_THR = BEST_THR_GE_F1\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Save artifacts\n",
    "# ----------------------------\n",
    "thr_table_all = pd.concat([thr_table_ge, thr_table_gt], axis=0, ignore_index=True)\n",
    "\n",
    "# save top1000 by F1 primary\n",
    "top1000 = thr_table_all.sort_values(\n",
    "    [\"f1\",\"mcc\",\"balanced_accuracy\",\"recall\",\"precision\",\"pos_pred_gap\"],\n",
    "    ascending=[False, False, False, False, False, True]\n",
    ").head(1000).reset_index(drop=True)\n",
    "\n",
    "out_json = OOF_DIR / \"threshold_tuning.json\"\n",
    "out_txt  = OOF_DIR / \"threshold_report.txt\"\n",
    "out_csv  = OOF_DIR / \"threshold_table_top1000.csv\"\n",
    "top1000.to_csv(out_csv, index=False)\n",
    "\n",
    "payload = {\n",
    "    \"version\": \"v4.4\",\n",
    "    \"source\": src,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"n_total_rows\": int(N_all),\n",
    "    \"n_valid_rows\": int(N),\n",
    "    \"pos_valid\": int(pos),\n",
    "    \"neg_valid\": int(neg),\n",
    "    \"pos_rate_valid\": float(pos / max(N, 1)),\n",
    "    \"oof_auc_valid_only\": float(auc_oof),\n",
    "    \"oof_ap_valid_only\": float(ap_oof),\n",
    "    \"prevalence_match_thr_ge\": float(thr_prev),\n",
    "    \"baseline_thr_0p5\": {\"ge\": base05_ge, \"gt\": base05_gt},\n",
    "    \"best_ge\": {\n",
    "        \"best_thr_f1\":   best_ge_f1.to_dict(),\n",
    "        \"best_thr_f05\":  best_ge_f05.to_dict(),\n",
    "        \"best_thr_f2\":   best_ge_f2.to_dict(),\n",
    "        \"best_thr_mcc\":  best_ge_mcc.to_dict(),\n",
    "        \"best_thr_bacc\": best_ge_bacc.to_dict(),\n",
    "        \"best_thr_f1_constrained\": (None if best_ge_f1_con is None else best_ge_f1_con.to_dict()),\n",
    "        \"constraints\": {\"min_precision\": MIN_PREC, \"min_recall\": MIN_REC},\n",
    "    },\n",
    "    \"best_gt\": {\n",
    "        \"best_thr_f1\":   best_gt_f1.to_dict(),\n",
    "        \"best_thr_f05\":  best_gt_f05.to_dict(),\n",
    "        \"best_thr_f2\":   best_gt_f2.to_dict(),\n",
    "        \"best_thr_mcc\":  best_gt_mcc.to_dict(),\n",
    "        \"best_thr_bacc\": best_gt_bacc.to_dict(),\n",
    "        \"ge_equiv_for_downstream_using_ge\": {\n",
    "            \"f1\": float(BEST_THR_GT_F1_GE_EQUIV),\n",
    "            \"mcc\": float(BEST_THR_GT_MCC_GE_EQUIV),\n",
    "            \"bacc\": float(BEST_THR_GT_BACC_GE_EQUIV),\n",
    "        },\n",
    "    },\n",
    "    \"default_best_thr\": {\"metric\": \"f1\", \"rule\": \"ge\", \"thr\": float(BEST_THR)},\n",
    "}\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "def _fmt_row(d):\n",
    "    return (f\"thr={d['thr']:.6f} | F1={d['f1']:.6f} | F0.5={d['f05']:.6f} | F2={d['f2']:.6f} | \"\n",
    "            f\"P={d['precision']:.6f} R={d['recall']:.6f} | BACC={d['balanced_accuracy']:.6f} | MCC={d['mcc']:.6f} | \"\n",
    "            f\"pos_pred={int(d['pos_pred'])} (gap={int(d['pos_pred_gap'])})\")\n",
    "\n",
    "lines = []\n",
    "lines.append(\"OOF Threshold Tuning Report (v4.4)\")\n",
    "lines.append(f\"- source={src}\")\n",
    "lines.append(f\"- target_col={TARGET_COL}\")\n",
    "lines.append(f\"- total_rows={N_all} | valid_rows={N} | pos_valid={pos} | neg_valid={neg} | pos%={pos/max(N,1)*100:.6f}%\")\n",
    "lines.append(f\"- OOF AUC (valid-only) = {auc_oof:.6f}\")\n",
    "lines.append(f\"- OOF AP  (valid-only) = {ap_oof:.6f}\")\n",
    "lines.append(f\"- prevalence-match thr (ge) ~ {thr_prev:.6f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"- rule=ge: {_fmt_row(base05_ge)}\")\n",
    "lines.append(f\"- rule=gt: {_fmt_row(base05_gt)}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"BEST (rule=ge)  [downstream default: pred = prob >= thr]\")\n",
    "lines.append(f\"- BEST-F1   : {_fmt_row(best_ge_f1.to_dict())}\")\n",
    "lines.append(f\"- BEST-F0.5 : {_fmt_row(best_ge_f05.to_dict())}\")\n",
    "lines.append(f\"- BEST-F2   : {_fmt_row(best_ge_f2.to_dict())}\")\n",
    "lines.append(f\"- BEST-MCC  : {_fmt_row(best_ge_mcc.to_dict())}\")\n",
    "lines.append(f\"- BEST-BACC : {_fmt_row(best_ge_bacc.to_dict())}\")\n",
    "if best_ge_f1_con is not None:\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"BEST-F1 constrained (minP={MIN_PREC}, minR={MIN_REC}): {_fmt_row(best_ge_f1_con.to_dict())}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"BEST (rule=gt)  [strict '>' boundary]\")\n",
    "lines.append(f\"- BEST-F1   : {_fmt_row(best_gt_f1.to_dict())} | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.6f}\")\n",
    "lines.append(f\"- BEST-MCC  : {_fmt_row(best_gt_mcc.to_dict())} | ge_equiv={BEST_THR_GT_MCC_GE_EQUIV:.6f}\")\n",
    "lines.append(f\"- BEST-BACC : {_fmt_row(best_gt_bacc.to_dict())} | ge_equiv={BEST_THR_GT_BACC_GE_EQUIV:.6f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 12 (by F1) overall:\")\n",
    "for i in range(min(12, len(top1000))):\n",
    "    r = top1000.iloc[i].to_dict()\n",
    "    lines.append(f\"{i+1:02d}. rule={r['rule']} | {_fmt_row(r)}\")\n",
    "\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print(\"[Stage 9] DONE\")\n",
    "print(f\"- Saved: {out_json}\")\n",
    "print(f\"- Saved: {out_txt}\")\n",
    "print(f\"- Saved: {out_csv}\")\n",
    "print(f\"- OOF AUC (valid-only): {auc_oof:.6f} | AP: {ap_oof:.6f}\")\n",
    "print(f\"- DEFAULT BEST_THR (ge/F1) = {BEST_THR:.6f} | F1={float(best_ge_f1['f1']):.6f} (P={float(best_ge_f1['precision']):.6f} R={float(best_ge_f1['recall']):.6f})\")\n",
    "print(f\"- BEST_THR_GE_MCC          = {BEST_THR_GE_MCC:.6f} | MCC={float(best_ge_mcc['mcc']):.6f}\")\n",
    "print(f\"- BEST_THR_GE_BACC         = {BEST_THR_GE_BACC:.6f} | BACC={float(best_ge_bacc['balanced_accuracy']):.6f}\")\n",
    "print(f\"- BEST_THR_GT_F1           = {BEST_THR_GT_F1:.6f} (gt) | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.6f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"train_ids_oof_all\": train_ids,\n",
    "    \"train_ids_oof_valid\": train_ids_v,\n",
    "    \"oof_prob_all\": oof_prob,\n",
    "    \"oof_prob_valid\": p_v,\n",
    "    \"y_oof_valid\": y_v,\n",
    "\n",
    "    \"BEST_THR\": float(BEST_THR),                 # default: rule=ge, metric=F1\n",
    "    \"BEST_THR_GE_F1\": float(BEST_THR_GE_F1),\n",
    "    \"BEST_THR_GE_F05\": float(BEST_THR_GE_F05),\n",
    "    \"BEST_THR_GE_F2\": float(BEST_THR_GE_F2),\n",
    "    \"BEST_THR_GE_MCC\": float(BEST_THR_GE_MCC),\n",
    "    \"BEST_THR_GE_BACC\": float(BEST_THR_GE_BACC),\n",
    "\n",
    "    \"BEST_THR_GT_F1\": float(BEST_THR_GT_F1),\n",
    "    \"BEST_THR_GT_F05\": float(BEST_THR_GT_F05),\n",
    "    \"BEST_THR_GT_F2\": float(BEST_THR_GT_F2),\n",
    "    \"BEST_THR_GT_MCC\": float(BEST_THR_GT_MCC),\n",
    "    \"BEST_THR_GT_BACC\": float(BEST_THR_GT_BACC),\n",
    "\n",
    "    # use these if downstream always uses >= but you want strict \">\"\n",
    "    \"BEST_THR_GT_F1_GE_EQUIV\": float(BEST_THR_GT_F1_GE_EQUIV),\n",
    "    \"BEST_THR_GT_MCC_GE_EQUIV\": float(BEST_THR_GT_MCC_GE_EQUIV),\n",
    "    \"BEST_THR_GT_BACC_GE_EQUIV\": float(BEST_THR_GT_BACC_GE_EQUIV),\n",
    "\n",
    "    \"thr_table_ge\": thr_table_ge,\n",
    "    \"thr_table_gt\": thr_table_gt,\n",
    "    \"thr_table_top1000\": top1000,\n",
    "    \"THR_JSON_PATH\": out_json,\n",
    "    \"THR_REPORT_PATH\": out_txt,\n",
    "    \"THR_TABLE_CSV_PATH\": out_csv,\n",
    "    \"OOF_AUC_VALID_ONLY\": float(auc_oof),\n",
    "    \"OOF_AP_VALID_ONLY\": float(ap_oof),\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f56fbc",
   "metadata": {
    "papermill": {
     "duration": 0.026216,
     "end_time": "2026-01-07T19:45:33.808248",
     "exception": false,
     "start_time": "2026-01-07T19:45:33.782032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Inference (Fold Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d487911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T19:45:33.862388Z",
     "iopub.status.busy": "2026-01-07T19:45:33.861937Z",
     "iopub.status.idle": "2026-01-07T19:52:20.522239Z",
     "shell.execute_reply": "2026-01-07T19:52:20.521029Z"
    },
    "papermill": {
     "duration": 406.690347,
     "end_time": "2026-01-07T19:52:20.524596",
     "exception": false,
     "start_time": "2026-01-07T19:45:33.834249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 10] token_mode=asinh | VAL_FEAT=signal | VAL_IS_MAG=False | X_dtype=<class 'numpy.float32'>\n",
      "[Stage 10] SHIFT_BAND_IDS=True | PAD_BAND_ID=0 | N_BANDS=6\n",
      "[Stage 10] META_COLS=['Z', 'Z_err', 'EBV_used', 'Z_missing', 'Z_err_missing', 'EBV_missing', 'is_photoz'] | need_agg=True\n",
      "[Stage 10] USE_EMA_WEIGHTS_FOR_INFER=True | STRICT_GDIM=True\n",
      "[Stage 10] Building TEST global features (then cached)...\n",
      "[Stage 10] TEST G built: shape=(7135, 38) | time=0.9s | cached=/kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/fixed_seq/global_features_test_raw.npy\n",
      "[Stage 10] Test inference: N_test=7,135 | folds=5 | batch=64 | ensemble=mean_logits_then_sigmoid\n",
      "  fold 0: d_model=160 n_heads=4 g_dim=38 | x_ln=True g_ln=False | ema=True (hits=52) | logit_mean=-0.429430 | prob_mean=0.408914 | prob_std=0.175861\n",
      "  fold 1: d_model=160 n_heads=4 g_dim=38 | x_ln=True g_ln=False | ema=True (hits=52) | logit_mean=-0.451708 | prob_mean=0.405091 | prob_std=0.176140\n",
      "  fold 2: d_model=160 n_heads=4 g_dim=38 | x_ln=True g_ln=False | ema=True (hits=52) | logit_mean=-0.390987 | prob_mean=0.417141 | prob_std=0.175105\n",
      "  fold 3: d_model=160 n_heads=4 g_dim=38 | x_ln=True g_ln=False | ema=True (hits=52) | logit_mean=-0.406420 | prob_mean=0.415224 | prob_std=0.178238\n",
      "  fold 4: d_model=160 n_heads=4 g_dim=38 | x_ln=True g_ln=False | ema=True (hits=52) | logit_mean=-0.397284 | prob_mean=0.415832 | prob_std=0.170029\n",
      "\n",
      "[Stage 10] DONE\n",
      "- Saved logits folds: /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/preds/test_logit_folds.npy\n",
      "- Saved logits ens  : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/preds/test_logit_ens.npy\n",
      "- Saved probs folds : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/preds/test_prob_folds.npy\n",
      "- Saved probs ens   : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/preds/test_prob_ens.npy\n",
      "- Saved csv         : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/preds/test_prob_ens.csv\n",
      "- Saved pred 0/1    : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/preds/test_pred_01.csv (thr=0.647303)\n",
      "- Saved config      : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/artifacts/preds/test_infer_config.json\n",
      "- ens prob mean=0.412152 | std=0.172765 | min=0.054225 | max=0.841255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 10 — Test Inference (Fold Ensemble) (ONE CELL)\n",
    "# REVISI FULL v4.6 (FIX x_ln/g_ln keys + TEST G-FEAT CACHE + STRICT GDIM + BAND RANGE CHECK + EMA INFER)\n",
    "#\n",
    "# Fix v4.6:\n",
    "# - Support ckpt with x_ln.weight/x_ln.bias (LayerNorm on token embeddings)\n",
    "# - (Optional) support g_ln if present\n",
    "# - Everything else from v4.5 unchanged\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, re, math, time, warnings, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True.*\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require previous stages\n",
    "# ----------------------------\n",
    "need = [\"ART_DIR\",\"FIX_DIR\",\"MAX_LEN\",\"SEQ_FEATURE_NAMES\",\"df_test_meta\",\"CKPT_DIR\",\"n_splits\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing `{k}`. Jalankan STAGE 0..9 dulu.\")\n",
    "\n",
    "# Torch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch tidak tersedia di environment ini.\") from e\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "SEED = int(globals().get(\"SEED\", 2025))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Thread guard (CPU)\n",
    "try:\n",
    "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"2\")))\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FIX_DIR = Path(globals()[\"FIX_DIR\"])\n",
    "ART_DIR = Path(globals()[\"ART_DIR\"]); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR = Path(globals()[\"CKPT_DIR\"])\n",
    "\n",
    "OUT_DIR = ART_DIR / \"preds\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Settings\n",
    "# ----------------------------\n",
    "USE_EMA_WEIGHTS_FOR_INFER = True      # use EMA if ckpt has ema_shadow\n",
    "EXPORT_TEST_PRED_01 = True            # export 0/1 csv using BEST_THR if available\n",
    "DEFAULT_THR_IF_MISSING = 0.5\n",
    "EXPORT_TEST_PROB_FOLDS_CSV = False    # debug: wide CSV per fold\n",
    "STRICT_GDIM = True                   # if computed G_raw < g_dim expected -> raise\n",
    "\n",
    "# ----------------------------\n",
    "# helper: normalize id robustly\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_, bytearray)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _load_ids_npy(path: Path):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    xs = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n",
    "    return [_norm_id(z) for z in xs]\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    x = np.clip(x, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Read Stage6 policy (SHIFT_BAND_IDS, PAD_BAND_ID, dtype_X, token/value hints)\n",
    "# ----------------------------\n",
    "SHIFT_BAND_IDS = False\n",
    "PAD_BAND_ID = 0\n",
    "DTYPE_X_MEMMAP = np.float32\n",
    "POL_TOKEN_MODE = None\n",
    "POL_SCORE_VALUE_FEAT = None\n",
    "\n",
    "policy_path = FIX_DIR / \"length_policy_config.json\"\n",
    "if policy_path.exists():\n",
    "    try:\n",
    "        pol = json.loads(policy_path.read_text())\n",
    "        SHIFT_BAND_IDS = bool(pol.get(\"padding\", {}).get(\"SHIFT_BAND_IDS\", False))\n",
    "        PAD_BAND_ID = int(pol.get(\"padding\", {}).get(\"PAD_BAND_ID\", 0))\n",
    "        dt = str(pol.get(\"dtype_X\", \"float32\")).lower()\n",
    "        DTYPE_X_MEMMAP = np.float16 if ((\"float16\" in dt) or (\"fp16\" in dt)) else np.float32\n",
    "        POL_TOKEN_MODE = pol.get(\"token_mode\", None)\n",
    "        POL_SCORE_VALUE_FEAT = pol.get(\"score_value_feat\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load TEST ordering (must match STAGE 6)\n",
    "# ----------------------------\n",
    "test_ids_path = FIX_DIR / \"test_ids.npy\"\n",
    "if not test_ids_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_ids_path}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "test_ids = _load_ids_npy(test_ids_path)\n",
    "NTE = len(test_ids)\n",
    "if NTE <= 0:\n",
    "    raise RuntimeError(\"test_ids kosong (NTE=0). Pastikan STAGE 6 sukses membuat test_ids.npy.\")\n",
    "\n",
    "# Align df_test_meta.index via string-map (HARD)\n",
    "df_test_meta = globals()[\"df_test_meta\"].copy(deep=False)\n",
    "df_test_meta.index = pd.Index([_norm_id(z) for z in df_test_meta.index.tolist()], name=df_test_meta.index.name)\n",
    "\n",
    "pos_idx = df_test_meta.index.get_indexer(test_ids)\n",
    "if (pos_idx < 0).any():\n",
    "    bad = [test_ids[i] for i in np.where(pos_idx < 0)[0][:10]]\n",
    "    raise KeyError(f\"Some test_ids not found in df_test_meta.index. ex={bad} | missing_n={int((pos_idx<0).sum())}\")\n",
    "pos_idx = pos_idx.astype(np.int32)\n",
    "\n",
    "if len(set(test_ids)) != len(test_ids):\n",
    "    s = pd.Series(test_ids)\n",
    "    dup = s[s.duplicated()].head(10).tolist()\n",
    "    raise ValueError(f\"Duplicate object_id in test_ids ordering (examples): {dup}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Open fixed-length TEST memmaps (dtype from Stage6 policy)\n",
    "# ----------------------------\n",
    "SEQ_FEATURE_NAMES = list(globals()[\"SEQ_FEATURE_NAMES\"])\n",
    "Fdim = len(SEQ_FEATURE_NAMES)\n",
    "L = int(globals()[\"MAX_LEN\"])\n",
    "\n",
    "test_X_path = FIX_DIR / \"test_X.dat\"\n",
    "test_B_path = FIX_DIR / \"test_B.dat\"\n",
    "test_M_path = FIX_DIR / \"test_M.dat\"\n",
    "for p in [test_X_path, test_B_path, test_M_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing fixed cache file: {p}. Pastikan STAGE 6 sukses.\")\n",
    "\n",
    "Xte = np.memmap(test_X_path, dtype=DTYPE_X_MEMMAP, mode=\"r\", shape=(NTE, L, Fdim))\n",
    "Bte = np.memmap(test_B_path, dtype=np.int8,        mode=\"r\", shape=(NTE, L))\n",
    "Mte = np.memmap(test_M_path, dtype=np.int8,        mode=\"r\", shape=(NTE, L))\n",
    "\n",
    "feat = {n:i for i,n in enumerate(SEQ_FEATURE_NAMES)}\n",
    "for k in [\"snr_tanh\",\"detected\"]:\n",
    "    if k not in feat:\n",
    "        raise RuntimeError(f\"Feature '{k}' not found in SEQ_FEATURE_NAMES.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Checkpoints (fold_*.pt)\n",
    "# ----------------------------\n",
    "n_splits = int(globals()[\"n_splits\"])\n",
    "ckpts = []\n",
    "for f in range(n_splits):\n",
    "    p = CKPT_DIR / f\"fold_{f}.pt\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {p}. Pastikan STAGE 8 menyimpan ckpt per fold.\")\n",
    "    ckpts.append(p)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Safe/compat checkpoint loader\n",
    "# ----------------------------\n",
    "def torch_load_compat(path: Path):\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "        if isinstance(obj, dict) and (\"model_state\" in obj or \"global_scaler\" in obj or \"cfg\" in obj):\n",
    "            return obj\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        return torch.load(path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "def extract_state_and_meta(ckpt_obj):\n",
    "    if isinstance(ckpt_obj, dict) and \"model_state\" in ckpt_obj and isinstance(ckpt_obj[\"model_state\"], dict):\n",
    "        return ckpt_obj[\"model_state\"], ckpt_obj\n",
    "    if isinstance(ckpt_obj, dict):\n",
    "        any_tensor = any(torch.is_tensor(v) for v in ckpt_obj.values())\n",
    "        if any_tensor:\n",
    "            return ckpt_obj, {}\n",
    "        return ckpt_obj, ckpt_obj\n",
    "    raise RuntimeError(f\"Unsupported ckpt object type: {type(ckpt_obj)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Infer architecture from state_dict (v4.6: detect x_ln/g_ln)\n",
    "# ----------------------------\n",
    "def infer_from_state(sd: dict):\n",
    "    keys = set(sd.keys())\n",
    "\n",
    "    if \"band_emb.weight\" not in sd:\n",
    "        raise RuntimeError(\"state_dict missing band_emb.weight.\")\n",
    "    n_bands = int(sd[\"band_emb.weight\"].shape[0])\n",
    "    d_model = int(sd[\"band_emb.weight\"].shape[1])\n",
    "\n",
    "    if \"pos_emb\" not in sd:\n",
    "        raise RuntimeError(\"state_dict missing pos_emb.\")\n",
    "    max_len_ckpt = int(sd[\"pos_emb\"].shape[1])\n",
    "\n",
    "    if \"x_proj.0.weight\" in keys:\n",
    "        feat_dim = int(sd[\"x_proj.0.weight\"].shape[1])\n",
    "    elif \"x_proj.weight\" in keys:\n",
    "        feat_dim = int(sd[\"x_proj.weight\"].shape[1])\n",
    "    else:\n",
    "        raise RuntimeError(\"state_dict missing x_proj.*.weight\")\n",
    "\n",
    "    # NEW: optional x_ln\n",
    "    has_x_ln = (\"x_ln.weight\" in keys and \"x_ln.bias\" in keys)\n",
    "    x_ln_dim = int(sd[\"x_ln.weight\"].shape[0]) if has_x_ln else 0\n",
    "\n",
    "    if \"g_proj.0.weight\" in keys:\n",
    "        g_dim = int(sd[\"g_proj.0.weight\"].shape[1])\n",
    "        g_hidden = int(sd[\"g_proj.0.weight\"].shape[0])\n",
    "    else:\n",
    "        g_dim = 0\n",
    "        g_hidden = 0\n",
    "\n",
    "    # NEW: optional g_ln\n",
    "    has_g_ln = (\"g_ln.weight\" in keys and \"g_ln.bias\" in keys)\n",
    "    g_ln_dim = int(sd[\"g_ln.weight\"].shape[0]) if has_g_ln else 0\n",
    "\n",
    "    layer_ids = set()\n",
    "    for k in keys:\n",
    "        m = re.match(r\"encoder\\.layers\\.(\\d+)\\.\", k)\n",
    "        if m:\n",
    "            layer_ids.add(int(m.group(1)))\n",
    "    n_layers = (max(layer_ids) + 1) if layer_ids else 0\n",
    "    if n_layers <= 0:\n",
    "        raise RuntimeError(\"Cannot infer n_layers (encoder.layers.* not found).\")\n",
    "\n",
    "    k_lin1 = \"encoder.layers.0.linear1.weight\"\n",
    "    if k_lin1 in sd:\n",
    "        dim_ff = int(sd[k_lin1].shape[0])\n",
    "    else:\n",
    "        lin1_keys = [k for k in keys if k.endswith(\"linear1.weight\")]\n",
    "        if not lin1_keys:\n",
    "            raise RuntimeError(\"Cannot infer dim_feedforward (linear1.weight not found).\")\n",
    "        dim_ff = int(sd[sorted(lin1_keys)[0]].shape[0])\n",
    "\n",
    "    has_pool_ln = (\"pool_ln.weight\" in keys and \"pool_ln.bias\" in keys)\n",
    "\n",
    "    head_w_idx = []\n",
    "    for k in keys:\n",
    "        m = re.match(r\"head\\.(\\d+)\\.weight\", k)\n",
    "        if m:\n",
    "            head_w_idx.append(int(m.group(1)))\n",
    "    if not head_w_idx:\n",
    "        raise RuntimeError(\"Cannot infer head structure (head.*.weight not found).\")\n",
    "    head_final_idx = max(sorted(set(head_w_idx)))\n",
    "\n",
    "    return {\n",
    "        \"n_bands\": n_bands,\n",
    "        \"d_model\": d_model,\n",
    "        \"max_len_ckpt\": max_len_ckpt,\n",
    "        \"feat_dim\": feat_dim,\n",
    "        \"g_dim\": g_dim,\n",
    "        \"g_hidden\": g_hidden,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"dim_ff\": dim_ff,\n",
    "        \"has_pool_ln\": has_pool_ln,\n",
    "        \"head_final_idx\": head_final_idx,\n",
    "        \"has_x_ln\": bool(has_x_ln),\n",
    "        \"x_ln_dim\": int(x_ln_dim),\n",
    "        \"has_g_ln\": bool(has_g_ln),\n",
    "        \"g_ln_dim\": int(g_ln_dim),\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Resolve token/value feature from CKPT meta + fallback\n",
    "# ----------------------------\n",
    "def _pick_val_feat(feat_map, prefer=None):\n",
    "    cand = []\n",
    "    if prefer is not None:\n",
    "        cand.append(str(prefer))\n",
    "    cand += [\n",
    "        \"signal\", \"value\", \"flux\",\n",
    "        \"flux_asinh\", \"flux_asinh_clip\", \"flux_asinh_norm\", \"flux_asinh_scaled\",\n",
    "        \"mag\", \"mag_norm\", \"mag_clip\", \"mag_scaled\",\n",
    "        \"delta_signal\", \"delta_flux\", \"signal_clip\", \"signal_norm\",\n",
    "    ]\n",
    "    for c in cand:\n",
    "        if c and (c in feat_map):\n",
    "            return c\n",
    "    keys = list(feat_map.keys())\n",
    "    fuzzy = [k for k in keys if any(t in k for t in [\"signal\", \"flux\", \"value\", \"asinh\", \"mag\"])]\n",
    "    if fuzzy:\n",
    "        return sorted(fuzzy)[0]\n",
    "    return None\n",
    "\n",
    "first_obj = torch_load_compat(ckpts[0])\n",
    "_, first_meta = extract_state_and_meta(first_obj)\n",
    "\n",
    "CKPT_TOKEN_MODE = None\n",
    "CKPT_VAL_FEAT = None\n",
    "CKPT_VAL_IS_MAG = None\n",
    "CKPT_SHIFT_BAND_IDS = None\n",
    "CKPT_META_COLS = None\n",
    "\n",
    "if isinstance(first_meta, dict):\n",
    "    CKPT_TOKEN_MODE = first_meta.get(\"token_mode\", None)\n",
    "    CKPT_VAL_FEAT = first_meta.get(\"val_feat\", None)\n",
    "    CKPT_VAL_IS_MAG = first_meta.get(\"val_is_mag\", None)\n",
    "    CKPT_SHIFT_BAND_IDS = first_meta.get(\"shift_band_ids_from_stage6\", None)\n",
    "    CKPT_META_COLS = first_meta.get(\"global_meta_cols\", None)\n",
    "\n",
    "# HARD guard: SHIFT_BAND_IDS must match ckpt if present\n",
    "if CKPT_SHIFT_BAND_IDS is not None and bool(CKPT_SHIFT_BAND_IDS) != bool(SHIFT_BAND_IDS):\n",
    "    raise RuntimeError(\n",
    "        \"[Stage 10] SHIFT_BAND_IDS mismatch between Stage6 policy and ckpt meta.\\n\"\n",
    "        f\"- Stage6 policy SHIFT_BAND_IDS={SHIFT_BAND_IDS}\\n\"\n",
    "        f\"- ckpt meta shift_band_ids_from_stage6={CKPT_SHIFT_BAND_IDS}\\n\"\n",
    "        \"Solusi: inference harus pakai FIX_DIR yang sama dengan training ckpt.\"\n",
    "    )\n",
    "\n",
    "SEQ_TOKEN_MODE = CKPT_TOKEN_MODE if CKPT_TOKEN_MODE is not None else POL_TOKEN_MODE\n",
    "if SEQ_TOKEN_MODE is None:\n",
    "    SEQ_TOKEN_MODE = \"mag\" if any(k.startswith(\"mag\") for k in feat.keys()) else \"generic\"\n",
    "SEQ_TOKEN_MODE = str(SEQ_TOKEN_MODE).lower().strip()\n",
    "\n",
    "VAL_FEAT = _pick_val_feat(feat, prefer=(CKPT_VAL_FEAT or POL_SCORE_VALUE_FEAT))\n",
    "if VAL_FEAT is None:\n",
    "    raise RuntimeError(\"Cannot resolve VAL_FEAT from SEQ_FEATURE_NAMES.\")\n",
    "VAL_IS_MAG = bool(CKPT_VAL_IS_MAG) if CKPT_VAL_IS_MAG is not None else (SEQ_TOKEN_MODE == \"mag\" and \"mag\" in VAL_FEAT)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Band range sanity (fail-fast)\n",
    "# ----------------------------\n",
    "def _band_sanity_check(Bmm, Mmm, n_bands, shift_flag, sample_n=512):\n",
    "    s = min(int(sample_n), int(Bmm.shape[0]))\n",
    "    if s <= 0:\n",
    "        return\n",
    "    Bc = np.asarray(Bmm[:s])\n",
    "    Mc = np.asarray(Mmm[:s])\n",
    "    real = (Mc == 1)\n",
    "    if not real.any():\n",
    "        return\n",
    "    br = Bc[real].astype(np.int64, copy=False)\n",
    "    bmin = int(br.min()); bmax = int(br.max())\n",
    "\n",
    "    if shift_flag:\n",
    "        ok = (bmin >= 0) and (bmax <= n_bands)\n",
    "        if not ok:\n",
    "            raise RuntimeError(\n",
    "                \"[Stage 10] Band-id range incompatible with SHIFT_BAND_IDS=True.\\n\"\n",
    "                f\"- observed real band min={bmin} max={bmax}\\n\"\n",
    "                f\"- expected ~ 1..{n_bands} (pad=0)\\n\"\n",
    "            )\n",
    "    else:\n",
    "        ok = (bmin >= 0) and (bmax <= (n_bands - 1))\n",
    "        if not ok:\n",
    "            raise RuntimeError(\n",
    "                \"[Stage 10] Band-id range incompatible with SHIFT_BAND_IDS=False.\\n\"\n",
    "                f\"- observed real band min={bmin} max={bmax}\\n\"\n",
    "                f\"- expected 0..{n_bands-1}\\n\"\n",
    "            )\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Determine META_COLS and whether need agg\n",
    "# ----------------------------\n",
    "DEFAULT_META_COLS = [\"Z\",\"Z_err\",\"EBV_used\",\"Z_missing\",\"Z_err_missing\",\"EBV_missing\",\"is_photoz\"]\n",
    "META_COLS = DEFAULT_META_COLS\n",
    "if isinstance(CKPT_META_COLS, (list, tuple)) and len(CKPT_META_COLS) > 0:\n",
    "    META_COLS = [str(c) for c in CKPT_META_COLS]\n",
    "\n",
    "fold_arch = []\n",
    "fold_meta_summary = []\n",
    "need_agg = False\n",
    "arch_used = None\n",
    "\n",
    "for fold, p in enumerate(ckpts):\n",
    "    obj = torch_load_compat(p)\n",
    "    sd, meta = extract_state_and_meta(obj)\n",
    "    arch = infer_from_state(sd)\n",
    "    fold_arch.append(arch)\n",
    "    if arch_used is None:\n",
    "        arch_used = dict(arch)\n",
    "\n",
    "    if int(arch.get(\"g_dim\", 0)) > len(META_COLS):\n",
    "        need_agg = True\n",
    "    if isinstance(meta, dict) and bool(meta.get(\"use_agg_seq_features\", False)):\n",
    "        need_agg = True\n",
    "\n",
    "    if isinstance(meta, dict):\n",
    "        vf = meta.get(\"val_feat\", None)\n",
    "        vim = meta.get(\"val_is_mag\", None)\n",
    "        sb = meta.get(\"shift_band_ids_from_stage6\", None)\n",
    "        if vf is not None and str(vf) != str(VAL_FEAT):\n",
    "            raise RuntimeError(f\"[Stage 10] Fold {fold}: val_feat mismatch. fold={vf} vs resolved={VAL_FEAT}\")\n",
    "        if vim is not None and bool(vim) != bool(VAL_IS_MAG):\n",
    "            raise RuntimeError(f\"[Stage 10] Fold {fold}: val_is_mag mismatch. fold={vim} vs resolved={VAL_IS_MAG}\")\n",
    "        if sb is not None and bool(sb) != bool(SHIFT_BAND_IDS):\n",
    "            raise RuntimeError(f\"[Stage 10] Fold {fold}: SHIFT_BAND_IDS mismatch. fold={sb} vs policy={SHIFT_BAND_IDS}\")\n",
    "\n",
    "    fold_meta_summary.append({\n",
    "        \"fold\": fold,\n",
    "        \"g_dim\": int(arch.get(\"g_dim\", 0)),\n",
    "        \"d_model\": int(arch.get(\"d_model\", 0)),\n",
    "        \"n_layers\": int(arch.get(\"n_layers\", 0)),\n",
    "        \"has_x_ln\": bool(arch.get(\"has_x_ln\", False)),\n",
    "        \"has_g_ln\": bool(arch.get(\"has_g_ln\", False)),\n",
    "    })\n",
    "\n",
    "N_BANDS = int(arch_used[\"n_bands\"])\n",
    "_band_sanity_check(Bte, Mte, n_bands=N_BANDS, shift_flag=SHIFT_BAND_IDS, sample_n=512)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Build TEST global features (meta + optional agg seq feats) + CACHE\n",
    "# ----------------------------\n",
    "def _hash_cfg(d: dict) -> str:\n",
    "    s = json.dumps(d, sort_keys=True, ensure_ascii=True)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "def _prepare_meta_cols(df):\n",
    "    df = df.copy(deep=False)\n",
    "\n",
    "    if \"EBV_used\" in META_COLS and \"EBV_used\" not in df.columns:\n",
    "        if (\"EBV_clip\" in df.columns):\n",
    "            df[\"EBV_used\"] = df[\"EBV_clip\"]\n",
    "        elif \"EBV\" in df.columns:\n",
    "            df[\"EBV_used\"] = df[\"EBV\"]\n",
    "        else:\n",
    "            df[\"EBV_used\"] = 0.0\n",
    "\n",
    "    for c in META_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "\n",
    "    Gm = df.iloc[pos_idx][META_COLS].copy()\n",
    "    for c in META_COLS:\n",
    "        Gm[c] = pd.to_numeric(Gm[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "    return Gm.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1.0)\n",
    "\n",
    "def build_agg_seq_features_memmap(Xmm, Bmm, Mmm, chunk=512):\n",
    "    snr_i = feat[\"snr_tanh\"]\n",
    "    det_i = feat[\"detected\"]\n",
    "    val_i = feat[VAL_FEAT]\n",
    "\n",
    "    agg_dim = 4 + 3 + (N_BANDS * 4)\n",
    "    out = np.zeros((NTE, agg_dim), dtype=np.float32)\n",
    "\n",
    "    for start in range(0, NTE, int(chunk)):\n",
    "        end = min(NTE, start + int(chunk))\n",
    "        Xc = np.asarray(Xmm[start:end])\n",
    "        Bc = np.asarray(Bmm[start:end])\n",
    "        Mc = np.asarray(Mmm[start:end])\n",
    "\n",
    "        real = (Mc == 1)\n",
    "\n",
    "        if SHIFT_BAND_IDS:\n",
    "            Bc2 = Bc.astype(np.int16, copy=True)\n",
    "            if real.any():\n",
    "                Bc2[real] = np.clip(Bc2[real] - 1, 0, N_BANDS - 1)\n",
    "            Bc2[~real] = 0\n",
    "            Bc = Bc2.astype(np.int8, copy=False)\n",
    "\n",
    "        tok_count = real.sum(axis=1).astype(np.float32)\n",
    "\n",
    "        snr = np.abs(Xc[:, :, snr_i]).astype(np.float32, copy=False)\n",
    "        det = (Xc[:, :, det_i] > 0.5).astype(np.float32, copy=False)\n",
    "        val = Xc[:, :, val_i].astype(np.float32, copy=False)\n",
    "\n",
    "        snr_r = snr * real\n",
    "        det_r = det * real\n",
    "\n",
    "        det_frac = _safe_div(det_r.sum(axis=1), tok_count)\n",
    "        mean_abs_snr = _safe_div(snr_r.sum(axis=1), tok_count)\n",
    "        max_abs_snr = np.where(tok_count > 0, snr_r.max(axis=1), 0.0).astype(np.float32)\n",
    "\n",
    "        if VAL_IS_MAG:\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            mean_val = np.nan_to_num(np.nanmean(val_r, axis=1).astype(np.float32), nan=0.0)\n",
    "            std_val  = np.nan_to_num(np.nanstd(val_r,  axis=1).astype(np.float32), nan=0.0)\n",
    "            min_val  = np.nan_to_num(np.nanmin(val_r,  axis=1).astype(np.float32), nan=0.0)\n",
    "            global_val_feats = np.stack([mean_val, std_val, min_val], axis=1).astype(np.float32)\n",
    "        else:\n",
    "            aval = np.abs(val).astype(np.float32, copy=False)\n",
    "            aval_r = aval * real\n",
    "            mean_aval = _safe_div(aval_r.sum(axis=1), tok_count).astype(np.float32)\n",
    "            val_r = np.where(real, val, np.nan)\n",
    "            std_val = np.nan_to_num(np.nanstd(val_r, axis=1).astype(np.float32), nan=0.0)\n",
    "            max_aval = np.where(tok_count > 0, aval_r.max(axis=1), 0.0).astype(np.float32)\n",
    "            global_val_feats = np.stack([mean_aval, std_val, max_aval], axis=1).astype(np.float32)\n",
    "\n",
    "        per_band = []\n",
    "        for b in range(N_BANDS):\n",
    "            bm = (Bc == b) & real\n",
    "            cnt = bm.sum(axis=1).astype(np.float32)\n",
    "\n",
    "            detb = (det * bm).sum(axis=1).astype(np.float32)\n",
    "            snrb = (snr * bm).sum(axis=1).astype(np.float32)\n",
    "\n",
    "            det_frac_b = _safe_div(detb, cnt)\n",
    "            mean_abs_snr_b = _safe_div(snrb, cnt)\n",
    "\n",
    "            if VAL_IS_MAG:\n",
    "                vb = np.where(bm, val, np.nan)\n",
    "                mean_val_b = np.nan_to_num(np.nanmean(vb, axis=1).astype(np.float32), nan=0.0)\n",
    "            else:\n",
    "                ab = (np.abs(val).astype(np.float32, copy=False) * bm).sum(axis=1).astype(np.float32)\n",
    "                mean_val_b = _safe_div(ab, cnt).astype(np.float32)\n",
    "\n",
    "            per_band.append(np.stack([cnt, det_frac_b, mean_abs_snr_b, mean_val_b], axis=1))\n",
    "\n",
    "        per_band = np.concatenate(per_band, axis=1).astype(np.float32)\n",
    "        glob = np.stack([tok_count, det_frac, mean_abs_snr, max_abs_snr], axis=1).astype(np.float32)\n",
    "        agg = np.concatenate([glob, global_val_feats, per_band], axis=1).astype(np.float32)\n",
    "\n",
    "        out[start:end] = agg\n",
    "\n",
    "        del Xc, Bc, Mc\n",
    "        if (start // int(chunk)) % 4 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    return out\n",
    "\n",
    "G_TEST_CACHE = FIX_DIR / \"global_features_test_raw.npy\"\n",
    "G_TEST_META  = FIX_DIR / \"global_features_test_raw_meta.json\"\n",
    "\n",
    "agg_spec = {\n",
    "    \"NTE\": int(NTE),\n",
    "    \"L\": int(L),\n",
    "    \"Fdim\": int(Fdim),\n",
    "    \"n_bands\": int(N_BANDS),\n",
    "    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n",
    "    \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n",
    "    \"meta_cols\": list(META_COLS),\n",
    "    \"val_feat\": str(VAL_FEAT),\n",
    "    \"val_is_mag\": bool(VAL_IS_MAG),\n",
    "    \"need_agg\": bool(need_agg),\n",
    "}\n",
    "agg_hash = _hash_cfg(agg_spec)\n",
    "\n",
    "print(f\"[Stage 10] token_mode={SEQ_TOKEN_MODE} | VAL_FEAT={VAL_FEAT} | VAL_IS_MAG={VAL_IS_MAG} | X_dtype={DTYPE_X_MEMMAP}\")\n",
    "print(f\"[Stage 10] SHIFT_BAND_IDS={SHIFT_BAND_IDS} | PAD_BAND_ID={PAD_BAND_ID} | N_BANDS={N_BANDS}\")\n",
    "print(f\"[Stage 10] META_COLS={META_COLS} | need_agg={need_agg}\")\n",
    "print(f\"[Stage 10] USE_EMA_WEIGHTS_FOR_INFER={USE_EMA_WEIGHTS_FOR_INFER} | STRICT_GDIM={STRICT_GDIM}\")\n",
    "\n",
    "G_raw_default = None\n",
    "if G_TEST_CACHE.exists() and G_TEST_META.exists():\n",
    "    try:\n",
    "        old = json.loads(G_TEST_META.read_text())\n",
    "        if old.get(\"agg_hash\") == agg_hash and int(old.get(\"NTE\", -1)) == int(NTE):\n",
    "            G_raw_default = np.load(G_TEST_CACHE, allow_pickle=False).astype(np.float32, copy=False)\n",
    "            if G_raw_default.shape[0] != NTE:\n",
    "                G_raw_default = None\n",
    "    except Exception:\n",
    "        G_raw_default = None\n",
    "\n",
    "if G_raw_default is None:\n",
    "    print(\"[Stage 10] Building TEST global features (then cached)...\")\n",
    "    t0 = time.time()\n",
    "    G_meta_np = _prepare_meta_cols(df_test_meta)\n",
    "\n",
    "    if need_agg:\n",
    "        G_seq_np = build_agg_seq_features_memmap(Xte, Bte, Mte, chunk=512)\n",
    "        G_raw_default = np.concatenate([G_meta_np, G_seq_np], axis=1).astype(np.float32)\n",
    "        agg_dim = int(G_seq_np.shape[1])\n",
    "    else:\n",
    "        G_raw_default = G_meta_np.astype(np.float32, copy=False)\n",
    "        agg_dim = 0\n",
    "\n",
    "    np.save(G_TEST_CACHE, G_raw_default.astype(np.float32, copy=False))\n",
    "    G_TEST_META.write_text(json.dumps(\n",
    "        {\"agg_hash\": agg_hash, \"NTE\": int(NTE), \"spec\": agg_spec, \"agg_dim\": int(agg_dim), \"g_dim\": int(G_raw_default.shape[1])},\n",
    "        indent=2\n",
    "    ))\n",
    "    print(f\"[Stage 10] TEST G built: shape={G_raw_default.shape} | time={time.time()-t0:.1f}s | cached={G_TEST_CACHE}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Model definition (v4.6: add x_ln/g_ln if present)\n",
    "# ----------------------------\n",
    "class FlexMultibandEventTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, max_len, n_bands, d_model, n_heads, n_layers, dim_ff, dropout,\n",
    "                 g_dim, g_hidden, has_pool_ln=True, head_final_idx=3,\n",
    "                 x_ln_dim=0, g_ln_dim=0):\n",
    "        super().__init__()\n",
    "        self.n_bands = int(n_bands)\n",
    "        self.max_len = int(max_len)\n",
    "        self.d_model = int(d_model)\n",
    "\n",
    "        self.x_proj = nn.Sequential(\n",
    "            nn.Linear(int(feat_dim), int(d_model)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(float(dropout)),\n",
    "        )\n",
    "\n",
    "        # NEW: optional x_ln (must match ckpt key: x_ln.*)\n",
    "        self.x_ln = nn.LayerNorm(int(x_ln_dim)) if int(x_ln_dim) > 0 else None\n",
    "\n",
    "        self.band_emb = nn.Embedding(int(n_bands), int(d_model))\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, int(max_len), int(d_model)))\n",
    "        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=int(d_model),\n",
    "            nhead=int(n_heads),\n",
    "            dim_feedforward=int(dim_ff),\n",
    "            dropout=float(dropout),\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=int(n_layers))\n",
    "\n",
    "        self.attn = nn.Linear(int(d_model), 1)\n",
    "\n",
    "        self.has_pool_ln = bool(has_pool_ln)\n",
    "        if self.has_pool_ln:\n",
    "            self.pool_ln = nn.LayerNorm(int(d_model))\n",
    "\n",
    "        self.g_dim = int(g_dim)\n",
    "        self.g_hidden = int(g_hidden)\n",
    "        if self.g_dim > 0 and self.g_hidden > 0:\n",
    "            self.g_proj = nn.Sequential(\n",
    "                nn.Linear(int(g_dim), int(g_hidden)),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(float(dropout)),\n",
    "            )\n",
    "        else:\n",
    "            self.g_proj = None\n",
    "\n",
    "        # NEW: optional g_ln (must match ckpt key: g_ln.*)\n",
    "        self.g_ln = nn.LayerNorm(int(g_ln_dim)) if int(g_ln_dim) > 0 else None\n",
    "\n",
    "        in_head = int(d_model + (g_hidden if (self.g_proj is not None) else 0))\n",
    "        if head_final_idx == 3:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, int(d_model)),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(float(dropout)),\n",
    "                nn.Linear(int(d_model), 1),\n",
    "            )\n",
    "        elif head_final_idx == 2:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, int(d_model)),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(int(d_model), 1),\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_head, int(d_model)),\n",
    "                nn.Linear(int(d_model), 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, X, band_id, mask, G):\n",
    "        X = X.to(torch.float32)\n",
    "        band_id = band_id.clamp(0, self.n_bands - 1).to(torch.long)\n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        pad_mask = (mask == 0)\n",
    "        all_pad = pad_mask.all(dim=1)\n",
    "        if all_pad.any():\n",
    "            pad_mask = pad_mask.clone()\n",
    "            pad_mask[all_pad, 0] = False\n",
    "\n",
    "        hx = self.x_proj(X)\n",
    "        if self.x_ln is not None:\n",
    "            hx = self.x_ln(hx)  # FIX: match ckpt with x_ln\n",
    "\n",
    "        h = hx + self.band_emb(band_id) + self.pos_emb[:, :X.shape[1], :]\n",
    "        h = self.encoder(h, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        a = self.attn(h).squeeze(-1)\n",
    "        a = a.masked_fill(pad_mask, -1e9)\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        pooled_attn = torch.sum(h * w.unsqueeze(-1), dim=1)\n",
    "\n",
    "        valid = (~pad_mask).to(h.dtype).unsqueeze(-1)\n",
    "        denom = valid.sum(dim=1).clamp_min(1.0)\n",
    "        pooled_mean = (h * valid).sum(dim=1) / denom\n",
    "\n",
    "        h_masked = h.masked_fill(pad_mask.unsqueeze(-1), -1e9)\n",
    "        pooled_max = torch.max(h_masked, dim=1).values\n",
    "        pooled_max = torch.where(torch.isfinite(pooled_max), pooled_max, torch.zeros_like(pooled_max))\n",
    "\n",
    "        pooled = (0.50 * pooled_attn) + (0.30 * pooled_mean) + (0.20 * pooled_max)\n",
    "        if self.has_pool_ln:\n",
    "            pooled = self.pool_ln(pooled)\n",
    "\n",
    "        if self.g_proj is not None:\n",
    "            g = self.g_proj(G.to(torch.float32))\n",
    "            if self.g_ln is not None:\n",
    "                g = self.g_ln(g)  # optional\n",
    "            z = torch.cat([pooled, g], dim=1)\n",
    "        else:\n",
    "            z = pooled\n",
    "\n",
    "        return self.head(z).squeeze(-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_logits_batchwise(model, Xmm, Bmm, Mmm, G_raw, mean, std, batch_size=64):\n",
    "    model.eval()\n",
    "    out = np.zeros((Xmm.shape[0],), dtype=np.float32)\n",
    "    N0 = int(Xmm.shape[0])\n",
    "\n",
    "    for i in range(0, N0, int(batch_size)):\n",
    "        j = min(N0, i + int(batch_size))\n",
    "        Xb_np = np.asarray(Xmm[i:j]).astype(np.float32, copy=False)\n",
    "        Bb_np = np.asarray(Bmm[i:j])\n",
    "        Mb_np = np.asarray(Mmm[i:j])\n",
    "\n",
    "        real = (Mb_np == 1)\n",
    "\n",
    "        if SHIFT_BAND_IDS:\n",
    "            Bb_np2 = Bb_np.astype(np.int16, copy=True)\n",
    "            if real.any():\n",
    "                Bb_np2[real] = np.clip(Bb_np2[real] - 1, 0, N_BANDS - 1)\n",
    "            Bb_np2[~real] = 0\n",
    "            Bb_np = Bb_np2.astype(np.int64, copy=False)\n",
    "        else:\n",
    "            Bb_np = Bb_np.astype(np.int64, copy=False)\n",
    "\n",
    "        Gb_np = G_raw[i:j]\n",
    "        Gb_np = ((Gb_np - mean) / std).astype(np.float32, copy=False)\n",
    "\n",
    "        Xb = torch.from_numpy(Xb_np)\n",
    "        Bb = torch.from_numpy(Bb_np)\n",
    "        Mb = torch.from_numpy(Mb_np.astype(np.int64, copy=False))\n",
    "        Gb = torch.from_numpy(Gb_np)\n",
    "\n",
    "        logit = model(Xb.to(device), Bb.to(device), Mb.to(device), Gb.to(device))\n",
    "        out[i:j] = logit.detach().cpu().numpy().astype(np.float32, copy=False)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Batch size heuristic\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"64\"))\n",
    "if L >= 512:\n",
    "    BATCH_SIZE = min(BATCH_SIZE, 32)\n",
    "BATCH_SIZE = max(4, BATCH_SIZE)\n",
    "\n",
    "test_logit_folds = np.zeros((NTE, n_splits), dtype=np.float32)\n",
    "\n",
    "print(f\"[Stage 10] Test inference: N_test={NTE:,} | folds={n_splits} | batch={BATCH_SIZE} | ensemble=mean_logits_then_sigmoid\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Fold inference\n",
    "# ----------------------------\n",
    "for fold, ckpt_path in enumerate(ckpts):\n",
    "    ckpt_obj = torch_load_compat(ckpt_path)\n",
    "    sd, meta = extract_state_and_meta(ckpt_obj)\n",
    "\n",
    "    arch = fold_arch[fold]\n",
    "    cfg = meta.get(\"cfg\", {}) if isinstance(meta, dict) else {}\n",
    "    dropout = float(cfg.get(\"dropout\", 0.0)) if isinstance(cfg, dict) else 0.0\n",
    "\n",
    "    n_heads = int(cfg.get(\"n_heads\", 4)) if isinstance(cfg, dict) else 4\n",
    "    if n_heads <= 0:\n",
    "        n_heads = 4\n",
    "    if (arch[\"d_model\"] % n_heads) != 0:\n",
    "        for h in [4, 8, 2, 1, 16, 32]:\n",
    "            if h > 0 and (arch[\"d_model\"] % h) == 0:\n",
    "                n_heads = h\n",
    "                break\n",
    "        if (arch[\"d_model\"] % n_heads) != 0:\n",
    "            raise RuntimeError(f\"Fold {fold}: cannot choose valid n_heads for d_model={arch['d_model']}\")\n",
    "\n",
    "    if arch[\"feat_dim\"] != Fdim:\n",
    "        raise RuntimeError(\n",
    "            f\"Fold {fold}: feature_dim mismatch.\\n\"\n",
    "            f\"- ckpt expects feat_dim={arch['feat_dim']}\\n\"\n",
    "            f\"- memmap has Fdim={Fdim}\\n\"\n",
    "        )\n",
    "    if arch[\"max_len_ckpt\"] != L:\n",
    "        raise RuntimeError(\n",
    "            f\"Fold {fold}: max_len mismatch.\\n\"\n",
    "            f\"- ckpt max_len={arch['max_len_ckpt']}\\n\"\n",
    "            f\"- memmap MAX_LEN={L}\\n\"\n",
    "        )\n",
    "\n",
    "    g_dim = int(arch[\"g_dim\"])\n",
    "    if g_dim <= 0:\n",
    "        G_raw = np.zeros((NTE, 0), dtype=np.float32)\n",
    "        g_mean = np.zeros((0,), dtype=np.float32)\n",
    "        g_std  = np.ones((0,), dtype=np.float32)\n",
    "    else:\n",
    "        if G_raw_default.shape[1] < g_dim:\n",
    "            msg = (\n",
    "                f\"[Stage 10] Fold {fold}: computed TEST G_raw dim is smaller than ckpt expects.\\n\"\n",
    "                f\"- G_raw_default_dim={G_raw_default.shape[1]}\\n\"\n",
    "                f\"- ckpt g_dim={g_dim}\\n\"\n",
    "            )\n",
    "            if STRICT_GDIM:\n",
    "                raise RuntimeError(msg)\n",
    "            else:\n",
    "                pad = np.zeros((NTE, g_dim - G_raw_default.shape[1]), dtype=np.float32)\n",
    "                G_raw = np.concatenate([G_raw_default, pad], axis=1).astype(np.float32, copy=False)\n",
    "        elif G_raw_default.shape[1] > g_dim:\n",
    "            G_raw = G_raw_default[:, :g_dim]\n",
    "        else:\n",
    "            G_raw = G_raw_default\n",
    "\n",
    "        scaler = meta.get(\"global_scaler\", None) if isinstance(meta, dict) else None\n",
    "        if scaler is None or not isinstance(scaler, dict) or (\"mean\" not in scaler) or (\"std\" not in scaler):\n",
    "            raise RuntimeError(f\"[Stage 10] Fold {fold}: missing global_scaler in checkpoint meta.\")\n",
    "        g_mean = np.asarray(scaler[\"mean\"], dtype=np.float32).reshape(-1)\n",
    "        g_std  = np.asarray(scaler[\"std\"],  dtype=np.float32).reshape(-1)\n",
    "\n",
    "        if g_mean.shape[0] != g_dim or g_std.shape[0] != g_dim:\n",
    "            raise RuntimeError(\n",
    "                f\"[Stage 10] Fold {fold}: global_scaler shape mismatch.\\n\"\n",
    "                f\"- mean/std len: {g_mean.shape[0]}/{g_std.shape[0]}\\n\"\n",
    "                f\"- g_dim: {g_dim}\\n\"\n",
    "            )\n",
    "        g_std = np.where(g_std < 1e-6, 1.0, g_std).astype(np.float32)\n",
    "\n",
    "    model = FlexMultibandEventTransformer(\n",
    "        feat_dim=arch[\"feat_dim\"],\n",
    "        max_len=arch[\"max_len_ckpt\"],\n",
    "        n_bands=arch[\"n_bands\"],\n",
    "        d_model=arch[\"d_model\"],\n",
    "        n_heads=n_heads,\n",
    "        n_layers=arch[\"n_layers\"],\n",
    "        dim_ff=arch[\"dim_ff\"],\n",
    "        dropout=dropout,\n",
    "        g_dim=g_dim,\n",
    "        g_hidden=arch[\"g_hidden\"],\n",
    "        has_pool_ln=arch[\"has_pool_ln\"],\n",
    "        head_final_idx=arch[\"head_final_idx\"],\n",
    "        x_ln_dim=(arch[\"x_ln_dim\"] if arch.get(\"has_x_ln\", False) else 0),\n",
    "        g_ln_dim=(arch[\"g_ln_dim\"] if arch.get(\"has_g_ln\", False) else 0),\n",
    "    ).to(device)\n",
    "\n",
    "    # STRICT load should now succeed if ckpt has x_ln/g_ln\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "\n",
    "    # OPTIONAL: apply EMA weights for inference (if present)\n",
    "    used_ema = False\n",
    "    ema_hits = 0\n",
    "    if USE_EMA_WEIGHTS_FOR_INFER and isinstance(meta, dict) and isinstance(meta.get(\"ema_shadow\", None), dict):\n",
    "        ema_shadow = meta[\"ema_shadow\"]\n",
    "        st = model.state_dict()\n",
    "        for k, v in ema_shadow.items():\n",
    "            if k in st and torch.is_tensor(v) and st[k].shape == v.shape:\n",
    "                st[k] = v.to(dtype=st[k].dtype, device=st[k].device)\n",
    "                ema_hits += 1\n",
    "        if ema_hits > 0:\n",
    "            model.load_state_dict(st, strict=True)\n",
    "            used_ema = True\n",
    "\n",
    "    logits = predict_logits_batchwise(\n",
    "        model, Xte, Bte, Mte, G_raw, mean=g_mean, std=g_std, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    if not np.isfinite(logits).all():\n",
    "        raise RuntimeError(f\"[Stage 10] Fold {fold}: logits has NaN/inf. Check inputs/scaler.\")\n",
    "\n",
    "    test_logit_folds[:, fold] = logits\n",
    "    probs_tmp = sigmoid_np(logits)\n",
    "\n",
    "    print(\n",
    "        f\"  fold {fold}: d_model={arch['d_model']} n_heads={n_heads} g_dim={g_dim} | \"\n",
    "        f\"x_ln={bool(arch.get('has_x_ln', False))} g_ln={bool(arch.get('has_g_ln', False))} | \"\n",
    "        f\"ema={used_ema} (hits={ema_hits}) | \"\n",
    "        f\"logit_mean={float(logits.mean()):.6f} | prob_mean={float(probs_tmp.mean()):.6f} | prob_std={float(probs_tmp.std()):.6f}\"\n",
    "    )\n",
    "\n",
    "    del model, logits, probs_tmp\n",
    "    gc.collect()\n",
    "\n",
    "# ensemble on logits\n",
    "test_logit_ens = test_logit_folds.mean(axis=1).astype(np.float32)\n",
    "test_prob_folds = sigmoid_np(test_logit_folds).astype(np.float32)\n",
    "test_prob_ens   = sigmoid_np(test_logit_ens).astype(np.float32)\n",
    "\n",
    "if not np.isfinite(test_prob_ens).all():\n",
    "    raise RuntimeError(\"[Stage 10] test_prob_ens contains NaN/inf (unexpected).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Save artifacts\n",
    "# ----------------------------\n",
    "logit_fold_path = OUT_DIR / \"test_logit_folds.npy\"\n",
    "logit_ens_path  = OUT_DIR / \"test_logit_ens.npy\"\n",
    "prob_fold_path  = OUT_DIR / \"test_prob_folds.npy\"\n",
    "prob_ens_path   = OUT_DIR / \"test_prob_ens.npy\"\n",
    "csv_path        = OUT_DIR / \"test_prob_ens.csv\"\n",
    "cfg_path        = OUT_DIR / \"test_infer_config.json\"\n",
    "\n",
    "np.save(logit_fold_path, test_logit_folds)\n",
    "np.save(logit_ens_path,  test_logit_ens)\n",
    "np.save(prob_fold_path,  test_prob_folds)\n",
    "np.save(prob_ens_path,   test_prob_ens)\n",
    "\n",
    "pd.DataFrame({\"object_id\": test_ids, \"prob\": test_prob_ens}).to_csv(csv_path, index=False)\n",
    "\n",
    "if EXPORT_TEST_PROB_FOLDS_CSV:\n",
    "    df_f = pd.DataFrame({\"object_id\": test_ids})\n",
    "    for f in range(n_splits):\n",
    "        df_f[f\"prob_fold{f}\"] = test_prob_folds[:, f]\n",
    "    (OUT_DIR / \"test_prob_folds.csv\").write_text(df_f.to_csv(index=False))\n",
    "\n",
    "thr_used = None\n",
    "pred01_path = None\n",
    "if EXPORT_TEST_PRED_01:\n",
    "    thr_used = float(globals().get(\"BEST_THR\", DEFAULT_THR_IF_MISSING))\n",
    "    thr_used = min(max(thr_used, 0.0), 1.0)\n",
    "    test_pred01 = (test_prob_ens >= thr_used).astype(np.int8)\n",
    "    pred01_path = OUT_DIR / \"test_pred_01.csv\"\n",
    "    pd.DataFrame({\"object_id\": test_ids, \"prediction\": test_pred01.astype(int)}).to_csv(pred01_path, index=False)\n",
    "\n",
    "infer_cfg = {\n",
    "    \"seed\": int(SEED),\n",
    "    \"n_splits\": int(n_splits),\n",
    "    \"ensemble\": \"mean_logits_then_sigmoid\",\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "    \"max_len\": int(L),\n",
    "    \"feature_dim\": int(Fdim),\n",
    "    \"token_mode\": str(SEQ_TOKEN_MODE),\n",
    "    \"val_feat\": str(VAL_FEAT),\n",
    "    \"val_is_mag\": bool(VAL_IS_MAG),\n",
    "    \"shift_band_ids_from_stage6\": bool(SHIFT_BAND_IDS),\n",
    "    \"pad_band_id_from_stage6\": int(PAD_BAND_ID),\n",
    "    \"dtype_X_memmap\": str(DTYPE_X_MEMMAP),\n",
    "    \"global_meta_cols\": META_COLS,\n",
    "    \"need_agg_seq\": bool(need_agg),\n",
    "    \"global_default_dim\": int(G_raw_default.shape[1]),\n",
    "    \"use_ema_weights_for_infer\": bool(USE_EMA_WEIGHTS_FOR_INFER),\n",
    "    \"export_test_pred_01\": bool(EXPORT_TEST_PRED_01),\n",
    "    \"thr_used_for_pred01\": (None if thr_used is None else float(thr_used)),\n",
    "    \"ckpt_dir\": str(CKPT_DIR),\n",
    "    \"ckpts\": [str(p) for p in ckpts],\n",
    "    \"arch_inferred_from_first_fold\": arch_used,\n",
    "    \"fold_meta_summary\": fold_meta_summary,\n",
    "    \"test_global_cache\": {\"path\": str(G_TEST_CACHE), \"meta\": str(G_TEST_META), \"agg_hash\": agg_hash},\n",
    "    \"outputs\": {\n",
    "        \"test_logit_folds\": str(logit_fold_path),\n",
    "        \"test_logit_ens\": str(logit_ens_path),\n",
    "        \"test_prob_folds\": str(prob_fold_path),\n",
    "        \"test_prob_ens\": str(prob_ens_path),\n",
    "        \"test_prob_ens_csv\": str(csv_path),\n",
    "        \"test_pred_01_csv\": (None if pred01_path is None else str(pred01_path)),\n",
    "    }\n",
    "}\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(infer_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\n[Stage 10] DONE\")\n",
    "print(f\"- Saved logits folds: {logit_fold_path}\")\n",
    "print(f\"- Saved logits ens  : {logit_ens_path}\")\n",
    "print(f\"- Saved probs folds : {prob_fold_path}\")\n",
    "print(f\"- Saved probs ens   : {prob_ens_path}\")\n",
    "print(f\"- Saved csv         : {csv_path}\")\n",
    "if pred01_path is not None:\n",
    "    print(f\"- Saved pred 0/1    : {pred01_path} (thr={thr_used:.6f})\")\n",
    "print(f\"- Saved config      : {cfg_path}\")\n",
    "print(f\"- ens prob mean={float(test_prob_ens.mean()):.6f} | std={float(test_prob_ens.std()):.6f} | \"\n",
    "      f\"min={float(test_prob_ens.min()):.6f} | max={float(test_prob_ens.max()):.6f}\")\n",
    "\n",
    "globals().update({\n",
    "    \"test_ids\": test_ids,\n",
    "    \"test_logit_folds\": test_logit_folds,\n",
    "    \"test_logit_ens\": test_logit_ens,\n",
    "    \"test_prob_folds\": test_prob_folds,\n",
    "    \"test_prob_ens\": test_prob_ens,\n",
    "    \"TEST_LOGIT_FOLDS_PATH\": logit_fold_path,\n",
    "    \"TEST_LOGIT_ENS_PATH\": logit_ens_path,\n",
    "    \"TEST_PROB_FOLDS_PATH\": prob_fold_path,\n",
    "    \"TEST_PROB_ENS_PATH\": prob_ens_path,\n",
    "    \"TEST_PROB_CSV_PATH\": csv_path,\n",
    "    \"TEST_INFER_CFG_PATH\": cfg_path,\n",
    "    \"STAGE10_VAL_FEAT\": VAL_FEAT,\n",
    "    \"STAGE10_VAL_IS_MAG\": VAL_IS_MAG,\n",
    "    \"STAGE10_USE_EMA_INFER\": bool(USE_EMA_WEIGHTS_FOR_INFER),\n",
    "    \"TEST_PRED01_PATH\": (pred01_path if pred01_path is not None else None),\n",
    "    \"TEST_PRED01_THR_USED\": (thr_used if thr_used is not None else None),\n",
    "    \"TEST_GLOBAL_FEAT_CACHE_PATH\": G_TEST_CACHE,\n",
    "    \"TEST_GLOBAL_FEAT_META_PATH\": G_TEST_META,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bead2",
   "metadata": {
    "papermill": {
     "duration": 0.024795,
     "end_time": "2026-01-07T19:52:20.576010",
     "exception": false,
     "start_time": "2026-01-07T19:52:20.551215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f740a6a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T19:52:20.628617Z",
     "iopub.status.busy": "2026-01-07T19:52:20.627616Z",
     "iopub.status.idle": "2026-01-07T19:52:21.146696Z",
     "shell.execute_reply": "2026-01-07T19:52:21.145644Z"
    },
    "papermill": {
     "duration": 0.54835,
     "end_time": "2026-01-07T19:52:21.148761",
     "exception": false,
     "start_time": "2026-01-07T19:52:20.600411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] OOF source=csv(oof_prob.csv) | target_col=target\n",
      "[Eval] rows: valid=3,043 / total=3,043 | pos=148 neg=2,895 pos%=4.863621%\n",
      "\n",
      "EVALUATION (OOF) — Precision/Recall/F-scores (+BACC/MCC)\n",
      "- ROC-AUC=0.842018 | PR-AUC=0.224057\n",
      "- prevalence-match thr (ge) ~ 0.682562\n",
      "\n",
      "Baseline @ thr=0.5\n",
      "- rule=ge: F1=0.197472 | P=0.111807 | R=0.844595 | ACC=0.666119 | BACC=0.750795 | MCC=0.223804 | pos_pred=1118\n",
      "- rule=gt: F1=0.197472 | P=0.111807 | R=0.844595 | ACC=0.666119 | BACC=0.750795 | MCC=0.223804 | pos_pred=1118\n",
      "\n",
      "BEST (rule=ge) — default downstream: pred = prob >= thr\n",
      "- BEST-F1   @ thr=0.647290 | F1=0.310905 | P=0.236749 | R=0.452703 | pos_pred=283\n",
      "- BEST-F0.5 @ thr=0.681278 | F0.5=0.284974 | P=0.282051 | R=0.297297\n",
      "- BEST-F2   @ thr=0.565520 | F2=0.413505 | P=0.150138 | R=0.736486\n",
      "\n",
      "BEST (rule=gt) — strict boundary (>)\n",
      "- BEST-F1 @ thr=0.647290 (gt) | ge_equiv=0.647290 | F1=0.310905 | pos_pred=283\n",
      "\n",
      "Top 10 by F1 (mixed rules):\n",
      "01. rule=ge thr=0.647290 | f1=0.310905 | P=0.236749 R=0.452703 | mcc=0.280030 bacc=0.689046 | pos_pred=283\n",
      "02. rule=ge thr=0.647291 | f1=0.310905 | P=0.236749 R=0.452703 | mcc=0.280030 bacc=0.689046 | pos_pred=283\n",
      "03. rule=ge thr=0.647303 | f1=0.310905 | P=0.236749 R=0.452703 | mcc=0.280030 bacc=0.689046 | pos_pred=283\n",
      "04. rule=gt thr=0.647290 | f1=0.310905 | P=0.236749 R=0.452703 | mcc=0.280030 bacc=0.689046 | pos_pred=283\n",
      "05. rule=gt thr=0.647290 | f1=0.310905 | P=0.236749 R=0.452703 | mcc=0.280030 bacc=0.689046 | pos_pred=283\n",
      "06. rule=gt thr=0.647291 | f1=0.310905 | P=0.236749 R=0.452703 | mcc=0.280030 bacc=0.689046 | pos_pred=283\n",
      "07. rule=ge thr=0.647177 | f1=0.310185 | P=0.235915 R=0.452703 | mcc=0.279331 bacc=0.688873 | pos_pred=284\n",
      "08. rule=ge thr=0.647290 | f1=0.310185 | P=0.235915 R=0.452703 | mcc=0.279331 bacc=0.688873 | pos_pred=284\n",
      "09. rule=gt thr=0.647177 | f1=0.310185 | P=0.235915 R=0.452703 | mcc=0.279331 bacc=0.688873 | pos_pred=284\n",
      "10. rule=gt thr=0.647177 | f1=0.310185 | P=0.235915 R=0.452703 | mcc=0.279331 bacc=0.688873 | pos_pred=284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/3924154281.py:314: RuntimeWarning: invalid value encountered in divide\n",
      "  mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved:\n",
      "- /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/eval_report.txt\n",
      "- /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/eval_threshold_table.csv\n",
      "- /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/eval_threshold_table_top500.csv\n",
      "- /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/oof/eval_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ONE CELL — EVALUATION (Precision / Recall / F1) + Threshold Sweep (OOF)\n",
    "# REVISI FULL v3.4 (HOLDOUT-SAFE + GE/GT + PREVALENCE THR + EXTRA CANDS + GT->GE_EQUIV)\n",
    "#\n",
    "# Default behavior:\n",
    "# - HOLDOUT_SAFE=True: drop non-finite probs (NaN/inf) from tuning\n",
    "# - Evaluate BOTH rules: ge (>=) and gt (>)\n",
    "# - Default BEST_THR uses rule=ge, metric=F1\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, math, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal\n",
    "# ----------------------------\n",
    "if \"df_train_meta\" not in globals():\n",
    "    raise RuntimeError(\"Missing df_train_meta. Jalankan stage meta dulu.\")\n",
    "\n",
    "ART_DIR = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "OOF_DIR = Path(globals().get(\"OOF_DIR\", ART_DIR / \"oof\"))\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Switches\n",
    "HOLDOUT_SAFE = True            # drop non-finite oof probs from tuning (recommended)\n",
    "DO_BOTH_RULES = True           # compute ge and gt tables\n",
    "ADD_NEXTAFTER_CANDIDATES = True  # add nextafter(unique_probs) as thr candidates (recommended)\n",
    "\n",
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a.reshape(1)\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return a / np.maximum(b, 1e-12)\n",
    "\n",
    "def _to_np_bool(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(bool, copy=False)\n",
    "    if hasattr(x, \"to_numpy\"):\n",
    "        return x.to_numpy(dtype=bool, copy=False)\n",
    "    return np.asarray(x, dtype=bool)\n",
    "\n",
    "# HOLDOUT_SAFE: keep NaN as invalid (do not nan_to_num -> 0 for tuning)\n",
    "def _sanitize_prob(p, holdout_safe=True):\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    if holdout_safe:\n",
    "        # keep non-finite for filtering later\n",
    "        p = np.clip(p, 0.0, 1.0, out=p, where=np.isfinite(p))\n",
    "        return p.astype(np.float32, copy=False)\n",
    "    # legacy: force finite\n",
    "    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    return p.astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 0a) Normalize meta index\n",
    "# ----------------------------\n",
    "df_train_meta = df_train_meta.copy(deep=False)\n",
    "df_train_meta.index = pd.Index([_norm_id(z) for z in df_train_meta.index], name=df_train_meta.index.name)\n",
    "\n",
    "# ----------------------------\n",
    "# 0b) Detect target column\n",
    "# ----------------------------\n",
    "def _detect_target_col(df):\n",
    "    for cand in [\"target\", \"y\", \"label\", \"class\", \"is_tde\", \"binary_target\", \"target_id\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "TARGET_COL = _detect_target_col(df_train_meta)\n",
    "if TARGET_COL is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot detect target column in df_train_meta. \"\n",
    "        f\"Columns sample: {list(df_train_meta.columns)[:60]}\"\n",
    "    )\n",
    "\n",
    "y_series = pd.to_numeric(df_train_meta[TARGET_COL], errors=\"coerce\").fillna(0.0)\n",
    "y_bin = (y_series.to_numpy(dtype=np.float32) > 0).astype(np.int8)\n",
    "y_map = pd.Series(y_bin, index=df_train_meta.index)\n",
    "if y_map.index.has_duplicates:\n",
    "    y_map = y_map.groupby(level=0).max()\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load oof_prob (prefer csv)\n",
    "# ----------------------------\n",
    "def load_oof():\n",
    "    pcsv = OOF_DIR / \"oof_prob.csv\"\n",
    "    if pcsv.exists():\n",
    "        df = pd.read_csv(pcsv)\n",
    "        if (\"object_id\" in df.columns) and (\"oof_prob\" in df.columns):\n",
    "            df = df[[\"object_id\", \"oof_prob\"]].copy()\n",
    "            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "            p = _sanitize_prob(_as_1d_float32(df[\"oof_prob\"].to_numpy()), holdout_safe=HOLDOUT_SAFE)\n",
    "            if len(p) != len(df):\n",
    "                raise RuntimeError(\"oof_prob.csv: length mismatch after parsing.\")\n",
    "            df[\"oof_prob\"] = p\n",
    "            return p, df, \"csv(oof_prob.csv)\"\n",
    "\n",
    "    if \"oof_prob\" in globals():\n",
    "        p = _sanitize_prob(_as_1d_float32(globals()[\"oof_prob\"]), holdout_safe=HOLDOUT_SAFE)\n",
    "        return p, None, \"globals(oof_prob)\"\n",
    "\n",
    "    pnpy = OOF_DIR / \"oof_prob.npy\"\n",
    "    if pnpy.exists():\n",
    "        p = _sanitize_prob(_as_1d_float32(np.load(pnpy, allow_pickle=False)), holdout_safe=HOLDOUT_SAFE)\n",
    "        return p, None, \"npy(oof_prob.npy)\"\n",
    "\n",
    "    raise FileNotFoundError(\"OOF prob tidak ditemukan (oof_prob.csv / globals oof_prob / oof_prob.npy).\")\n",
    "\n",
    "oof_prob_all, df_oof_csv, oof_src = load_oof()\n",
    "if not isinstance(oof_prob_all, np.ndarray) or oof_prob_all.ndim != 1:\n",
    "    raise TypeError(f\"Invalid oof_prob. type={type(oof_prob_all)} ndim={getattr(oof_prob_all,'ndim',None)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Align y to OOF order\n",
    "# ----------------------------\n",
    "if df_oof_csv is not None:\n",
    "    # de-dup ids in oof: mean per id but preserve first order\n",
    "    ids_first = pd.unique(df_oof_csv[\"object_id\"].to_numpy())\n",
    "    if len(ids_first) != len(df_oof_csv):\n",
    "        df_mean = df_oof_csv.groupby(\"object_id\", as_index=True)[\"oof_prob\"].mean()\n",
    "        df_oof_csv = pd.DataFrame({\"object_id\": ids_first})\n",
    "        df_oof_csv[\"oof_prob\"] = df_mean.reindex(ids_first).to_numpy(dtype=np.float32)\n",
    "        oof_prob_all = df_oof_csv[\"oof_prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    train_ids_all = df_oof_csv[\"object_id\"].tolist()\n",
    "\n",
    "    ok_raw = pd.Index(train_ids_all).isin(y_map.index)\n",
    "    ok = _to_np_bool(ok_raw)\n",
    "    if not ok.all():\n",
    "        bad = [train_ids_all[i] for i in np.where(~ok)[0][:10]]\n",
    "        print(f\"[WARN] oof ids not in df_train_meta: missing_n={int((~ok).sum())} examples={bad}\")\n",
    "        df_oof_csv = df_oof_csv.loc[ok].reset_index(drop=True)\n",
    "        train_ids_all = df_oof_csv[\"object_id\"].tolist()\n",
    "        oof_prob_all = df_oof_csv[\"oof_prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    y_all = y_map.reindex(train_ids_all).to_numpy(dtype=np.int8, copy=True)\n",
    "\n",
    "elif \"train_ids_ordered\" in globals():\n",
    "    ids = [_norm_id(z) for z in list(globals()[\"train_ids_ordered\"])]\n",
    "    if len(ids) != len(oof_prob_all):\n",
    "        raise RuntimeError(\"train_ids_ordered length mismatch with oof_prob. Gunakan oof_prob.csv agar alignment aman.\")\n",
    "    missing = [oid for oid in ids if oid not in y_map.index]\n",
    "    if missing:\n",
    "        raise KeyError(f\"train_ids_ordered contains ids not in df_train_meta. ex={missing[:10]} missing_n={len(missing)}\")\n",
    "    train_ids_all = ids\n",
    "    y_all = y_map.reindex(train_ids_all).to_numpy(dtype=np.int8, copy=True)\n",
    "\n",
    "else:\n",
    "    if len(oof_prob_all) != len(df_train_meta):\n",
    "        raise RuntimeError(\n",
    "            f\"Tidak bisa align y. len(oof_prob)={len(oof_prob_all)} != len(df_train_meta)={len(df_train_meta)} \"\n",
    "            \"dan tidak ada oof_prob.csv atau train_ids_ordered.\"\n",
    "        )\n",
    "    if df_train_meta.index.has_duplicates:\n",
    "        raise RuntimeError(\n",
    "            \"df_train_meta.index has duplicates, tapi oof source tidak punya object_id ordering. \"\n",
    "            \"Solusi: simpan oof_prob.csv (object_id + oof_prob) atau sediakan train_ids_ordered.\"\n",
    "        )\n",
    "    train_ids_all = df_train_meta.index.astype(str).tolist()\n",
    "    y_all = y_map.reindex(train_ids_all).to_numpy(dtype=np.int8, copy=True)\n",
    "\n",
    "if len(y_all) != len(oof_prob_all):\n",
    "    raise RuntimeError(f\"Length mismatch: y={len(y_all)} vs oof_prob={len(oof_prob_all)}\")\n",
    "\n",
    "uy = set(np.unique(y_all).tolist())\n",
    "if not uy.issubset({0, 1}):\n",
    "    raise ValueError(f\"y must be binary 0/1. Found: {sorted(list(uy))}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2b) HOLDOUT_SAFE filtering (valid only)\n",
    "# ----------------------------\n",
    "valid = np.isfinite(oof_prob_all)\n",
    "if HOLDOUT_SAFE:\n",
    "    train_ids = [train_ids_all[i] for i in np.where(valid)[0]]\n",
    "    oof_prob = np.clip(oof_prob_all[valid].astype(np.float32), 0.0, 1.0)\n",
    "    y = y_all[valid].astype(np.int8)\n",
    "else:\n",
    "    train_ids = train_ids_all\n",
    "    oof_prob = np.clip(np.nan_to_num(oof_prob_all, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32), 0.0, 1.0)\n",
    "    y = y_all.astype(np.int8)\n",
    "\n",
    "N_all = int(len(y_all))\n",
    "N = int(len(y))\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "\n",
    "print(f\"[Eval] OOF source={oof_src} | target_col={TARGET_COL}\")\n",
    "print(f\"[Eval] rows: valid={N:,} / total={N_all:,} | pos={pos:,} neg={neg:,} pos%={pos/max(N,1)*100:.6f}%\")\n",
    "if HOLDOUT_SAFE and (N < N_all):\n",
    "    print(f\"[Eval] HOLDOUT_SAFE dropped non-finite rows: {N_all - N} rows\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Ranking metrics (threshold-free)\n",
    "# ----------------------------\n",
    "roc_auc = None\n",
    "pr_auc = None\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    if (y.max() == 1) and (y.min() == 0) and N > 1:\n",
    "        roc_auc = float(roc_auc_score(y, oof_prob))\n",
    "        pr_auc  = float(average_precision_score(y, oof_prob))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Threshold candidates (grid + quantiles + unique + nextafter + extras)\n",
    "# ----------------------------\n",
    "grid = np.concatenate([\n",
    "    np.linspace(0.00, 0.10, 41, dtype=np.float32),\n",
    "    np.linspace(0.10, 0.90, 161, dtype=np.float32),\n",
    "    np.linspace(0.90, 1.00, 41, dtype=np.float32),\n",
    "]).astype(np.float32)\n",
    "\n",
    "qs = np.linspace(0.001, 0.999, 999, dtype=np.float32)\n",
    "try:\n",
    "    quant_thr = np.quantile(oof_prob, qs).astype(np.float32) if N > 0 else np.array([], dtype=np.float32)\n",
    "except Exception:\n",
    "    quant_thr = np.array([], dtype=np.float32)\n",
    "\n",
    "uniq = np.unique(oof_prob.astype(np.float32))\n",
    "if len(uniq) > 8000:\n",
    "    take = np.linspace(0, len(uniq) - 1, 8000, dtype=int)\n",
    "    uniq = uniq[take].astype(np.float32)\n",
    "\n",
    "uniq_up = np.nextafter(uniq, np.float32(1.0)).astype(np.float32) if (ADD_NEXTAFTER_CANDIDATES and len(uniq) > 0) else np.array([], dtype=np.float32)\n",
    "\n",
    "# prevalence-match threshold (for ge): choose thr so predicted positives roughly == pos\n",
    "if pos > 0:\n",
    "    p_sorted_tmp = np.sort(oof_prob)[::-1]\n",
    "    thr_prev = float(p_sorted_tmp[min(pos - 1, N - 1)])\n",
    "else:\n",
    "    thr_prev = 1.0\n",
    "\n",
    "extra = [0.0, 0.5, 1.0, float(thr_prev)]\n",
    "for cand_name in [\"BEST_THR\", \"OOF_BEST_THR_F1\", \"BEST_THR_F1\"]:\n",
    "    if cand_name in globals() and globals()[cand_name] is not None:\n",
    "        try:\n",
    "            extra.append(float(globals()[cand_name]))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "thr_candidates = np.unique(\n",
    "    np.clip(np.concatenate([grid, quant_thr, uniq, uniq_up, np.array(extra, dtype=np.float32)]), 0.0, 1.0)\n",
    ").astype(np.float32)\n",
    "\n",
    "if len(thr_candidates) > 20000:\n",
    "    take = np.linspace(0, len(thr_candidates) - 1, 20000, dtype=int)\n",
    "    thr_candidates = thr_candidates[take].astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) FAST sweep using sorted probabilities\n",
    "# ----------------------------\n",
    "ord_desc = np.argsort(-oof_prob)\n",
    "p_sorted = oof_prob[ord_desc]\n",
    "y_sorted = y[ord_desc].astype(np.int8)\n",
    "\n",
    "pos_prefix = np.cumsum(y_sorted == 1).astype(np.int64)\n",
    "neg_prefix = np.cumsum(y_sorted == 0).astype(np.int64)\n",
    "pos_total = int(pos_prefix[-1]) if N > 0 else 0\n",
    "neg_total = int(neg_prefix[-1]) if N > 0 else 0\n",
    "\n",
    "def _metrics_from_counts(tp, fp, fn, tn):\n",
    "    tp = tp.astype(np.float64); fp = fp.astype(np.float64)\n",
    "    fn = fn.astype(np.float64); tn = tn.astype(np.float64)\n",
    "\n",
    "    prec = _safe_div(tp, tp + fp)\n",
    "    rec  = _safe_div(tp, tp + fn)\n",
    "    f1   = _safe_div(2 * prec * rec, prec + rec)\n",
    "\n",
    "    def fbeta(prec, rec, beta):\n",
    "        b2 = beta * beta\n",
    "        return _safe_div((1.0 + b2) * prec * rec, b2 * prec + rec)\n",
    "\n",
    "    f05 = fbeta(prec, rec, 0.5)\n",
    "    f2  = fbeta(prec, rec, 2.0)\n",
    "\n",
    "    acc  = _safe_div(tp + tn, tp + fp + fn + tn)\n",
    "    tpr  = _safe_div(tp, tp + fn)\n",
    "    tnr  = _safe_div(tn, tn + fp)\n",
    "    bacc = 0.5 * (tpr + tnr)\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    mcc = np.where(den > 0, num / np.sqrt(den), 0.0)\n",
    "\n",
    "    return f1, f05, f2, prec, rec, acc, bacc, mcc\n",
    "\n",
    "def _sweep(rule: str):\n",
    "    # k = number of predicted positives\n",
    "    if rule == \"ge\":\n",
    "        k = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"right\").astype(np.int64)\n",
    "    elif rule == \"gt\":\n",
    "        k = np.searchsorted(-p_sorted, -thr_candidates.astype(np.float32), side=\"left\").astype(np.int64)\n",
    "    else:\n",
    "        raise ValueError(\"rule must be 'ge' or 'gt'\")\n",
    "\n",
    "    k = np.clip(k, 0, N).astype(np.int64)\n",
    "\n",
    "    tp = np.where(k > 0, pos_prefix[k - 1], 0).astype(np.int64)\n",
    "    fp = np.where(k > 0, neg_prefix[k - 1], 0).astype(np.int64)\n",
    "    fn = (pos_total - tp).astype(np.int64)\n",
    "    tn = (neg_total - fp).astype(np.int64)\n",
    "\n",
    "    f1, f05, f2, prec, rec, acc, bacc, mcc = _metrics_from_counts(tp, fp, fn, tn)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"thr\": thr_candidates.astype(np.float32),\n",
    "        \"rule\": rule,\n",
    "        \"f1\": f1.astype(np.float32),\n",
    "        \"f0.5\": f05.astype(np.float32),\n",
    "        \"f2\": f2.astype(np.float32),\n",
    "        \"precision\": prec.astype(np.float32),\n",
    "        \"recall\": rec.astype(np.float32),\n",
    "        \"acc\": acc.astype(np.float32),\n",
    "        \"bacc\": bacc.astype(np.float32),\n",
    "        \"mcc\": mcc.astype(np.float32),\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "        \"pos_pred\": k.astype(np.int64),\n",
    "    })\n",
    "\n",
    "thr_ge = _sweep(\"ge\")\n",
    "thr_gt = _sweep(\"gt\") if DO_BOTH_RULES else None\n",
    "\n",
    "def _pick_best(df, primary, tie_cols):\n",
    "    sort_cols = [primary] + tie_cols\n",
    "    asc = [False] * len(sort_cols)\n",
    "    return df.sort_values(sort_cols, ascending=asc).iloc[0]\n",
    "\n",
    "def _eval_at(thr, rule):\n",
    "    thr = float(thr)\n",
    "    if rule == \"ge\":\n",
    "        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"right\"))\n",
    "    else:\n",
    "        k0 = int(np.searchsorted(-p_sorted, -np.float32(thr), side=\"left\"))\n",
    "    k0 = max(0, min(k0, N))\n",
    "\n",
    "    tp0 = int(pos_prefix[k0 - 1]) if k0 > 0 else 0\n",
    "    fp0 = int(neg_prefix[k0 - 1]) if k0 > 0 else 0\n",
    "    fn0 = int(pos_total - tp0)\n",
    "    tn0 = int(neg_total - fp0)\n",
    "\n",
    "    p0 = tp0 / max(tp0 + fp0, 1)\n",
    "    r0 = tp0 / max(tp0 + fn0, 1)\n",
    "    f10 = 0.0 if (tp0 == 0 or (p0 + r0) == 0) else (2 * p0 * r0 / (p0 + r0))\n",
    "    f05 = 0.0 if (0.25 * p0 + r0) == 0 else ((1.25) * p0 * r0 / (0.25 * p0 + r0))\n",
    "    f2  = 0.0 if (4.0 * p0 + r0) == 0 else ((5.0) * p0 * r0 / (4.0 * p0 + r0))\n",
    "\n",
    "    acc0  = (tp0 + tn0) / max(tp0 + fp0 + fn0 + tn0, 1)\n",
    "    bacc0 = 0.5 * ((tp0 / max(tp0 + fn0, 1)) + (tn0 / max(tn0 + fp0, 1)))\n",
    "\n",
    "    den0 = (tp0 + fp0) * (tp0 + fn0) * (tn0 + fp0) * (tn0 + fn0)\n",
    "    mcc0 = 0.0 if den0 <= 0 else ((tp0 * tn0 - fp0 * fn0) / math.sqrt(den0))\n",
    "\n",
    "    return {\n",
    "        \"thr\": thr, \"rule\": rule,\n",
    "        \"tp\": tp0, \"fp\": fp0, \"fn\": fn0, \"tn\": tn0, \"pos_pred\": k0,\n",
    "        \"precision\": float(p0), \"recall\": float(r0),\n",
    "        \"f1\": float(f10), \"f0.5\": float(f05), \"f2\": float(f2),\n",
    "        \"acc\": float(acc0), \"bacc\": float(bacc0), \"mcc\": float(mcc0),\n",
    "    }\n",
    "\n",
    "base_ge = _eval_at(0.5, \"ge\")\n",
    "base_gt = _eval_at(0.5, \"gt\") if DO_BOTH_RULES else None\n",
    "\n",
    "# Bests (rule=ge)\n",
    "best_f1_ge  = _pick_best(thr_ge, \"f1\",   [\"mcc\",\"bacc\",\"recall\",\"precision\",\"acc\"])\n",
    "best_f05_ge = _pick_best(thr_ge, \"f0.5\", [\"precision\",\"mcc\",\"f1\",\"acc\"])\n",
    "best_f2_ge  = _pick_best(thr_ge, \"f2\",   [\"recall\",\"mcc\",\"f1\",\"bacc\",\"acc\"])\n",
    "\n",
    "BEST_THR_GE_F1  = float(best_f1_ge[\"thr\"])\n",
    "BEST_THR_GE_F05 = float(best_f05_ge[\"thr\"])\n",
    "BEST_THR_GE_F2  = float(best_f2_ge[\"thr\"])\n",
    "\n",
    "best_ge_f1  = _eval_at(BEST_THR_GE_F1, \"ge\")\n",
    "best_ge_f05 = _eval_at(BEST_THR_GE_F05, \"ge\")\n",
    "best_ge_f2  = _eval_at(BEST_THR_GE_F2, \"ge\")\n",
    "\n",
    "# Bests (rule=gt)\n",
    "if DO_BOTH_RULES:\n",
    "    best_f1_gt  = _pick_best(thr_gt, \"f1\",   [\"mcc\",\"bacc\",\"recall\",\"precision\",\"acc\"])\n",
    "    BEST_THR_GT_F1 = float(best_f1_gt[\"thr\"])\n",
    "    best_gt_f1 = _eval_at(BEST_THR_GT_F1, \"gt\")\n",
    "\n",
    "    # gt -> ge equivalent (so downstream can still use prob >= thr)\n",
    "    BEST_THR_GT_F1_GE_EQUIV = float(np.nextafter(np.float32(BEST_THR_GT_F1), np.float32(1.0)))\n",
    "else:\n",
    "    BEST_THR_GT_F1 = None\n",
    "    BEST_THR_GT_F1_GE_EQUIV = None\n",
    "    best_gt_f1 = None\n",
    "\n",
    "# Default export for downstream (keep old variable names)\n",
    "BEST_THR_F1  = BEST_THR_GE_F1\n",
    "BEST_THR_F05 = BEST_THR_GE_F05\n",
    "BEST_THR_F2  = BEST_THR_GE_F2\n",
    "BEST_THR     = BEST_THR_GE_F1\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Report + tables\n",
    "# ----------------------------\n",
    "print(\"\\nEVALUATION (OOF) — Precision/Recall/F-scores (+BACC/MCC)\")\n",
    "if roc_auc is not None:\n",
    "    print(f\"- ROC-AUC={roc_auc:.6f} | PR-AUC={pr_auc:.6f}\")\n",
    "print(f\"- prevalence-match thr (ge) ~ {thr_prev:.6f}\")\n",
    "\n",
    "print(\"\\nBaseline @ thr=0.5\")\n",
    "print(f\"- rule=ge: F1={base_ge['f1']:.6f} | P={base_ge['precision']:.6f} | R={base_ge['recall']:.6f} | \"\n",
    "      f\"ACC={base_ge['acc']:.6f} | BACC={base_ge['bacc']:.6f} | MCC={base_ge['mcc']:.6f} | pos_pred={base_ge['pos_pred']}\")\n",
    "if DO_BOTH_RULES:\n",
    "    print(f\"- rule=gt: F1={base_gt['f1']:.6f} | P={base_gt['precision']:.6f} | R={base_gt['recall']:.6f} | \"\n",
    "          f\"ACC={base_gt['acc']:.6f} | BACC={base_gt['bacc']:.6f} | MCC={base_gt['mcc']:.6f} | pos_pred={base_gt['pos_pred']}\")\n",
    "\n",
    "print(f\"\\nBEST (rule=ge) — default downstream: pred = prob >= thr\")\n",
    "print(f\"- BEST-F1   @ thr={best_ge_f1['thr']:.6f} | F1={best_ge_f1['f1']:.6f} | P={best_ge_f1['precision']:.6f} | R={best_ge_f1['recall']:.6f} | pos_pred={best_ge_f1['pos_pred']}\")\n",
    "print(f\"- BEST-F0.5 @ thr={best_ge_f05['thr']:.6f} | F0.5={best_ge_f05['f0.5']:.6f} | P={best_ge_f05['precision']:.6f} | R={best_ge_f05['recall']:.6f}\")\n",
    "print(f\"- BEST-F2   @ thr={best_ge_f2['thr']:.6f} | F2={best_ge_f2['f2']:.6f} | P={best_ge_f2['precision']:.6f} | R={best_ge_f2['recall']:.6f}\")\n",
    "\n",
    "if DO_BOTH_RULES:\n",
    "    print(f\"\\nBEST (rule=gt) — strict boundary (>)\")\n",
    "    print(f\"- BEST-F1 @ thr={best_gt_f1['thr']:.6f} (gt) | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.6f} | \"\n",
    "          f\"F1={best_gt_f1['f1']:.6f} | pos_pred={best_gt_f1['pos_pred']}\")\n",
    "\n",
    "# Combine + sort for top view\n",
    "thr_all = pd.concat([thr_ge, thr_gt], ignore_index=True) if DO_BOTH_RULES else thr_ge.copy()\n",
    "thr_sorted = thr_all.sort_values(\n",
    "    [\"f1\",\"mcc\",\"bacc\",\"recall\",\"precision\"],\n",
    "    ascending=[False, False, False, False, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTop 10 by F1 (mixed rules):\")\n",
    "for i in range(min(10, len(thr_sorted))):\n",
    "    r = thr_sorted.iloc[i]\n",
    "    print(f\"{i+1:02d}. rule={r['rule']} thr={float(r['thr']):.6f} | f1={float(r['f1']):.6f} | \"\n",
    "          f\"P={float(r['precision']):.6f} R={float(r['recall']):.6f} | mcc={float(r['mcc']):.6f} bacc={float(r['bacc']):.6f} | \"\n",
    "          f\"pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Save artifacts\n",
    "# ----------------------------\n",
    "out_txt   = OOF_DIR / \"eval_report.txt\"\n",
    "out_csv   = OOF_DIR / \"eval_threshold_table.csv\"\n",
    "out_csv_t = OOF_DIR / \"eval_threshold_table_top500.csv\"\n",
    "out_json  = OOF_DIR / \"eval_summary.json\"\n",
    "\n",
    "thr_sorted.to_csv(out_csv, index=False)\n",
    "thr_sorted.head(500).to_csv(out_csv_t, index=False)\n",
    "\n",
    "payload = {\n",
    "    \"version\": \"v3.4\",\n",
    "    \"source\": oof_src,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"n_total_rows\": int(N_all),\n",
    "    \"n_valid_rows\": int(N),\n",
    "    \"pos_valid\": int(pos),\n",
    "    \"neg_valid\": int(neg),\n",
    "    \"roc_auc_valid_only\": roc_auc,\n",
    "    \"pr_auc_valid_only\": pr_auc,\n",
    "    \"prevalence_match_thr_ge\": float(thr_prev),\n",
    "    \"baseline_thr_0p5\": {\"ge\": base_ge, \"gt\": base_gt},\n",
    "    \"best_ge\": {\"f1\": best_ge_f1, \"f0.5\": best_ge_f05, \"f2\": best_ge_f2},\n",
    "    \"best_gt\": {\"f1\": best_gt_f1, \"ge_equiv_for_downstream_using_ge\": {\"f1\": BEST_THR_GT_F1_GE_EQUIV}},\n",
    "    \"default_best_thr\": {\"metric\": \"f1\", \"rule\": \"ge\", \"thr\": float(BEST_THR)},\n",
    "    \"switches\": {\n",
    "        \"HOLDOUT_SAFE\": bool(HOLDOUT_SAFE),\n",
    "        \"DO_BOTH_RULES\": bool(DO_BOTH_RULES),\n",
    "        \"ADD_NEXTAFTER_CANDIDATES\": bool(ADD_NEXTAFTER_CANDIDATES),\n",
    "    },\n",
    "    \"paths\": {\"report\": str(out_txt), \"table\": str(out_csv), \"table_top500\": str(out_csv_t), \"summary\": str(out_json)},\n",
    "}\n",
    "\n",
    "# text report (ringkas tapi jelas)\n",
    "lines = []\n",
    "lines.append(\"OOF Evaluation Report (v3.4)\")\n",
    "lines.append(f\"source={oof_src} | target_col={TARGET_COL}\")\n",
    "lines.append(f\"valid_rows={N} / total_rows={N_all} | pos={pos} | neg={neg} | pos%={pos/max(N,1)*100:.10f}%\")\n",
    "if roc_auc is not None:\n",
    "    lines.append(f\"ROC-AUC(valid-only)={roc_auc:.10f} | PR-AUC(valid-only)={pr_auc:.10f}\")\n",
    "lines.append(f\"prevalence_match_thr_ge={thr_prev:.10f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Baseline @ thr=0.5\")\n",
    "lines.append(f\"ge: F1={base_ge['f1']:.10f} P={base_ge['precision']:.10f} R={base_ge['recall']:.10f} \"\n",
    "             f\"ACC={base_ge['acc']:.10f} BACC={base_ge['bacc']:.10f} MCC={base_ge['mcc']:.10f} pos_pred={base_ge['pos_pred']}\")\n",
    "if DO_BOTH_RULES and base_gt is not None:\n",
    "    lines.append(f\"gt: F1={base_gt['f1']:.10f} P={base_gt['precision']:.10f} R={base_gt['recall']:.10f} \"\n",
    "                 f\"ACC={base_gt['acc']:.10f} BACC={base_gt['bacc']:.10f} MCC={base_gt['mcc']:.10f} pos_pred={base_gt['pos_pred']}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"BEST (ge/F1) thr={BEST_THR_GE_F1:.10f} | F1={best_ge_f1['f1']:.10f} P={best_ge_f1['precision']:.10f} R={best_ge_f1['recall']:.10f}\")\n",
    "if DO_BOTH_RULES and best_gt_f1 is not None:\n",
    "    lines.append(f\"BEST (gt/F1) thr={BEST_THR_GT_F1:.10f} | ge_equiv={BEST_THR_GT_F1_GE_EQUIV:.10f} | F1={best_gt_f1['f1']:.10f}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Top 10 by F1 (mixed rules):\")\n",
    "for i in range(min(10, len(thr_sorted))):\n",
    "    r = thr_sorted.iloc[i]\n",
    "    lines.append(f\"{i+1:02d}. rule={r['rule']} thr={float(r['thr']):.10f} f1={float(r['f1']):.10f} \"\n",
    "                 f\"P={float(r['precision']):.10f} R={float(r['recall']):.10f} \"\n",
    "                 f\"mcc={float(r['mcc']):.10f} bacc={float(r['bacc']):.10f} pos_pred={int(r['pos_pred'])}\")\n",
    "\n",
    "out_txt.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "out_json.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(f\"- {out_txt}\")\n",
    "print(f\"- {out_csv}\")\n",
    "print(f\"- {out_csv_t}\")\n",
    "print(f\"- {out_json}\")\n",
    "\n",
    "globals().update({\n",
    "    \"BEST_THR\": float(BEST_THR),\n",
    "    \"BEST_THR_F1\": float(BEST_THR_F1),\n",
    "    \"BEST_THR_F05\": float(BEST_THR_F05),\n",
    "    \"BEST_THR_F2\": float(BEST_THR_F2),\n",
    "\n",
    "    \"BEST_THR_GE_F1\": float(BEST_THR_GE_F1),\n",
    "    \"BEST_THR_GE_F05\": float(BEST_THR_GE_F05),\n",
    "    \"BEST_THR_GE_F2\": float(BEST_THR_GE_F2),\n",
    "\n",
    "    \"BEST_THR_GT_F1\": (None if BEST_THR_GT_F1 is None else float(BEST_THR_GT_F1)),\n",
    "    \"BEST_THR_GT_F1_GE_EQUIV\": (None if BEST_THR_GT_F1_GE_EQUIV is None else float(BEST_THR_GT_F1_GE_EQUIV)),\n",
    "\n",
    "    \"thr_table_eval\": thr_sorted,\n",
    "    \"EVAL_REPORT_PATH\": out_txt,\n",
    "    \"EVAL_TABLE_PATH\": out_csv,\n",
    "    \"EVAL_TABLE_TOP500_PATH\": out_csv_t,\n",
    "    \"EVAL_SUMMARY_PATH\": out_json,\n",
    "    \"OOF_AUC_VALID_ONLY\": roc_auc,\n",
    "    \"OOF_AP_VALID_ONLY\": pr_auc,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25cc11",
   "metadata": {
    "papermill": {
     "duration": 0.024874,
     "end_time": "2026-01-07T19:52:21.198894",
     "exception": false,
     "start_time": "2026-01-07T19:52:21.174020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5c43ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T19:52:21.252153Z",
     "iopub.status.busy": "2026-01-07T19:52:21.251169Z",
     "iopub.status.idle": "2026-01-07T19:52:21.580301Z",
     "shell.execute_reply": "2026-01-07T19:52:21.579309Z"
    },
    "papermill": {
     "duration": 0.358506,
     "end_time": "2026-01-07T19:52:21.582473",
     "exception": false,
     "start_time": "2026-01-07T19:52:21.223967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11] Loaded test predictions (PROB)\n",
      "- source: globals(test_ids + test_prob_ens)\n",
      "- N_pred=7,135 | prob_mean=0.412152 | std=0.172765 | min=0.054225 | max=0.841255\n",
      "[Stage 11] ID coverage check:\n",
      "- sample_in_pred = 7,135 / 7,135\n",
      "- pred_in_sample = 7,135 / 7,135\n",
      "\n",
      "[Stage 11] SUBMISSION READY (BINARY 0/1)\n",
      "- threshold_used=0.647290 | thr_source=globals(BEST_THR_F1)\n",
      "- rows=7,135 | pos_pred=606 (8.493343%)\n",
      "- wrote: /kaggle/working/submission.csv\n",
      "- copy : /kaggle/working/mallorn_run/run_20260107_164508_3223fb1d5c/submissions/submission.csv\n",
      "\n",
      "Preview:\n",
      "                   object_id  prediction\n",
      "    Eluwaith_Mithrim_nothrim           0\n",
      "          Eru_heledir_archam           0\n",
      "           Gonhir_anann_fuin           0\n",
      "Gwathuirim_haradrim_tegilbor           0\n",
      "            achas_minai_maen           0\n",
      "               adab_fae_gath           0\n",
      "             adel_draug_gaur           0\n",
      "     aderthad_cuil_galadhrim           0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 11 — Submission Build (ONE CELL) — REVISI FULL v3.5\n",
    "#\n",
    "# Upgrade v3.5:\n",
    "# - Threshold JSON fallback FIX (supports Stage9 threshold_tuning.json & Eval v3.4 summary)\n",
    "# - Auto-handle if pred file already 0/1 (test_pred_01.csv)\n",
    "# - Stronger diagnostics + strict order = sample_submission\n",
    "#\n",
    "# Output:\n",
    "# - /kaggle/working/submission.csv\n",
    "# - SUB_DIR/submission.csv (copy)\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 0 globals\n",
    "# ----------------------------\n",
    "for need in [\"PATHS\", \"SUB_DIR\"]:\n",
    "    if need not in globals():\n",
    "        raise RuntimeError(f\"Missing `{need}`. Jalankan STAGE 0 dulu (setup).\")\n",
    "\n",
    "sample_path = Path(PATHS[\"SAMPLE_SUB\"])\n",
    "if not sample_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing sample_submission.csv: {sample_path}\")\n",
    "\n",
    "# IMPORTANT: dtype object_id=str to prevent ID corruption\n",
    "df_sub = pd.read_csv(sample_path, dtype={\"object_id\": str})\n",
    "if not {\"object_id\", \"prediction\"}.issubset(df_sub.columns):\n",
    "    raise ValueError(f\"sample_submission must have columns object_id,prediction. Found: {list(df_sub.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _norm_id(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    s = str(x).strip()\n",
    "    if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
    "        s = s[2:-1]\n",
    "    return s.strip()\n",
    "\n",
    "def _as_1d_float32(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.dtype == object and a.ndim == 0:\n",
    "        try:\n",
    "            a = np.asarray(a.item())\n",
    "        except Exception:\n",
    "            pass\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        return a.reshape(1)          # IMPORTANT: always 1D\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    return a\n",
    "\n",
    "def _sanitize_prob(p):\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    p = np.nan_to_num(p, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    return p.astype(np.float32)\n",
    "\n",
    "def _load_ids_npy(path: Path):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    ids = arr.tolist() if hasattr(arr, \"tolist\") else list(arr)\n",
    "    return [_norm_id(x) for x in ids]\n",
    "\n",
    "def _try_load_json(p):\n",
    "    try:\n",
    "        p = Path(p)\n",
    "        if not p.exists():\n",
    "            return None\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _try_load_stage10_cfg():\n",
    "    p = globals().get(\"TEST_INFER_CFG_PATH\", None)\n",
    "    if p is None:\n",
    "        return None\n",
    "    return _try_load_json(p)\n",
    "\n",
    "def _detect_prob_col(df):\n",
    "    # Prefer explicit names (probability)\n",
    "    for c in [\"prob\", \"proba\", \"oof_prob\", \"pred\", \"p\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # Some users store probability in \"prediction\"\n",
    "    if \"prediction\" in df.columns:\n",
    "        # if it looks float-ish (not only 0/1), treat as prob\n",
    "        s = pd.to_numeric(df[\"prediction\"], errors=\"coerce\")\n",
    "        if s.notna().mean() > 0.95:\n",
    "            u = set(np.unique(s.dropna().astype(float).to_numpy()).tolist())\n",
    "            if not u.issubset({0.0, 1.0}):\n",
    "                return \"prediction\"\n",
    "\n",
    "    # else: pick a single mostly-numeric column besides object_id\n",
    "    cand = [c for c in df.columns if c != \"object_id\"]\n",
    "    floatish = []\n",
    "    for c in cand:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.notna().mean() > 0.95:\n",
    "            floatish.append(c)\n",
    "    if len(floatish) == 1:\n",
    "        return floatish[0]\n",
    "    return None\n",
    "\n",
    "def _detect_binary_col(df):\n",
    "    # Prefer explicit binary prediction column\n",
    "    for c in [\"prediction\", \"pred\", \"label\", \"y\"]:\n",
    "        if c in df.columns:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().mean() > 0.95:\n",
    "                u = set(np.unique(s.dropna().astype(int).to_numpy()).tolist())\n",
    "                if u.issubset({0, 1}):\n",
    "                    return c\n",
    "    return None\n",
    "\n",
    "def _load_pred_df():\n",
    "    \"\"\"\n",
    "    Return (df_pred, mode, src_str)\n",
    "    mode:\n",
    "      - \"prob\": df_pred has columns object_id, prob (float [0,1])\n",
    "      - \"bin\" : df_pred has columns object_id, prediction (int 0/1)\n",
    "    Priority:\n",
    "      A) globals: test_ids + test_prob_ens\n",
    "      B) STAGE10 config json -> outputs.test_pred_01_csv (bin) OR outputs.test_prob_ens_csv (prob)\n",
    "      C) csv fallbacks\n",
    "      D) npy fallback: FIX_DIR/test_ids.npy + test_prob_ens.npy\n",
    "    \"\"\"\n",
    "    # ---- A) globals ----\n",
    "    if (\"test_prob_ens\" in globals()) and (globals()[\"test_prob_ens\"] is not None) and \\\n",
    "       (\"test_ids\" in globals()) and (globals()[\"test_ids\"] is not None):\n",
    "        ids = [_norm_id(x) for x in list(globals()[\"test_ids\"])]\n",
    "        prob = _sanitize_prob(_as_1d_float32(globals()[\"test_prob_ens\"]))\n",
    "        if len(ids) == len(prob) and len(ids) > 0:\n",
    "            return pd.DataFrame({\"object_id\": ids, \"prob\": prob}), \"prob\", \"globals(test_ids + test_prob_ens)\"\n",
    "\n",
    "    # ---- B) STAGE 10 config json ----\n",
    "    cfg = _try_load_stage10_cfg()\n",
    "    if isinstance(cfg, dict):\n",
    "        out = cfg.get(\"outputs\", {}) if isinstance(cfg.get(\"outputs\", {}), dict) else {}\n",
    "        # prefer binary if exists\n",
    "        pred01 = out.get(\"test_pred_01_csv\", None)\n",
    "        if pred01:\n",
    "            p = Path(pred01)\n",
    "            if p.exists():\n",
    "                df = pd.read_csv(p, dtype={\"object_id\": str})\n",
    "                if \"object_id\" in df.columns:\n",
    "                    colb = _detect_binary_col(df)\n",
    "                    if colb is None:\n",
    "                        raise RuntimeError(f\"Cannot detect binary column in: {p} | cols={list(df.columns)}\")\n",
    "                    df = df.copy()\n",
    "                    df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "                    y = pd.to_numeric(df[colb], errors=\"coerce\").fillna(0).astype(int).clip(0,1).to_numpy()\n",
    "                    return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prediction\": y}), \"bin\", f\"stage10_cfg_bin({p})\"\n",
    "\n",
    "        csvp = out.get(\"test_prob_ens_csv\", None)\n",
    "        if csvp:\n",
    "            p = Path(csvp)\n",
    "            if p.exists():\n",
    "                df = pd.read_csv(p, dtype={\"object_id\": str})\n",
    "                if \"object_id\" in df.columns:\n",
    "                    colp = _detect_prob_col(df)\n",
    "                    if colp is None:\n",
    "                        raise RuntimeError(f\"Cannot detect prob column in: {p} | cols={list(df.columns)}\")\n",
    "                    df = df.copy()\n",
    "                    df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "                    prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n",
    "                    if len(prob) != len(df):\n",
    "                        raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n",
    "                    return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob}), \"prob\", f\"stage10_cfg_csv({p})\"\n",
    "\n",
    "    # ---- C) csv fallback ----\n",
    "    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "    preds_dir = art_dir / \"preds\"\n",
    "\n",
    "    cand_csv = []\n",
    "    if \"TEST_PRED01_PATH\" in globals() and globals()[\"TEST_PRED01_PATH\"] is not None:\n",
    "        cand_csv.append(Path(globals()[\"TEST_PRED01_PATH\"]))\n",
    "    if \"TEST_PROB_CSV_PATH\" in globals() and globals()[\"TEST_PROB_CSV_PATH\"] is not None:\n",
    "        cand_csv.append(Path(globals()[\"TEST_PROB_CSV_PATH\"]))\n",
    "\n",
    "    cand_csv += [\n",
    "        preds_dir / \"test_pred_01.csv\",\n",
    "        preds_dir / \"test_prob_ens.csv\",\n",
    "        art_dir / \"test_pred_01.csv\",\n",
    "        art_dir / \"test_prob_ens.csv\",\n",
    "    ]\n",
    "\n",
    "    for p in cand_csv:\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p, dtype={\"object_id\": str})\n",
    "            if \"object_id\" not in df.columns:\n",
    "                continue\n",
    "\n",
    "            # If binary file\n",
    "            colb = _detect_binary_col(df)\n",
    "            if colb is not None:\n",
    "                df = df.copy()\n",
    "                df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "                y = pd.to_numeric(df[colb], errors=\"coerce\").fillna(0).astype(int).clip(0,1).to_numpy()\n",
    "                return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prediction\": y}), \"bin\", f\"csv_bin({p})\"\n",
    "\n",
    "            # Else probability file\n",
    "            colp = _detect_prob_col(df)\n",
    "            if colp is None:\n",
    "                raise RuntimeError(f\"Cannot detect prob/binary column in: {p} | cols={list(df.columns)}\")\n",
    "            df = df.copy()\n",
    "            df[\"object_id\"] = df[\"object_id\"].apply(_norm_id)\n",
    "            prob = _sanitize_prob(_as_1d_float32(df[colp].to_numpy()))\n",
    "            if len(prob) != len(df):\n",
    "                raise RuntimeError(f\"CSV prob length mismatch: {p}\")\n",
    "            return pd.DataFrame({\"object_id\": df[\"object_id\"].tolist(), \"prob\": prob}), \"prob\", f\"csv_prob({p})\"\n",
    "\n",
    "    # ---- D) npy fallback ----\n",
    "    fix_dir = Path(globals().get(\"FIX_DIR\", \"/kaggle/working/mallorn_run/artifacts/fixed_seq\"))\n",
    "    p_ids = fix_dir / \"test_ids.npy\"\n",
    "    if not p_ids.exists():\n",
    "        raise RuntimeError(\"Missing test_ids. Pastikan STAGE 6 membuat fixed_seq/test_ids.npy atau STAGE 10 export test_ids.\")\n",
    "    ids = _load_ids_npy(p_ids)\n",
    "    if len(ids) == 0:\n",
    "        raise RuntimeError(\"test_ids.npy kosong.\")\n",
    "\n",
    "    cand_npy = []\n",
    "    if \"TEST_PROB_ENS_PATH\" in globals() and globals()[\"TEST_PROB_ENS_PATH\"] is not None:\n",
    "        cand_npy.append(Path(globals()[\"TEST_PROB_ENS_PATH\"]))\n",
    "    cand_npy += [preds_dir / \"test_prob_ens.npy\", art_dir / \"test_prob_ens.npy\"]\n",
    "\n",
    "    prob = None\n",
    "    used = None\n",
    "    for p in cand_npy:\n",
    "        if p.exists():\n",
    "            prob = _sanitize_prob(_as_1d_float32(np.load(p, allow_pickle=False)))\n",
    "            used = p\n",
    "            break\n",
    "    if prob is None:\n",
    "        raise RuntimeError(\"Missing test_prob_ens. Jalankan STAGE 10 dulu (Test Inference).\")\n",
    "    if len(prob) != len(ids):\n",
    "        raise RuntimeError(f\"Length mismatch (NPY): test_prob={len(prob)} vs test_ids={len(ids)}\")\n",
    "\n",
    "    return pd.DataFrame({\"object_id\": ids, \"prob\": prob}), \"prob\", f\"npy({used}) + ids({p_ids})\"\n",
    "\n",
    "def _load_best_threshold_fallback():\n",
    "    \"\"\"\n",
    "    Correctly parse thresholds from:\n",
    "      - Stage 9: OOF_DIR/threshold_tuning.json\n",
    "          * payload[\"default_best_thr\"][\"thr\"]\n",
    "          * payload[\"best_ge\"][\"best_thr_f1\"][\"thr\"]  (if exists)\n",
    "      - Eval v3.4/v3.5 summary: OOF_DIR/eval_summary.json\n",
    "          * payload[\"default_best_thr\"][\"thr\"]\n",
    "          * payload[\"best_ge\"][\"f1\"][\"thr\"]\n",
    "    \"\"\"\n",
    "    art_dir = Path(globals().get(\"ART_DIR\", \"/kaggle/working\"))\n",
    "    oof_dir = Path(globals().get(\"OOF_DIR\", art_dir / \"oof\"))\n",
    "\n",
    "    cand = []\n",
    "    for k in [\"THR_JSON_PATH\", \"EVAL_SUMMARY_PATH\"]:\n",
    "        if k in globals() and globals()[k] is not None:\n",
    "            cand.append(Path(globals()[k]))\n",
    "    cand += [oof_dir / \"threshold_tuning.json\", oof_dir / \"eval_summary.json\"]\n",
    "\n",
    "    def _dig(obj, path_list):\n",
    "        cur = obj\n",
    "        for key in path_list:\n",
    "            if not isinstance(cur, dict) or key not in cur:\n",
    "                return None\n",
    "            cur = cur[key]\n",
    "        return cur\n",
    "\n",
    "    for p in cand:\n",
    "        obj = _try_load_json(p)\n",
    "        if not isinstance(obj, dict):\n",
    "            continue\n",
    "        name = Path(p).name\n",
    "\n",
    "        # generic: default_best_thr.thr\n",
    "        v = _dig(obj, [\"default_best_thr\", \"thr\"])\n",
    "        if v is not None:\n",
    "            try:\n",
    "                return float(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if name == \"threshold_tuning.json\":\n",
    "            v = _dig(obj, [\"best_ge\", \"best_thr_f1\", \"thr\"])\n",
    "            if v is not None:\n",
    "                try:\n",
    "                    return float(v)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if name == \"eval_summary.json\":\n",
    "            v = _dig(obj, [\"best_ge\", \"f1\", \"thr\"])\n",
    "            if v is not None:\n",
    "                try:\n",
    "                    return float(v)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # last resort: scan known keys if user saved them\n",
    "        for k in [\"BEST_THR\", \"BEST_THR_F1\", \"BEST_THR_GE_F1\"]:\n",
    "            if k in obj:\n",
    "                try:\n",
    "                    return float(obj[k])\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load prediction df\n",
    "# ----------------------------\n",
    "df_pred, pred_mode, pred_src = _load_pred_df()\n",
    "if df_pred is None or df_pred.empty:\n",
    "    raise RuntimeError(\"df_pred empty (unexpected).\")\n",
    "\n",
    "df_pred = df_pred.copy()\n",
    "df_pred[\"object_id\"] = df_pred[\"object_id\"].apply(_norm_id)\n",
    "\n",
    "# strict: no duplicate ids\n",
    "if df_pred[\"object_id\"].duplicated().any():\n",
    "    dup = df_pred.loc[df_pred[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(f\"Duplicated object_id in predictions (examples): {dup}\")\n",
    "\n",
    "if pred_mode == \"prob\":\n",
    "    p = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    if not np.isfinite(p).all():\n",
    "        bad = int((~np.isfinite(p)).sum())\n",
    "        raise ValueError(f\"Found non-finite probabilities in test predictions: {bad} rows\")\n",
    "    df_pred[\"prob\"] = _sanitize_prob(p)\n",
    "\n",
    "    p2 = df_pred[\"prob\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    print(\"[Stage 11] Loaded test predictions (PROB)\")\n",
    "    print(f\"- source: {pred_src}\")\n",
    "    print(f\"- N_pred={len(df_pred):,} | prob_mean={float(p2.mean()):.6f} | std={float(p2.std()):.6f} | min={float(p2.min()):.6f} | max={float(p2.max()):.6f}\")\n",
    "else:\n",
    "    yb = pd.to_numeric(df_pred[\"prediction\"], errors=\"coerce\").fillna(0).astype(int).clip(0,1).to_numpy()\n",
    "    df_pred[\"prediction\"] = yb.astype(np.int8)\n",
    "    print(\"[Stage 11] Loaded test predictions (BINARY 0/1)\")\n",
    "    print(f\"- source: {pred_src}\")\n",
    "    print(f\"- N_pred={len(df_pred):,} | pos_pred={int(df_pred['prediction'].sum()):,} ({float(df_pred['prediction'].mean())*100:.6f}%)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Threshold selection (only if needed)\n",
    "# ----------------------------\n",
    "FORCE_THR = None  # set manual if you want, e.g. 0.37\n",
    "\n",
    "thr_src = None\n",
    "thr = None\n",
    "\n",
    "if pred_mode == \"prob\":\n",
    "    if FORCE_THR is not None:\n",
    "        thr = float(FORCE_THR); thr_src = \"FORCE_THR\"\n",
    "    elif \"BEST_THR_F1\" in globals() and globals()[\"BEST_THR_F1\"] is not None:\n",
    "        thr = float(globals()[\"BEST_THR_F1\"]); thr_src = \"globals(BEST_THR_F1)\"\n",
    "    elif \"BEST_THR\" in globals() and globals()[\"BEST_THR\"] is not None:\n",
    "        thr = float(globals()[\"BEST_THR\"]); thr_src = \"globals(BEST_THR)\"\n",
    "    else:\n",
    "        fb = _load_best_threshold_fallback()\n",
    "        if fb is not None:\n",
    "            thr = float(fb); thr_src = \"json_fallback(threshold_tuning/eval_summary)\"\n",
    "        else:\n",
    "            thr = 0.5; thr_src = \"default(0.5)\"\n",
    "    thr = float(np.clip(thr, 0.0, 1.0))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Align to sample_submission order + build output\n",
    "# ----------------------------\n",
    "df_sub = df_sub.copy()\n",
    "df_sub[\"object_id\"] = df_sub[\"object_id\"].apply(_norm_id)\n",
    "\n",
    "if df_sub[\"object_id\"].duplicated().any():\n",
    "    dup = df_sub.loc[df_sub[\"object_id\"].duplicated(), \"object_id\"].iloc[:10].tolist()\n",
    "    raise ValueError(f\"sample_submission has duplicate object_id (unexpected). examples={dup}\")\n",
    "\n",
    "sample_ids = pd.Index(df_sub[\"object_id\"].tolist())\n",
    "pred_ids = pd.Index(df_pred[\"object_id\"].tolist())\n",
    "\n",
    "in_pred = sample_ids.isin(pred_ids)\n",
    "in_sample = pred_ids.isin(sample_ids)\n",
    "\n",
    "print(f\"[Stage 11] ID coverage check:\")\n",
    "print(f\"- sample_in_pred = {int(in_pred.sum()):,} / {len(sample_ids):,}\")\n",
    "print(f\"- pred_in_sample = {int(in_sample.sum()):,} / {len(pred_ids):,}\")\n",
    "\n",
    "if (~in_sample).any():\n",
    "    extra = pred_ids[~in_sample][:10].tolist()\n",
    "    print(f\"[Stage 11] WARN: predictions contain extra ids not in sample (show 10): {extra}\")\n",
    "if (~in_pred).any():\n",
    "    miss = sample_ids[~in_pred][:10].tolist()\n",
    "    print(f\"[Stage 11] WARN: sample contains ids missing in predictions (show 10): {miss}\")\n",
    "\n",
    "# Merge in sample order (STRICT)\n",
    "df_out = df_sub[[\"object_id\"]].merge(df_pred, on=\"object_id\", how=\"left\", sort=False)\n",
    "\n",
    "if pred_mode == \"prob\":\n",
    "    if df_out[\"prob\"].isna().any():\n",
    "        missing_n = int(df_out[\"prob\"].isna().sum())\n",
    "        miss_ids = df_out.loc[df_out[\"prob\"].isna(), \"object_id\"].iloc[:10].tolist()\n",
    "        raise ValueError(\n",
    "            f\"Some sample_submission object_id have no prediction: missing_n={missing_n}. Examples: {miss_ids}\\n\"\n",
    "            \"Penyebab umum: object_id kebaca numeric (leading zero hilang) atau pred tidak lengkap.\"\n",
    "        )\n",
    "    df_out[\"prediction\"] = (df_out[\"prob\"].to_numpy(dtype=np.float32) >= np.float32(thr)).astype(np.int8)\n",
    "else:\n",
    "    if df_out[\"prediction\"].isna().any():\n",
    "        missing_n = int(df_out[\"prediction\"].isna().sum())\n",
    "        miss_ids = df_out.loc[df_out[\"prediction\"].isna(), \"object_id\"].iloc[:10].tolist()\n",
    "        raise ValueError(\n",
    "            f\"Some sample_submission object_id have no binary prediction: missing_n={missing_n}. Examples: {miss_ids}\"\n",
    "        )\n",
    "    df_out[\"prediction\"] = pd.to_numeric(df_out[\"prediction\"], errors=\"coerce\").fillna(0).astype(int).clip(0,1).astype(np.int8)\n",
    "\n",
    "df_out = df_out[[\"object_id\", \"prediction\"]]\n",
    "\n",
    "# Strict checks\n",
    "if len(df_out) != len(df_sub):\n",
    "    raise RuntimeError(\"submission row count mismatch with sample_submission.\")\n",
    "if not df_out[\"object_id\"].equals(df_sub[\"object_id\"]):\n",
    "    raise RuntimeError(\"submission order mismatch with sample_submission (must be identical).\")\n",
    "\n",
    "u = set(np.unique(df_out[\"prediction\"].to_numpy()).tolist())\n",
    "if not u.issubset({0, 1}):\n",
    "    raise RuntimeError(f\"submission prediction contains values outside {{0,1}}: {sorted(list(u))}\")\n",
    "\n",
    "pos_pred = int(df_out[\"prediction\"].sum())\n",
    "print(\"\\n[Stage 11] SUBMISSION READY (BINARY 0/1)\")\n",
    "if pred_mode == \"prob\":\n",
    "    print(f\"- threshold_used={thr:.6f} | thr_source={thr_src}\")\n",
    "print(f\"- rows={len(df_out):,} | pos_pred={pos_pred:,} ({pos_pred/max(len(df_out),1)*100:.6f}%)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Write files\n",
    "# ----------------------------\n",
    "SUB_DIR = Path(SUB_DIR)\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_main = Path(\"/kaggle/working/submission.csv\")\n",
    "out_copy = SUB_DIR / \"submission.csv\"\n",
    "\n",
    "df_out.to_csv(out_main, index=False)\n",
    "df_out.to_csv(out_copy, index=False)\n",
    "\n",
    "print(f\"- wrote: {out_main}\")\n",
    "print(f\"- copy : {out_copy}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(df_out.head(8).to_string(index=False))\n",
    "\n",
    "globals().update({\n",
    "    \"SUBMISSION_PATH\": out_main,\n",
    "    \"SUBMISSION_COPY_PATH\": out_copy,\n",
    "    \"SUBMISSION_MODE\": \"binary\",\n",
    "    \"SUBMISSION_THRESHOLD\": (None if pred_mode != \"prob\" else float(thr)),\n",
    "    \"SUBMISSION_THRESHOLD_SOURCE\": (None if pred_mode != \"prob\" else thr_src),\n",
    "    \"SUBMISSION_PRED_SOURCE\": pred_src,\n",
    "})\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8925232,
     "sourceId": 14010596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11244.703662,
   "end_time": "2026-01-07T19:52:24.309077",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-07T16:44:59.605415",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
